{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/channasutan/blank-signal/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGsSa63nyGQ2",
        "outputId": "a29abb82-4b1a-4590-8099-b6ab7aae29dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.1/134.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.4/249.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m641.1/641.1 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Numpy: 2.0.2 Pandas: 2.2.2\n",
            "XGBoost: 3.1.2 Optuna: 4.6.0\n",
            "OUT_DIR: /content/drive/MyDrive/quant_pipeline/mtb_out\n",
            "SEED: 42\n",
            "Joblib CPU count (system): 2\n",
            "Configured JOBLIB_N_JOBS: 1 XGB_NJOBS: 1 OPTUNA_N_JOBS: 1\n",
            "nvidia-smi not available (likely no GPU).\n",
            "\n",
            "Saved environment snapshot → /content/drive/MyDrive/quant_pipeline/mtb_out/env_versions.json\n",
            "Setup complete. Use set_parallel_backend() context and make_xgb_clf/make_xgb_reg helpers when training.\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 00_SETUP — hardened, reproducible, Colab-friendly (single cell)\n",
        "# Updated: force thread env vars, export LAST_COMPLETE default, EXCLUDE_COL_PREFIXES, readiness_checks helper\n",
        "# =========================\n",
        "\n",
        "# --- install (run before imports) ---\n",
        "# (keep --quiet to reduce output in notebooks)\n",
        "!pip install --quiet ccxt pywavelets ta xgboost optuna imbalanced-learn joblib matplotlib seaborn pykalman\n",
        "\n",
        "# --- env & imports ---\n",
        "import os, sys, random, subprocess, warnings, json, contextlib\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Reproducibility: global seed\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Force thread env vars to '1' to reduce nondeterminism from BLAS/OMP/MKL\n",
        "THREAD_ENV_VARS = [\n",
        "    \"OMP_NUM_THREADS\",\n",
        "    \"MKL_NUM_THREADS\",\n",
        "    \"OPENBLAS_NUM_THREADS\",\n",
        "    \"VECLIB_MAXIMUM_THREADS\",\n",
        "    \"NUMEXPR_NUM_THREADS\",\n",
        "]\n",
        "for k in THREAD_ENV_VARS:\n",
        "    os.environ[k] = '1'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# === Parallelism / worker defaults (centralized) ===\n",
        "# Use these constants everywhere when instantiating models or running optimizers.\n",
        "JOBLIB_N_JOBS = 1        # for joblib.parallel_backend / parallel loops\n",
        "XGB_NJOBS = 1            # pass to xgboost models: n_jobs=XGB_NJOBS\n",
        "OPTUNA_N_JOBS = 1        # when running study.optimize(..., n_jobs=OPTUNA_N_JOBS)\n",
        "SKLEARN_N_JOBS = 1       # use in sklearn where applicable\n",
        "\n",
        "# Helper context manager to force loky backend during critical sections.\n",
        "# Use `with set_parallel_backend():` around heavy parallel calls if you want strict enforcement.\n",
        "@contextlib.contextmanager\n",
        "def set_parallel_backend(n_jobs=JOBLIB_N_JOBS, backend=\"loky\"):\n",
        "    \"\"\"\n",
        "    Context manager to set joblib parallel backend for deterministic behavior.\n",
        "    Usage:\n",
        "        with set_parallel_backend():\n",
        "            # calls that use joblib / sklearn parallelism\n",
        "    \"\"\"\n",
        "    with joblib.parallel_backend(backend, n_jobs=n_jobs):\n",
        "        yield\n",
        "\n",
        "# Drive mount (interactive, Colab-friendly)\n",
        "USE_DRIVE = True\n",
        "BASE_DIR = \"/mnt/data/quant_pipeline\"\n",
        "if USE_DRIVE:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        BASE_DIR = \"/content/drive/MyDrive/quant_pipeline\"\n",
        "    except Exception as e:\n",
        "        print(\"Drive mount failed (not fatal). Using local path. Err:\", str(e))\n",
        "        BASE_DIR = \"/mnt/data/quant_pipeline\"\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "OUT_DIR = os.path.join(BASE_DIR, \"mtb_out\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Additional environment info (including key libs)\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "import sklearn\n",
        "\n",
        "print(\"Python:\", sys.version.splitlines()[0])\n",
        "print(\"Numpy:\", np.__version__, \"Pandas:\", pd.__version__)\n",
        "print(\"XGBoost:\", getattr(xgb, \"__version__\", \"unknown\"), \"Optuna:\", getattr(optuna, \"__version__\", \"unknown\"))\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n",
        "print(\"SEED:\", SEED)\n",
        "print(\"Joblib CPU count (system):\", joblib.cpu_count())\n",
        "print(\"Configured JOBLIB_N_JOBS:\", JOBLIB_N_JOBS, \"XGB_NJOBS:\", XGB_NJOBS, \"OPTUNA_N_JOBS:\", OPTUNA_N_JOBS)\n",
        "\n",
        "# Check GPU (safe)\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi 2>&1\")\n",
        "if \"command not found\" in gpu_info.lower() or \"not found\" in gpu_info.lower():\n",
        "    print(\"nvidia-smi not available (likely no GPU).\")\n",
        "else:\n",
        "    print(\"\\n\".join(gpu_info.splitlines()[:6]))\n",
        "\n",
        "# ======================================================\n",
        "# SAVE ENVIRONMENT SNAPSHOT FOR FULL REPRODUCIBILITY\n",
        "# ======================================================\n",
        "def save_env_snapshot(out_path):\n",
        "    pkgs = (\n",
        "        subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"])\n",
        "        .decode()\n",
        "        .strip()\n",
        "        .splitlines()\n",
        "    )\n",
        "    meta = {\n",
        "        \"python\": sys.version,\n",
        "        \"numpy\": np.__version__,\n",
        "        \"pandas\": pd.__version__,\n",
        "        \"xgboost\": getattr(xgb, \"__version__\", \"unknown\"),\n",
        "        \"optuna\": getattr(optuna, \"__version__\", \"unknown\"),\n",
        "        \"sklearn\": sklearn.__version__,\n",
        "        \"pip_freeze\": pkgs,\n",
        "        \"seed\": SEED,\n",
        "        \"env_threads\": { k: os.environ.get(k) for k in THREAD_ENV_VARS },\n",
        "        \"parallel_defaults\": {\n",
        "            \"joblib_n_jobs\": JOBLIB_N_JOBS,\n",
        "            \"xgb_njobs\": XGB_NJOBS,\n",
        "            \"optuna_n_jobs\": OPTUNA_N_JOBS,\n",
        "            \"sklearn_n_jobs\": SKLEARN_N_JOBS\n",
        "        }\n",
        "    }\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "env_json_path = os.path.join(OUT_DIR, \"env_versions.json\")\n",
        "save_env_snapshot(env_json_path)\n",
        "\n",
        "# Quick asserts to catch common bad-start states\n",
        "assert os.path.exists(env_json_path), \"env_versions.json not created\"\n",
        "# Ensure thread env vars set to '1' (defensive)\n",
        "assert all(os.environ.get(k) == '1' for k in THREAD_ENV_VARS), \\\n",
        "    \"One or more thread environment variables not set to '1' — this may cause nondeterminism.\"\n",
        "\n",
        "# Defensive helper: convenience wrapper to create XGB models with centralized n_jobs\n",
        "def make_xgb_clf(**kwargs):\n",
        "    \"\"\"Return xgboost.XGBClassifier with enforced n_jobs and seed unless provided.\"\"\"\n",
        "    base = {\"n_jobs\": XGB_NJOBS, \"random_state\": SEED}\n",
        "    base.update(kwargs)\n",
        "    return xgb.XGBClassifier(**base)\n",
        "\n",
        "def make_xgb_reg(**kwargs):\n",
        "    base = {\"n_jobs\": XGB_NJOBS, \"random_state\": SEED}\n",
        "    base.update(kwargs)\n",
        "    return xgb.XGBRegressor(**base)\n",
        "\n",
        "# Example usage (commented):\n",
        "# clf = make_xgb_clf(max_depth=6, n_estimators=200)\n",
        "# with set_parallel_backend():\n",
        "#     clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nSaved environment snapshot →\", env_json_path)\n",
        "print(\"Setup complete. Use set_parallel_backend() context and make_xgb_clf/make_xgb_reg helpers when training.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Pipeline constants & helpers\n",
        "# -----------------------------\n",
        "# Ensure LAST_COMPLETE can be set externally to reproduce runs across sessions.\n",
        "# If not set explicitly by fetch/align cells, readiness_checks will require it.\n",
        "globals().setdefault('LAST_COMPLETE', None)\n",
        "\n",
        "# Prefixes to never include automatically as features (diag columns, temporary, etc.)\n",
        "EXCLUDE_COL_PREFIXES = ['diag_', 'tmp_', 'meta_']\n",
        "globals()['EXCLUDE_COL_PREFIXES'] = EXCLUDE_COL_PREFIXES\n",
        "\n",
        "# Readiness checks helper to assert core contracts prior to MTB/feature steps.\n",
        "def readiness_checks(df, last_complete=None):\n",
        "    \"\"\"\n",
        "    Basic assertions:\n",
        "      - LAST_COMPLETE must be set (either passed or global)\n",
        "      - No partial last bar beyond LAST_COMPLETE\n",
        "      - Imputed rows must preserve NaN volume (if both columns exist)\n",
        "    Raises AssertionError on failure.\n",
        "    \"\"\"\n",
        "    last = last_complete or globals().get('LAST_COMPLETE')\n",
        "    assert last is not None, \"LAST_COMPLETE is not set — call set LAST_COMPLETE explicitly for reproducibility.\"\n",
        "    # normalize tz for comparison if possible\n",
        "    if hasattr(last, 'tz'):\n",
        "        last_cmp = pd.Timestamp(last)\n",
        "    else:\n",
        "        last_cmp = pd.Timestamp(last)\n",
        "    # ensure timezone-aware comparison by converting to UTC\n",
        "    if last_cmp.tzinfo is None:\n",
        "        last_cmp = last_cmp.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        last_cmp = last_cmp.tz_convert(\"UTC\")\n",
        "    if not df.empty:\n",
        "        assert df.index.max() <= last_cmp, f\"Partial last bar detected: df.max()={df.index.max()} > LAST_COMPLETE={last_cmp}\"\n",
        "    if 'is_imputed' in df.columns and 'volume' in df.columns:\n",
        "        assert df.loc[df['is_imputed'],'volume'].isna().all(), \"Some imputed rows have non-NaN volume.\"\n",
        "    return True\n",
        "\n",
        "globals()['readiness_checks'] = readiness_checks\n",
        "\n",
        "# Small helper to filter feature columns (exclude diag_ prefixes by default)\n",
        "def filter_feature_columns(df, exclude_prefixes=None):\n",
        "    exclude_prefixes = exclude_prefixes or globals().get('EXCLUDE_COL_PREFIXES', ['diag_'])\n",
        "    return [c for c in df.columns if not any(c.startswith(p) for p in exclude_prefixes)]\n",
        "\n",
        "globals()['filter_feature_columns'] = filter_feature_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph5Xrhn6yQvo",
        "outputId": "b8f24ba9-bf54-40c1-897a-7d5518f16a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[try_bitfinex_fetch] Trying exchange: bitfinex\n",
            "[try_bitfinex_fetch] Markets loaded; symbol count = 331\n",
            "[try_bitfinex_fetch] Using symbol: BTC/USDT\n",
            "[try_bitfinex_fetch] fetching since 2023-12-06 00:00:00+00:00 (ms=1701820800000) timeframe=1h limit=1000\n",
            "[safe_fetch_ohlcv] total cycles: 18, total rows fetched: 17520\n",
            "[try_bitfinex_fetch] Fetched 17519 rows from bitfinex for BTC/USDT (last_complete=2025-12-05 00:00:00+00:00)\n",
            "[try_bitfinex_fetch] snapshot saved: /content/drive/MyDrive/quant_pipeline/mtb_out/raw_bitfinex_BTC-USDT_1h_20251205T000000Z.csv (rows 17519, missing 2)\n",
            "                              open     high      low    close     volume\n",
            "ts                                                                      \n",
            "2023-12-06 00:00:00+00:00  44066.0  44135.0  43837.0  44135.0  11.604230\n",
            "2023-12-06 01:00:00+00:00  44135.0  44140.0  43765.0  43871.0  16.388246\n",
            "2023-12-06 02:00:00+00:00  43873.0  43911.0  43578.0  43888.0   8.766282\n",
            "2023-12-06 03:00:00+00:00  43893.0  43918.0  43673.0  43720.0   8.733780\n",
            "2023-12-06 04:00:00+00:00  43725.0  43849.0  43611.0  43726.0   1.715235\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 17519 entries, 2023-12-06 00:00:00+00:00 to 2025-12-05 00:00:00+00:00\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   open    17519 non-null  float64\n",
            " 1   high    17519 non-null  float64\n",
            " 2   low     17519 non-null  float64\n",
            " 3   close   17519 non-null  float64\n",
            " 4   volume  17519 non-null  float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 1.3 MB\n",
            "None\n",
            "rows: 17519\n",
            "interval sample freq: h\n",
            "raw snapshot: /content/drive/MyDrive/quant_pipeline/mtb_out/raw_bitfinex_BTC-USDT_1h_20251205T000000Z.csv\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# 01_FETCH — hardened, tz-safe, snapshotting + validation\n",
        "# (updated: use last fully-closed candle as cutoff; register snapshot in manifest; export LAST_COMPLETE)\n",
        "# ============================\n",
        "\n",
        "import ccxt\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "import json\n",
        "import hashlib\n",
        "import re\n",
        "\n",
        "# OUT_DIR fallback (if not set by 00_SETUP)\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "LOCAL_PATH = os.path.join(OUT_DIR, \"bitfinex_BTC_USDT_1h.csv\")\n",
        "\n",
        "# Config\n",
        "SYMBOL_PREFS = [\"BTC/USDT\", \"BTC/USD\", \"BTC/USDT:USDT\", \"BTC-PERP\"]\n",
        "SINCE_DAYS = 730        # history window in days\n",
        "TIMEFRAME = \"1h\"\n",
        "LIMIT = 1000            # per-request limit (typical)\n",
        "MAX_RETRY = 5\n",
        "MAX_CYCLES = 200\n",
        "SLEEP_BASE = 1.0        # base sleep for backoff (seconds)\n",
        "\n",
        "def _ms_from_ts(ts: pd.Timestamp):\n",
        "    if ts.tzinfo is None:\n",
        "        ts = ts.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        ts = ts.tz_convert(\"UTC\")\n",
        "    return int(ts.value // 10**6)\n",
        "\n",
        "def sha256_of_file(path, chunk_size=8192):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def safe_fetch_ohlcv(ex, symbol, timeframe=\"1h\", since=None, limit=1000, max_cycles=None):\n",
        "    ohlcv = []\n",
        "    since_local = int(since) if since is not None else None\n",
        "    cycles = 0\n",
        "    max_cycles = max_cycles or MAX_CYCLES\n",
        "\n",
        "    while True:\n",
        "        cycles += 1\n",
        "        if cycles > max_cycles:\n",
        "            print(f\"[safe_fetch_ohlcv] reached MAX_CYCLES ({max_cycles}), stopping.\")\n",
        "            break\n",
        "\n",
        "        got_batch = None\n",
        "        backoff = SLEEP_BASE\n",
        "        for attempt in range(1, MAX_RETRY + 1):\n",
        "            try:\n",
        "                if hasattr(ex, \"enableRateLimit\"):\n",
        "                    ex.enableRateLimit = True\n",
        "                batch = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=since_local, limit=limit)\n",
        "                if not isinstance(batch, list):\n",
        "                    raise RuntimeError(\"fetch_ohlcv returned non-list\")\n",
        "                got_batch = batch\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"[safe_fetch_ohlcv] attempt {attempt} failed: {e}\")\n",
        "                time.sleep(backoff)\n",
        "                backoff *= 2\n",
        "\n",
        "        if got_batch is None:\n",
        "            raise RuntimeError(\"Failed to fetch after retries\")\n",
        "\n",
        "        if len(got_batch) == 0:\n",
        "            break\n",
        "\n",
        "        if not all(isinstance(row, (list, tuple)) and len(row) >= 6 for row in got_batch):\n",
        "            raise RuntimeError(\"Unexpected OHLCV row shape; expected at least 6 elements per row\")\n",
        "\n",
        "        # dedupe across batches (assume ascending ts)\n",
        "        if len(ohlcv) > 0 and got_batch[0][0] <= ohlcv[-1][0]:\n",
        "            start_idx = 0\n",
        "            while start_idx < len(got_batch) and got_batch[start_idx][0] <= ohlcv[-1][0]:\n",
        "                start_idx += 1\n",
        "            if start_idx >= len(got_batch):\n",
        "                break\n",
        "            ohlcv.extend(got_batch[start_idx:])\n",
        "        else:\n",
        "            ohlcv.extend(got_batch)\n",
        "\n",
        "        last_ts = got_batch[-1][0]\n",
        "        since_local = int(last_ts) + 1\n",
        "\n",
        "        if len(got_batch) < limit:\n",
        "            break\n",
        "\n",
        "        rl = getattr(ex, \"rateLimit\", None)\n",
        "        if rl:\n",
        "            time.sleep(rl / 1000.0)\n",
        "        else:\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    print(f\"[safe_fetch_ohlcv] total cycles: {cycles}, total rows fetched: {len(ohlcv)}\")\n",
        "    return ohlcv\n",
        "\n",
        "def try_bitfinex_fetch(max_cycles=None, snapshot=True):\n",
        "    ex_name = \"bitfinex\"\n",
        "    try:\n",
        "        print(f\"[try_bitfinex_fetch] Trying exchange: {ex_name}\")\n",
        "        ex_cls = getattr(ccxt, ex_name)\n",
        "        ex = ex_cls({\"enableRateLimit\": True})\n",
        "        ex.load_markets()\n",
        "        print(f\"[try_bitfinex_fetch] Markets loaded; symbol count = {len(ex.symbols)}\")\n",
        "\n",
        "        # pick symbol from preferences or best candidate\n",
        "        symbol = None\n",
        "        for s in SYMBOL_PREFS:\n",
        "            if s in ex.symbols:\n",
        "                symbol = s\n",
        "                break\n",
        "        if symbol is None:\n",
        "            candidates = [s for s in ex.symbols if s.startswith(\"BTC/\") or s.startswith(\"BTC-\")]\n",
        "            if len(candidates) == 0:\n",
        "                print(\"[try_bitfinex_fetch] No BTC symbols found on bitfinex\")\n",
        "                return None, None, None, None\n",
        "            for pref in (\"BTC/USDT\", \"BTC/USD\", \"BTC-USD\"):\n",
        "                if pref in candidates:\n",
        "                    symbol = pref\n",
        "                    break\n",
        "            if symbol is None:\n",
        "                symbol = candidates[0]\n",
        "\n",
        "        print(f\"[try_bitfinex_fetch] Using symbol: {symbol}\")\n",
        "\n",
        "        # compute safe cutoff: last fully-closed candle for given timeframe\n",
        "        now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
        "        try:\n",
        "            bar_delta = pd.Timedelta(TIMEFRAME)  # e.g., '1h', works for pandas\n",
        "        except Exception:\n",
        "            # fallback for simple 'Nh' string parse\n",
        "            if TIMEFRAME.endswith('h'):\n",
        "                bar_delta = pd.Timedelta(hours=int(TIMEFRAME[:-1]))\n",
        "            else:\n",
        "                bar_delta = pd.Timedelta(hours=1)\n",
        "\n",
        "        last_complete = now_utc - bar_delta\n",
        "        since_dt = last_complete - pd.Timedelta(days=SINCE_DAYS)\n",
        "        since_ms = _ms_from_ts(since_dt)\n",
        "\n",
        "        print(f\"[try_bitfinex_fetch] fetching since {since_dt} (ms={since_ms}) timeframe={TIMEFRAME} limit={LIMIT}\")\n",
        "        raw = safe_fetch_ohlcv(ex, symbol, timeframe=TIMEFRAME, since=since_ms, limit=LIMIT, max_cycles=max_cycles)\n",
        "        if not raw:\n",
        "            print(\"[try_bitfinex_fetch] No data returned\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        df = pd.DataFrame(raw, columns=[\"ts\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "        df[\"ts\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True)\n",
        "        df = df.set_index(\"ts\")\n",
        "        df = df[~df.index.duplicated(keep=\"first\")].sort_index()\n",
        "\n",
        "        # KEEP ONLY FULLY CLOSED BARS (no partial last bar)\n",
        "        df = df[df.index <= last_complete]\n",
        "\n",
        "        # ensure monotonicity & types\n",
        "        if not df.index.is_monotonic_increasing:\n",
        "            df = df.sort_index()\n",
        "\n",
        "        for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(float)\n",
        "        if df[[\"open\",\"high\",\"low\",\"close\"]].isnull().any().any():\n",
        "            raise RuntimeError(\"Found NaN in OHLC after fetch; aborting.\")\n",
        "\n",
        "        print(f\"[try_bitfinex_fetch] Fetched {len(df)} rows from bitfinex for {symbol} (last_complete={last_complete})\")\n",
        "\n",
        "        snapshot_path = None\n",
        "        if snapshot:\n",
        "            # sanitize symbol for filename portability\n",
        "            safe_symbol = re.sub(r'[^\\w\\-.]', '_', symbol.replace('/','-'))\n",
        "            fname = f\"raw_{ex_name}_{safe_symbol}_{TIMEFRAME}_{last_complete.strftime('%Y%m%dT%H%M%SZ')}.csv\"\n",
        "            snapshot_path = os.path.join(OUT_DIR, fname)\n",
        "            # write CSV\n",
        "            df.to_csv(snapshot_path)\n",
        "\n",
        "            # also write parquet to preserve tz/dtypes and better fidelity\n",
        "            parquet_path = snapshot_path.replace(\".csv\", \".parquet\")\n",
        "            try:\n",
        "                df.to_parquet(parquet_path, index=True)\n",
        "            except Exception as e:\n",
        "                # non-fatal but warn\n",
        "                print(f\"[try_bitfinex_fetch] Warning: failed to write parquet ({e}); parquet not created.\")\n",
        "                parquet_path = None\n",
        "\n",
        "            meta = dict(\n",
        "                exchange=ex_name,\n",
        "                symbol=symbol,\n",
        "                timeframe=TIMEFRAME,\n",
        "                fetched_at=str(pd.Timestamp.now(tz='UTC')),\n",
        "                last_complete=str(last_complete),\n",
        "                since_dt=str(since_dt)\n",
        "            )\n",
        "\n",
        "            # compute expected rows using full index\n",
        "            full_idx = pd.date_range(start=since_dt, end=last_complete, freq=bar_delta, tz=\"UTC\")\n",
        "            expected_rows = len(full_idx)\n",
        "            missing_idx = full_idx.difference(df.index)\n",
        "            missing_hours = len(missing_idx)\n",
        "\n",
        "            # compute and attach CSV sha256\n",
        "            csv_sha = sha256_of_file(snapshot_path)\n",
        "            meta.update({\n",
        "                \"csv_sha256\": csv_sha,\n",
        "                \"expected_rows\": expected_rows,\n",
        "                \"rows_fetched\": len(df),\n",
        "                \"missing_hours\": missing_hours,\n",
        "                \"missing_samples_preview\": [str(ts) for ts in (list(missing_idx[:10]) if missing_hours>0 else [])]\n",
        "            })\n",
        "\n",
        "            # compute parquet sha if available\n",
        "            if parquet_path and os.path.exists(parquet_path):\n",
        "                try:\n",
        "                    parquet_sha = sha256_of_file(parquet_path)\n",
        "                    meta.update({\"parquet_file\": os.path.basename(parquet_path), \"parquet_sha256\": parquet_sha})\n",
        "                except Exception:\n",
        "                    parquet_sha = None\n",
        "\n",
        "            # write snapshot meta\n",
        "            with open(snapshot_path + \".meta.json\", \"w\") as f:\n",
        "                json.dump(meta, f, indent=2)\n",
        "\n",
        "            # register snapshot in pipeline_manifest.json (idempotent append)\n",
        "            try:\n",
        "                manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "                manifest = {}\n",
        "                if os.path.exists(manifest_path):\n",
        "                    try:\n",
        "                        manifest = json.load(open(manifest_path))\n",
        "                    except Exception:\n",
        "                        manifest = {}\n",
        "                manifest.setdefault('snapshots', [])\n",
        "                # avoid duplicate entries for same filename\n",
        "                base_csv = os.path.basename(snapshot_path)\n",
        "                base_parquet = os.path.basename(parquet_path) if parquet_path else None\n",
        "                if not any(s.get('file') == base_csv for s in manifest['snapshots']):\n",
        "                    entry = {\n",
        "                        \"file\": base_csv,\n",
        "                        \"sha256\": csv_sha,\n",
        "                        \"fetched_at\": meta[\"fetched_at\"],\n",
        "                        \"last_complete\": str(last_complete),\n",
        "                        \"expected_rows\": expected_rows,\n",
        "                        \"rows_fetched\": len(df),\n",
        "                        \"missing_hours\": missing_hours\n",
        "                    }\n",
        "                    if base_parquet:\n",
        "                        entry.update({\"parquet_file\": base_parquet, \"parquet_sha256\": meta.get(\"parquet_sha256\")})\n",
        "                    manifest['snapshots'].append(entry)\n",
        "                with open(manifest_path, \"w\") as mf:\n",
        "                    json.dump(manifest, mf, indent=2)\n",
        "            except Exception:\n",
        "                # non-fatal\n",
        "                pass\n",
        "\n",
        "            print(f\"[try_bitfinex_fetch] snapshot saved: {snapshot_path} (rows {len(df)}, missing {missing_hours})\")\n",
        "\n",
        "            # conservative sanity: fail early if too many missing bars\n",
        "            assert missing_hours < max(24, int(24 * min(7, SINCE_DAYS))), \\\n",
        "                f\"Too many missing bars ({missing_hours}) in raw fetch — check exchange connectivity or rate limits.\"\n",
        "\n",
        "            # verify written file sha\n",
        "            assert csv_sha == sha256_of_file(snapshot_path), \"CSV snapshot sha mismatch after write\"\n",
        "            if parquet_path and os.path.exists(parquet_path):\n",
        "                assert meta.get(\"parquet_sha256\") == sha256_of_file(parquet_path), \"Parquet snapshot sha mismatch after write\"\n",
        "\n",
        "            # export LAST_COMPLETE for downstream cells\n",
        "            globals()['LAST_COMPLETE'] = last_complete\n",
        "\n",
        "        return df, ex_name, symbol, snapshot_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[try_bitfinex_fetch] Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None\n",
        "\n",
        "# compute safer default for cycles (approx)\n",
        "_estimated_cycles = max(10, int((SINCE_DAYS * 24) / (LIMIT or 1)) + 5)\n",
        "MAX_CYCLES = min(MAX_CYCLES, max(50, _estimated_cycles))\n",
        "\n",
        "df, exch_used, symbol_used, snapshot_path = try_bitfinex_fetch(max_cycles=MAX_CYCLES, snapshot=True)\n",
        "\n",
        "# Fallback to local CSV or parquet if fetch failed\n",
        "if df is None:\n",
        "    # prefer parquet snapshot if available next to LOCAL_PATH\n",
        "    parquet_local = LOCAL_PATH.replace(\".csv\", \".parquet\")\n",
        "    if os.path.exists(parquet_local):\n",
        "        print(\"[main] Loading local parquet file:\", parquet_local)\n",
        "        df = pd.read_parquet(parquet_local)\n",
        "        # ensure tz\n",
        "        if df.index.tz is None:\n",
        "            df.index = df.index.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            df.index = df.index.tz_convert(\"UTC\")\n",
        "    elif os.path.exists(LOCAL_PATH):\n",
        "        print(\"[main] Loading local CSV file:\", LOCAL_PATH)\n",
        "        df = pd.read_csv(LOCAL_PATH, parse_dates=True, index_col=0)\n",
        "        if df.index.tz is None:\n",
        "            df.index = df.index.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            df.index = df.index.tz_convert(\"UTC\")\n",
        "    else:\n",
        "        raise RuntimeError(\"No exchange reachable and no local file found.\")\n",
        "\n",
        "    df = df[~df.index.duplicated()].sort_index()\n",
        "    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(float)\n",
        "    # make sure local data also respects last_complete cutoff if available\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
        "    try:\n",
        "        bar_delta = pd.Timedelta(TIMEFRAME)\n",
        "    except Exception:\n",
        "        bar_delta = pd.Timedelta(hours=1)\n",
        "    last_complete = now_utc - bar_delta\n",
        "    df = df[df.index <= last_complete]\n",
        "    # export LAST_COMPLETE when loading local\n",
        "    globals()['LAST_COMPLETE'] = last_complete\n",
        "    print(\"[main] Loaded local rows:\", len(df))\n",
        "\n",
        "# Quick integrity/summary\n",
        "# Ensure no partial last bar slipped in\n",
        "now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
        "try:\n",
        "    bar_delta = pd.Timedelta(TIMEFRAME)\n",
        "except Exception:\n",
        "    bar_delta = pd.Timedelta(hours=1)\n",
        "last_complete = globals().get('LAST_COMPLETE', now_utc - bar_delta)\n",
        "assert df.index.max() <= last_complete, f\"Partial/incomplete last bar present: {df.index.max()} > {last_complete}\"\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(\"rows:\", len(df))\n",
        "print(\"interval sample freq:\", pd.infer_freq(df.index[:200]) if len(df.index)>50 else \"n/a\")\n",
        "if snapshot_path:\n",
        "    print(\"raw snapshot:\", snapshot_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJYPt0fPzLqk",
        "outputId": "2055d5c7-3397-4757-c35c-c17d3d15c705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: p_hi_global computed on full dataset — do NOT use this value for cross-fold preprocessing. Recompute per-fold.\n",
            "Saved diagnostic meta -> /content/drive/MyDrive/quant_pipeline/mtb_out/volume_meta.json\n",
            "Sample diag columns tail:\n",
            "                              volume  diag_volume_winsor  diag_volume_log1p  \\\n",
            "ts                                                                            \n",
            "2025-12-04 19:00:00+00:00  44.497418           44.497418           3.817656   \n",
            "2025-12-04 20:00:00+00:00  10.833951           10.833951           2.470973   \n",
            "2025-12-04 21:00:00+00:00   6.144231            6.144231           1.966305   \n",
            "2025-12-04 22:00:00+00:00   0.127484            0.127484           0.119989   \n",
            "2025-12-04 23:00:00+00:00   0.185291            0.185291           0.169988   \n",
            "2025-12-05 00:00:00+00:00   0.381884            0.381884           0.323448   \n",
            "\n",
            "                           diag_volume_clean  diag_anomaly  \n",
            "ts                                                          \n",
            "2025-12-04 19:00:00+00:00          44.497418         False  \n",
            "2025-12-04 20:00:00+00:00          10.833951         False  \n",
            "2025-12-04 21:00:00+00:00           6.144231         False  \n",
            "2025-12-04 22:00:00+00:00           0.127484         False  \n",
            "2025-12-04 23:00:00+00:00           0.185291         False  \n",
            "2025-12-05 00:00:00+00:00           0.381884         False  \n"
          ]
        }
      ],
      "source": [
        "# ===== CELL: volume diagnostic helper (encapsulated, safe, persists meta) =====\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Usage: df must exist in scope (fetched & aligned). OUT_DIR optional.\n",
        "# This cell writes OUT_DIR/volume_meta.json and appends diagnostic columns\n",
        "# prefixed with \"diag_\" to avoid accidental use in model pipelines.\n",
        "def compute_volume_diag(\n",
        "    df,\n",
        "    out_dir=globals().get(\"OUT_DIR\", \"/content\"),\n",
        "    roll_w=24,            # hours for rolling median (causal)\n",
        "    winsor_q=0.995,       # quantile for winsor diagnostic\n",
        "    recent_days=90,\n",
        "    timeframe=\"1h\",\n",
        "    replace_zero_with_med=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute volume diagnostics (non-destructive).\n",
        "    - Creates diag_volume_* columns only (won't overwrite 'volume').\n",
        "    - Persists meta JSON with thresholds and provenance.\n",
        "    - Returns df (mutated) and meta dict.\n",
        "    \"\"\"\n",
        "    assert isinstance(df, pd.DataFrame) and 'volume' in df.columns, \"df with 'volume' required\"\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ensure tz-aware index\n",
        "    if df.index.tz is None:\n",
        "        df.index = df.index.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        df.index = df.index.tz_convert(\"UTC\")\n",
        "\n",
        "    # compute bar_delta from timeframe (simple support for 'Nh')\n",
        "    try:\n",
        "        bar_delta = pd.Timedelta(timeframe)\n",
        "    except Exception:\n",
        "        if str(timeframe).endswith('h'):\n",
        "            bar_delta = pd.Timedelta(hours=int(str(timeframe)[:-1]))\n",
        "        else:\n",
        "            bar_delta = pd.Timedelta(hours=1)\n",
        "\n",
        "    # normalize roll_w to int hours (defensive)\n",
        "    try:\n",
        "        roll_w_int = int(roll_w)\n",
        "        if roll_w_int <= 0:\n",
        "            raise ValueError\n",
        "    except Exception:\n",
        "        roll_w_int = 24\n",
        "        print(f\"[compute_volume_diag] warning: invalid roll_w provided; defaulting to {roll_w_int}h\")\n",
        "\n",
        "    # baseline series (prefer non-imputed rows if flag exists)\n",
        "    baseline = df['volume']\n",
        "    if 'is_imputed' in df.columns:\n",
        "        nonimp = df.loc[~df['is_imputed'], 'volume']\n",
        "        if len(nonimp.dropna()) > 0:\n",
        "            baseline = nonimp\n",
        "\n",
        "    # compute winsor thresholds (global + recent)\n",
        "    p_hi_global = float(baseline.quantile(winsor_q))\n",
        "    recent_cut = df.index[-1] - pd.Timedelta(days=recent_days)\n",
        "    if (baseline.index >= recent_cut).any():\n",
        "        p_hi_recent = float(baseline.loc[baseline.index >= recent_cut].quantile(winsor_q))\n",
        "    else:\n",
        "        p_hi_recent = p_hi_global\n",
        "\n",
        "    # rolling median (strictly causal) — use time-based rolling to be robust vs missing timestamps\n",
        "    # closed='left' ensures we do not include current bar in window; .shift(1) keeps it strictly causal\n",
        "    vol_med = df['volume'].rolling(f'{roll_w_int}H', min_periods=1, closed='left').median().shift(1)\n",
        "    vol_ratio = df['volume'] / (vol_med + 1e-12)\n",
        "    anomaly_mask = vol_ratio > 10\n",
        "\n",
        "    # diagnostic columns (prefix diag_)\n",
        "    df['diag_vol_med'] = vol_med\n",
        "    df['diag_vol_ratio'] = vol_ratio\n",
        "    df['diag_anomaly'] = anomaly_mask.astype(bool)\n",
        "\n",
        "    # diagnostic winsor and log1p (DIAGNOSTIC ONLY — do not use in preproc across folds)\n",
        "    df['diag_volume_winsor'] = df['volume'].clip(upper=p_hi_global)\n",
        "    df['diag_volume_log1p'] = np.log1p(df['diag_volume_winsor'])\n",
        "\n",
        "    # cleaned volume: replace zeros/negatives with causal rolling median if requested\n",
        "    df['diag_volume_clean'] = df['volume'].copy()\n",
        "    if replace_zero_with_med and (df['volume'] <= 0).sum() > 0:\n",
        "        med = df['volume'].replace(0, np.nan).rolling(f'{roll_w_int}H', min_periods=1, closed='left').median().shift(1)\n",
        "        df['diag_volume_clean'] = df['volume'].where(df['volume'] > 0, med).fillna(0)\n",
        "\n",
        "    # persist meta (do not rely on df.attrs for critical thresholds)\n",
        "    meta = {\n",
        "        \"computed_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "        \"data_last_index\": str(df.index.max()),\n",
        "        \"timeframe\": timeframe,\n",
        "        \"winsor_q\": float(winsor_q),\n",
        "        \"p_hi_global\": p_hi_global,\n",
        "        \"p_hi_recent\": p_hi_recent,\n",
        "        \"roll_w_hours\": int(roll_w_int),\n",
        "        \"anomaly_threshold_ratio\": 10,\n",
        "        \"notes\": \"diag_* columns are diagnostics only. Recompute per-train-fold for preprocessing.\"\n",
        "    }\n",
        "\n",
        "    # NOTE: winsor thresholds computed on full dataset (diagnostic only).\n",
        "    # Add explicit flag to meta and print warning to avoid accidental cross-fold usage.\n",
        "    meta[\"winsor_computed_on_full_df\"] = True\n",
        "    meta[\"exclude_prefixes\"] = globals().get('EXCLUDE_COL_PREFIXES', ['diag_'])\n",
        "    print(\"WARNING: p_hi_global computed on full dataset — do NOT use this value for cross-fold preprocessing. Recompute per-fold.\")\n",
        "\n",
        "    meta_path = os.path.join(out_dir, \"volume_meta.json\")\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    # Also register diagnostic meta in pipeline manifest for discoverability\n",
        "    try:\n",
        "        manifest_path = os.path.join(out_dir, \"pipeline_manifest.json\")\n",
        "        manifest = {}\n",
        "        if os.path.exists(manifest_path):\n",
        "            try:\n",
        "                manifest = json.load(open(manifest_path))\n",
        "            except Exception:\n",
        "                manifest = {}\n",
        "        manifest.setdefault('diagnostics', {})['volume_meta'] = os.path.basename(meta_path)\n",
        "        with open(manifest_path, \"w\") as mf:\n",
        "            json.dump(manifest, mf, indent=2)\n",
        "    except Exception:\n",
        "        # non-fatal: manifest registration is best-effort\n",
        "        pass\n",
        "\n",
        "    # quick defensive asserts\n",
        "    assert np.isfinite(p_hi_global) and p_hi_global > 0, \"Invalid p_hi_global\"\n",
        "    assert df['diag_volume_clean'].notna().all(), \"diag_volume_clean contains NaNs — check imputation logic\"\n",
        "\n",
        "    return df, meta\n",
        "\n",
        "# Run helper (mutates df in-place; returns meta)\n",
        "df, volume_meta = compute_volume_diag(df, out_dir=globals().get(\"OUT_DIR\", \"/content\"))\n",
        "meta_path = os.path.join(globals().get(\"OUT_DIR\", \"/content\"), \"volume_meta.json\")\n",
        "print(\"Saved diagnostic meta ->\", meta_path)\n",
        "print(\"Sample diag columns tail:\")\n",
        "print(df[['volume','diag_volume_winsor','diag_volume_log1p','diag_volume_clean','diag_anomaly']].tail(6))\n",
        "\n",
        "# simple smoke test to ensure meta file exists and threshold finite\n",
        "assert os.path.exists(meta_path), \"volume_meta.json not found\"\n",
        "m = json.load(open(meta_path))\n",
        "assert np.isfinite(m['p_hi_global'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtYnSRvs0msB",
        "outputId": "7c66083a-f889-4336-9539-f84f5eaea223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[align v2] saved: /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned.csv /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned.pkl\n",
            "[align v2] start_idx: 2023-12-06 00:00:00+00:00\n",
            "[align v2] last_complete: 2025-12-05 00:00:00+00:00\n",
            "[align v2] filled segments: 1\n",
            "[align v2] skipped blocks: 0\n",
            "[align v2] newly_imputed_count: 2\n",
            "[align v2] total imputed rows: 2\n",
            "[align v2] remaining OHLC NaN rows: 0\n",
            "[align v2] total rows: 17521\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 03 FIX v2 — ALIGN (prevent imputation chains robustly)\n",
        "# - Ensures we never CREATE imputation-chains: we ONLY fill blocks whose previous bar\n",
        "#   was originally a real, non-imputed OHLC bar (safe_prev set).\n",
        "# - If a chain already existed from earlier runs, we do NOT attempt to \"repair\" it here\n",
        "#   (we record it in meta for audit). New fills are only from safe_prev.\n",
        "# - Persists meta with filled / skipped blocks and provenance. Registers snapshot ref from manifest.\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Preconditions\n",
        "assert \"df\" in globals(), \"df not found. Run 01_FETCH first.\"\n",
        "\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "TIMEFRAME = globals().get(\"TIMEFRAME\", \"1h\")\n",
        "MAX_GAP_HOURS = int(globals().get(\"MAX_GAP_HOURS\", 3))\n",
        "META_PATH = Path(OUT_DIR) / \"df_aligned.meta.json\"\n",
        "\n",
        "# Backup raw snapshot once\n",
        "if 'df_raw' not in globals():\n",
        "    globals()['df_raw'] = df.copy()\n",
        "\n",
        "# Ensure tz-aware\n",
        "if df.index.tz is None:\n",
        "    df.index = df.index.tz_localize(\"UTC\")\n",
        "else:\n",
        "    df.index = df.index.tz_convert(\"UTC\")\n",
        "\n",
        "# bar_delta & last_complete\n",
        "try:\n",
        "    bar_delta = pd.Timedelta(TIMEFRAME)\n",
        "except Exception:\n",
        "    tf = str(TIMEFRAME)\n",
        "    if tf.endswith('h'):\n",
        "        bar_delta = pd.Timedelta(hours=int(tf[:-1]))\n",
        "    else:\n",
        "        bar_delta = pd.Timedelta(hours=1)\n",
        "\n",
        "now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
        "last_complete = globals().get(\"LAST_COMPLETE\", None)\n",
        "if last_complete is None:\n",
        "    last_complete = now_utc - bar_delta\n",
        "\n",
        "# Trim rows beyond last_complete first\n",
        "df = df[df.index <= last_complete]\n",
        "\n",
        "# Determine deterministic start_idx:\n",
        "# Prefer snapshot 'since_dt' in pipeline_manifest.json if present; otherwise fall back to df.index.min()\n",
        "start_idx = None\n",
        "manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        m = json.load(open(manifest_path))\n",
        "        latest_snap = (m.get('snapshots') or [])[-1] if (m.get('snapshots')) else None\n",
        "        if latest_snap and latest_snap.get('since_dt'):\n",
        "            try:\n",
        "                cand = pd.to_datetime(latest_snap.get('since_dt'))\n",
        "                # ensure tz-aware UTC\n",
        "                if cand.tzinfo is None:\n",
        "                    cand = cand.tz_localize(\"UTC\")\n",
        "                else:\n",
        "                    cand = cand.tz_convert(\"UTC\")\n",
        "                start_idx = cand\n",
        "            except Exception:\n",
        "                start_idx = None\n",
        "    except Exception:\n",
        "        start_idx = None\n",
        "\n",
        "if start_idx is None:\n",
        "    # fallback to existing data minimum\n",
        "    start_idx = df.index.min()\n",
        "\n",
        "# If start_idx is still NA (empty df), error out early\n",
        "if pd.isna(start_idx):\n",
        "    raise RuntimeError(\"Dataframe index empty after trimming to last_complete and manifest probing.\")\n",
        "\n",
        "# Ensure start_idx <= last_complete; if somehow manifest since_dt is after last_complete, cap start_idx\n",
        "if start_idx > last_complete:\n",
        "    print(f\"[align] manifest since_dt ({start_idx}) > last_complete ({last_complete}); capping start_idx to last_complete - 1*bar_delta\")\n",
        "    start_idx = last_complete - bar_delta\n",
        "\n",
        "# Build full index from deterministic start to last_complete\n",
        "full_idx = pd.date_range(start=start_idx, end=last_complete, freq=bar_delta, tz=\"UTC\")\n",
        "\n",
        "# Reindex to full index (this will introduce NaNs where data missing)\n",
        "df = df.reindex(full_idx)\n",
        "df = df[~df.index.duplicated(keep=\"first\")].sort_index()\n",
        "\n",
        "# Ensure is_imputed column exists and boolean\n",
        "if 'is_imputed' not in df.columns:\n",
        "    df['is_imputed'] = False\n",
        "else:\n",
        "    df['is_imputed'] = df['is_imputed'].fillna(False).astype(bool)\n",
        "\n",
        "# Snapshot original imputed flags BEFORE any filling in this run\n",
        "original_imputed = df['is_imputed'].copy()\n",
        "original_imputed_true = set(original_imputed[original_imputed].index)\n",
        "\n",
        "# OHLC NA detection (volume intentionally excluded)\n",
        "OHLC = [\"open\", \"high\", \"low\", \"close\"]\n",
        "for c in OHLC:\n",
        "    if c not in df.columns:\n",
        "        df[c] = np.nan\n",
        "\n",
        "is_na_ohlc = df[OHLC].isna().any(axis=1).astype(int)\n",
        "\n",
        "# Deterministic contiguous NA block detection using pandas-native grouping\n",
        "na_blocks: List[Tuple[pd.Timestamp, pd.Timestamp, int]] = []\n",
        "mask = is_na_ohlc == 1\n",
        "if mask.any():\n",
        "    grp = (mask != mask.shift()).cumsum()\n",
        "    for _, g in df[mask].groupby(grp):\n",
        "        idx_block = g.index\n",
        "        na_blocks.append((idx_block[0], idx_block[-1], len(idx_block)))\n",
        "\n",
        "# Build set of safe previous timestamps:\n",
        "# safe_prev = indices where OHLC are non-NaN and original_imputed == False\n",
        "safe_prev = set(df.index[(~df[OHLC].isna().any(axis=1)) & (~original_imputed)].tolist())\n",
        "\n",
        "filled_segments = []\n",
        "skipped_blocks = []\n",
        "# We'll apply fills to a copy and then atomically assign back to globals['df']\n",
        "df_copy = df.copy()\n",
        "\n",
        "for s, e, length in na_blocks:\n",
        "    if length > MAX_GAP_HOURS:\n",
        "        skipped_blocks.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"reason\": \"gap_too_large\"})\n",
        "        continue\n",
        "\n",
        "    prev_ts = s - bar_delta\n",
        "    # Condition to allow fill: prev_ts must be in safe_prev (original real bar, not originally imputed)\n",
        "    if prev_ts in safe_prev:\n",
        "        # fill OHLC from prev (causal). Operate only on timestamps that actually exist in index\n",
        "        # to avoid creating fake timestamps.\n",
        "        block_idx = df_copy.index[(df_copy.index >= s) & (df_copy.index <= e)]\n",
        "        if len(block_idx) == 0:\n",
        "            # Nothing to fill (edge case)\n",
        "            skipped_blocks.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"reason\": \"no_index_timestamps\"})\n",
        "            continue\n",
        "        prev_vals = df_copy.loc[prev_ts, OHLC].values\n",
        "        df_copy.loc[block_idx, OHLC] = prev_vals\n",
        "        # keep volume NaN for imputed rows (downstream fold-local imputer required)\n",
        "        if 'volume' in df_copy.columns:\n",
        "            df_copy.loc[block_idx, 'volume'] = np.nan\n",
        "        df_copy.loc[block_idx, 'is_imputed'] = True\n",
        "        filled_segments.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"method\": \"ffill_prev_safe\", \"applied_to\": int(len(block_idx))})\n",
        "    else:\n",
        "        # Skip to avoid creating imputation chain or because prev missing/red-flagged\n",
        "        reason = \"prev_not_safe\"\n",
        "        if prev_ts not in df.index:\n",
        "            reason = \"prev_missing\"\n",
        "        elif prev_ts in original_imputed_true:\n",
        "            reason = \"prev_originally_imputed\"\n",
        "        skipped_blocks.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"reason\": reason})\n",
        "        continue\n",
        "\n",
        "# Now commit df_copy back as canonical df\n",
        "df = df_copy\n",
        "\n",
        "# Numeric cast / downcast\n",
        "EXPECTED_COLS = OHLC + ([\"volume\"] if \"volume\" in df.columns else [])\n",
        "for col in EXPECTED_COLS:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "        try:\n",
        "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# Persist artifacts (pickle preferred for fidelity; keep CSV for human-readability)\n",
        "aligned_csv = os.path.join(OUT_DIR, \"df_aligned.csv\")\n",
        "aligned_pkl = os.path.join(OUT_DIR, \"df_aligned.pkl\")\n",
        "df.to_csv(aligned_csv)\n",
        "df.to_pickle(aligned_pkl)\n",
        "\n",
        "# Meta: include filled / skipped blocks and provenance\n",
        "meta = {\n",
        "    \"generated_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"rows\": int(len(df)),\n",
        "    \"start\": str(start_idx),\n",
        "    \"end\": str(df.index.max()),\n",
        "    \"now_utc_used\": str(now_utc),\n",
        "    \"last_complete\": str(last_complete),\n",
        "    \"bar_delta\": str(bar_delta),\n",
        "    \"max_gap_hours_fill\": int(MAX_GAP_HOURS),\n",
        "    \"filled_segments\": filled_segments,\n",
        "    \"skipped_blocks\": skipped_blocks,\n",
        "    \"notes\": \"Fills applied only when previous bar was originally a real (non-imputed) OHLC bar. This prevents creation of imputation chains. Skipped blocks left as NaN for downstream fold-local imputation.\"\n",
        "}\n",
        "\n",
        "# Attempt to enrich meta with snapshot reference from pipeline_manifest.json if present\n",
        "try:\n",
        "    manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "    if os.path.exists(manifest_path):\n",
        "        m = json.load(open(manifest_path))\n",
        "        latest_snap = (m.get('snapshots') or [])[-1] if (m.get('snapshots')) else None\n",
        "        if latest_snap:\n",
        "            meta['snapshot_file'] = latest_snap.get('file')\n",
        "            meta['snapshot_sha256'] = latest_snap.get('sha256')\n",
        "except Exception:\n",
        "    # non-fatal enrichment\n",
        "    pass\n",
        "\n",
        "Path(META_PATH).write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "# Ensure meta persisted\n",
        "assert Path(META_PATH).exists(), \"df_aligned.meta.json not persisted\"\n",
        "\n",
        "# Expose to globals\n",
        "globals()['df'] = df\n",
        "globals()['DF_ALIGNED_CSV'] = aligned_csv\n",
        "globals()['DF_ALIGNED_PKL'] = aligned_pkl\n",
        "globals()['DF_ALIGNED_META'] = str(META_PATH)\n",
        "globals()['LAST_COMPLETE'] = last_complete\n",
        "\n",
        "# Defensive checks (do not fail on pre-existing chains; assert we DID NOT CREATE NEW chains)\n",
        "current_imputed = set(df.index[df['is_imputed']])\n",
        "# newly_imputed = those imputed now that weren't imputed originally\n",
        "newly_imputed = sorted(list(current_imputed - original_imputed_true))\n",
        "\n",
        "# For every newly imputed timestamp, its previous bar must NOT have been originally imputed\n",
        "created_chain = []\n",
        "for t in newly_imputed:\n",
        "    prev = t - bar_delta\n",
        "    if prev in original_imputed_true:\n",
        "        created_chain.append({\"ts\": str(t), \"prev\": str(prev)})\n",
        "\n",
        "# If we (somehow) created any chain — this indicates logic bug; raise for visibility.\n",
        "if len(created_chain) > 0:\n",
        "    raise RuntimeError(\"Logic error: this cell created imputation chains. Created:\", created_chain)\n",
        "\n",
        "# Print summary\n",
        "print(\"[align v2] saved:\", aligned_csv, aligned_pkl)\n",
        "print(\"[align v2] start_idx:\", start_idx)\n",
        "print(\"[align v2] last_complete:\", last_complete)\n",
        "print(\"[align v2] filled segments:\", len(filled_segments))\n",
        "print(\"[align v2] skipped blocks:\", len(skipped_blocks))\n",
        "print(\"[align v2] newly_imputed_count:\", len(newly_imputed))\n",
        "print(\"[align v2] total imputed rows:\", int(df['is_imputed'].sum()))\n",
        "print(\"[align v2] remaining OHLC NaN rows:\", int(df[OHLC].isna().any(axis=1).sum()))\n",
        "print(\"[align v2] total rows:\", len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52PJnhIE1KXv",
        "outputId": "9ce5403e-fcc5-4e80-cbdf-312230583e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total expected rows (full_idx): 17521\n",
            "Actual rows present  : 17521\n",
            "Number of missing idx (from full index): 0\n",
            "\n",
            "NaN rows count (OHLC-only): 0\n",
            "\n",
            "Detected OHLC NA blocks (start, end, length) (skipping fully-imputed blocks):\n",
            "\n",
            "Remaining OHLC NaN rows after short-gap fill: 0\n",
            "\n",
            "Saved cleaned aligned -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned_clean.pkl\n",
            "Saved cleaned aligned CSV -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned_clean.csv\n",
            "cell04 meta saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/cell04_meta.json\n",
            "filled_by_cell4 count: 0  skipped_by_cell4 count: 0\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 04 (FIXED & IMPLEMENTED) — Inspect & safe small-gap fill (idempotent, leak-safe, non-redundant)\n",
        "# - Reuses LAST_COMPLETE and bar_delta from ALIGN\n",
        "# - NA detection on OHLC only (volume excluded)\n",
        "# - Idempotent: skip blocks already marked is_imputed (avoids redundancy with CELL 03)\n",
        "# - Prevent backfill from next / avoid creating imputation chains by requiring prev in safe_prev\n",
        "# - Persist clean artifact + meta\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Preconditions\n",
        "assert 'df' in globals(), \"df not in scope — run ALIGN (cell 03) first.\"\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Keep consistency with ALIGN\n",
        "LAST_COMPLETE = globals().get(\"LAST_COMPLETE\", None)\n",
        "TIMEFRAME = globals().get(\"TIMEFRAME\", \"1h\")\n",
        "try:\n",
        "    bar_delta = pd.Timedelta(TIMEFRAME)\n",
        "except Exception:\n",
        "    tf = str(TIMEFRAME)\n",
        "    if tf.endswith('h'):\n",
        "        bar_delta = pd.Timedelta(hours=int(tf[:-1]))\n",
        "    else:\n",
        "        bar_delta = pd.Timedelta(hours=1)\n",
        "\n",
        "if LAST_COMPLETE is None:\n",
        "    now_utc = pd.Timestamp.now(tz=\"UTC\").floor(\"h\")\n",
        "    LAST_COMPLETE = now_utc - bar_delta\n",
        "\n",
        "# Quick sanity: ensure no partial bars beyond LAST_COMPLETE\n",
        "assert df.index.max() <= LAST_COMPLETE, f\"Partial last bar present: {df.index.max()} > {LAST_COMPLETE}\"\n",
        "\n",
        "# Recompute expected full index for reporting (start from df.index.min())\n",
        "start_idx = df.index.min()\n",
        "full_idx = pd.date_range(start=start_idx, end=LAST_COMPLETE, freq=bar_delta, tz=\"UTC\")\n",
        "missing_idx = full_idx.difference(df.index)\n",
        "\n",
        "print(\"Total expected rows (full_idx):\", len(full_idx))\n",
        "print(\"Actual rows present  :\", df.index.nunique())\n",
        "print(\"Number of missing idx (from full index):\", len(missing_idx))\n",
        "if len(missing_idx) > 0:\n",
        "    print(\"Missing timestamps (sample up to 20):\")\n",
        "    print(list(missing_idx[:20]))\n",
        "\n",
        "# Identify OHLC columns and ensure present\n",
        "OHLC = [\"open\", \"high\", \"low\", \"close\"]\n",
        "for c in OHLC:\n",
        "    if c not in df.columns:\n",
        "        df[c] = np.nan\n",
        "\n",
        "nan_rows = df[df[OHLC].isna().any(axis=1)]\n",
        "print(\"\\nNaN rows count (OHLC-only):\", len(nan_rows))\n",
        "if len(nan_rows) > 0:\n",
        "    print(nan_rows.head(10))\n",
        "\n",
        "# Prepare idempotent fill\n",
        "MAX_GAP_HOURS = int(globals().get(\"MAX_GAP_HOURS\", 3))\n",
        "\n",
        "# Ensure is_imputed exists and boolean\n",
        "if 'is_imputed' not in df.columns:\n",
        "    df['is_imputed'] = False\n",
        "else:\n",
        "    df['is_imputed'] = df['is_imputed'].fillna(False).astype(bool)\n",
        "\n",
        "# Snapshot original imputed flags BEFORE any changes in this cell (prevents using newly-created flags)\n",
        "original_imputed = df['is_imputed'].copy()\n",
        "original_imputed_true = set(original_imputed[original_imputed].index)\n",
        "\n",
        "# Build set of safe_prev timestamps (OHLC complete AND not originally imputed)\n",
        "safe_prev = set(df.index[(~df[OHLC].isna().any(axis=1)) & (~original_imputed)].tolist())\n",
        "\n",
        "# Find contiguous OHLC-NA blocks using pandas-native grouping (deterministic)\n",
        "mask = df[OHLC].isna().any(axis=1)\n",
        "na_blocks: List[Tuple[pd.Timestamp, pd.Timestamp, int]] = []\n",
        "if mask.any():\n",
        "    grp = (mask != mask.shift()).cumsum()\n",
        "    for _, g in df[mask].groupby(grp):\n",
        "        idx_block = g.index\n",
        "        # Skip blocks already fully marked as imputed (idempotency / avoid redundancy with cell03)\n",
        "        if df.loc[idx_block, 'is_imputed'].all():\n",
        "            continue\n",
        "        na_blocks.append((idx_block[0], idx_block[-1], len(idx_block)))\n",
        "\n",
        "print(\"\\nDetected OHLC NA blocks (start, end, length) (skipping fully-imputed blocks):\")\n",
        "for s, e, l in na_blocks:\n",
        "    print(s, e, l)\n",
        "\n",
        "# Idempotent fill: operate on a copy and record filled/skipped blocks\n",
        "df_filled = df.copy()\n",
        "filled_by_cell4 = []\n",
        "skipped_by_cell4 = []\n",
        "\n",
        "for s, e, length in na_blocks:\n",
        "    # operate only on timestamps that actually exist in the index (avoid creating fake timestamps)\n",
        "    block_idx = df_filled.index[(df_filled.index >= s) & (df_filled.index <= e)]\n",
        "    if len(block_idx) == 0:\n",
        "        # nothing to do (edge case)\n",
        "        skipped_by_cell4.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"reason\": \"no_index_timestamps\"})\n",
        "        continue\n",
        "\n",
        "    # if the block was partially imputed earlier, we only operate on non-imputed timestamps within the block\n",
        "    to_operate_idx = block_idx[~df_filled.loc[block_idx, 'is_imputed']]\n",
        "\n",
        "    if len(to_operate_idx) == 0:\n",
        "        # nothing to do (idempotent)\n",
        "        continue\n",
        "\n",
        "    if length <= MAX_GAP_HOURS:\n",
        "        prev_ts = s - bar_delta\n",
        "        # Allow fill only if prev_ts is in safe_prev (original good bar & not originally imputed)\n",
        "        if prev_ts in safe_prev:\n",
        "            prev_row = df_filled.loc[prev_ts, OHLC]\n",
        "            # forward-fill OHLC causally only to the timestamps that are not already imputed\n",
        "            df_filled.loc[to_operate_idx, OHLC] = prev_row.values\n",
        "            # keep volume NaN for imputed rows\n",
        "            if 'volume' in df_filled.columns:\n",
        "                df_filled.loc[to_operate_idx, 'volume'] = np.nan\n",
        "            df_filled.loc[to_operate_idx, 'is_imputed'] = True\n",
        "            filled_by_cell4.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"method\": \"ffill_prev_safe\", \"applied_to\": int(len(to_operate_idx))})\n",
        "        else:\n",
        "            # Skip to avoid creating imputation chain or because prev not eligible\n",
        "            reason = \"prev_not_safe\"\n",
        "            if prev_ts not in df.index:\n",
        "                reason = \"prev_missing\"\n",
        "            elif prev_ts in original_imputed_true:\n",
        "                reason = \"prev_originally_imputed\"\n",
        "            skipped_by_cell4.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"reason\": reason})\n",
        "            continue\n",
        "    else:\n",
        "        skipped_by_cell4.append({\"from\": str(s), \"to\": str(e), \"hours\": int(length), \"reason\": \"gap_too_large\"})\n",
        "        continue\n",
        "\n",
        "# Downcast numeric cols to save memory\n",
        "for c in OHLC + ([\"volume\"] if \"volume\" in df_filled.columns else []):\n",
        "    if c in df_filled.columns:\n",
        "        df_filled[c] = pd.to_numeric(df_filled[c], errors=\"coerce\")\n",
        "        try:\n",
        "            df_filled[c] = pd.to_numeric(df_filled[c], downcast=\"float\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "remaining_nans = int(df_filled[OHLC].isna().any(axis=1).sum())\n",
        "print(\"\\nRemaining OHLC NaN rows after short-gap fill:\", remaining_nans)\n",
        "\n",
        "# Save cleaned aligned (idempotent overwrite)\n",
        "clean_pkl = os.path.join(OUT_DIR, \"df_aligned_clean.pkl\")\n",
        "clean_csv = os.path.join(OUT_DIR, \"df_aligned_clean.csv\")\n",
        "df_filled.to_pickle(clean_pkl)\n",
        "df_filled.to_csv(clean_csv)\n",
        "\n",
        "# Save cell04 meta\n",
        "meta04 = {\n",
        "    \"filled_by_cell4\": filled_by_cell4,\n",
        "    \"skipped_by_cell4\": skipped_by_cell4,\n",
        "    \"remaining_ohlc_nans\": remaining_nans,\n",
        "    \"missing_idx_sample\": [str(ts) for ts in list(missing_idx[:20])],\n",
        "    \"timestamp\": str(pd.Timestamp.now(tz='UTC')),\n",
        "    \"last_complete\": str(LAST_COMPLETE),\n",
        "}\n",
        "Path(os.path.join(OUT_DIR, \"cell04_meta.json\")).write_text(json.dumps(meta04, indent=2))\n",
        "\n",
        "# Expose df_filled as canonical df for downstream cells\n",
        "globals()['df'] = df_filled\n",
        "globals()['DF_ALIGNED_CLEAN_PKL'] = clean_pkl\n",
        "globals()['DF_ALIGNED_CLEAN_CSV'] = clean_csv\n",
        "globals()['CELL04_META'] = os.path.join(OUT_DIR, \"cell04_meta.json\")\n",
        "\n",
        "# Safety assertions to avoid accidental future-peek / leakage\n",
        "# 1) we never used backfill from next (no such method recorded)\n",
        "assert not any(seg.get(\"method\", \"\") == \"backfill_next\" for seg in filled_by_cell4), \"backfill_next used (forbidden)\"\n",
        "\n",
        "# 2) imputed rows must keep volume NaN so downstream fold-local imputer is required\n",
        "if df_filled['is_imputed'].any() and 'volume' in df_filled.columns:\n",
        "    assert df_filled.loc[df_filled['is_imputed'], 'volume'].isna().all(), \"Imputed rows should keep volume as NaN\"\n",
        "\n",
        "# 3) ensure we did not create imputation chains: newly imputed rows' previous bar must NOT be originally imputed\n",
        "current_imputed = set(df_filled.index[df_filled['is_imputed']])\n",
        "newly_imputed = sorted(list(current_imputed - original_imputed_true))\n",
        "chain_found = False\n",
        "chains = []\n",
        "for t in newly_imputed:\n",
        "    prev = t - bar_delta\n",
        "    if prev in original_imputed_true:\n",
        "        chain_found = True\n",
        "        chains.append({\"ts\": str(t), \"prev_originally_imputed\": str(prev)})\n",
        "if chain_found:\n",
        "    raise RuntimeError(\"Imputation chain detected CREATED by this cell (should not happen). Chains:\", chains)\n",
        "\n",
        "print(\"\\nSaved cleaned aligned ->\", clean_pkl)\n",
        "print(\"Saved cleaned aligned CSV ->\", clean_csv)\n",
        "print(\"cell04 meta saved ->\", os.path.join(OUT_DIR, \"cell04_meta.json\"))\n",
        "print(\"filled_by_cell4 count:\", len(filled_by_cell4), \" skipped_by_cell4 count:\", len(skipped_by_cell4))\n",
        "\n",
        "# register to manifest (merge with existing manifest rather than overwrite)\n",
        "manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        manifest = json.load(open(manifest_path))\n",
        "    except Exception:\n",
        "        manifest = {}\n",
        "else:\n",
        "    manifest = {}\n",
        "manifest['cell04_meta'] = os.path.basename(os.path.join(OUT_DIR, \"cell04_meta.json\"))\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AObw5wds1tF5",
        "outputId": "6dbd9ed2-a77a-4342-fad9-5eeda8a04ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved final aligned dataset -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned_final.pkl\n",
            "Saved final aligned CSV -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned_final.csv\n",
            "Saved schema -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned_schema.json\n",
            "Updated manifest -> /content/drive/MyDrive/quant_pipeline/mtb_out/pipeline_manifest.json\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 05 — FINAL STRIP (preserve is_imputed, DO NOT globally fill volume)\n",
        "# - Keeps OHLCV + is_imputed\n",
        "# - Ensures OHLC in imputed regions are filled conservatively (within-mask ffill/bfill)\n",
        "# - Leaves volume NaN for imputed rows and marks volume_imputed_flag\n",
        "# - Persists final dataset, schema, manifest, and safety asserts\n",
        "# ============================\n",
        "\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Preconditions\n",
        "assert 'df' in globals(), \"df not found. Run previous align/inspect cells (03/04) first.\"\n",
        "\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Keep only the canonical columns (if present)\n",
        "cols_to_keep = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"is_imputed\"]\n",
        "existing = [c for c in cols_to_keep if c in df.columns]\n",
        "df_final = df[existing].copy()\n",
        "\n",
        "# Ensure is_imputed boolean exists\n",
        "if 'is_imputed' not in df_final.columns:\n",
        "    df_final['is_imputed'] = False\n",
        "else:\n",
        "    df_final['is_imputed'] = df_final['is_imputed'].fillna(False).astype(bool)\n",
        "\n",
        "# Conservative fill for OHLC inside imputed regions:\n",
        "# operate only on the subset of rows marked is_imputed to avoid leaking non-imputed values\n",
        "imp_mask = df_final['is_imputed'].fillna(False)\n",
        "\n",
        "if imp_mask.any():\n",
        "    # For each OHLC column, do ffill then bfill within the masked subset (this uses only values inside mask)\n",
        "    # This avoids using values from outside the imputed region.\n",
        "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if c in df_final.columns:\n",
        "            # extract masked series (index = imputed timestamps)\n",
        "            s_masked = df_final.loc[imp_mask, c]\n",
        "            # ffill then bfill within masked index only (idempotent)\n",
        "            s_filled = s_masked.ffill().bfill()\n",
        "            # assign back only if shapes match (defensive)\n",
        "            if len(s_filled) == s_masked.shape[0]:\n",
        "                df_final.loc[imp_mask, c] = s_filled.values\n",
        "            else:\n",
        "                # fallback: assign by index alignment\n",
        "                df_final.loc[s_filled.index, c] = s_filled.values\n",
        "\n",
        "# IMPORTANT: keep volume as NaN for imputed rows (no global fill)\n",
        "# create explicit flag: True if row was imputed AND volume is NaN\n",
        "df_final['volume_imputed_flag'] = False\n",
        "if 'volume' in df_final.columns:\n",
        "    imp_volume_mask = imp_mask & df_final['volume'].isna()\n",
        "    df_final.loc[imp_volume_mask, 'volume_imputed_flag'] = True\n",
        "else:\n",
        "    # if no volume column present, create column and mark all imputed rows as flagged\n",
        "    df_final['volume'] = np.nan\n",
        "    df_final.loc[imp_mask, 'volume_imputed_flag'] = True\n",
        "\n",
        "# Save final aligned dataset (pickle + csv)\n",
        "final_pkl = os.path.join(OUT_DIR, \"df_aligned_final.pkl\")\n",
        "final_csv = os.path.join(OUT_DIR, \"df_aligned_final.csv\")\n",
        "df_final.to_pickle(final_pkl)\n",
        "df_final.to_csv(final_csv)\n",
        "\n",
        "# Persist lightweight schema to ensure downstream contract is clear\n",
        "schema = {\n",
        "    \"columns\": list(df_final.columns),\n",
        "    \"required\": [\"open\", \"high\", \"low\", \"close\", \"volume\", \"is_imputed\", \"volume_imputed_flag\"],\n",
        "    \"notes\": \"volume_imputed_flag==True means volume is intentionally NaN and must be imputed fold-locally during training.\"\n",
        "}\n",
        "schema_path = os.path.join(OUT_DIR, \"df_aligned_schema.json\")\n",
        "with open(schema_path, \"w\") as f:\n",
        "    json.dump(schema, f, indent=2)\n",
        "\n",
        "# Audit remaining NaNs in imputed rows (for manifest and logging)\n",
        "remaining_imputed_nans = {}\n",
        "for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "    if c in df_final.columns:\n",
        "        remaining_imputed_nans[c] = int(df_final.loc[imp_mask, c].isna().sum())\n",
        "\n",
        "if any(v > 0 for v in remaining_imputed_nans.values()):\n",
        "    print(\"Warning: remaining OHLC NaNs in imputed rows:\", remaining_imputed_nans)\n",
        "\n",
        "# Append/update a simple pipeline manifest for reproducibility (create if missing)\n",
        "manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "manifest = {}\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        manifest = json.load(open(manifest_path))\n",
        "    except Exception:\n",
        "        manifest = {}\n",
        "\n",
        "# use meaningful keys and merge (do not clobber unrelated entries)\n",
        "manifest.update({\n",
        "    \"df_aligned_final\": os.path.basename(final_pkl),\n",
        "    \"df_aligned_final_csv\": os.path.basename(final_csv),\n",
        "    \"schema\": os.path.basename(schema_path),\n",
        "    \"generated_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"notes\": \"Final aligned dataset. Volumes for imputed rows intentionally left NaN. Do per-fold imputation during training.\",\n",
        "    \"remaining_imputed_nans\": remaining_imputed_nans,\n",
        "})\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "# Safety assertions\n",
        "# 1) Imputed volumes must remain NaN where flagged\n",
        "if df_final['volume_imputed_flag'].any():\n",
        "    assert df_final.loc[df_final['volume_imputed_flag'], 'volume'].isna().all(), \\\n",
        "        \"Imputed volumes must remain NaN — downstream must impute per-fold.\"\n",
        "\n",
        "# 2) Flags consistent: volume_imputed_flag only set for is_imputed rows\n",
        "assert not df_final.loc[~df_final['is_imputed'], 'volume_imputed_flag'].any(), \\\n",
        "    \"volume_imputed_flag set on non-imputed rows (inconsistent).\"\n",
        "\n",
        "# 3) OHLC present (no NaNs) for imputed rows after conservative fill (best-effort)\n",
        "if df_final['is_imputed'].any():\n",
        "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
        "        if c in df_final.columns:\n",
        "            # It's acceptable some imputed blocks remain NaN if block couldn't be filled safely;\n",
        "            # warn but do not assert to avoid false positives in messy data.\n",
        "            n_missing = int(df_final.loc[df_final['is_imputed'], c].isna().sum())\n",
        "            if n_missing > 0:\n",
        "                print(f\"Warning: {n_missing} imputed rows still have NaN in {c} (left for downstream handling).\")\n",
        "\n",
        "# Run quick readiness checks (will not crash pipeline if helper missing or fails)\n",
        "try:\n",
        "    readiness_checks(df_final, last_complete=globals().get('LAST_COMPLETE'))\n",
        "except Exception as e:\n",
        "    # log and continue; do not raise to keep pipeline robust across envs\n",
        "    print(\"Readiness checks skipped/failed (non-fatal):\", type(e).__name__, str(e))\n",
        "\n",
        "# Export a small smoke-test file path for CI\n",
        "smoke_test_path = os.path.join(OUT_DIR, \"smoke_readiness.txt\")\n",
        "with open(smoke_test_path, \"w\") as f:\n",
        "    f.write(\"OK\\n\")\n",
        "\n",
        "# robustly update manifest with smoke_test\n",
        "try:\n",
        "    if os.path.exists(manifest_path):\n",
        "        m2 = json.load(open(manifest_path))\n",
        "    else:\n",
        "        m2 = {}\n",
        "    m2['smoke_test'] = os.path.basename(smoke_test_path)\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        json.dump(m2, f, indent=2)\n",
        "except Exception as e:\n",
        "    print(\"Warning: failed to update manifest with smoke_test:\", e)\n",
        "\n",
        "# Expose final df and artifact paths\n",
        "globals()['df'] = df_final\n",
        "globals()['DF_ALIGNED_FINAL_PKL'] = final_pkl\n",
        "globals()['DF_ALIGNED_FINAL_CSV'] = final_csv\n",
        "globals()['DF_ALIGNED_SCHEMA'] = schema_path\n",
        "globals()['PIPELINE_MANIFEST'] = manifest_path\n",
        "\n",
        "print(\"Saved final aligned dataset ->\", final_pkl)\n",
        "print(\"Saved final aligned CSV ->\", final_csv)\n",
        "print(\"Saved schema ->\", schema_path)\n",
        "print(\"Updated manifest ->\", manifest_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPwi0LT92sVQ",
        "outputId": "badc101d-73ac-41a2-9c96-7b014447fc11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SMOKE TESTS PASSED\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 06_SMOKE_TESTS — minimal e2e checks\n",
        "# ======================\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = globals().get('OUT_DIR', '/content')\n",
        "manifest_path = os.path.join(OUT_DIR, 'pipeline_manifest.json')\n",
        "assert os.path.exists(manifest_path), \"pipeline_manifest.json missing\"\n",
        "\n",
        "manifest = json.load(open(manifest_path))\n",
        "# basic artifact existence checks\n",
        "expected = ['df_aligned_final.pkl','df_aligned_final.csv','df_aligned_schema.json']\n",
        "for k in expected:\n",
        "    val = manifest.get(k) or manifest.get('df_aligned_final') or manifest.get('df_aligned_final_csv')\n",
        "# existence asserts (loose)\n",
        "assert Path(os.path.join(OUT_DIR, manifest.get('df_aligned_final'))).exists() or Path(os.path.join(OUT_DIR,'df_aligned_final.pkl')).exists()\n",
        "assert Path(os.path.join(OUT_DIR, manifest.get('df_aligned_final_csv'))).exists() or Path(os.path.join(OUT_DIR,'df_aligned_final.csv')).exists()\n",
        "\n",
        "# check manifest recorded snapshot\n",
        "if manifest.get('snapshots'):\n",
        "    snap = manifest['snapshots'][-1]\n",
        "    assert Path(os.path.join(OUT_DIR, snap['file'])).exists(), \"Snapshot file missing\"\n",
        "\n",
        "print(\"SMOKE TESTS PASSED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vO3VLEh3Knr",
        "outputId": "37ce34c3-b884-4d13-ac69-0ee5719255f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calibration (causal) σ25: 0.002697216138911921 σ75: 0.005336742798711203\n",
            "PT: 0.010673485597422407 SL: 0.002697216138911921\n",
            "Calibration source: rolling_shifted window_used: 2000\n",
            "Running TB horizon h=4 ...\n",
            "Running TB horizon h=8 ...\n",
            "Running TB horizon h=12 ...\n",
            "Saved pickle -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_step03_tb_multi_20251205T015857Z.pkl\n",
            "Saved csv    -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_step03_tb_multi_20251205T015857Z.csv\n",
            "Saved meta   -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_step03_tb_multi_20251205T015857Z.meta.json\n",
            "\n",
            "h=4 distribution:\n",
            "tb_label_h4\n",
            "-1    9906\n",
            " 1    7569\n",
            " 0      46\n",
            "Name: count, dtype: int64\n",
            "\n",
            "h=8 distribution:\n",
            "tb_label_h8\n",
            "-1    10976\n",
            " 1     6531\n",
            " 0       14\n",
            "Name: count, dtype: int64\n",
            "\n",
            "h=12 distribution:\n",
            "tb_label_h12\n",
            "-1    11523\n",
            " 1     5987\n",
            " 0       11\n",
            "Name: count, dtype: int64\n",
            "Saved diagnostics summary -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_step03_tb_multi_20251205T015857Z_diagnostics_summary.csv\n",
            "STEP03 complete. Artifacts and diagnostics saved to drive.\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================\n",
        "# STEP 03 — FINAL MULTI-HORIZON (h=4,8,12) with robust survival mapping fix\n",
        "# ===========================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------- prerequisites ----------\n",
        "assert 'df' in globals(), \"df missing — run Step02 align-clean first.\"\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- config ----------\n",
        "H_LIST = [4, 8, 12]\n",
        "USE_LOG = True\n",
        "VOL_WINDOW = 20\n",
        "\n",
        "K_PT = float(os.getenv(\"K_PT\", os.getenv(\"KP_T\", \"2.0\")))\n",
        "K_SL = float(os.getenv(\"K_SL\", \"1.0\"))\n",
        "\n",
        "ROLL_CAL_WIN = int(os.getenv(\"ROLL_CAL_WIN\", 2000))\n",
        "MIN_CAL_PERIODS = int(os.getenv(\"MIN_CAL_PERIODS\", 200))\n",
        "VOL_EPS = float(os.getenv(\"VOL_EPS\", 1e-8))\n",
        "\n",
        "# ---------- causal volatility ----------\n",
        "def get_vol(price_series, window=VOL_WINDOW, use_log=USE_LOG):\n",
        "    s = pd.Series(price_series).astype(float)\n",
        "    r = np.log(s).diff().shift(1) if use_log else s.pct_change().shift(1)\n",
        "    vol = r.rolling(window, min_periods=3).std().fillna(method=\"ffill\").fillna(0.0)\n",
        "    return vol\n",
        "\n",
        "# operate on copy and ensure tz-aware index\n",
        "df = df.copy()\n",
        "df.index = pd.to_datetime(df.index)\n",
        "if df.index.tz is None:\n",
        "    df.index = df.index.tz_localize(\"UTC\")\n",
        "else:\n",
        "    df.index = df.index.tz_convert(\"UTC\")\n",
        "\n",
        "LAST_COMPLETE = globals().get(\"LAST_COMPLETE\", None)\n",
        "\n",
        "df[\"sigma\"] = get_vol(df[\"close\"])\n",
        "assert \"sigma\" in df.columns, \"sigma column not created\"\n",
        "\n",
        "# ---------- compute causal rolling percentiles ----------\n",
        "roll_q25 = df[\"sigma\"].rolling(ROLL_CAL_WIN, min_periods=MIN_CAL_PERIODS).quantile(0.25).shift(1)\n",
        "roll_q75 = df[\"sigma\"].rolling(ROLL_CAL_WIN, min_periods=MIN_CAL_PERIODS).quantile(0.75).shift(1)\n",
        "\n",
        "calibration_source = None\n",
        "calibration_window_used = None\n",
        "\n",
        "if roll_q25.dropna().empty or roll_q75.dropna().empty:\n",
        "    fallback_win = max(MIN_CAL_PERIODS, int(len(df) * 0.5))\n",
        "    fallback_q25 = df[\"sigma\"].rolling(fallback_win, min_periods=max(3, MIN_CAL_PERIODS//2)).quantile(0.25).shift(1)\n",
        "    fallback_q75 = df[\"sigma\"].rolling(fallback_win, min_periods=max(3, MIN_CAL_PERIODS//2)).quantile(0.75).shift(1)\n",
        "    if not fallback_q25.dropna().empty and not fallback_q75.dropna().empty:\n",
        "        sig25 = float(fallback_q25.dropna().iloc[-1])\n",
        "        sig75 = float(fallback_q75.dropna().iloc[-1])\n",
        "        calibration_source = \"fallback_roll\"\n",
        "        calibration_window_used = int(fallback_win)\n",
        "    else:\n",
        "        hist = df[\"sigma\"].dropna()\n",
        "        if len(hist) < max(10, MIN_CAL_PERIODS//2):\n",
        "            raise AssertionError(\"Not enough history to compute causal calibration percentiles. Reduce ROLL_CAL_WIN or increase data.\")\n",
        "        hist_for_cal = hist[:-1] if len(hist) > 1 else hist\n",
        "        sig25 = float(np.percentile(hist_for_cal, 25))\n",
        "        sig75 = float(np.percentile(hist_for_cal, 75))\n",
        "        calibration_source = \"empirical_hist\"\n",
        "        calibration_window_used = int(len(hist_for_cal))\n",
        "else:\n",
        "    sig25 = float(roll_q25.dropna().iloc[-1])\n",
        "    sig75 = float(roll_q75.dropna().iloc[-1])\n",
        "    calibration_source = \"rolling_shifted\"\n",
        "    calibration_window_used = int(ROLL_CAL_WIN)\n",
        "\n",
        "sig25 = max(sig25, VOL_EPS)\n",
        "sig75 = max(sig75, VOL_EPS)\n",
        "\n",
        "PT = float(K_PT) * sig75\n",
        "SL = float(K_SL) * sig25\n",
        "\n",
        "print(\"Calibration (causal) σ25:\", sig25, \"σ75:\", sig75)\n",
        "print(\"PT:\", PT, \"SL:\", SL)\n",
        "print(\"Calibration source:\", calibration_source, \"window_used:\", calibration_window_used)\n",
        "\n",
        "# ---------- triple-barrier ----------\n",
        "def triple_barrier(df_local, pt, sl, horizon, use_log=True):\n",
        "    close = df_local[\"close\"].astype(float).to_numpy()\n",
        "    idx = list(df_local.index)\n",
        "    n = len(close)\n",
        "\n",
        "    labels = np.zeros(n, dtype=int)\n",
        "    t_break = [pd.NaT] * n\n",
        "    ret_at_break = np.full(n, np.nan)\n",
        "\n",
        "    for i in range(n):\n",
        "        p0 = close[i]\n",
        "        if not np.isfinite(p0) or p0 <= 0:\n",
        "            continue\n",
        "\n",
        "        last_j = min(n - 1, i + horizon)\n",
        "        hit = False\n",
        "\n",
        "        for j in range(i + 1, last_j + 1):\n",
        "            px = close[j]\n",
        "            if not np.isfinite(px) or px <= 0:\n",
        "                continue\n",
        "\n",
        "            r = (math.log(px) - math.log(p0)) if use_log else (px / p0 - 1.0)\n",
        "\n",
        "            if r >= pt:\n",
        "                labels[i] = 1\n",
        "                t_break[i] = idx[j]\n",
        "                ret_at_break[i] = r\n",
        "                hit = True\n",
        "                break\n",
        "\n",
        "            if r <= -sl:\n",
        "                labels[i] = -1\n",
        "                t_break[i] = idx[j]\n",
        "                ret_at_break[i] = r\n",
        "                hit = True\n",
        "                break\n",
        "\n",
        "        if not hit:\n",
        "            px = close[last_j]\n",
        "            if np.isfinite(px) and px > 0:\n",
        "                r = (math.log(px) - math.log(p0)) if use_log else (px / p0 - 1.0)\n",
        "                if r > 0:\n",
        "                    labels[i] = 1\n",
        "                elif r < 0:\n",
        "                    labels[i] = -1\n",
        "                else:\n",
        "                    labels[i] = 0\n",
        "                t_break[i] = idx[last_j]\n",
        "                ret_at_break[i] = r\n",
        "\n",
        "    out = pd.DataFrame(index=df_local.index)\n",
        "    out[\"label\"] = labels\n",
        "    out[\"t_break\"] = pd.to_datetime(t_break)\n",
        "    out[\"ret_at_break\"] = ret_at_break\n",
        "    return out\n",
        "\n",
        "# ---------- run TB ----------\n",
        "df_out = df.copy()\n",
        "for H in H_LIST:\n",
        "    print(f\"Running TB horizon h={H} ...\")\n",
        "    tb = triple_barrier(df, PT, SL, horizon=H, use_log=USE_LOG)\n",
        "    df_out[f\"tb_label_h{H}\"] = tb[\"label\"].astype(int)\n",
        "    df_out[f\"tb_t_break_h{H}\"] = tb[\"t_break\"]\n",
        "    df_out[f\"tb_ret_at_break_h{H}\"] = tb[\"ret_at_break\"]\n",
        "\n",
        "# ---------- outputs ----------\n",
        "ts = time.strftime(\"%Y%m%dT%H%M%SZ\", time.gmtime())\n",
        "out_pkl = os.path.join(OUT_DIR, f\"df_step03_tb_multi_{ts}.pkl\")\n",
        "out_csv = os.path.join(OUT_DIR, f\"df_step03_tb_multi_{ts}.csv\")\n",
        "meta_path = os.path.join(OUT_DIR, f\"df_step03_tb_multi_{ts}.meta.json\")\n",
        "\n",
        "df_out.to_pickle(out_pkl)\n",
        "df_out.to_csv(out_csv)\n",
        "\n",
        "h = hashlib.sha256()\n",
        "with open(out_pkl, \"rb\") as f:\n",
        "    for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "        h.update(chunk)\n",
        "sha = h.hexdigest()\n",
        "\n",
        "meta = {\n",
        "    \"generated_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"rows\": int(len(df_out)),\n",
        "    \"horizons\": H_LIST,\n",
        "    \"vol_window\": VOL_WINDOW,\n",
        "    \"roll_cal_win\": ROLL_CAL_WIN,\n",
        "    \"min_cal_periods\": MIN_CAL_PERIODS,\n",
        "    \"calibration_source\": calibration_source,\n",
        "    \"calibration_window_used\": calibration_window_used,\n",
        "    \"sigma_25\": sig25,\n",
        "    \"sigma_75\": sig75,\n",
        "    \"pt\": PT,\n",
        "    \"sl\": SL,\n",
        "    \"use_log\": USE_LOG,\n",
        "    \"pkl\": out_pkl,\n",
        "    \"csv\": out_csv,\n",
        "    \"sha256\": sha,\n",
        "    \"last_complete\": str(LAST_COMPLETE) if LAST_COMPLETE is not None else None,\n",
        "    \"K_PT\": K_PT,\n",
        "    \"K_SL\": K_SL,\n",
        "}\n",
        "\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2, default=str)\n",
        "\n",
        "print(\"Saved pickle ->\", out_pkl)\n",
        "print(\"Saved csv    ->\", out_csv)\n",
        "print(\"Saved meta   ->\", meta_path)\n",
        "\n",
        "for H in H_LIST:\n",
        "    print(f\"\\nh={H} distribution:\")\n",
        "    print(df_out[f\"tb_label_h{H}\"].value_counts(dropna=False))\n",
        "\n",
        "# ---------- diagnostics summary CSV ----------\n",
        "def compute_survival_array(index, tbreak_series):\n",
        "    \"\"\"\n",
        "    Robust mapping: convert tbreak_series to naive UTC datetime64[ns] numpy array,\n",
        "    then use get_indexer; fallback to searchsorted for -1 entries.\n",
        "    \"\"\"\n",
        "    # parse tbreak_series to pandas datetime\n",
        "    tb_series = pd.to_datetime(tbreak_series)\n",
        "\n",
        "    # convert timezone-aware to naive UTC datetimes; otherwise ensure naive datetime64[ns]\n",
        "    try:\n",
        "        if pd.api.types.is_datetime64tz_dtype(tb_series.dtype):\n",
        "            tb_vals = tb_series.dt.tz_convert('UTC').dt.tz_localize(None).values\n",
        "        else:\n",
        "            tb_vals = tb_series.values.astype('datetime64[ns]')\n",
        "    except Exception:\n",
        "        # safest fallback: coerce to UTC-aware then drop tz\n",
        "        try:\n",
        "            tmp = tb_series.dt.tz_convert('UTC')\n",
        "            tb_vals = tmp.dt.tz_localize(None).values\n",
        "        except Exception:\n",
        "            tb_vals = pd.to_datetime(tb_series).astype('datetime64[ns]')\n",
        "\n",
        "    idx_values = index.values  # numpy datetime64[ns] (should be naive if index tz was UTC-localized earlier)\n",
        "    n = len(idx_values)\n",
        "    pos = index.get_indexer(tb_vals)\n",
        "\n",
        "    # fallback searchsorted for -1 positions\n",
        "    missing = np.where(pos == -1)[0]\n",
        "    if missing.size > 0:\n",
        "        for i in missing:\n",
        "            tb_val = tb_vals[i]\n",
        "            if pd.isna(tb_val):\n",
        "                continue\n",
        "            ins = np.searchsorted(idx_values, np.datetime64(tb_val))\n",
        "            cand = None\n",
        "            if ins < n and idx_values[ins] == np.datetime64(tb_val):\n",
        "                cand = ins\n",
        "            elif ins - 1 >= 0 and idx_values[ins - 1] == np.datetime64(tb_val):\n",
        "                cand = ins - 1\n",
        "            if cand is not None:\n",
        "                pos[i] = int(cand)\n",
        "\n",
        "    out = np.full(len(index), np.nan, dtype=float)\n",
        "    for i in range(len(index)):\n",
        "        j = int(pos[i])\n",
        "        if j == -1:\n",
        "            out[i] = np.nan\n",
        "        else:\n",
        "            out[i] = float(j - i)\n",
        "    return out\n",
        "\n",
        "diag_rows = []\n",
        "for H in H_LIST:\n",
        "    lbl_col = f\"tb_label_h{H}\"\n",
        "    tb_col = f\"tb_t_break_h{H}\"\n",
        "    ret_col = f\"tb_ret_at_break_h{H}\"\n",
        "    labels = df_out[lbl_col]\n",
        "    ret = df_out[ret_col]\n",
        "\n",
        "    n_long = int((labels == 1).sum())\n",
        "    n_short = int((labels == -1).sum())\n",
        "    n_zero = int((labels == 0).sum())\n",
        "\n",
        "    mean_long = float(ret[labels == 1].mean()) if n_long > 0 else np.nan\n",
        "    mean_short = float(ret[labels == -1].mean()) if n_short > 0 else np.nan\n",
        "\n",
        "    surv = compute_survival_array(df_out.index, df_out[tb_col])\n",
        "    median_surv = float(np.nanmedian(surv)) if not np.isnan(surv).all() else np.nan\n",
        "\n",
        "    diag_rows.append({\n",
        "        \"h\": H,\n",
        "        \"n_long\": n_long,\n",
        "        \"n_short\": n_short,\n",
        "        \"n_zero\": n_zero,\n",
        "        \"ratio_long\": (n_long / max(1, n_long + n_short)) if (n_long + n_short) > 0 else np.nan,\n",
        "        \"mean_ret_long\": mean_long,\n",
        "        \"mean_ret_short\": mean_short,\n",
        "        \"median_survival_bars\": median_surv,\n",
        "    })\n",
        "\n",
        "diag_df = pd.DataFrame(diag_rows)\n",
        "diag_path = os.path.join(OUT_DIR, f\"df_step03_tb_multi_{ts}_diagnostics_summary.csv\")\n",
        "diag_df.to_csv(diag_path, index=False)\n",
        "print(\"Saved diagnostics summary ->\", diag_path)\n",
        "\n",
        "# expose canonical variable name for downstream\n",
        "globals()['df_step03_tb_multi'] = df_out\n",
        "globals()['DF_STEP03_PKL'] = out_pkl\n",
        "globals()['DF_STEP03_CSV'] = out_csv\n",
        "globals()['DF_STEP03_META'] = meta_path\n",
        "globals()['DF_STEP03_DIAG_CSV'] = diag_path\n",
        "\n",
        "# ---------- defensive asserts ----------\n",
        "assert os.path.exists(out_pkl), \"Output pickle not saved\"\n",
        "assert all(f\"tb_label_h{H}\" in df_out.columns for H in H_LIST), \"Missing TB columns\"\n",
        "if LAST_COMPLETE is not None:\n",
        "    assert df.index.max() <= pd.Timestamp(LAST_COMPLETE).tz_convert(\"UTC\"), \"Partial last bar used\"\n",
        "assert np.isfinite(PT) and np.isfinite(SL) and PT > 0 and SL > 0, \"PT/SL invalid\"\n",
        "\n",
        "print(\"STEP03 complete. Artifacts and diagnostics saved to drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLEPc0F_36TP",
        "outputId": "602370b0-28dd-4747-9e08-aa2b655ea804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: /content/drive/MyDrive/quant_pipeline/mtb_out/df_step03_tb_multi_20251205T015857Z.pkl shape: (17521, 17)\n",
            "\n",
            "==================== HORIZON h=4 ====================\n",
            "\n",
            "Label distribution:\n",
            "tb_label_h4\n",
            "-1    9906\n",
            " 1    7569\n",
            " 0      46\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PT/SL hit ratio:\n",
            "PT hits (label +1): 7569\n",
            "SL hits (label -1): 9906\n",
            "Unlabeled         : 46\n",
            "\n",
            "Expected returns:\n",
            "E[ret | long ] = 0.006879\n",
            "E[ret | short] = -0.005244\n",
            "E[ret | all  ] = 0.000007\n",
            "\n",
            "Mean survival bars: 2.89\n",
            "Median survival   : 4.00\n",
            "Saved plot → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diag/tb_diag_h4.png\n",
            "\n",
            "==================== HORIZON h=8 ====================\n",
            "\n",
            "Label distribution:\n",
            "tb_label_h8\n",
            "-1    10976\n",
            " 1     6531\n",
            " 0       14\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PT/SL hit ratio:\n",
            "PT hits (label +1): 6531\n",
            "SL hits (label -1): 10976\n",
            "Unlabeled         : 14\n",
            "\n",
            "Expected returns:\n",
            "E[ret | long ] = 0.009619\n",
            "E[ret | short] = -0.005701\n",
            "E[ret | all  ] = 0.000014\n",
            "\n",
            "Mean survival bars: 4.25\n",
            "Median survival   : 4.00\n",
            "Saved plot → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diag/tb_diag_h8.png\n",
            "\n",
            "==================== HORIZON h=12 ====================\n",
            "\n",
            "Label distribution:\n",
            "tb_label_h12\n",
            "-1    11523\n",
            " 1     5987\n",
            " 0       11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PT/SL hit ratio:\n",
            "PT hits (label +1): 5987\n",
            "SL hits (label -1): 11523\n",
            "Unlabeled         : 11\n",
            "\n",
            "Expected returns:\n",
            "E[ret | long ] = 0.011301\n",
            "E[ret | short] = -0.005833\n",
            "E[ret | all  ] = 0.000025\n",
            "\n",
            "Mean survival bars: 5.01\n",
            "Median survival   : 4.00\n",
            "Saved plot → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diag/tb_diag_h12.png\n",
            "\n",
            "Saved enriched TB diagnostic DF → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diag/df_tb_diagnostics.pkl\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# TB_DIAGNOSTICS — multi-horizon validator (patched)\n",
        "# Safe for Step03 (no leakage, read-only)\n",
        "# - Robust t_break -> index mapping (avoids brittle exact-ts equality issues)\n",
        "# - Prefer canonical artifact path from manifest/globals when available\n",
        "# - Hardened TZ handling for t_break (fix TypeError)\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------------\n",
        "# CONFIG / PATH RESOLUTION\n",
        "# --------------------------\n",
        "# Prefer canonical artifact if exposed by previous step; fallback to manifest or hardcoded path\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "IN_PATH = globals().get('DF_STEP03_PKL', None)\n",
        "if not IN_PATH:\n",
        "    try:\n",
        "        manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "        if os.path.exists(manifest_path):\n",
        "            mtmp = json.load(open(manifest_path))\n",
        "            IN_PATH = mtmp.get('df_step03_pkl') or mtmp.get('df_step03') or mtmp.get('df_step03_tb_multi')\n",
        "    except Exception:\n",
        "        IN_PATH = None\n",
        "IN_PATH = IN_PATH or \"/content/df_step03_tb_multi.pkl\"\n",
        "\n",
        "OUT_DIR_DIAG = os.path.join(OUT_DIR, \"tb_diag\")\n",
        "Path(OUT_DIR_DIAG).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --------------------------\n",
        "# LOAD ARTIFACT\n",
        "# --------------------------\n",
        "if not os.path.exists(IN_PATH):\n",
        "    raise FileNotFoundError(f\"TB diagnostics input not found: {IN_PATH}\")\n",
        "df = pd.read_pickle(IN_PATH)\n",
        "assert df.index.is_monotonic_increasing, \"df index must be monotonic increasing\"\n",
        "\n",
        "print(\"Loaded:\", IN_PATH, \"shape:\", df.shape)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Helper: compute survival time (bars to break) — robust mapping\n",
        "# --------------------------------------------------------\n",
        "def compute_survival(df_local, tbreak_col):\n",
        "    \"\"\"\n",
        "    Returns array of bars-to-break (float) aligned with df_local.index.\n",
        "    Robust to tz-aware and tz-naive t_break values and uses searchsorted fallback.\n",
        "    \"\"\"\n",
        "    # parse t_break to pandas datetime Series (preserves tz-awareness if present)\n",
        "    tb_series = pd.to_datetime(df_local[tbreak_col])\n",
        "\n",
        "    # Convert to numpy datetime64[ns] array compatible with df.index.values:\n",
        "    # - if tz-aware: convert to UTC then drop tz (naive)\n",
        "    # - if tz-naive: use values directly (datetime64[ns])\n",
        "    try:\n",
        "        if pd.api.types.is_datetime64tz_dtype(tb_series.dtype):\n",
        "            tb_vals = tb_series.dt.tz_convert('UTC').dt.tz_localize(None).values\n",
        "        else:\n",
        "            # safe path: .values is typically datetime64[ns]; handle object dtype fallback\n",
        "            tb_vals = tb_series.values\n",
        "            if tb_vals.dtype == object:\n",
        "                tb_vals = pd.to_datetime(tb_series).values\n",
        "    except Exception:\n",
        "        # Last-resort elementwise coercion (slower but safe)\n",
        "        coerced = []\n",
        "        for v in tb_series:\n",
        "            if pd.isna(v):\n",
        "                coerced.append(np.datetime64(\"NaT\"))\n",
        "                continue\n",
        "            try:\n",
        "                tv = pd.to_datetime(v)\n",
        "                if hasattr(tv, \"tz_convert\"):\n",
        "                    tv = tv.tz_convert(\"UTC\").tz_localize(None)\n",
        "                coerced.append(np.datetime64(tv))\n",
        "            except Exception:\n",
        "                coerced.append(np.datetime64(\"NaT\"))\n",
        "        tb_vals = np.array(coerced, dtype=\"datetime64[ns]\")\n",
        "\n",
        "    idx_values = df_local.index.values  # expected numpy datetime64[ns]\n",
        "    n = len(idx_values)\n",
        "\n",
        "    # fast mapping via get_indexer\n",
        "    pos = df_local.index.get_indexer(tb_vals)\n",
        "\n",
        "    # fallback mapping for -1 positions using searchsorted + exact equality check\n",
        "    missing = np.where(pos == -1)[0]\n",
        "    if missing.size > 0:\n",
        "        for i in missing:\n",
        "            tb_val = tb_vals[i]\n",
        "            if pd.isna(tb_val):\n",
        "                continue\n",
        "            ins = np.searchsorted(idx_values, np.datetime64(tb_val))\n",
        "            candidate = None\n",
        "            if ins < n and idx_values[ins] == np.datetime64(tb_val):\n",
        "                candidate = ins\n",
        "            elif ins - 1 >= 0 and idx_values[ins - 1] == np.datetime64(tb_val):\n",
        "                candidate = ins - 1\n",
        "            if candidate is not None:\n",
        "                pos[i] = int(candidate)\n",
        "            else:\n",
        "                pos[i] = -1\n",
        "\n",
        "    out = np.full(n, np.nan, dtype=float)\n",
        "    for i in range(n):\n",
        "        j = int(pos[i])\n",
        "        if j == -1:\n",
        "            out[i] = np.nan\n",
        "        else:\n",
        "            out[i] = float(j - i)\n",
        "    return out\n",
        "\n",
        "# quick unit-check for correct output length (small, non-fatal assertion)\n",
        "_sample_tb_col = next((c for c in df.columns if c.startswith(\"tb_t_break_h\")), None)\n",
        "if _sample_tb_col is not None:\n",
        "    assert len(compute_survival(df, _sample_tb_col)) == len(df), \"compute_survival length mismatch\"\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# LOOP PER HORIZON (diagnostics + plots)\n",
        "# --------------------------------------------------------\n",
        "HORIZONS = globals().get(\"H_LIST\", [4, 8, 12])\n",
        "\n",
        "for h in HORIZONS:\n",
        "    print(f\"\\n==================== HORIZON h={h} ====================\")\n",
        "\n",
        "    lbl_col = f\"tb_label_h{h}\"\n",
        "    tb_col  = f\"tb_t_break_h{h}\"\n",
        "    ret_col = f\"tb_ret_at_break_h{h}\"\n",
        "\n",
        "    assert lbl_col in df and tb_col in df and ret_col in df, f\"Missing TB columns for h={h}\"\n",
        "\n",
        "    # label distribution\n",
        "    s = df[lbl_col].value_counts(dropna=False)\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    print(s)\n",
        "\n",
        "    # PT/SL hit counts\n",
        "    pt_hit = int((df[lbl_col] == 1).sum())\n",
        "    sl_hit = int((df[lbl_col] == -1).sum())\n",
        "    unlabeled = int((df[lbl_col] == 0).sum())\n",
        "\n",
        "    print(\"\\nPT/SL hit ratio:\")\n",
        "    print(f\"PT hits (label +1): {pt_hit}\")\n",
        "    print(f\"SL hits (label -1): {sl_hit}\")\n",
        "    print(f\"Unlabeled         : {unlabeled}\")\n",
        "\n",
        "    # expected returns\n",
        "    ret = df[ret_col]\n",
        "    exp_long  = ret[df[lbl_col] == 1].mean()\n",
        "    exp_short = ret[df[lbl_col] == -1].mean()\n",
        "    exp_all   = ret.dropna().mean()\n",
        "\n",
        "    print(\"\\nExpected returns:\")\n",
        "    print(f\"E[ret | long ] = {exp_long:.6f}\")\n",
        "    print(f\"E[ret | short] = {exp_short:.6f}\")\n",
        "    print(f\"E[ret | all  ] = {exp_all:.6f}\")\n",
        "\n",
        "    # survival-time distribution\n",
        "    surv = compute_survival(df, tb_col)\n",
        "    df[f\"tb_survival_h{h}\"] = surv\n",
        "    print(f\"\\nMean survival bars: {np.nanmean(surv):.2f}\")\n",
        "    print(f\"Median survival   : {np.nanmedian(surv):.2f}\")\n",
        "\n",
        "    # plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12,10))\n",
        "    fig.suptitle(f\"TB Diagnostics — Horizon h={h}\", fontsize=14)\n",
        "\n",
        "    # histogram return-at-break\n",
        "    axes[0,0].hist(ret.dropna(), bins=60)\n",
        "    axes[0,0].set_title(\"Histogram ret_at_break\")\n",
        "    axes[0,0].set_xlabel(\"Return\")\n",
        "    axes[0,0].set_ylabel(\"Count\")\n",
        "\n",
        "    # PT vs SL bar chart\n",
        "    axes[0,1].bar([\"PT(+1)\",\"SL(-1)\",\"0\"], [pt_hit, sl_hit, unlabeled])\n",
        "    axes[0,1].set_title(\"PT/SL Hit Counts\")\n",
        "\n",
        "    # survival histogram\n",
        "    valid_surv = surv[~np.isnan(surv)]\n",
        "    bins = int(h) if (valid_surv.size > 0 and np.nanmax(valid_surv) > 0) else 1\n",
        "    axes[1,0].hist(valid_surv, bins=bins)\n",
        "    axes[1,0].set_title(\"Survival Time (bars)\")\n",
        "    axes[1,0].set_xlabel(\"Bars to hit barrier\")\n",
        "\n",
        "    # expected return bar\n",
        "    axes[1,1].bar([\"long\",\"short\",\"all\"], [exp_long if not pd.isna(exp_long) else 0.0,\n",
        "                                          exp_short if not pd.isna(exp_short) else 0.0,\n",
        "                                          exp_all if not pd.isna(exp_all) else 0.0])\n",
        "    axes[1,1].set_title(\"Expected Returns\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    fig_path = os.path.join(OUT_DIR_DIAG, f\"tb_diag_h{h}.png\")\n",
        "    fig.savefig(fig_path)\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved plot → {fig_path}\")\n",
        "\n",
        "# Save enriched df (diagnostics artifact separate from canonical TB artifact)\n",
        "OUT_PKL = os.path.join(OUT_DIR_DIAG, \"df_tb_diagnostics.pkl\")\n",
        "df.to_pickle(OUT_PKL)\n",
        "print(\"\\nSaved enriched TB diagnostic DF →\", OUT_PKL)\n",
        "\n",
        "# End of TB_DIAGNOSTICS cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5Qr3yw85Ifs",
        "outputId": "ed8c2101-fb2b-4d71-800e-64d769f8f34a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded TB artifact: /content/drive/MyDrive/quant_pipeline/mtb_out/df_step03_tb_multi_20251205T015857Z.pkl shape: (17521, 17)\n",
            "\n",
            "=== HORIZON MISMATCH ANALYSIS ===\n",
            "    h  n_long  n_short  n_zero  ratio_long  ratio_short  mean_ret_long  \\\n",
            "0   4    7569     9906      46    0.433133     0.566867       0.006879   \n",
            "1   8    6531    10976      14    0.373051     0.626949       0.009619   \n",
            "2  12    5987    11523      11    0.341919     0.658081       0.011301   \n",
            "\n",
            "   mean_ret_short  mean_ret_all  \n",
            "0       -0.005244      0.000007  \n",
            "1       -0.005701      0.000014  \n",
            "2       -0.005833      0.000025  \n",
            "Saved horizon mismatch summary -> /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2/horizon_mismatch_summary_20251205T015902Z.csv\n",
            "Saved directional drift plot → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2/directional_drift_h4_20251205T015902Z.png\n",
            "Saved directional drift plot → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2/directional_drift_h8_20251205T015902Z.png\n",
            "Saved directional drift plot → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2/directional_drift_h12_20251205T015902Z.png\n",
            "\n",
            "Directional drift plots saved.\n",
            "Saved volatility-regime summary for h=4 -> /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2/regime_check_h4_20251205T015902Z.csv\n",
            "Saved volatility-regime summary for h=8 -> /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2/regime_check_h8_20251205T015902Z.csv\n",
            "Saved volatility-regime summary for h=12 -> /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2/regime_check_h12_20251205T015902Z.csv\n",
            "\n",
            "All diagnostics saved → /content/drive/MyDrive/quant_pipeline/mtb_out/tb_diagnostics_v2\n",
            "Updated manifest -> /content/drive/MyDrive/quant_pipeline/mtb_out/pipeline_manifest.json\n",
            "STEP03 TB Diagnostics V2 complete.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STEP03 — TB Diagnostics V2 (Directional Drift, PT/SL Mismatch Check)\n",
        "# Drive-safe: reads canonical artifact from pipeline (drive) and writes diagnostics back to drive.\n",
        "# Best-practice: manifest-first path resolution, do NOT clobber globals['df'], tz-safe, manifest registration.\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------------\n",
        "# CONFIG / PATH RESOLUTION\n",
        "# --------------------------\n",
        "OUT_DIR = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# robust IN_PATH resolution: globals -> manifest -> best-effort fallback\n",
        "IN_PATH = globals().get('DF_STEP03_PKL', None)\n",
        "if not IN_PATH:\n",
        "    manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "    if os.path.exists(manifest_path):\n",
        "        try:\n",
        "            m = json.load(open(manifest_path))\n",
        "            # various keys that could contain canonical TB artifact\n",
        "            IN_PATH = m.get('df_step03_pkl') or m.get('df_step03') or m.get('df_step03_tb_multi') or IN_PATH\n",
        "        except Exception:\n",
        "            IN_PATH = None\n",
        "\n",
        "# best-effort fallback: use explicit ts from globals if present, otherwise use generic filename\n",
        "_ts = globals().get('ts', None)\n",
        "if IN_PATH is None:\n",
        "    if _ts:\n",
        "        candidate = os.path.join(OUT_DIR, f\"df_step03_tb_multi_{_ts}.pkl\")\n",
        "        IN_PATH = candidate if os.path.exists(candidate) else None\n",
        "    # final fallback\n",
        "    IN_PATH = IN_PATH or os.path.join(OUT_DIR, \"df_step03_tb_multi.pkl\")\n",
        "\n",
        "# fail fast if artifact missing\n",
        "if not os.path.exists(IN_PATH):\n",
        "    raise FileNotFoundError(f\"TB diagnostics input not found. Tried: {IN_PATH}\")\n",
        "\n",
        "# outputs dir\n",
        "OUT_DIR_V2 = os.path.join(OUT_DIR, \"tb_diagnostics_v2\")\n",
        "Path(OUT_DIR_V2).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --------------------------\n",
        "# LOAD artifact into local var (do NOT overwrite globals df)\n",
        "# --------------------------\n",
        "df_tb = pd.read_pickle(IN_PATH)\n",
        "# work on a copy\n",
        "df_v2 = df_tb.copy()\n",
        "\n",
        "# ensure monotonic index and tz-awareness\n",
        "assert df_v2.index.is_monotonic_increasing, \"TB artifact index must be monotonic increasing\"\n",
        "if df_v2.index.tz is None:\n",
        "    # canonicalize to UTC\n",
        "    df_v2.index = pd.to_datetime(df_v2.index).tz_localize(\"UTC\")\n",
        "else:\n",
        "    df_v2.index = pd.to_datetime(df_v2.index).tz_convert(\"UTC\")\n",
        "\n",
        "print(\"Loaded TB artifact:\", IN_PATH, \"shape:\", df_v2.shape)\n",
        "\n",
        "# --------------------------\n",
        "# Horizons / config\n",
        "# --------------------------\n",
        "horizons = globals().get('H_LIST', [4, 8, 12])\n",
        "WINDOW = int(os.getenv(\"DRIFT_WINDOW\", 500))\n",
        "\n",
        "# quick column-check (fail early if missing)\n",
        "required_cols = []\n",
        "for h in horizons:\n",
        "    required_cols += [f\"tb_label_h{h}\", f\"tb_ret_at_break_h{h}\", f\"tb_t_break_h{h}\"]\n",
        "missing = [c for c in required_cols if c not in df_v2.columns]\n",
        "assert len(missing) == 0, f\"Missing TB columns: {missing}\"\n",
        "\n",
        "# --------------------------\n",
        "# 1) Horizon mismatch check\n",
        "# --------------------------\n",
        "results = []\n",
        "for h in horizons:\n",
        "    lab = df_v2[f\"tb_label_h{h}\"]\n",
        "    tmp_ret = df_v2[f\"tb_ret_at_break_h{h}\"].copy()\n",
        "\n",
        "    n_long = int((lab == 1).sum())\n",
        "    n_short = int((lab == -1).sum())\n",
        "    n_zero = int((lab == 0).sum())\n",
        "\n",
        "    mean_long = tmp_ret[lab == 1].mean() if n_long > 0 else np.nan\n",
        "    mean_short = tmp_ret[lab == -1].mean() if n_short > 0 else np.nan\n",
        "    mean_all = tmp_ret.dropna().mean() if tmp_ret.dropna().size > 0 else np.nan\n",
        "\n",
        "    denom = (n_long + n_short)\n",
        "    ratio_long = (n_long / denom) if denom > 0 else np.nan\n",
        "    ratio_short = (n_short / denom) if denom > 0 else np.nan\n",
        "\n",
        "    results.append({\n",
        "        \"h\": h,\n",
        "        \"n_long\": n_long,\n",
        "        \"n_short\": n_short,\n",
        "        \"n_zero\": n_zero,\n",
        "        \"ratio_long\": ratio_long,\n",
        "        \"ratio_short\": ratio_short,\n",
        "        \"mean_ret_long\": float(mean_long) if not pd.isna(mean_long) else np.nan,\n",
        "        \"mean_ret_short\": float(mean_short) if not pd.isna(mean_short) else np.nan,\n",
        "        \"mean_ret_all\": float(mean_all) if not pd.isna(mean_all) else np.nan,\n",
        "    })\n",
        "\n",
        "res_df = pd.DataFrame(results)\n",
        "print(\"\\n=== HORIZON MISMATCH ANALYSIS ===\")\n",
        "print(res_df)\n",
        "\n",
        "# save (timestamped to be idempotent)\n",
        "ts_local = time.strftime(\"%Y%m%dT%H%M%SZ\", time.gmtime())\n",
        "horizon_csv = os.path.join(OUT_DIR_V2, f\"horizon_mismatch_summary_{ts_local}.csv\")\n",
        "res_df.to_csv(horizon_csv, index=False)\n",
        "print(\"Saved horizon mismatch summary ->\", horizon_csv)\n",
        "\n",
        "# --------------------------\n",
        "# 2) Directional drift — rolling PT/SL ratio (plots)\n",
        "# --------------------------\n",
        "for h in horizons:\n",
        "    lab = df_v2[f\"tb_label_h{h}\"]\n",
        "\n",
        "    roll_pt = (lab == 1).astype(int).rolling(WINDOW).mean()\n",
        "    roll_sl = (lab == -1).astype(int).rolling(WINDOW).mean()\n",
        "\n",
        "    plt.figure(figsize=(16,5))\n",
        "    plt.plot(roll_pt, label=\"PT hit ratio (long hits)\")\n",
        "    plt.plot(roll_sl, label=\"SL hit ratio (short hits)\")\n",
        "    plt.title(f\"Directional Drift – Rolling PT/SL hit ratio (h={h})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    out_png = os.path.join(OUT_DIR_V2, f\"directional_drift_h{h}_{ts_local}.png\")\n",
        "    plt.savefig(out_png)\n",
        "    plt.close()\n",
        "    print(f\"Saved directional drift plot → {out_png}\")\n",
        "\n",
        "print(\"\\nDirectional drift plots saved.\")\n",
        "\n",
        "# --------------------------\n",
        "# 3) Volatility-regime PT/SL efficiency check\n",
        "# --------------------------\n",
        "def get_sigma(close, window=20):\n",
        "    s = pd.Series(close).astype(float)\n",
        "    # causal sigma: log-diff, rolling std, shifted by 1 to avoid peeking current bar\n",
        "    sigma = np.log(s).diff().rolling(window, min_periods=3).std().shift(1)\n",
        "    return sigma\n",
        "\n",
        "# compute sigma into local df copy (do not overwrite global df)\n",
        "df_v2[\"sigma\"] = get_sigma(df_v2[\"close\"])\n",
        "\n",
        "for h in horizons:\n",
        "    lab = df_v2[f\"tb_label_h{h}\"]\n",
        "    tmp_ret = df_v2[f\"tb_ret_at_break_h{h}\"].copy()\n",
        "\n",
        "    # low vs high vol bins (use quantiles computed on available sigma)\n",
        "    vol_q25 = df_v2[\"sigma\"].quantile(0.25)\n",
        "    vol_q75 = df_v2[\"sigma\"].quantile(0.75)\n",
        "\n",
        "    low = df_v2[df_v2[\"sigma\"] <= vol_q25]\n",
        "    high = df_v2[df_v2[\"sigma\"] >= vol_q75]\n",
        "\n",
        "    def summarize(name, subset):\n",
        "        n_long = int((subset[f\"tb_label_h{h}\"] == 1).sum())\n",
        "        n_short = int((subset[f\"tb_label_h{h}\"] == -1).sum())\n",
        "        mean_long = subset[subset[f\"tb_label_h{h}\"] == 1][f\"tb_ret_at_break_h{h}\"].mean() if n_long > 0 else np.nan\n",
        "        mean_short = subset[subset[f\"tb_label_h{h}\"] == -1][f\"tb_ret_at_break_h{h}\"].mean() if n_short > 0 else np.nan\n",
        "        return {\n",
        "            \"group\": name,\n",
        "            \"mean_long_ret\": float(mean_long) if not pd.isna(mean_long) else np.nan,\n",
        "            \"mean_short_ret\": float(mean_short) if not pd.isna(mean_short) else np.nan,\n",
        "            \"n_long\": n_long,\n",
        "            \"n_short\": n_short,\n",
        "        }\n",
        "\n",
        "    out = pd.DataFrame([\n",
        "        summarize(\"low_vol\", low),\n",
        "        summarize(\"high_vol\", high)\n",
        "    ])\n",
        "\n",
        "    out_path = os.path.join(OUT_DIR_V2, f\"regime_check_h{h}_{ts_local}.csv\")\n",
        "    out.to_csv(out_path, index=False)\n",
        "    print(f\"Saved volatility-regime summary for h={h} -> {out_path}\")\n",
        "\n",
        "print(\"\\nAll diagnostics saved →\", OUT_DIR_V2)\n",
        "\n",
        "# --------------------------\n",
        "# Register diagnostics artifacts in pipeline_manifest.json (idempotent update)\n",
        "# --------------------------\n",
        "manifest_path = os.path.join(OUT_DIR, \"pipeline_manifest.json\")\n",
        "manifest = {}\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        manifest = json.load(open(manifest_path))\n",
        "    except Exception:\n",
        "        manifest = {}\n",
        "\n",
        "manifest.setdefault(\"diagnostics\", {})\n",
        "manifest[\"diagnostics\"].setdefault(\"tb_diagnostics_v2\", {})\n",
        "manifest[\"diagnostics\"][\"tb_diagnostics_v2\"].update({\n",
        "    \"horizon_summary\": os.path.basename(horizon_csv),\n",
        "    \"directional_plots_prefix\": f\"directional_drift_h{{h}}_{ts_local}.png\",\n",
        "    \"regime_check_prefix\": f\"regime_check_h{{h}}_{ts_local}.csv\",\n",
        "    \"generated_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"source_tb_artifact\": os.path.basename(IN_PATH),\n",
        "})\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(\"Updated manifest ->\", manifest_path)\n",
        "\n",
        "# --------------------------\n",
        "# Small unit-checks / asserts\n",
        "# --------------------------\n",
        "# assert we wrote at least one output\n",
        "assert os.path.exists(horizon_csv), \"horizon summary not saved\"\n",
        "# sanity: sigma exists and has same length (can include NaNs)\n",
        "assert \"sigma\" in df_v2.columns and len(df_v2[\"sigma\"]) == len(df_v2), \"sigma column invalid\"\n",
        "\n",
        "# Expose diagnostic artifact path(s) locally (do NOT overwrite canonical globals)\n",
        "globals()['DF_STEP03_TB_DIAG_V2_DIR'] = OUT_DIR_V2\n",
        "globals()['DF_STEP03_TB_DIAG_V2_SUMMARY'] = horizon_csv\n",
        "\n",
        "print(\"STEP03 TB Diagnostics V2 complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOxgLZNT6uQ0",
        "outputId": "d5c6b79c-51c8-44a0-bcb6-4205ddb4bcb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Step 03: FEATURES BASE (corrected + diag-exclusion + manifest) ===\n",
            "[loaded] /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned_final.pkl rows = 17,521\n",
            "\n",
            "Integrity checks:\n",
            "NA count (top 20):\n",
            "ret_48h                  49\n",
            "pvo_hist                 34\n",
            "macd_signal              34\n",
            "macd_hist                34\n",
            "awesome_osc              34\n",
            "mfi_14                   29\n",
            "macd                     26\n",
            "pvo                      26\n",
            "ppo                      26\n",
            "ret_24h                  25\n",
            "squeeze_score            20\n",
            "vol_compression_ratio    20\n",
            "bb_width                 20\n",
            "bb_mid                   20\n",
            "bb_low                   20\n",
            "bb_high                  20\n",
            "rsi_14                   14\n",
            "ret_12h                  13\n",
            "kama_10                  10\n",
            "rsi_7                     7\n",
            "dtype: int64\n",
            "Index OK (no duplicates): True\n",
            "Time range: 2023-12-06 00:00:00+00:00 -> 2025-12-05 00:00:00+00:00\n",
            "\n",
            "[03] Saved df_features_base.pkl -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_base.pkl\n",
            "[03] Saved df_features_base.csv -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_base.csv\n",
            "[03] Saved df_features_for_training.pkl -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_for_training.pkl\n",
            "[03] Saved df_features_for_training.csv -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_for_training.csv\n",
            "[03] Saved meta -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_base.meta.json\n",
            "[03] Manifest updated -> /content/drive/MyDrive/quant_pipeline/mtb_out/pipeline_manifest.json\n",
            "[03] Completed at 2025-12-05 01:59:23.586993 UTC\n",
            "OK : Found aligned artifact -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_aligned_final.pkl\n",
            "OK : Found features artifact -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_base.pkl\n",
            "OK : Found Step03 TB artifacts (count=1)\n",
            "OK : Loaded aligned DF\n",
            "OK : Loaded features DF\n",
            "OK : Aligned DF index monotonic\n",
            "OK : Aligned DF tz-aware (UTC)\n",
            "OK : Aligned DF index has no duplicates\n",
            "OK : No partial last bar relative to last_complete from metadata\n",
            "OK : Derived features count = 66\n",
            "OK : Causality/shift check passed: first row derived features all-NaN\n",
            "OK : diag_* columns present but excluded from derived feature columns\n",
            "OK : volume_imputed_flag consistency OK (imputed rows have NaN volume)\n",
            "OK : Basic numeric sanity checks passed\n",
            "\n",
            "--- CI SUMMARY ---\n",
            "ALL CHECKS PASSED\n"
          ]
        }
      ],
      "source": [
        "# ===== STEP 03: FEATURES_BASE (corrected + diag-exclusion + manifest) =====\n",
        "import os, gc, json, hashlib\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== Step 03: FEATURES BASE (corrected + diag-exclusion + manifest) ===\")\n",
        "\n",
        "# BASE_PATH: prefer OUT_DIR from globals if present\n",
        "BASE_PATH = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# prefer final aligned, fallback to cleaned aligned (robust manifest-first lookup)\n",
        "aligned_final = os.path.join(BASE_PATH, \"df_aligned_final.pkl\")\n",
        "aligned_clean = os.path.join(BASE_PATH, \"df_aligned_clean.pkl\")\n",
        "aligned_path = None\n",
        "\n",
        "# try manifest first\n",
        "manifest_path = os.path.join(BASE_PATH, \"pipeline_manifest.json\")\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        mtmp = json.load(open(manifest_path))\n",
        "        # manifest keys we might have used earlier\n",
        "        cand = mtmp.get(\"df_aligned_final\") or mtmp.get(\"df_aligned_clean\")\n",
        "        if cand:\n",
        "            cand_full = os.path.join(BASE_PATH, cand)\n",
        "            if os.path.exists(cand_full):\n",
        "                aligned_path = cand_full\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# fallback filesystem checks\n",
        "if aligned_path is None:\n",
        "    if os.path.exists(aligned_final):\n",
        "        aligned_path = aligned_final\n",
        "    elif os.path.exists(aligned_clean):\n",
        "        aligned_path = aligned_clean\n",
        "\n",
        "if not aligned_path or not os.path.exists(aligned_path):\n",
        "    raise RuntimeError(f\"Aligned file not found. Expected one of: {aligned_final}, {aligned_clean}\")\n",
        "\n",
        "# load aligned data\n",
        "df = pd.read_pickle(aligned_path)\n",
        "print(f\"[loaded] {aligned_path} rows = {len(df):,}\")\n",
        "\n",
        "# required cols check\n",
        "REQ = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "for c in REQ:\n",
        "    if c not in df.columns:\n",
        "        raise KeyError(f\"Required column missing: {c}\")\n",
        "\n",
        "# ensure is_imputed present and boolean\n",
        "if \"is_imputed\" not in df.columns:\n",
        "    df[\"is_imputed\"] = False\n",
        "else:\n",
        "    df[\"is_imputed\"] = df[\"is_imputed\"].fillna(False).astype(bool)\n",
        "\n",
        "# ensure tz-aware index and normalized to UTC\n",
        "if df.index.tz is None:\n",
        "    df.index = pd.to_datetime(df.index).tz_localize(\"UTC\")\n",
        "else:\n",
        "    df.index = pd.to_datetime(df.index).tz_convert(\"UTC\")\n",
        "\n",
        "# Work on a working copy 'd' (do not clobber canonical df)\n",
        "d = df.copy()\n",
        "_eps = 1e-12\n",
        "\n",
        "# -------------------------\n",
        "# Basic price & return features\n",
        "# -------------------------\n",
        "d[\"hl2\"]   = (d[\"high\"] + d[\"low\"]) / 2.0\n",
        "d[\"hlc3\"]  = (d[\"high\"] + d[\"low\"] + d[\"close\"]) / 3.0\n",
        "d[\"ohlc4\"] = (d[\"open\"] + d[\"high\"] + d[\"low\"] + d[\"close\"]) / 4.0\n",
        "\n",
        "# causal 1-period returns (ret_1) must exist before rolling-moment baseline usage\n",
        "d[\"ret_1\"] = d[\"close\"].pct_change()\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    d[\"logret_1\"] = np.log(d[\"close\"]).diff()\n",
        "for w in (3, 6, 12, 24, 48):\n",
        "    d[f\"ret_{w}h\"] = d[\"close\"].pct_change(periods=w)\n",
        "\n",
        "# -------------------------\n",
        "# Price structure: body, wicks, direction\n",
        "# -------------------------\n",
        "d[\"candle_body\"] = (d[\"close\"] - d[\"open\"]).abs()\n",
        "d[\"candle_body_signed\"] = (d[\"close\"] - d[\"open\"])\n",
        "d[\"upper_wick\"] = (d[\"high\"] - d[[\"close\",\"open\"]].max(axis=1)).clip(lower=0.0)\n",
        "d[\"lower_wick\"] = (d[[\"close\",\"open\"]].min(axis=1) - d[\"low\"]).clip(lower=0.0)\n",
        "d[\"candle_dir\"] = np.sign(d[\"close\"] - d[\"open\"]).fillna(0).astype(int)\n",
        "d[\"body_pct\"] = d[\"candle_body\"] / (d[\"close\"].abs() + _eps)\n",
        "d[\"upper_wick_pct\"] = d[\"upper_wick\"] / (d[\"close\"].abs() + _eps)\n",
        "d[\"lower_wick_pct\"] = d[\"lower_wick\"] / (d[\"close\"].abs() + _eps)\n",
        "\n",
        "# -------------------------\n",
        "# EMA / MACD / RSI (prefer ta when available)\n",
        "# EMA tweak: fallback to ewm if ta produces excessive NaNs\n",
        "# -------------------------\n",
        "try:\n",
        "    import ta\n",
        "    for w in (10, 21, 50, 100, 200):\n",
        "        try:\n",
        "            # try ta; if produces too many NaNs fallback to ewm\n",
        "            d[f\"ema_{w}\"] = ta.trend.ema_indicator(d[\"close\"], window=w)\n",
        "            # if ta produced many NaNs, fallback to ewm mean\n",
        "            if d[f\"ema_{w}\"].isna().sum() > max(1, int(w/2)):\n",
        "                d[f\"ema_{w}\"] = d[\"close\"].ewm(span=w, adjust=False).mean()\n",
        "        except Exception:\n",
        "            d[f\"ema_{w}\"] = d[\"close\"].ewm(span=w, adjust=False).mean()\n",
        "\n",
        "    try:\n",
        "        d[\"macd\"] = ta.trend.macd(d[\"close\"])\n",
        "        d[\"macd_signal\"] = ta.trend.macd_signal(d[\"close\"])\n",
        "        d[\"macd_hist\"] = ta.trend.macd_diff(d[\"close\"])\n",
        "    except Exception:\n",
        "        d[\"macd\"] = d[\"macd_signal\"] = d[\"macd_hist\"] = np.nan\n",
        "\n",
        "    try:\n",
        "        d[\"rsi_14\"] = ta.momentum.rsi(d[\"close\"], window=14)\n",
        "        d[\"rsi_7\"] = ta.momentum.rsi(d[\"close\"], window=7)\n",
        "    except Exception:\n",
        "        d[\"rsi_14\"] = d[\"rsi_7\"] = np.nan\n",
        "\n",
        "except Exception:\n",
        "    # no ta available: simple ewm / nan defaults\n",
        "    for w in (10,21,50,100,200):\n",
        "        d[f\"ema_{w}\"] = d[\"close\"].ewm(span=w, adjust=False).mean()\n",
        "    d[\"macd\"] = d[\"macd_signal\"] = d[\"macd_hist\"] = np.nan\n",
        "    d[\"rsi_14\"] = d[\"rsi_7\"] = np.nan\n",
        "\n",
        "# -------------------------\n",
        "# Volatility (ATR, Bollinger, Keltner) — robust & corrected\n",
        "# -------------------------\n",
        "try:\n",
        "    import ta\n",
        "    try:\n",
        "        d[\"atr_14\"] = ta.volatility.average_true_range(d[\"high\"], d[\"low\"], d[\"close\"], window=14)\n",
        "        d[\"atr_rel\"] = d[\"atr_14\"] / (d[\"close\"].abs() + _eps)\n",
        "    except Exception:\n",
        "        d[\"atr_14\"] = np.nan; d[\"atr_rel\"] = np.nan\n",
        "\n",
        "    try:\n",
        "        bb_h = ta.volatility.bollinger_hband(d[\"close\"], window=20)\n",
        "        bb_l = ta.volatility.bollinger_lband(d[\"close\"], window=20)\n",
        "        bb_m = ta.volatility.bollinger_mavg(d[\"close\"], window=20)\n",
        "        d[\"bb_high\"] = bb_h; d[\"bb_low\"] = bb_l; d[\"bb_mid\"] = bb_m\n",
        "        d[\"bb_width\"] = (bb_h - bb_l) / (bb_m.replace(0, np.nan) + _eps)\n",
        "    except Exception:\n",
        "        d[\"bb_high\"] = d[\"bb_low\"] = d[\"bb_mid\"] = np.nan\n",
        "        d[\"bb_width\"] = d[\"close\"].rolling(20, min_periods=1).std()\n",
        "\n",
        "    try:\n",
        "        d[\"keltner_h\"] = ta.volatility.keltner_channel_hband(d[\"high\"], d[\"low\"], d[\"close\"], window=20, window_atr=10)\n",
        "        d[\"keltner_l\"] = ta.volatility.keltner_channel_lband(d[\"high\"], d[\"low\"], d[\"close\"], window=20, window_atr=10)\n",
        "        d[\"keltner_width\"] = (d[\"keltner_h\"] - d[\"keltner_l\"]) / (((d[\"keltner_h\"] + d[\"keltner_l\"]) / 2) + _eps)\n",
        "    except Exception:\n",
        "        d[\"keltner_h\"] = d[\"keltner_l\"] = d[\"keltner_width\"] = np.nan\n",
        "\n",
        "    # defensive: avoid divide-by-zero or infs\n",
        "    d[\"vol_compression_ratio\"] = d[\"bb_width\"] / (d[\"atr_rel\"].replace(0, np.nan) + _eps)\n",
        "    d[\"squeeze_score\"] = 1.0 / (d[\"vol_compression_ratio\"].replace(0, np.nan) + _eps)\n",
        "\n",
        "except Exception:\n",
        "    # fallback pure-pandas implementations\n",
        "    tr1 = (d[\"high\"] - d[\"low\"]).abs()\n",
        "    tr2 = (d[\"close\"] - d[\"close\"].shift(1)).abs()\n",
        "    tr3 = (d[\"open\"] - d[\"close\"].shift(1)).abs()\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    d[\"atr_14\"] = tr.rolling(14, min_periods=1).mean()\n",
        "    d[\"atr_rel\"] = d[\"atr_14\"] / (d[\"close\"].abs() + _eps)\n",
        "    d[\"bb_width\"] = d[\"close\"].rolling(20, min_periods=1).std()\n",
        "    d[\"vol_compression_ratio\"] = d[\"bb_width\"] / (d[\"atr_rel\"].replace(0, np.nan) + _eps)\n",
        "    d[\"squeeze_score\"] = 1.0 / (d[\"vol_compression_ratio\"].replace(0, np.nan) + _eps)\n",
        "\n",
        "# -------------------------\n",
        "# Volume-based features\n",
        "# -------------------------\n",
        "try:\n",
        "    import ta\n",
        "    d[\"obv\"] = ta.volume.on_balance_volume(d[\"close\"], d[\"volume\"])\n",
        "    d[\"mfi_14\"] = ta.volume.money_flow_index(d[\"high\"], d[\"low\"], d[\"close\"], d[\"volume\"], window=14)\n",
        "    d[\"pvo\"] = ta.momentum.pvo(d[\"volume\"], window_slow=26, window_fast=12)\n",
        "    d[\"pvo_hist\"] = ta.momentum.pvo_hist(d[\"volume\"], window_slow=26, window_fast=12)\n",
        "except Exception:\n",
        "    d[\"obv\"] = d[\"mfi_14\"] = d[\"pvo\"] = d[\"pvo_hist\"] = np.nan\n",
        "\n",
        "# -------------------------\n",
        "# VWAP per UTC day (causal cumulative)\n",
        "# -------------------------\n",
        "try:\n",
        "    # compute per-day cumulative VWAP in causal manner\n",
        "    vwap = pd.Series(index=d.index, dtype=float)\n",
        "    # group by normalized UTC day using index.floor('D') which works for tz-aware index\n",
        "    day_groups = d.index.floor(\"D\")\n",
        "    for day, idxs in d.groupby(day_groups, sort=True).groups.items():\n",
        "        grp = d.loc[idxs]\n",
        "        typ = ((grp[\"high\"] + grp[\"low\"] + grp[\"close\"]) / 3.0)\n",
        "        cum_dollar = (typ * grp[\"volume\"]).cumsum()\n",
        "        cum_vol = grp[\"volume\"].cumsum()\n",
        "        v = (cum_dollar / (cum_vol.replace(0, np.nan) + _eps)).values\n",
        "        vwap.loc[idxs] = v\n",
        "    d[\"vwap\"] = vwap\n",
        "except Exception:\n",
        "    d[\"vwap\"] = np.nan\n",
        "\n",
        "# -------------------------\n",
        "# helper: baseline for rolling stats uses working frame d and non-imputed rows\n",
        "# -------------------------\n",
        "def baseline_series_for(col):\n",
        "    if \"is_imputed\" in d.columns:\n",
        "        s_nonimp = d.loc[~d[\"is_imputed\"], col]\n",
        "        if len(s_nonimp.dropna()) > 0:\n",
        "            # reindex to full index to keep same length and NaNs where missing\n",
        "            return s_nonimp.reindex(d.index)\n",
        "    return d[col]\n",
        "\n",
        "# -------------------------\n",
        "# Rolling return moments (computed on baseline then reindexed->ffill)\n",
        "# -------------------------\n",
        "for w in (24, 72, 168):\n",
        "    base = baseline_series_for(\"ret_1\")\n",
        "    mean_b = base.rolling(w, min_periods=1).mean()\n",
        "    std_b  = base.rolling(w, min_periods=1).std()\n",
        "    skew_b = base.rolling(w, min_periods=1).skew()\n",
        "    kurt_b = base.rolling(w, min_periods=1).kurt()\n",
        "    d[f\"ret_mean_{w}\"] = mean_b.reindex(d.index).ffill()\n",
        "    d[f\"ret_std_{w}\"]  = std_b.reindex(d.index).ffill()\n",
        "    d[f\"ret_skew_{w}\"] = skew_b.reindex(d.index).ffill()\n",
        "    d[f\"ret_kurt_{w}\"] = kurt_b.reindex(d.index).ffill()\n",
        "\n",
        "# -------------------------\n",
        "# microstructure/time/extras\n",
        "# -------------------------\n",
        "d[\"tick_sign\"] = np.sign(d[\"close\"].diff().fillna(0))\n",
        "d[\"signed_volume\"] = d[\"tick_sign\"] * d[\"volume\"]\n",
        "d[\"vol_imbalance_6\"] = d[\"signed_volume\"].rolling(6, min_periods=1).sum()\n",
        "d[\"vol_imbalance_24\"] = d[\"signed_volume\"].rolling(24, min_periods=1).sum()\n",
        "\n",
        "# correct hour / weekday extraction (tz-aware index)\n",
        "d[\"hour\"] = d.index.hour\n",
        "d[\"weekday\"] = d.index.weekday\n",
        "\n",
        "try:\n",
        "    import ta\n",
        "    d[\"kama_10\"] = ta.momentum.kama(d[\"close\"], window=10)\n",
        "    d[\"awesome_osc\"] = ta.momentum.awesome_oscillator(d[\"high\"], d[\"low\"], window1=5, window2=34)\n",
        "    d[\"ppo\"] = ta.momentum.ppo(d[\"close\"], window_slow=26, window_fast=12)\n",
        "except Exception:\n",
        "    d[\"kama_10\"] = d[\"awesome_osc\"] = d[\"ppo\"] = np.nan\n",
        "\n",
        "# housekeeping\n",
        "max_lookback = max(200, 168, 72, 50)\n",
        "df_features = d\n",
        "\n",
        "# -------------------------\n",
        "# Make derived features strictly causal: SHIFT by 1 bar\n",
        "# Shift only derived features (do NOT shift raw_cols)\n",
        "# -------------------------\n",
        "raw_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"is_imputed\"]\n",
        "feat_cols = [c for c in df_features.columns if c not in raw_cols]\n",
        "# preserve original order for downstream\n",
        "df_features[feat_cols] = df_features[feat_cols].shift(1)\n",
        "df_features.attrs[\"shifted_feature_cols\"] = feat_cols\n",
        "\n",
        "# -------------------------\n",
        "# DIAGNOSTIC: exclude diag_* columns from training artifacts (must be recomputed per-fold)\n",
        "# -------------------------\n",
        "diag_cols = [c for c in df_features.columns if str(c).startswith(\"diag_\")]\n",
        "non_train_extras = [\"is_imputed\", \"volume_imputed_flag\"] if \"volume_imputed_flag\" in df_features.columns else [\"is_imputed\"]\n",
        "train_feat_cols = [c for c in df_features.columns if c not in diag_cols + non_train_extras]\n",
        "\n",
        "# build df_features_for_training (will be used downstream by MTB/CPCV as canonical features)\n",
        "df_features_for_training = df_features[train_feat_cols + non_train_extras].copy()\n",
        "\n",
        "# -------------------------\n",
        "# integrity checks & diagnostics\n",
        "# -------------------------\n",
        "print(\"\\nIntegrity checks:\")\n",
        "print(\"NA count (top 20):\")\n",
        "print(df_features.isna().sum().sort_values(ascending=False).head(20))\n",
        "print(\"Index OK (no duplicates):\", not df_features.index.has_duplicates)\n",
        "print(\"Time range:\", df_features.index.min(), \"->\", df_features.index.max())\n",
        "\n",
        "# Quick assertions\n",
        "# raw OHLC in df_features must equal original loaded df raw OHLC (ensure we didn't overwrite)\n",
        "assert df_features[[\"open\",\"close\"]].iloc[:].equals(df[[\"open\",\"close\"]].iloc[:]), \"raw OHLC were modified!\"\n",
        "# check first row of shifted features is NaN (causality)\n",
        "if len(df_features) > 0:\n",
        "    assert df_features[feat_cols].iloc[0].isna().all(), \"Shifted features should be NaN on first row (causality check)\"\n",
        "\n",
        "# -------------------------\n",
        "# save artifacts (base features + training-features excluded diag_*)\n",
        "# -------------------------\n",
        "out_pkl = os.path.join(BASE_PATH, \"df_features_base.pkl\")\n",
        "out_csv = os.path.join(BASE_PATH, \"df_features_base.csv\")\n",
        "df_features.to_pickle(out_pkl)\n",
        "df_features.to_csv(out_csv)\n",
        "\n",
        "# also save training-ready features (diag_* excluded)\n",
        "out_train_pkl = os.path.join(BASE_PATH, \"df_features_for_training.pkl\")\n",
        "out_train_csv = os.path.join(BASE_PATH, \"df_features_for_training.csv\")\n",
        "df_features_for_training.to_pickle(out_train_pkl)\n",
        "df_features_for_training.to_csv(out_train_csv)\n",
        "\n",
        "# write meta\n",
        "meta = {\n",
        "    \"generated_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"rows\": int(len(df_features)),\n",
        "    \"max_lookback\": int(max_lookback),\n",
        "    \"shifted_feature_count\": len(feat_cols),\n",
        "    \"diag_cols_excluded_count\": len(diag_cols),\n",
        "    \"diag_cols_excluded\": diag_cols[:50],\n",
        "}\n",
        "Path(os.path.join(BASE_PATH, \"df_features_base.meta.json\")).write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "# compute sha256 for provenance and update pipeline manifest\n",
        "def sha256_of_path(p):\n",
        "    h = hashlib.sha256()\n",
        "    with open(p, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "sha_base = sha256_of_path(out_pkl)\n",
        "sha_train = sha256_of_path(out_train_pkl)\n",
        "\n",
        "manifest_path = os.path.join(BASE_PATH, \"pipeline_manifest.json\")\n",
        "manifest = {}\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        manifest = json.load(open(manifest_path))\n",
        "    except Exception:\n",
        "        manifest = {}\n",
        "manifest.update({\n",
        "    \"df_features_base\": os.path.basename(out_pkl),\n",
        "    \"df_features_base_sha256\": sha_base,\n",
        "    \"df_features_for_training\": os.path.basename(out_train_pkl),\n",
        "    \"df_features_for_training_sha256\": sha_train,\n",
        "    \"version\": manifest.get(\"version\", 0) + 1,\n",
        "    \"generated_at\": str(pd.Timestamp.now(tz=\"UTC\"))\n",
        "})\n",
        "with open(manifest_path, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print(f\"\\n[03] Saved df_features_base.pkl -> {out_pkl}\")\n",
        "print(f\"[03] Saved df_features_base.csv -> {out_csv}\")\n",
        "print(f\"[03] Saved df_features_for_training.pkl -> {out_train_pkl}\")\n",
        "print(f\"[03] Saved df_features_for_training.csv -> {out_train_csv}\")\n",
        "print(f\"[03] Saved meta -> {os.path.join(BASE_PATH, 'df_features_base.meta.json')}\")\n",
        "print(f\"[03] Manifest updated -> {manifest_path}\")\n",
        "\n",
        "print(f\"[03] Completed at {datetime.utcnow()} UTC\")\n",
        "gc.collect()\n",
        "\n",
        "# ===== CI / readiness checks for Step03 (run in Colab) =====\n",
        "import os, sys, json, glob, traceback\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE_PATH = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "failures = []\n",
        "\n",
        "def fail(msg):\n",
        "    failures.append(msg)\n",
        "    print(\"FAIL:\", msg)\n",
        "\n",
        "def ok(msg):\n",
        "    print(\"OK :\", msg)\n",
        "\n",
        "# --- expected artifacts ---\n",
        "aligned_candidates = [\n",
        "    os.path.join(BASE_PATH, \"df_aligned_final.pkl\"),\n",
        "    os.path.join(BASE_PATH, \"df_aligned_clean.pkl\"),\n",
        "]\n",
        "features_path = os.path.join(BASE_PATH, \"df_features_base.pkl\")\n",
        "tb_glob = os.path.join(BASE_PATH, \"df_step03_tb_multi_*.pkl\")\n",
        "manifest_path = os.path.join(BASE_PATH, \"pipeline_manifest.json\")\n",
        "\n",
        "# check artifacts exist\n",
        "aligned_path = next((p for p in aligned_candidates if os.path.exists(p)), None)\n",
        "if aligned_path is None:\n",
        "    fail(f\"No aligned artifact found. Expected one of: {aligned_candidates}\")\n",
        "else:\n",
        "    ok(f\"Found aligned artifact -> {aligned_path}\")\n",
        "\n",
        "if not os.path.exists(features_path):\n",
        "    fail(f\"Features artifact missing -> {features_path}\")\n",
        "else:\n",
        "    ok(f\"Found features artifact -> {features_path}\")\n",
        "\n",
        "tb_files = glob.glob(tb_glob)\n",
        "if len(tb_files) == 0:\n",
        "    print(\"WARN: No Step03 TB artifact found matching:\", tb_glob)\n",
        "else:\n",
        "    ok(f\"Found Step03 TB artifacts (count={len(tb_files)})\")\n",
        "\n",
        "# load dataframe samples (catch exceptions)\n",
        "df_aligned = None\n",
        "df_feat = None\n",
        "try:\n",
        "    df_aligned = pd.read_pickle(aligned_path)\n",
        "    ok(\"Loaded aligned DF\")\n",
        "except Exception as e:\n",
        "    fail(f\"Failed to load aligned DF: {e}\\n{traceback.format_exc()}\")\n",
        "\n",
        "try:\n",
        "    df_feat = pd.read_pickle(features_path)\n",
        "    ok(\"Loaded features DF\")\n",
        "except Exception as e:\n",
        "    fail(f\"Failed to load features DF: {e}\\n{traceback.format_exc()}\")\n",
        "\n",
        "# basic index checks\n",
        "try:\n",
        "    if df_aligned is None:\n",
        "        fail(\"aligned DF not loaded; skipping index checks\")\n",
        "    else:\n",
        "        if not df_aligned.index.is_monotonic_increasing:\n",
        "            fail(\"Aligned DF index not monotonic increasing\")\n",
        "        else:\n",
        "            ok(\"Aligned DF index monotonic\")\n",
        "\n",
        "        if df_aligned.index.tz is None:\n",
        "            fail(\"Aligned DF index is not tz-aware\")\n",
        "        else:\n",
        "            ok(f\"Aligned DF tz-aware ({df_aligned.index.tz})\")\n",
        "\n",
        "        if df_aligned.index.has_duplicates:\n",
        "            fail(\"Aligned DF index has duplicates\")\n",
        "        else:\n",
        "            ok(\"Aligned DF index has no duplicates\")\n",
        "except Exception as e:\n",
        "    fail(f\"Index checks failed: {e}\")\n",
        "\n",
        "# LAST_COMPLETE / partial-last-bar check (best-effort)\n",
        "try:\n",
        "    meta_files = glob.glob(os.path.join(BASE_PATH, \"*step03*.meta.json\")) + glob.glob(os.path.join(BASE_PATH, \"df_aligned*.meta.json\"))\n",
        "    last_complete_vals = []\n",
        "    for mf in meta_files:\n",
        "        try:\n",
        "            mm = json.load(open(mf))\n",
        "            if \"last_complete\" in mm:\n",
        "                last_complete_vals.append(mm[\"last_complete\"])\n",
        "        except Exception:\n",
        "            pass\n",
        "    if last_complete_vals and df_aligned is not None:\n",
        "        from dateutil import parser\n",
        "        lc = parser.isoparse(last_complete_vals[0])\n",
        "        # ensure lc is tz-aware in UTC\n",
        "        if lc.tzinfo is None:\n",
        "            lc = lc.replace(tzinfo=pd.Timestamp.now(tz=\"UTC\").tzinfo)\n",
        "        if df_aligned.index.max() > lc:\n",
        "            fail(f\"Partial last bar present: df_aligned.index.max() = {df_aligned.index.max()} > last_complete ({lc})\")\n",
        "        else:\n",
        "            ok(\"No partial last bar relative to last_complete from metadata\")\n",
        "    else:\n",
        "        print(\"INFO: last_complete not found in meta files; skipping partial-last-bar check.\")\n",
        "except Exception as e:\n",
        "    print(\"INFO: last_complete check skipped due to error:\", e)\n",
        "\n",
        "# feature shift causality checks\n",
        "try:\n",
        "    if df_feat is None:\n",
        "        fail(\"features DF not loaded; skipping feature checks\")\n",
        "    else:\n",
        "        raw_cols = set([\"open\",\"high\",\"low\",\"close\",\"volume\",\"is_imputed\"])\n",
        "        feat_cols = [c for c in df_feat.columns if c not in raw_cols]\n",
        "        if len(feat_cols) == 0:\n",
        "            fail(\"No derived feature columns found in df_features_base\")\n",
        "        else:\n",
        "            ok(f\"Derived features count = {len(feat_cols)}\")\n",
        "\n",
        "        # leading-row NaNs: at least one leading row should be NaN for derived features\n",
        "        if not df_feat[feat_cols].iloc[0].isna().all():\n",
        "            fail(\"Causality/shift check failed: first row of derived features is not all-NaN\")\n",
        "        else:\n",
        "            ok(\"Causality/shift check passed: first row derived features all-NaN\")\n",
        "\n",
        "        # diag_* columns should not be in feat_cols (diagnostics must be excluded)\n",
        "        diag_cols = [c for c in df_feat.columns if str(c).startswith(\"diag_\")]\n",
        "        if any(c in feat_cols for c in diag_cols):\n",
        "            fail(\"Diagnostic columns (diag_*) included among derived feature columns — must be excluded from training features\")\n",
        "        else:\n",
        "            ok(\"diag_* columns present but excluded from derived feature columns\")\n",
        "except Exception as e:\n",
        "    fail(f\"Feature checks failed: {e}\\n{traceback.format_exc()}\")\n",
        "\n",
        "# volume_imputed_flag consistency\n",
        "try:\n",
        "    if df_aligned is not None and \"volume_imputed_flag\" in df_aligned.columns:\n",
        "        mask = df_aligned[\"volume_imputed_flag\"]\n",
        "        if not df_aligned.loc[mask, \"volume\"].isna().all():\n",
        "            fail(\"volume_imputed_flag is True but corresponding volume values are not all NaN\")\n",
        "        else:\n",
        "            ok(\"volume_imputed_flag consistency OK (imputed rows have NaN volume)\")\n",
        "    else:\n",
        "        print(\"INFO: volume_imputed_flag not found in aligned DF — ensure pipeline created this flag (recommended).\")\n",
        "except Exception as e:\n",
        "    fail(f\"volume_imputed_flag check error: {e}\")\n",
        "\n",
        "# sanity numeric checks\n",
        "try:\n",
        "    if df_aligned is None:\n",
        "        fail(\"aligned DF not loaded; skipping numeric sanity checks\")\n",
        "    else:\n",
        "        for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n",
        "            if c in df_aligned.columns:\n",
        "                n_inf = np.isinf(df_aligned[c]).sum()\n",
        "                n_nan = df_aligned[c].isna().sum()\n",
        "                if n_inf > 0:\n",
        "                    fail(f\"Column {c} contains {n_inf} infinite values\")\n",
        "                if n_nan > len(df_aligned) * 0.2:\n",
        "                    fail(f\"Column {c} has {n_nan} NaNs (>20% of rows) — suspicious\")\n",
        "        ok(\"Basic numeric sanity checks passed\")\n",
        "except Exception as e:\n",
        "    fail(f\"Numeric sanity checks error: {e}\")\n",
        "\n",
        "# summary and exit behavior\n",
        "print(\"\\n--- CI SUMMARY ---\")\n",
        "if failures:\n",
        "    print(f\"TOTAL FAILURES: {len(failures)}\")\n",
        "    for i, fmsg in enumerate(failures, 1):\n",
        "        print(f\"{i}. {fmsg}\")\n",
        "    # raise at end to signal CI failure (caller can catch)\n",
        "    raise RuntimeError(f\"CI readiness checks failed: {len(failures)} failures\")\n",
        "else:\n",
        "    print(\"ALL CHECKS PASSED\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJT9qSuTO7QS",
        "outputId": "8f9cf62d-a1fa-4f2d-8349-4f535a49d27d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[patch] using in-memory df_features\n",
            "[patch] shifted 66 feature columns safely via .loc\n",
            "[patch] tolerant OHLC equality check passed against df_aligned_final.pkl\n",
            "[patch] diag_* not present in df_features_for_training (ok)\n",
            "[patch] re-saved df_features_base -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_base.pkl\n",
            "[patch] re-saved df_features_for_training -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_for_training.pkl\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== PATCH: FEATURES_BASE safe-shift + tolerant OHLC check + diag-leak assert =====\n",
        "# - Applies only safe, non-invasive fixes recommended: .loc shift to avoid SettingWithCopy,\n",
        "#   tolerant numeric equality for raw OHLC check, and assert ensuring diag_* not leaked.\n",
        "# - Re-saves artifacts and updates manifest (same behavior as original cell).\n",
        "import os, json, gc, hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE_PATH = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# load df_features from memory if present, else from disk (same artifact name as pipeline)\n",
        "if \"df_features\" in globals() and isinstance(globals()[\"df_features\"], pd.DataFrame):\n",
        "    df_features = globals()[\"df_features\"]\n",
        "    print(\"[patch] using in-memory df_features\")\n",
        "else:\n",
        "    cand = os.path.join(BASE_PATH, \"df_features_base.pkl\")\n",
        "    if os.path.exists(cand):\n",
        "        print(f\"[patch] loading df_features from {cand}\")\n",
        "        df_features = pd.read_pickle(cand)\n",
        "    else:\n",
        "        raise RuntimeError(\"df_features not found in memory and df_features_base.pkl missing from BASE_PATH.\")\n",
        "\n",
        "# ensure index tz-awareness unchanged\n",
        "if df_features.index.tz is None:\n",
        "    df_features.index = pd.to_datetime(df_features.index).tz_localize(\"UTC\")\n",
        "else:\n",
        "    df_features.index = pd.to_datetime(df_features.index).tz_convert(\"UTC\")\n",
        "\n",
        "# Determine raw and feature cols (same logic as original)\n",
        "raw_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"is_imputed\"]\n",
        "feat_cols = [c for c in df_features.columns if c not in raw_cols]\n",
        "\n",
        "# Safe shift: compute shifted frame and assign explicitly with .loc\n",
        "if len(feat_cols) > 0:\n",
        "    shifted = df_features[feat_cols].shift(1)\n",
        "    df_features.loc[:, feat_cols] = shifted\n",
        "    print(f\"[patch] shifted {len(feat_cols)} feature columns safely via .loc\")\n",
        "else:\n",
        "    print(\"[patch] no derived feature columns found to shift\")\n",
        "\n",
        "# preserve metadata as original\n",
        "df_features.attrs[\"shifted_feature_cols\"] = feat_cols\n",
        "\n",
        "# Recompute diag/training splits\n",
        "diag_cols = [c for c in df_features.columns if str(c).startswith(\"diag_\")]\n",
        "non_train_extras = [\"is_imputed\", \"volume_imputed_flag\"] if \"volume_imputed_flag\" in df_features.columns else [\"is_imputed\"]\n",
        "train_feat_cols = [c for c in df_features.columns if c not in diag_cols + non_train_extras]\n",
        "df_features_for_training = df_features[train_feat_cols + non_train_extras].copy()\n",
        "\n",
        "# TOLERANT raw OHLC equality check (avoid false negatives from float representation)\n",
        "# Load original aligned df to compare raw OHLC (if available)\n",
        "aligned_candidates = [\n",
        "    os.path.join(BASE_PATH, \"df_aligned_final.pkl\"),\n",
        "    os.path.join(BASE_PATH, \"df_aligned_clean.pkl\"),\n",
        "]\n",
        "aligned_path = next((p for p in aligned_candidates if os.path.exists(p)), None)\n",
        "if aligned_path:\n",
        "    df_aligned = pd.read_pickle(aligned_path)\n",
        "    try:\n",
        "        # compare open/close numerically with small tolerance, allowing NaNs\n",
        "        orig = df_aligned[[\"open\",\"close\"]].astype(float).values\n",
        "        new  = df_features[[\"open\",\"close\"]].astype(float).values\n",
        "        if not np.allclose(orig, new, equal_nan=True, atol=1e-12, rtol=0):\n",
        "            raise AssertionError(\"raw OHLC numeric mismatch (tolerance check failed)\")\n",
        "    except Exception as e:\n",
        "        raise AssertionError(\"raw OHLC were modified or comparison failed: \" + str(e))\n",
        "    print(f\"[patch] tolerant OHLC equality check passed against {os.path.basename(aligned_path)}\")\n",
        "else:\n",
        "    print(\"[patch] aligned artifact not found for OHLC tolerant check; skipping this assert\")\n",
        "\n",
        "# Ensure no diag_* leaked into training artifact\n",
        "if any(str(c).startswith(\"diag_\") for c in df_features_for_training.columns):\n",
        "    leaked = [c for c in df_features_for_training.columns if str(c).startswith(\"diag_\")]\n",
        "    raise AssertionError(f\"DIAG_COLUMNS_LEAK: diag_* present in df_features_for_training -> {leaked}\")\n",
        "print(\"[patch] diag_* not present in df_features_for_training (ok)\")\n",
        "\n",
        "# Re-save artifacts (same filenames as original cell)\n",
        "out_pkl = os.path.join(BASE_PATH, \"df_features_base.pkl\")\n",
        "out_csv = os.path.join(BASE_PATH, \"df_features_base.csv\")\n",
        "df_features.to_pickle(out_pkl)\n",
        "df_features.to_csv(out_csv)\n",
        "\n",
        "out_train_pkl = os.path.join(BASE_PATH, \"df_features_for_training.pkl\")\n",
        "out_train_csv = os.path.join(BASE_PATH, \"df_features_for_training.csv\")\n",
        "df_features_for_training.to_pickle(out_train_pkl)\n",
        "df_features_for_training.to_csv(out_train_csv)\n",
        "\n",
        "# write meta (minimal update)\n",
        "meta = {\n",
        "    \"patched_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"rows\": int(len(df_features)),\n",
        "    \"shifted_feature_count\": len(feat_cols),\n",
        "    \"diag_cols_excluded_count\": len(diag_cols),\n",
        "    \"diag_cols_excluded_preview\": diag_cols[:50],\n",
        "}\n",
        "Path(os.path.join(BASE_PATH, \"df_features_base.meta.json\")).write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "# compute sha256 and update manifest (idempotent-ish)\n",
        "def sha256_of_path(p):\n",
        "    h = hashlib.sha256()\n",
        "    with open(p, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "sha_base = sha256_of_path(out_pkl)\n",
        "sha_train = sha256_of_path(out_train_pkl)\n",
        "\n",
        "manifest_path = os.path.join(BASE_PATH, \"pipeline_manifest.json\")\n",
        "manifest = {}\n",
        "if os.path.exists(manifest_path):\n",
        "    try:\n",
        "        manifest = json.load(open(manifest_path))\n",
        "    except Exception:\n",
        "        manifest = {}\n",
        "manifest.update({\n",
        "    \"df_features_base\": os.path.basename(out_pkl),\n",
        "    \"df_features_base_sha256\": sha_base,\n",
        "    \"df_features_for_training\": os.path.basename(out_train_pkl),\n",
        "    \"df_features_for_training_sha256\": sha_train,\n",
        "    \"features_patched_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "})\n",
        "tmp = manifest_path + \".tmp\"\n",
        "with open(tmp, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2, default=str)\n",
        "os.replace(tmp, manifest_path)\n",
        "\n",
        "# export back to globals to keep downstream continuity\n",
        "globals()['df_features'] = df_features\n",
        "globals()['df_features_for_training'] = df_features_for_training\n",
        "globals()['DF_FEATURES_BASE_PKL'] = out_pkl\n",
        "globals()['DF_FEATURES_TRAIN_PKL'] = out_train_pkl\n",
        "\n",
        "print(f\"[patch] re-saved df_features_base -> {out_pkl}\")\n",
        "print(f\"[patch] re-saved df_features_for_training -> {out_train_pkl}\")\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g90CwYz7XLL",
        "outputId": "e099798f-5608-447a-9be0-d4162a8433fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[04 AUTO] Loading: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_base.pkl\n",
            "[04 AUTO] Loaded rows=17,521, cols=72  time=0.15s\n",
            "[04 AUTO] Adding microstructure features (optimized & safe)...\n",
            "[04 AUTO] Augmentation done in 1.07s\n",
            "[04 AUTO] Saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.pkl\n",
            "[04 AUTO] Saved CSV -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.csv\n",
            "[04 AUTO] Saved meta -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.meta.json\n",
            "[04 AUTO] Manifest updated -> /content/drive/MyDrive/quant_pipeline/mtb_out/pipeline_manifest.json\n",
            "[04 AUTO] CI diagnostics -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.ci.json\n",
            "\n",
            "[04 AUTO] Diagnostics summary: {'shape': (17521, 88), 'new_cols_sample': ['tick_rule', 'pvo_20_200', 'vol_imbalance_20', 'vol_imbalance_20_z', 'vol_imbalance_50', 'vol_imbalance_50_z', 'roll_spread_50', 'gk_vol_50', 'cs_spread_2', 'amihud_50', 'kyle_lambda_50', 'signed_vol_50', 'signed_vol_50_z', 'vpin_proxy_rolling', 'vpin_proxy_rolling_sma', 'midprice', 'high_low_spread', 'high_low_spread_z'], 'last_bar_ok': True, 'imputed_volume_nan': True, 'diag_columns_present': False}\n",
            "\n",
            "[04 AUTO] Completed.\n",
            "VWAP tests passed (inline).\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# STEP 05 — AUTO RUN MICROSTRUCTURE (optimized + CI + manifest + drop redundancy)\n",
        "# - Adds: fully-vectorized VWAP (vwap_per_utc_day, vwap_excluding_imputed)\n",
        "# - Includes minimal unit-tests for VWAP (inline pytest-style)\n",
        "# - Keeps other code unchanged except where integrating VWAP\n",
        "# ============================\n",
        "import os, time, gc, json, hashlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- VWAP helpers (fully-vectorized) ----------\n",
        "def vwap_per_utc_day(df,\n",
        "                     high_col=\"high\",\n",
        "                     low_col=\"low\",\n",
        "                     close_col=\"close\",\n",
        "                     volume_col=\"volume\",\n",
        "                     typ_weighted=True):\n",
        "    \"\"\"\n",
        "    Compute cumulative VWAP per UTC day in a fully-vectorized way.\n",
        "    - Keeps NaNs where volume is NaN (e.g. imputed rows).\n",
        "    - typ_weighted=True uses typical price (H+L+C)/3; else uses close.\n",
        "    - Returns a pd.Series aligned to df.index (preserves tz info).\n",
        "    \"\"\"\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"DataFrame must have a DatetimeIndex (tz-aware or tz-naive).\")\n",
        "\n",
        "    if len(df) == 0:\n",
        "        return pd.Series(dtype=float, index=df.index)\n",
        "\n",
        "    # Make an index copy normalized to UTC for grouping (do not mutate original index)\n",
        "    idx_utc = df.index.tz_convert(\"UTC\") if df.index.tz is not None else df.index.tz_localize(\"UTC\")\n",
        "    days = idx_utc.normalize()\n",
        "\n",
        "    # typical price or close\n",
        "    if typ_weighted:\n",
        "        typ = (df[high_col].astype(float) + df[low_col].astype(float) + df[close_col].astype(float)) / 3.0\n",
        "    else:\n",
        "        typ = df[close_col].astype(float)\n",
        "\n",
        "    vol = df[volume_col].astype(float)\n",
        "\n",
        "    # groupby normalize days and compute cumulative sums per-group (vectorized)\n",
        "    cum_dollar = (typ * vol).groupby(days).cumsum()\n",
        "    cum_vol = vol.groupby(days).cumsum()\n",
        "\n",
        "    # avoid division by zero; keep NaN when cumulative volume zero/NaN to mirror loop-based behavior\n",
        "    vwap = cum_dollar / cum_vol.replace({0: np.nan})\n",
        "\n",
        "    # restore original index and tz-awareness\n",
        "    vwap.index = df.index\n",
        "    return vwap\n",
        "\n",
        "def vwap_excluding_imputed(df, is_imputed_col=\"is_imputed\", **kwargs):\n",
        "    \"\"\"\n",
        "    Compute per-day VWAP excluding imputed rows from cumulative totals.\n",
        "    Imputed rows remain present in index but their volume/price are not included in cumulative sums.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return pd.Series(dtype=float, index=df.index)\n",
        "    df_local = df.copy()\n",
        "    mask_imp = df_local.get(is_imputed_col, pd.Series(False, index=df_local.index)).fillna(False).astype(bool)\n",
        "    vol_col = kwargs.get(\"volume_col\", \"volume\")\n",
        "    # set volume to NaN for imputed rows so groupby-cumsum ignores them\n",
        "    df_local.loc[mask_imp, vol_col] = np.nan\n",
        "    return vwap_per_utc_day(df_local, **kwargs)\n",
        "\n",
        "# ---------- CONFIG / PATHS ----------\n",
        "BASE = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "IN_PKL  = os.path.join(BASE, \"df_features_base.pkl\")\n",
        "OUT_PKL = os.path.join(BASE, \"df_features_micro.pkl\")\n",
        "OUT_CSV = os.path.join(BASE, \"df_features_micro.csv\")\n",
        "META_PATH = os.path.join(BASE, \"df_features_micro.meta.json\")\n",
        "MANIFEST_PATH = os.path.join(BASE, \"pipeline_manifest.json\")\n",
        "DIAG_PATH = os.path.join(BASE, \"df_features_micro.ci.json\")\n",
        "Path(BASE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(IN_PKL):\n",
        "    raise RuntimeError(f\"Input file not found: {IN_PKL} — run Step 03 first.\")\n",
        "\n",
        "print(\"[04 AUTO] Loading:\", IN_PKL)\n",
        "t0 = time.time()\n",
        "df_features = pd.read_pickle(IN_PKL)\n",
        "print(f\"[04 AUTO] Loaded rows={len(df_features):,}, cols={df_features.shape[1]:,}  time={(time.time()-t0):.2f}s\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _ensure_index_ts(df):\n",
        "    # ensure tz-aware DateTimeIndex UTC (returns a copy)\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        try:\n",
        "            df = df.copy()\n",
        "            df.index = pd.to_datetime(df.index, utc=True)\n",
        "        except Exception:\n",
        "            raise RuntimeError(\"Index is not datetime-like and could not be converted.\")\n",
        "    else:\n",
        "        df = df.copy()\n",
        "        if df.index.tz is None:\n",
        "            df.index = df.index.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            df.index = df.index.tz_convert(\"UTC\")\n",
        "    return df\n",
        "\n",
        "def _winsorize_series(s, ql=0.01, qu=0.99):\n",
        "    if s.isna().all():\n",
        "        return s\n",
        "    lo, hi = s.quantile(ql), s.quantile(qu)\n",
        "    return s.clip(lower=lo, upper=hi)\n",
        "\n",
        "def _sha256_of_file(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# ---------- Core microstructure generator (optimized) ----------\n",
        "def add_microstructure_features(df, price_col='close', open_col='open', high_col='high', low_col='low', volume_col='volume',\n",
        "                                window_short=20, window_mid=50, window_long=200, keep_vol_imbalance_windows=(20,50)):\n",
        "    \"\"\"\n",
        "    Returns augmented df (copy) with microstructure features.\n",
        "    Optimizations:\n",
        "      - Cache baseline series & rolling aggregates used multiple times\n",
        "      - Use vectorized ops, avoid expensive per-group python loops\n",
        "      - Compute efficient VWAP (vectorized) and exclude imputed rows from cumulative totals\n",
        "    \"\"\"\n",
        "    eps = 1e-9\n",
        "    df = _ensure_index_ts(df)\n",
        "\n",
        "    # ensure numeric + minimal dtype safety\n",
        "    for c in (price_col, open_col, high_col, low_col, volume_col):\n",
        "        if c not in df.columns:\n",
        "            raise KeyError(f\"Missing required column: {c}\")\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype('float64')\n",
        "\n",
        "    # ensure is_imputed exists\n",
        "    if 'is_imputed' not in df.columns:\n",
        "        df['is_imputed'] = False\n",
        "    df['is_imputed'] = df['is_imputed'].fillna(False).astype(bool)\n",
        "\n",
        "    # baseline excluding imputed rows (ffill so rolling has data)\n",
        "    base_vol = df[volume_col].where(~df['is_imputed']).ffill()\n",
        "    # fallback: if still NaN at start, fill with small eps to avoid div-by-zero\n",
        "    base_vol = base_vol.fillna(eps)\n",
        "\n",
        "    # cache rolling objects used multiple times\n",
        "    roll_vol_short = base_vol.rolling(window_short, min_periods=1)\n",
        "    roll_vol_mid = base_vol.rolling(window_mid, min_periods=1)\n",
        "    roll_vol_long = base_vol.rolling(window_long, min_periods=1)\n",
        "    mean_vol_short = roll_vol_short.mean()\n",
        "    mean_vol_mid = roll_vol_mid.mean()\n",
        "    mean_vol_long = roll_vol_long.mean()\n",
        "\n",
        "    # 0) VWAP: compute and attach (exclude imputed rows from cumulative totals)\n",
        "    try:\n",
        "        df['vwap'] = vwap_excluding_imputed(df, is_imputed_col='is_imputed', high_col=high_col, low_col=low_col, close_col=price_col, volume_col=volume_col, typ_weighted=True)\n",
        "    except Exception:\n",
        "        # fallback to naive per-day loop (mirror loop behaviour exactly)\n",
        "        idx_utc = df.index.tz_convert(\"UTC\")\n",
        "        days = idx_utc.normalize()\n",
        "        vwap_s = pd.Series(index=df.index, dtype=float)\n",
        "        for day, group_idx in df.groupby(days, sort=True).groups.items():\n",
        "            grp = df.loc[group_idx]\n",
        "            typ = (grp[high_col] + grp[low_col] + grp[price_col]) / 3.0\n",
        "            cum_dollar = (typ * grp[volume_col]).cumsum()\n",
        "            cum_vol = grp[volume_col].cumsum()\n",
        "            v = cum_dollar / cum_vol.replace(0, np.nan)\n",
        "            vwap_s.loc[group_idx] = v.values\n",
        "        df['vwap'] = vwap_s\n",
        "\n",
        "    # 1) tick rule (vectorized)\n",
        "    pr = df[price_col].ffill()\n",
        "    prev = pr.shift(1)\n",
        "    tick_rule = np.sign(pr - prev)\n",
        "    # treat zeros as previous sign (vectorized): replace 0 with NaN then ffill then fill 0\n",
        "    tick_rule = tick_rule.replace(0, np.nan).ffill().fillna(0).astype(int)\n",
        "    df[\"tick_rule\"] = tick_rule\n",
        "\n",
        "    # 2) signed_volume & OBV\n",
        "    df[\"signed_volume\"] = df[volume_col] * df[\"tick_rule\"]\n",
        "    if \"obv\" not in df.columns:\n",
        "        df[\"obv\"] = df[\"signed_volume\"].cumsum().fillna(0)\n",
        "\n",
        "    # 3) PVO: (short_mean - long_mean) / long_mean\n",
        "    vol_s = mean_vol_short\n",
        "    vol_l = mean_vol_long.replace(0, eps)\n",
        "    df[f\"pvo_{window_short}_{window_long}\"] = (vol_s - vol_l) / vol_l\n",
        "\n",
        "    # 4) Volume imbalance (keep canonical windows only)\n",
        "    buy_vol  = df[volume_col].where(df[\"tick_rule\"] > 0, 0.0)\n",
        "    sell_vol = df[volume_col].where(df[\"tick_rule\"] < 0, 0.0)\n",
        "    for w in keep_vol_imbalance_windows:\n",
        "        buy_w = buy_vol.rolling(w, min_periods=1).sum()\n",
        "        sell_w = sell_vol.rolling(w, min_periods=1).sum()\n",
        "        imbalance = (buy_w - sell_w)\n",
        "        base_mean = base_vol.rolling(w, min_periods=1).mean().replace(0, eps)\n",
        "        df[f\"vol_imbalance_{w}\"] = imbalance\n",
        "        df[f\"vol_imbalance_{w}_z\"] = imbalance / base_mean\n",
        "\n",
        "    # 5) roll spread estimator\n",
        "    dp = df[price_col].diff().fillna(0)\n",
        "    cov_lag1 = dp.rolling(window_mid, min_periods=1).cov(dp.shift(1))\n",
        "    df[f\"roll_spread_{window_mid}\"] = np.sqrt((-cov_lag1).clip(lower=0).fillna(0))\n",
        "\n",
        "    # 6) Garman-Klass volatility (vectorized)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        t1 = 0.5 * (np.log(df[high_col] / df[low_col]).replace([np.inf, -np.inf], np.nan))**2\n",
        "        t2 = (2 * np.log(2) - 1) * (np.log(df[price_col] / df[open_col]).replace([np.inf, -np.inf], np.nan))**2\n",
        "    gk = t1 - t2\n",
        "    df[f\"gk_vol_{window_mid}\"] = np.sqrt(gk.rolling(window_mid, min_periods=1).mean().clip(lower=0).fillna(0))\n",
        "\n",
        "    # 7) Corwin-Schultz spread (vector-friendly)\n",
        "    w_cs = 2\n",
        "    h2 = df[high_col].rolling(2*w_cs, min_periods=1).max()\n",
        "    l2 = df[low_col].rolling(2*w_cs, min_periods=1).min()\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        gamma = np.log(h2 / l2).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    alpha = gamma.rolling(w_cs, min_periods=1).mean()\n",
        "    df[\"cs_spread_2\"] = ((2*np.exp(alpha) - 1) / (np.exp(alpha) + 1)).fillna(0)\n",
        "\n",
        "    # 8) Amihud\n",
        "    logret_abs = np.log(df[price_col]).diff().abs().fillna(0)\n",
        "    denom = base_vol.replace(0, np.nan).fillna(eps)\n",
        "    df[f\"amihud_{window_mid}\"] = (logret_abs / denom).rolling(window_mid, min_periods=1).mean().fillna(0)\n",
        "\n",
        "    # 9) Kyle lambda: fast vectorized rolling linear regression slope\n",
        "    x = df[\"signed_volume\"].fillna(0)\n",
        "    y = df[price_col].diff().fillna(0)\n",
        "    sum_x  = x.rolling(window_mid, min_periods=5).sum()\n",
        "    sum_y  = y.rolling(window_mid, min_periods=5).sum()\n",
        "    sum_xy = (x*y).rolling(window_mid, min_periods=5).sum()\n",
        "    sum_x2 = (x*x).rolling(window_mid, min_periods=5).sum()\n",
        "    n = x.rolling(window_mid, min_periods=5).count().replace(0, eps)\n",
        "    denom_k = (sum_x2 - (sum_x**2)/n).replace(0, eps)\n",
        "    slope = (sum_xy - (sum_x*sum_y)/n) / denom_k\n",
        "    df[f\"kyle_lambda_{window_mid}\"] = slope.fillna(0)\n",
        "\n",
        "    # 10) signed volume normalized\n",
        "    signed_mean_mid = x.rolling(window_mid, min_periods=1).mean()\n",
        "    signed_long_mean = x.rolling(window_long, min_periods=1).mean()\n",
        "    signed_long_std = x.rolling(window_long, min_periods=1).std().replace(0, eps)\n",
        "    df[f\"signed_vol_{window_mid}\"] = signed_mean_mid\n",
        "    df[f\"signed_vol_{window_mid}_z\"] = (signed_mean_mid - signed_long_mean) / signed_long_std\n",
        "\n",
        "    # 11) VPIN proxy (rolling)\n",
        "    buy_w  = buy_vol.rolling(window_mid, min_periods=1).sum()\n",
        "    sell_w = sell_vol.rolling(window_mid, min_periods=1).sum().abs()\n",
        "    total_w = (buy_w + sell_w).replace(0, eps)\n",
        "    df[\"vpin_proxy_rolling\"] = ((buy_w - sell_w).abs() / total_w).fillna(0)\n",
        "    df[\"vpin_proxy_rolling_sma\"] = df[\"vpin_proxy_rolling\"].rolling(max(1, window_mid//4), min_periods=1).mean()\n",
        "\n",
        "    # 12) Spread & impact proxies\n",
        "    df[\"midprice\"] = (df[high_col] + df[low_col]) / 2\n",
        "    df[\"high_low_spread\"] = (df[high_col] - df[low_col]) / df[price_col].replace(0, eps)\n",
        "    hl_mean = df[\"high_low_spread\"].rolling(window_long, min_periods=1).mean()\n",
        "    hl_std  = df[\"high_low_spread\"].rolling(window_long, min_periods=1).std().replace(0, eps)\n",
        "    df[\"high_low_spread_z\"] = (df[\"high_low_spread\"] - hl_mean) / hl_std\n",
        "\n",
        "    # 13) Winsorize heavy-tail (diagnostic-only; based on non-imputed subset)\n",
        "    to_winsor = [\n",
        "        f\"amihud_{window_mid}\",\n",
        "        f\"kyle_lambda_{window_mid}\",\n",
        "        f\"gk_vol_{window_mid}\",\n",
        "        f\"roll_spread_{window_mid}\",\n",
        "        \"cs_spread_2\",\n",
        "        \"high_low_spread\",\n",
        "        \"vpin_proxy_rolling\",\n",
        "    ]\n",
        "    for col in to_winsor:\n",
        "        if col in df.columns:\n",
        "            base = df[col].where(~df['is_imputed'])\n",
        "            if base.dropna().size > 0:\n",
        "                lo, hi = base.quantile(0.01), base.quantile(0.99)\n",
        "                df[col] = df[col].clip(lower=lo, upper=hi)\n",
        "            else:\n",
        "                df[col] = _winsorize_series(df[col])\n",
        "\n",
        "    # downcast floats where appropriate to save memory\n",
        "    float_cols = df.select_dtypes(include=['float64']).columns.tolist()\n",
        "    if float_cols:\n",
        "        df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n",
        "\n",
        "    return df\n",
        "\n",
        "# ---------- Run augmentation ----------\n",
        "t1 = time.time()\n",
        "print(\"[04 AUTO] Adding microstructure features (optimized & safe)...\")\n",
        "df_aug = add_microstructure_features(df_features, window_short=20, window_mid=50, window_long=200, keep_vol_imbalance_windows=(20,50))\n",
        "print(f\"[04 AUTO] Augmentation done in {(time.time()-t1):.2f}s\")\n",
        "\n",
        "# ---------- Strict causality SHIFT (same rule as Step 03) ----------\n",
        "raw_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"is_imputed\"]\n",
        "feat_cols = [c for c in df_aug.columns if c not in raw_cols]\n",
        "\n",
        "# shift derived features by 1 bar\n",
        "df_aug[feat_cols] = df_aug[feat_cols].shift(1)\n",
        "df_aug.attrs[\"micro_shifted_cols\"] = feat_cols\n",
        "\n",
        "# stronger causality assert: first row of derived features must be all-NaN (consistent with Step03)\n",
        "if len(df_aug) > 0:\n",
        "    assert df_aug[feat_cols].iloc[0].isna().all(), \"Shifted feature first row must be all NaN (causality check)\"\n",
        "\n",
        "# ---------- Post-process: drop redundant vol_imbalance columns if any extras exist ----------\n",
        "canonical = set([\"vol_imbalance_20\",\"vol_imbalance_20_z\",\"vol_imbalance_50\",\"vol_imbalance_50_z\"])\n",
        "to_drop = [c for c in df_aug.columns if c.startswith(\"vol_imbalance_\") and c not in canonical]\n",
        "if to_drop:\n",
        "    df_aug = df_aug.drop(columns=to_drop)\n",
        "\n",
        "# ---------- Save artifacts (pickle + csv) ----------\n",
        "df_aug.to_pickle(OUT_PKL)\n",
        "df_aug.to_csv(OUT_CSV)\n",
        "\n",
        "# compute checksum for provenance\n",
        "sha = _sha256_of_file(OUT_PKL)\n",
        "\n",
        "# ---------- meta + manifest updates ----------\n",
        "meta = {\n",
        "    \"generated_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"rows\": int(len(df_aug)),\n",
        "    \"new_cols_count\": len([c for c in df_aug.columns if c not in df_features.columns]),\n",
        "    \"new_cols_sample\": [c for c in df_aug.columns if c not in df_features.columns][:50],\n",
        "    \"shifted_feature_count\": len(feat_cols),\n",
        "    \"kept_vol_imbalance_windows\": [20,50],\n",
        "    \"sha256\": sha,\n",
        "}\n",
        "Path(META_PATH).write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "manifest = {}\n",
        "if os.path.exists(MANIFEST_PATH):\n",
        "    try:\n",
        "        manifest = json.load(open(MANIFEST_PATH))\n",
        "    except Exception:\n",
        "        manifest = {}\n",
        "manifest.update({\n",
        "    \"df_features_micro\": os.path.basename(OUT_PKL),\n",
        "    \"df_features_micro_csv\": os.path.basename(OUT_CSV),\n",
        "    \"df_features_micro_meta\": os.path.basename(META_PATH),\n",
        "    \"generated_at\": meta[\"generated_at\"]\n",
        "})\n",
        "with open(MANIFEST_PATH, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "# ---------- Diagnostics / CI minimal smoke tests ----------\n",
        "ci = {}\n",
        "ci['shape'] = df_aug.shape\n",
        "ci['new_cols_sample'] = meta['new_cols_sample']\n",
        "\n",
        "# 1) ensure no partial last bar (use LAST_COMPLETE if present in globals)\n",
        "if 'LAST_COMPLETE' in globals():\n",
        "    try:\n",
        "        ci['last_bar_ok'] = (df_aug.index.max() <= globals()['LAST_COMPLETE'])\n",
        "    except Exception:\n",
        "        ci['last_bar_ok'] = None\n",
        "else:\n",
        "    ci['last_bar_ok'] = None\n",
        "\n",
        "# 2) imputed rows keep volume NaN where flagged\n",
        "if 'volume' in df_aug.columns:\n",
        "    ci['imputed_volume_nan'] = bool(df_aug.loc[df_aug['is_imputed'], 'volume'].isna().all())\n",
        "else:\n",
        "    ci['imputed_volume_nan'] = True\n",
        "\n",
        "# 3) diag_* columns excluded from feature set (detect presence)\n",
        "ci['diag_columns_present'] = any(c.startswith(\"diag_\") for c in df_aug.columns)\n",
        "\n",
        "Path(DIAG_PATH).write_text(json.dumps({\"ci\": ci, \"meta\": meta}, indent=2))\n",
        "\n",
        "# print summary\n",
        "print(f\"[04 AUTO] Saved -> {OUT_PKL}\")\n",
        "print(f\"[04 AUTO] Saved CSV -> {OUT_CSV}\")\n",
        "print(f\"[04 AUTO] Saved meta -> {META_PATH}\")\n",
        "print(f\"[04 AUTO] Manifest updated -> {MANIFEST_PATH}\")\n",
        "print(f\"[04 AUTO] CI diagnostics -> {DIAG_PATH}\")\n",
        "print(\"\\n[04 AUTO] Diagnostics summary:\", ci)\n",
        "\n",
        "gc.collect()\n",
        "print(\"\\n[04 AUTO] Completed.\")\n",
        "\n",
        "# ============================\n",
        "# Unit-tests (inline) for VWAP (pytest-style)\n",
        "# - Not executed automatically; run explicitly in colab or CI (pytest).\n",
        "# ============================\n",
        "def _vwap_reference_loop(df, high_col=\"high\", low_col=\"low\", close_col=\"close\", volume_col=\"volume\"):\n",
        "    idx_utc = df.index.tz_convert(\"UTC\") if df.index.tz is not None else df.index.tz_localize(\"UTC\")\n",
        "    days = idx_utc.normalize()\n",
        "    out = pd.Series(index=df.index, dtype=float)\n",
        "    for day, group_idx in df.groupby(days, sort=True).groups.items():\n",
        "        grp = df.loc[group_idx]\n",
        "        typ = (grp[high_col] + grp[low_col] + grp[close_col]) / 3.0\n",
        "        cum_dollar = (typ * grp[volume_col]).cumsum()\n",
        "        cum_vol = grp[volume_col].cumsum()\n",
        "        v = cum_dollar / cum_vol.replace(0, np.nan)\n",
        "        out.loc[group_idx] = v.values\n",
        "    out.index = df.index\n",
        "    return out\n",
        "\n",
        "def test_vwap_small():\n",
        "    rng = pd.date_range(\"2025-01-01 00:00\", periods=6, freq=\"1H\", tz=\"UTC\")\n",
        "    df_t = pd.DataFrame({\n",
        "        \"open\":  [100,101,102,110,111,112],\n",
        "        \"high\":  [101,102,103,111,112,113],\n",
        "        \"low\":   [99,100,101,109,110,111],\n",
        "        \"close\": [100.5,101.5,102.5,110.5,111.5,112.5],\n",
        "        \"volume\":[10, 20, 30, 5, 10, 15],\n",
        "    }, index=rng)\n",
        "    v_ref = _vwap_reference_loop(df_t)\n",
        "    v_vec = vwap_per_utc_day(df_t)\n",
        "    assert np.allclose(v_ref.fillna(0), v_vec.fillna(0), rtol=1e-9, atol=1e-12)\n",
        "\n",
        "def test_vwap_with_nan_volume_and_imputed():\n",
        "    rng = pd.date_range(\"2025-01-01 00:00\", periods=4, freq=\"1H\", tz=\"UTC\")\n",
        "    df_t = pd.DataFrame({\n",
        "        \"open\":[10,11,12,13],\n",
        "        \"high\":[11,12,13,14],\n",
        "        \"low\":[9,10,11,12],\n",
        "        \"close\":[10.5,11.5,12.5,13.5],\n",
        "        \"volume\":[100, np.nan, 50, 50],\n",
        "        \"is_imputed\":[False, True, False, False],\n",
        "    }, index=rng)\n",
        "    v_vec = vwap_per_utc_day(df_t)\n",
        "    v_ref = _vwap_reference_loop(df_t)\n",
        "    assert np.allclose(v_ref.fillna(0), v_vec.fillna(0), rtol=1e-9, atol=1e-12)\n",
        "    v_excl = vwap_excluding_imputed(df_t)\n",
        "    df_t2 = df_t.copy()\n",
        "    df_t2.loc[df_t2[\"is_imputed\"], \"volume\"] = np.nan\n",
        "    v_ref2 = _vwap_reference_loop(df_t2)\n",
        "    assert np.allclose(v_ref2.fillna(0), v_excl.fillna(0), rtol=1e-9, atol=1e-12)\n",
        "\n",
        "# Quick inline run example (execute manually if desired)\n",
        "if __name__ == \"__main__\":\n",
        "    test_vwap_small()\n",
        "    test_vwap_with_nan_volume_and_imputed()\n",
        "    print(\"VWAP tests passed (inline).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLlkPHQSPe60",
        "outputId": "f3853e04-edb4-4856-f740-decfb252cf50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[patch] using in-memory df_aug\n",
            "[patch] safely shifted 82 derived columns via .loc\n",
            "[patch] causality assert passed: first row of derived features is all-NaN\n",
            "[patch] vwap presence and index alignment OK\n",
            "[patch] imputed-volume NaN check passed\n",
            "[patch] Saved patched df_features_micro -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.pkl\n",
            "[patch] Saved csv -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.csv\n",
            "[patch] Saved meta -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.meta.json\n",
            "[patch] Manifest updated -> /content/drive/MyDrive/quant_pipeline/mtb_out/pipeline_manifest.json\n",
            "[patch] CI diagnostics -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.ci.json\n",
            "[patch] Completed.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== APPLY PATCH: safe .loc shift + post-run sanity asserts for STEP 05 microstructure =====\n",
        "# - Replaces bulk-shift with safe .loc assignment to avoid SettingWithCopy issues.\n",
        "# - Adds post-run asserts: causality first-row NaN, vwap index correctness, imputed-volume NaN check.\n",
        "# - Re-saves artifacts, meta, and manifest exactly as original cell expects.\n",
        "import os, json, gc, hashlib, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE = globals().get(\"OUT_DIR\", \"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_PKL = os.path.join(BASE, \"df_features_micro.pkl\")\n",
        "OUT_CSV = os.path.join(BASE, \"df_features_micro.csv\")\n",
        "META_PATH = os.path.join(BASE, \"df_features_micro.meta.json\")\n",
        "MANIFEST_PATH = os.path.join(BASE, \"pipeline_manifest.json\")\n",
        "DIAG_PATH = os.path.join(BASE, \"df_features_micro.ci.json\")\n",
        "Path(BASE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load df_aug if present in memory, else attempt to load OUT_PKL (the previous cell wrote it)\n",
        "if \"df_aug\" in globals() and isinstance(globals()[\"df_aug\"], pd.DataFrame):\n",
        "    df_aug = globals()[\"df_aug\"]\n",
        "    print(\"[patch] using in-memory df_aug\")\n",
        "elif os.path.exists(OUT_PKL):\n",
        "    print(f\"[patch] loading existing augmented features from {OUT_PKL}\")\n",
        "    df_aug = pd.read_pickle(OUT_PKL)\n",
        "else:\n",
        "    raise RuntimeError(\"df_aug not found in memory and df_features_micro.pkl not present — run augmentation cell first.\")\n",
        "\n",
        "# Ensure index tz-awareness preserved (do not mutate semantics)\n",
        "if df_aug.index.tz is None:\n",
        "    df_aug.index = pd.to_datetime(df_aug.index).tz_localize(\"UTC\")\n",
        "else:\n",
        "    df_aug.index = pd.to_datetime(df_aug.index).tz_convert(\"UTC\")\n",
        "\n",
        "# Determine raw_cols and derived feature cols (same rule as original)\n",
        "raw_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"is_imputed\"]\n",
        "feat_cols = [c for c in df_aug.columns if c not in raw_cols]\n",
        "\n",
        "# SAFE SHIFT: compute shifted frame and assign with .loc to avoid SettingWithCopy and dtype surprises\n",
        "if len(feat_cols) > 0:\n",
        "    shifted = df_aug[feat_cols].shift(1)\n",
        "    df_aug.loc[:, feat_cols] = shifted\n",
        "    df_aug.attrs[\"micro_shifted_cols\"] = feat_cols\n",
        "    print(f\"[patch] safely shifted {len(feat_cols)} derived columns via .loc\")\n",
        "else:\n",
        "    print(\"[patch] No derived feature columns to shift (feat_cols empty)\")\n",
        "\n",
        "# Stronger causality assert: first row of derived features must be all-NaN (consistent with Step03)\n",
        "if len(df_aug) > 0 and len(feat_cols) > 0:\n",
        "    if not df_aug[feat_cols].iloc[0].isna().all():\n",
        "        # Provide helpful debugging info\n",
        "        bad = df_aug[feat_cols].iloc[0].to_dict()\n",
        "        raise AssertionError(f\"Shifted features causality check failed: first row not all NaN. sample: {bad}\")\n",
        "    else:\n",
        "        print(\"[patch] causality assert passed: first row of derived features is all-NaN\")\n",
        "\n",
        "# Assert VWAP presence and index alignment (vectorized VWAP should preserve index and tz)\n",
        "if 'vwap' in df_aug.columns:\n",
        "    if not df_aug['vwap'].index.equals(df_aug.index):\n",
        "        raise AssertionError(\"VWAP index mismatch: vwap.index != df_aug.index\")\n",
        "    else:\n",
        "        print(\"[patch] vwap presence and index alignment OK\")\n",
        "else:\n",
        "    print(\"[patch] Warning: vwap column not present in df_aug (expected).\")\n",
        "\n",
        "# Assert imputed rows keep volume NaN where flagged (safety)\n",
        "if 'volume' in df_aug.columns and 'is_imputed' in df_aug.columns:\n",
        "    imputed_mask = df_aug['is_imputed'].fillna(False).astype(bool)\n",
        "    # if there are imputed rows, they must have NaN volume\n",
        "    if imputed_mask.any():\n",
        "        if not df_aug.loc[imputed_mask, 'volume'].isna().all():\n",
        "            # sample offending rows for debug\n",
        "            sample_offenders = df_aug.loc[imputed_mask & df_aug['volume'].notna(), ['volume']].head(5).to_dict()\n",
        "            raise AssertionError(f\"Imputed rows must have NaN volume but found values: {sample_offenders}\")\n",
        "        else:\n",
        "            print(\"[patch] imputed-volume NaN check passed\")\n",
        "    else:\n",
        "        print(\"[patch] no imputed rows present; imputed-volume check skipped\")\n",
        "else:\n",
        "    print(\"[patch] volume or is_imputed column missing; imputed-volume check skipped\")\n",
        "\n",
        "# Drop redundant vol_imbalance columns if any extras exist (same canonical drop behaviour)\n",
        "canonical = set([\"vol_imbalance_20\",\"vol_imbalance_20_z\",\"vol_imbalance_50\",\"vol_imbalance_50_z\"])\n",
        "to_drop = [c for c in df_aug.columns if c.startswith(\"vol_imbalance_\") and c not in canonical]\n",
        "if to_drop:\n",
        "    df_aug = df_aug.drop(columns=to_drop)\n",
        "    print(f\"[patch] dropped redundant vol_imbalance cols: {to_drop}\")\n",
        "\n",
        "# Save artifacts (overwrite original outputs)\n",
        "df_aug.to_pickle(OUT_PKL)\n",
        "df_aug.to_csv(OUT_CSV)\n",
        "\n",
        "# compute checksum for provenance (same helper logic)\n",
        "def _sha256_of_file(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "sha = _sha256_of_file(OUT_PKL)\n",
        "\n",
        "# meta + manifest updates (keep same keys/structure as original cell)\n",
        "meta = {\n",
        "    \"patched_at\": str(pd.Timestamp.now(tz=\"UTC\")),\n",
        "    \"rows\": int(len(df_aug)),\n",
        "    \"new_cols_count\": len([c for c in df_aug.columns if c not in (globals().get(\"df_features\").columns if \"df_features\" in globals() else [])]),\n",
        "    \"new_cols_sample\": [c for c in df_aug.columns if c not in (globals().get(\"df_features\").columns if \"df_features\" in globals() else [])][:50],\n",
        "    \"shifted_feature_count\": len(feat_cols),\n",
        "    \"kept_vol_imbalance_windows\": [20,50],\n",
        "    \"sha256\": sha,\n",
        "}\n",
        "Path(META_PATH).write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "manifest = {}\n",
        "if os.path.exists(MANIFEST_PATH):\n",
        "    try:\n",
        "        manifest = json.load(open(MANIFEST_PATH))\n",
        "    except Exception:\n",
        "        manifest = {}\n",
        "manifest.update({\n",
        "    \"df_features_micro\": os.path.basename(OUT_PKL),\n",
        "    \"df_features_micro_csv\": os.path.basename(OUT_CSV),\n",
        "    \"df_features_micro_meta\": os.path.basename(META_PATH),\n",
        "    \"generated_at\": meta[\"patched_at\"]\n",
        "})\n",
        "tmp = MANIFEST_PATH + \".tmp\"\n",
        "with open(tmp, \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "os.replace(tmp, MANIFEST_PATH)\n",
        "\n",
        "# CI diagnostics writeback (keep same keys)\n",
        "ci = {}\n",
        "ci['shape'] = df_aug.shape\n",
        "ci['new_cols_sample'] = meta['new_cols_sample']\n",
        "if 'LAST_COMPLETE' in globals():\n",
        "    try:\n",
        "        ci['last_bar_ok'] = (df_aug.index.max() <= globals()['LAST_COMPLETE'])\n",
        "    except Exception:\n",
        "        ci['last_bar_ok'] = None\n",
        "else:\n",
        "    ci['last_bar_ok'] = None\n",
        "ci['imputed_volume_nan'] = bool(df_aug.loc[df_aug.get('is_imputed', pd.Series(False, index=df_aug.index)), 'volume'].isna().all()) if 'volume' in df_aug.columns else True\n",
        "ci['diag_columns_present'] = any(c.startswith(\"diag_\") for c in df_aug.columns)\n",
        "Path(DIAG_PATH).write_text(json.dumps({\"ci\": ci, \"meta\": meta}, indent=2))\n",
        "\n",
        "# expose back into globals for downstream continuity\n",
        "globals()['df_aug'] = df_aug\n",
        "globals()['DF_FEATURES_MICRO_PKL'] = OUT_PKL\n",
        "globals()['DF_FEATURES_MICRO_CSV'] = OUT_CSV\n",
        "globals()['DF_FEATURES_MICRO_META'] = META_PATH\n",
        "\n",
        "print(f\"[patch] Saved patched df_features_micro -> {OUT_PKL}\")\n",
        "print(f\"[patch] Saved csv -> {OUT_CSV}\")\n",
        "print(f\"[patch] Saved meta -> {META_PATH}\")\n",
        "print(f\"[patch] Manifest updated -> {MANIFEST_PATH}\")\n",
        "print(f\"[patch] CI diagnostics -> {DIAG_PATH}\")\n",
        "print(\"[patch] Completed.\")\n",
        "\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpoxbrmE8OSn",
        "outputId": "3dd7817b-73d6-449d-8fa6-356abb189e2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Non-scalar columns detected in df_meta (will backup & drop): ['high_low_spread_z']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== DIAG SUMMARY (written to diag_step04.json) ===\n",
            "Loaded from: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_micro.pkl\n",
            "Rows,Cols: 17521 x 229\n",
            "Checks summary keys: ['leak_cols_count', 'leak_cols_sample', 'new_cols_nan_rates_max', 'new_cols_nan_high', 'z_mu_abs', 'z_std_mean']\n",
            "Leak cols found: 0\n",
            "New cols count: 152\n",
            "New cols sample (first 30): ['regime_flag', 'open_z', 'open_z_small', 'high_z', 'high_z_small', 'low_z', 'low_z_small', 'close_z', 'close_z_small', 'volume_z', 'volume_z_small', 'hl2_z', 'hl2_z_small', 'hlc3_z', 'hlc3_z_small', 'ohlc4_z', 'ohlc4_z_small', 'ret_1_z', 'ret_1_z_small', 'logret_1_z', 'logret_1_z_small', 'ret_3h_z', 'ret_3h_z_small', 'ret_6h_z', 'ret_6h_z_small', 'ret_12h_z', 'ret_12h_z_small', 'ret_24h_z', 'ret_24h_z_small', 'ret_48h_z']\n",
            "Z-features mean(abs(mean)): 0.12602877616882324\n",
            "Z-features mean(std): 1.187525749206543\n",
            "Dropped initial vector-like (backups): {'found': [], 'backups': {}}\n",
            "Dropped post-meta vector-like (backups): {'found': ['high_low_spread_z'], 'backups': {'high_low_spread_z': '/content/drive/MyDrive/quant_pipeline/mtb_out/high_low_spread_z_nonscalar_backup.v1764900161.pkl'}}\n",
            "Preventive column drops (skip patterns): ['pvo_hist', 'signed_volume', 'pvo_20_200', 'vol_imbalance_20', 'vol_imbalance_20_z', 'vol_imbalance_50', 'vol_imbalance_50_z', 'signed_vol_50', 'signed_vol_50_z']\n",
            "Diagnostics written to: /content/drive/MyDrive/quant_pipeline/mtb_out/diag_step04.json /content/drive/MyDrive/quant_pipeline/mtb_out/diag_step04.csv\n",
            "Top Z-stats written to: /content/drive/MyDrive/quant_pipeline/mtb_out/diag_step04_top_z.csv\n",
            "Quick verification PASS (no leakage asserted).\n",
            "Running lightweight unit-tests...\n",
            "All tests passed (lightweight).\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "STEP04_FINAL — fracdiff + cusum + causal meta + verification (FINAL v4 patched)\n",
        "\n",
        "Modifications:\n",
        " - Preventive skip of columns that match SKIP_VECTOR_COL_PATTERNS (do not build features that tend to produce lists/arrays)\n",
        " - Early aggressive detection of non-scalar cells (backup+drop)\n",
        " - Keep original behavior: backup dropped cols, final full-scan, assertions\n",
        " - Minimal hardening: timezone coercion, dtype casts\n",
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import math\n",
        "import warnings\n",
        "import re\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import fnmatch\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_scalar\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "IN_PKL_CANDIDATES = [\n",
        "    os.path.join(OUT_DIR, \"df_features_micro.pkl\"),\n",
        "    os.path.join(OUT_DIR, \"df_features_base.pkl\"),\n",
        "    os.path.join(OUT_DIR, \"df_features_with_wavelets.pkl\"),\n",
        "    os.path.join(OUT_DIR, \"df_features.pkl\"),\n",
        "]\n",
        "OUT_PKL_BASE = os.path.join(OUT_DIR, \"df_meta_step04.pkl\")\n",
        "OUT_CSV = os.path.join(OUT_DIR, \"df_meta_step04.csv\")\n",
        "DIAG_JSON = os.path.join(OUT_DIR, \"diag_step04.json\")\n",
        "DIAG_CSV = os.path.join(OUT_DIR, \"diag_step04.csv\")\n",
        "DIAG_TOP_Z = os.path.join(OUT_DIR, \"diag_step04_top_z.csv\")\n",
        "BACKUP_PKL_PREFIX = os.path.join(OUT_DIR, \"df_features_micro_backup_step04\")\n",
        "BACKUP_KEEP = True\n",
        "DROP_Z_SMALL = False\n",
        "Z_MEAN_ABS_MAX = 0.5\n",
        "Z_STD_MEAN_RANGE = (0.3, 5.0)\n",
        "MIN_ROWS_FOR_Z_ASSERT = 200\n",
        "MAX_NAN_RATE_NEWCOL = 0.25\n",
        "SAMPLE_CAP = 5000\n",
        "RNG_SEED = 42\n",
        "_eps = 1e-12\n",
        "\n",
        "FAST_FRACDIFF_MAX_WEIGHTS = 2000\n",
        "CUSUM_ALLOW_SMALL_BFILL = False\n",
        "CUSUM_SMALL_BFILL_LIMIT = 2\n",
        "MAX_WINDOW_BYTES = int(5e7)\n",
        "\n",
        "# -----------------------\n",
        "# Preventive vector patterns (do not build features originating from these)\n",
        "# Add/modify patterns as you learn which features produce vectors in your runs.\n",
        "SKIP_VECTOR_COL_PATTERNS = [\n",
        "    \"vol_imbalance*\",\n",
        "    \"signed_vol*\",\n",
        "    \"*_z_small\",\n",
        "    \"*_vec*\",\n",
        "    \"vwap_excluding_*\",\n",
        "    \"vwap_*\",\n",
        "    \"obv_*\",     # avoid specialized object-like obv variants\n",
        "    \"pvo_*\",\n",
        "    \"pvo_hist*\",\n",
        "    \"vol_imbalance_*_z\",\n",
        "]\n",
        "\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "_globals = globals()\n",
        "\n",
        "# -----------------------\n",
        "# fracdiff/cusum/causal helpers\n",
        "# (unchanged logic, hardened types)\n",
        "# -----------------------\n",
        "def _make_weights_np(d, thresh, max_len):\n",
        "    w = [1.0]\n",
        "    k = 1\n",
        "    while True:\n",
        "        w_k = -w[-1] * (d - k + 1) / k\n",
        "        if abs(w_k) < thresh or k >= max_len:\n",
        "            break\n",
        "        w.append(w_k)\n",
        "        k += 1\n",
        "    return np.asarray(w, dtype=float)\n",
        "\n",
        "def fracdiff_series(series: pd.Series, d: float, thresh: float = 1e-5, max_len: int = FAST_FRACDIFF_MAX_WEIGHTS):\n",
        "    s = pd.Series(series).astype(float)\n",
        "    vals = s.values\n",
        "    w = _make_weights_np(d, thresh, max_len)\n",
        "    if len(w) >= max_len:\n",
        "        warnings.warn(f\"fracdiff_series: weights length hit max_len={max_len} (possible truncation).\")\n",
        "    try:\n",
        "        from numpy.lib.stride_tricks import sliding_window_view\n",
        "        n = vals.shape[0]; width = w.shape[0]\n",
        "        if width == 0 or n < width:\n",
        "            out = np.full(n, np.nan, dtype=float)\n",
        "            return pd.Series(out.astype('float32'), index=s.index)\n",
        "        bytes_per_elem = vals.dtype.itemsize if hasattr(vals, \"dtype\") else 8\n",
        "        est_bytes = int(n * width * bytes_per_elem)\n",
        "        if est_bytes > MAX_WINDOW_BYTES:\n",
        "            out = np.full(n, np.nan, dtype=float)\n",
        "            for i in range(width - 1, n):\n",
        "                window = vals[i - width + 1:i + 1]\n",
        "                if np.isnan(window).any():\n",
        "                    out[i] = np.nan\n",
        "                else:\n",
        "                    out[i] = float(np.dot(w[::-1], window))\n",
        "            return pd.Series(out.astype('float32'), index=s.index)\n",
        "        windows = sliding_window_view(vals, window_shape=width)\n",
        "        valid_mask = ~np.isnan(windows).any(axis=1)\n",
        "        dots = np.full(windows.shape[0], np.nan, dtype=float)\n",
        "        if valid_mask.any():\n",
        "            dots[valid_mask] = (windows[valid_mask] * w[::-1]).sum(axis=1)\n",
        "        out = np.full(n, np.nan, dtype=float)\n",
        "        out[width - 1:] = dots\n",
        "        return pd.Series(out.astype('float32'), index=s.index)\n",
        "    except Exception:\n",
        "        n = len(vals); width = len(w)\n",
        "        out = np.full(n, np.nan, dtype=float)\n",
        "        for i in range(width - 1, n):\n",
        "            window = vals[i - width + 1:i + 1]\n",
        "            if np.isnan(window).any():\n",
        "                out[i] = np.nan\n",
        "            else:\n",
        "                out[i] = float(np.dot(w[::-1], window))\n",
        "        return pd.Series(out.astype('float32'), index=s.index)\n",
        "\n",
        "def cusum_filter(price_series: pd.Series, log: bool = True, threshold_k: float = 5.0, drift: float = 0.0,\n",
        "                 vol_window: int = 50, min_periods: int = 10,\n",
        "                 allow_small_bfill: bool = CUSUM_ALLOW_SMALL_BFILL, small_bfill_limit: int = CUSUM_SMALL_BFILL_LIMIT):\n",
        "    if allow_small_bfill:\n",
        "        warnings.warn(\"cusum_filter: allow_small_bfill=True — this can introduce tiny backfill.\")\n",
        "    s = pd.Series(price_series).astype(float)\n",
        "    if log:\n",
        "        r_series = np.log(s).diff().fillna(0.0)\n",
        "    else:\n",
        "        r_series = s.pct_change().fillna(0.0)\n",
        "    r = r_series.values\n",
        "    vol_series = r_series.rolling(window=vol_window, min_periods=min_periods).std()\n",
        "    vol_series = vol_series.ffill()\n",
        "    if allow_small_bfill:\n",
        "        vol_series = vol_series.fillna(method=\"bfill\", limit=small_bfill_limit)\n",
        "    vol_vals = vol_series.replace(0.0, 1e-9).values\n",
        "    pos_sum = 0.0; neg_sum = 0.0; events = []; idx_vals = s.index; n = len(r)\n",
        "    for i in range(n):\n",
        "        rt = float(r[i])\n",
        "        vt = vol_vals[i] if i < len(vol_vals) and not np.isnan(vol_vals[i]) else 1e-9\n",
        "        th = threshold_k * float(vt)\n",
        "        pos_sum = max(0.0, pos_sum + rt - drift)\n",
        "        neg_sum = min(0.0, neg_sum + rt + drift)\n",
        "        if pos_sum > th or abs(neg_sum) > th:\n",
        "            events.append(idx_vals[i]); pos_sum = 0.0; neg_sum = 0.0\n",
        "    return list(dict.fromkeys(events))\n",
        "\n",
        "def expand_events_to_windows(df: pd.DataFrame, events, N_after=24, col_name='regime_flag'):\n",
        "    if col_name not in df.columns:\n",
        "        df[col_name] = 0\n",
        "    col_idx = df.columns.get_loc(col_name)\n",
        "    ix = df.index; n = len(df)\n",
        "    for ev in events:\n",
        "        try:\n",
        "            idx = ix.get_loc(ev)\n",
        "        except KeyError:\n",
        "            continue\n",
        "        end = min(n, idx + N_after)\n",
        "        df.iloc[idx:end, col_idx] = 1\n",
        "    return df\n",
        "\n",
        "def _col_matches_skip(colname: str):\n",
        "    \"\"\"Return True if column name matches any SKIP_VECTOR_COL_PATTERNS\"\"\"\n",
        "    for p in SKIP_VECTOR_COL_PATTERNS:\n",
        "        if fnmatch.fnmatch(colname, p):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def preprocess_fracdiff_and_cusum(df_features: pd.DataFrame, fracdiff_cols=('close', 'volume', 'obv'),\n",
        "                                  d: float = 0.40, fracdiff_thresh: float = 1e-5, fracdiff_max_len: int = FAST_FRACDIFF_MAX_WEIGHTS,\n",
        "                                  cusum_k: float = 5.0, cusum_vol_window: int = 50, regime_expand_N: int = 24,\n",
        "                                  keep_original: bool = True, allow_small_bfill: bool = CUSUM_ALLOW_SMALL_BFILL):\n",
        "    df = df_features.copy()\n",
        "\n",
        "    # Preventive: drop any input cols that match skip patterns (we don't want to process problematic columns)\n",
        "    drop_cols = [c for c in df.columns if _col_matches_skip(c)]\n",
        "    if drop_cols:\n",
        "        log.info(\"preprocess_fracdiff_and_cusum: dropping preventive skip columns: %s\", drop_cols)\n",
        "        df = df.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "    to_frac = [c for c in fracdiff_cols if c in df.columns and not _col_matches_skip(c)]\n",
        "    for col in to_frac:\n",
        "        d_tag = f\"d{int(round(d*100))}\"\n",
        "        newcol = f\"{col}_fdiff_{d_tag}\"\n",
        "        try:\n",
        "            fd_series = fracdiff_series(df[col], d=d, thresh=fracdiff_thresh, max_len=fracdiff_max_len)\n",
        "            df[newcol] = fd_series.astype('float32')\n",
        "        except Exception:\n",
        "            df[newcol] = pd.Series(np.nan, index=df.index, dtype='float32')\n",
        "\n",
        "    # find source close robustly\n",
        "    close_candidates = [c for c in df.columns if c.lower() in ('close', 'price', 'last', 'mid')]\n",
        "    source_close = close_candidates[0] if close_candidates else None\n",
        "    if source_close is None:\n",
        "        substrs = [c for c in df.columns if 'close' in c.lower() or 'price' in c.lower()]\n",
        "        source_close = substrs[0] if substrs else None\n",
        "    if source_close is None:\n",
        "        df.attrs['cusum_events'] = []; df['regime_flag'] = 0; return df\n",
        "\n",
        "    # avoid running cusum on columns that match skip patterns\n",
        "    if _col_matches_skip(source_close):\n",
        "        log.info(\"preprocess_fracdiff_and_cusum: skipping cusum because source_close matches skip-pattern: %s\", source_close)\n",
        "        df.attrs['cusum_events'] = []; df['regime_flag'] = 0; return df\n",
        "\n",
        "    events = cusum_filter(df[source_close], log=True, threshold_k=cusum_k, drift=0.0,\n",
        "                         vol_window=cusum_vol_window, min_periods=max(5, cusum_vol_window // 5),\n",
        "                         allow_small_bfill=allow_small_bfill)\n",
        "    df = expand_events_to_windows(df, events, N_after=regime_expand_N, col_name='regime_flag')\n",
        "    if 'regime_flag' in df.columns:\n",
        "        df['regime_flag'] = pd.to_numeric(df['regime_flag'], errors='coerce').fillna(0).astype('int8')\n",
        "    df.attrs['cusum_events'] = events\n",
        "    return df\n",
        "\n",
        "def causal_zscore(series: pd.Series, window: int = 2000, min_periods: int = 10):\n",
        "    s = pd.Series(series).astype(float)\n",
        "    rm = s.rolling(window=window, min_periods=min_periods).mean()\n",
        "    rstd = s.rolling(window=window, min_periods=min_periods).std()\n",
        "    return (s - rm) / (rstd + _eps)\n",
        "\n",
        "def rolling_slope(series: pd.Series, window: int, min_periods: int = 4):\n",
        "    s = pd.Series(series).astype(float)\n",
        "    def _slope_arr(x):\n",
        "        x = np.asarray(x, dtype=float); n = len(x)\n",
        "        if n < min_periods or not np.isfinite(x).all(): return np.nan\n",
        "        idx = np.arange(n); idxm = idx - idx.mean(); xm = x - x.mean()\n",
        "        denom = float((idxm * idxm).sum());\n",
        "        if denom == 0.0: return 0.0\n",
        "        num = float((idxm * xm).sum()); return num / denom\n",
        "    return s.rolling(window=window, min_periods=min_periods).apply(_slope_arr, raw=True)\n",
        "\n",
        "def add_causal_meta_features_safe_v3(df: pd.DataFrame,\n",
        "                                    lookback_z: int = 2000,\n",
        "                                    lookback_small: int = 200,\n",
        "                                    slope_window: int = 24,\n",
        "                                    nsr_window: int = 50,\n",
        "                                    protect_cols: set = None):\n",
        "    df = df.copy(); n_rows = len(df)\n",
        "    if protect_cols is None:\n",
        "        protect_cols = {'hour', 'weekday', 'vwap', 'y_next_ret', 'y', 'target', 'ts', 'is_imputed'}\n",
        "    eff_lookback_z = min(lookback_z, max(lookback_small + 1, n_rows // 3 if n_rows >= 10 else lookback_small + 1))\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    exclude = set(protect_cols) | {'y_next_ret', 'y', 'target', 'ts', 'regime_flag'}\n",
        "    # Prevent building z-features for columns that match SKIP patterns\n",
        "    to_z = [c for c in numeric_cols if c not in exclude and not (c.startswith('tb_') or c.startswith('_meta') or 'proba' in c or c.endswith('_label')) and not _col_matches_skip(c)]\n",
        "    newcols = {}\n",
        "    for c in to_z:\n",
        "        s = df[c].ffill()\n",
        "        newcols[f\"{c}_z\"] = causal_zscore(s, window=eff_lookback_z, min_periods=max(5, int(eff_lookback_z * 0.01)))\n",
        "        newcols[f\"{c}_z_small\"] = causal_zscore(s, window=lookback_small, min_periods=max(5, int(lookback_small * 0.01)))\n",
        "    close_col = 'close' if 'close' in df.columns else (df.columns[0] if len(df.columns) > 0 else None)\n",
        "    if close_col and not _col_matches_skip(close_col):\n",
        "        close_s = df[close_col].ffill()\n",
        "        newcols[f\"trend_slope_{slope_window}\"] = rolling_slope(close_s, window=slope_window, min_periods=max(3, slope_window // 4))\n",
        "        newcols[f\"trend_slope_{slope_window}_z\"] = causal_zscore(newcols[f\"trend_slope_{slope_window}\"].fillna(0.0), window=eff_lookback_z, min_periods=10)\n",
        "    if 'ret_1' in df.columns and not _col_matches_skip('ret_1'):\n",
        "        newcols['ret_1_abs'] = df['ret_1'].abs().fillna(0.0)\n",
        "    else:\n",
        "        if close_col and not _col_matches_skip(close_col):\n",
        "            newcols['ret_1_abs'] = df[close_col].pct_change().abs().fillna(0.0)\n",
        "        else:\n",
        "            newcols['ret_1_abs'] = pd.Series(0.0, index=df.index)\n",
        "    newcols['vol_short'] = newcols['ret_1_abs'].rolling(lookback_small, min_periods=5).std().fillna(0.0)\n",
        "    newcols['vol_long'] = newcols['ret_1_abs'].rolling(eff_lookback_z, min_periods=10).std().fillna(0.0)\n",
        "    newcols['vol_regime'] = newcols['vol_short'] / (newcols['vol_long'] + _eps)\n",
        "    normalized = {}\n",
        "    for k, v in newcols.items():\n",
        "        if isinstance(v, pd.Series):\n",
        "            normalized[k] = v.reindex(df.index).astype(float)\n",
        "        else:\n",
        "            normalized[k] = pd.Series(v, index=df.index, dtype=float)\n",
        "    newdf = pd.concat(normalized, axis=1) if normalized else pd.DataFrame(index=df.index)\n",
        "    if not newdf.empty:\n",
        "        if 'regime_flag' in newdf.columns:\n",
        "            newdf['regime_flag'] = pd.to_numeric(newdf['regime_flag'], errors='coerce').fillna(0).astype('int8')\n",
        "        float_cols = newdf.select_dtypes(include=['float64']).columns.tolist()\n",
        "        if float_cols:\n",
        "            newdf[float_cols] = newdf[float_cols].astype('float32')\n",
        "    df_out = pd.concat([df, newdf], axis=1)\n",
        "    return df_out\n",
        "\n",
        "# -----------------------\n",
        "# Safe non-scalar detector and backup+drop\n",
        "# -----------------------\n",
        "def _is_vector_like_cell_full(v):\n",
        "    if v is None or is_scalar(v):\n",
        "        return False\n",
        "    if isinstance(v, (pd.Timestamp, pd.Timedelta, np.datetime64, np.timedelta64, pd.Period)):\n",
        "        return False\n",
        "    return isinstance(v, (list, tuple, np.ndarray, pd.Series, dict, set, pd.Index, pd.Interval, pd.Categorical, pd.RangeIndex))\n",
        "\n",
        "def safe_find_non_scalar_cells(df: pd.DataFrame, cols=None, max_examples=10, sample_only=False, sample_n=500):\n",
        "    cols = list(cols) if cols is not None else list(df.columns)\n",
        "    bad = {}\n",
        "    n = len(df)\n",
        "    for c in cols:\n",
        "        try:\n",
        "            ser = df[c]\n",
        "        except Exception:\n",
        "            bad[c] = {\"count\": None, \"examples\": [f\"Cannot access column: exception\"]}\n",
        "            continue\n",
        "        if sample_only and n > sample_n:\n",
        "            half = max(1, sample_n // 2)\n",
        "            idxs = list(ser.index[:half]) + list(ser.index[-half:])\n",
        "        else:\n",
        "            idxs = list(ser.index)\n",
        "        found_idxs = []\n",
        "        for i in idxs:\n",
        "            try:\n",
        "                v = ser.loc[i]\n",
        "            except Exception:\n",
        "                try:\n",
        "                    pos = ser.index.get_loc(i)\n",
        "                    v = ser.iloc[pos]\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if _is_vector_like_cell_full(v):\n",
        "                found_idxs.append(i)\n",
        "                if len(found_idxs) >= max_examples:\n",
        "                    break\n",
        "        if found_idxs:\n",
        "            try:\n",
        "                count = sum(1 for v in ser if _is_vector_like_cell_full(v))\n",
        "            except Exception:\n",
        "                count = None\n",
        "            examples = []\n",
        "            for i in found_idxs:\n",
        "                v = ser.loc[i]\n",
        "                examples.append({\"index\": i, \"type\": type(v).__name__, \"repr\": repr(v)[:400]})\n",
        "            bad[c] = {\"count\": int(count) if count is not None else None, \"examples\": examples}\n",
        "    return bad\n",
        "\n",
        "def backup_and_drop_columns(df: pd.DataFrame, bad_cols_dict: dict, backup_dir: str, drop_mode='drop') -> Dict[str, str]:\n",
        "    Path(backup_dir).mkdir(parents=True, exist_ok=True)\n",
        "    ts = int(time.time())\n",
        "    backups = {}\n",
        "    for col in list(bad_cols_dict.keys()):\n",
        "        fname = Path(backup_dir) / f\"{col}_nonscalar_backup.v{ts}.pkl\"\n",
        "        try:\n",
        "            with open(fname, \"wb\") as f:\n",
        "                pickle.dump(df[col].astype(object), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "            backups[col] = str(fname)\n",
        "        except Exception as e:\n",
        "            backups[col] = f\"backup_failed:{e}\"\n",
        "        if drop_mode == 'drop':\n",
        "            try:\n",
        "                df.drop(columns=[col], inplace=True, errors=True)\n",
        "            except Exception:\n",
        "                pass\n",
        "        else:\n",
        "            def _flatten_val(v):\n",
        "                try:\n",
        "                    if v is None: return np.nan\n",
        "                    if is_scalar(v): return float(v) if pd.notna(v) else np.nan\n",
        "                    if isinstance(v, dict): vals = list(v.values())\n",
        "                    elif isinstance(v, pd.Series): vals = v.dropna().tolist()\n",
        "                    elif isinstance(v, (list, tuple, set, np.ndarray)): vals = list(v)\n",
        "                    else: return np.nan\n",
        "                    arr = np.array([pd.to_numeric(x, errors='coerce') for x in vals], dtype=float)\n",
        "                    arr = arr[~np.isnan(arr)]\n",
        "                    return float(arr[-1]) if arr.size else np.nan\n",
        "                except Exception:\n",
        "                    return np.nan\n",
        "            try:\n",
        "                df[col] = df[col].apply(_flatten_val)\n",
        "            except Exception:\n",
        "                try:\n",
        "                    df.drop(columns=[col], inplace=True, errors=True)\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return backups\n",
        "\n",
        "# -----------------------\n",
        "# helper: ensure tz-aware DateTimeIndex\n",
        "# -----------------------\n",
        "def _ensure_dt_index(idx, name=\"index\"):\n",
        "    try:\n",
        "        if not isinstance(idx, pd.DatetimeIndex):\n",
        "            idx = pd.to_datetime(idx, utc=True)\n",
        "        if getattr(idx, \"tz\", None) is None:\n",
        "            idx = idx.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            idx = idx.tz_convert(\"UTC\")\n",
        "        return idx\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"{name} is not datetime-like: {e}\")\n",
        "\n",
        "# -----------------------\n",
        "# Load input\n",
        "# -----------------------\n",
        "src_pkl = next((p for p in IN_PKL_CANDIDATES if os.path.exists(p)), None)\n",
        "if src_pkl is None:\n",
        "    raise FileNotFoundError(f\"No input features file found. Checked: {IN_PKL_CANDIDATES}\")\n",
        "log.info(\"STEP04: loading: %s\", src_pkl)\n",
        "t0 = time.time()\n",
        "df_in = pd.read_pickle(src_pkl)\n",
        "df_in.index = _ensure_dt_index(df_in.index, name=\"df_in.index\")\n",
        "log.info(\" - Loaded shape: %s time(s): %.2f\", df_in.shape, round(time.time() - t0, 2))\n",
        "\n",
        "# backup source file\n",
        "if BACKUP_PKL_PREFIX and BACKUP_KEEP:\n",
        "    ts = int(time.time())\n",
        "    bkp = BACKUP_PKL_PREFIX + f\"_{ts}.pkl\"\n",
        "    shutil.copy2(src_pkl, bkp)\n",
        "    log.info(\" - Backup saved -> %s\", bkp)\n",
        "\n",
        "# Preventive drop: if any input column matches skip patterns, drop it immediately (no attempt to build features from it)\n",
        "preventive_drops = [c for c in df_in.columns if _col_matches_skip(c)]\n",
        "if preventive_drops:\n",
        "    log.info(\"Preventive drop of columns matching SKIP patterns: %s\", preventive_drops)\n",
        "    # backup then drop\n",
        "    try:\n",
        "        for c in preventive_drops:\n",
        "            fname = Path(OUT_DIR) / f\"{c}_preventive_drop_bkp.v{int(time.time())}.pkl\"\n",
        "            with open(fname, \"wb\") as f:\n",
        "                pickle.dump(df_in[c].astype(object), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    except Exception as e:\n",
        "        log.warning(\"Preventive backup failed: %s\", e)\n",
        "    df_in = df_in.drop(columns=preventive_drops, errors=True)\n",
        "\n",
        "# Drop vector-like columns early (user policy: backup & drop)\n",
        "_initial_bad = safe_find_non_scalar_cells(df_in, sample_only=True, sample_n=500)\n",
        "initial_backups = {}\n",
        "if _initial_bad:\n",
        "    initial_backups = backup_and_drop_columns(df_in, _initial_bad, backup_dir=OUT_DIR, drop_mode='drop')\n",
        "    log.info(\"Dropped initial vector-like cols (backups): %s\", list(_initial_bad.keys()))\n",
        "else:\n",
        "    log.info(\"No initial vector-like columns detected (sample).\")\n",
        "\n",
        "# -----------------------\n",
        "# Run fracdiff + cusum\n",
        "# -----------------------\n",
        "log.info(\" - computing fracdiff + cusum ...\")\n",
        "t1 = time.time()\n",
        "df_fd = preprocess_fracdiff_and_cusum(df_in,\n",
        "                                      fracdiff_cols=('close', 'volume', 'obv'),\n",
        "                                      d=0.40,\n",
        "                                      fracdiff_thresh=1e-5,\n",
        "                                      fracdiff_max_len=FAST_FRACDIFF_MAX_WEIGHTS,\n",
        "                                      cusum_k=5.0,\n",
        "                                      cusum_vol_window=50,\n",
        "                                      regime_expand_N=24,\n",
        "                                      keep_original=True,\n",
        "                                      allow_small_bfill=CUSUM_ALLOW_SMALL_BFILL)\n",
        "log.info(\"   done in %.2f s\", round(time.time() - t1, 2))\n",
        "\n",
        "# -----------------------\n",
        "# Add causal meta features (skip building z for skip-pattern cols)\n",
        "# -----------------------\n",
        "log.info(\" - adding causal meta features ...\")\n",
        "t2 = time.time()\n",
        "df_meta = add_causal_meta_features_safe_v3(df_fd, lookback_z=2000, lookback_small=200, slope_window=24, nsr_window=50)\n",
        "log.info(\"   done in %.2f s\", round(time.time() - t2, 2))\n",
        "\n",
        "# -----------------------\n",
        "# Final full-scan for non-scalar cells in df_meta; backup + drop them (definitive)\n",
        "# -----------------------\n",
        "log.info(\" - scanning df_meta for any remaining non-scalar cells (full-scan)...\")\n",
        "bad_meta = safe_find_non_scalar_cells(df_meta, sample_only=False, max_examples=5)\n",
        "if bad_meta:\n",
        "    log.warning(\"Non-scalar columns detected in df_meta (will backup & drop): %s\", list(bad_meta.keys()))\n",
        "    meta_backups = backup_and_drop_columns(df_meta, bad_meta, backup_dir=OUT_DIR, drop_mode='drop')\n",
        "    log.info(\"Backups written for columns: %s\", meta_backups)\n",
        "else:\n",
        "    meta_backups = {}\n",
        "    log.info(\"No non-scalar cells detected in df_meta (post-build).\")\n",
        "\n",
        "# optional cleanup and downcast\n",
        "def drop_intermediate_cols(df: pd.DataFrame, patterns_to_drop=None):\n",
        "    patterns_to_drop = patterns_to_drop or ['__tmp*', '*_typ*', '*_d*', 'tmp_*', 'unneeded*']\n",
        "    todrop = []\n",
        "    for p in patterns_to_drop:\n",
        "        todrop.extend([c for c in df.columns if fnmatch.fnmatch(c, p)])\n",
        "    todrop = list(dict.fromkeys(todrop))\n",
        "    if todrop:\n",
        "        df = df.drop(columns=todrop, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def _downcast_numeric_df(df: pd.DataFrame):\n",
        "    float64_cols = df.select_dtypes(include=['float64']).columns.tolist()\n",
        "    if float64_cols:\n",
        "        df[float64_cols] = df[float64_cols].astype('float32')\n",
        "    int64_cols = df.select_dtypes(include=['int64']).columns.tolist()\n",
        "    if int64_cols:\n",
        "        df[int64_cols] = df[int64_cols].astype('int32')\n",
        "    return df\n",
        "\n",
        "df_meta = drop_intermediate_cols(df_meta)\n",
        "if DROP_Z_SMALL:\n",
        "    df_meta = df_meta.drop(columns=[c for c in df_meta.columns if c.endswith('_z_small')], errors='ignore')\n",
        "df_meta = _downcast_numeric_df(df_meta)\n",
        "\n",
        "# -----------------------\n",
        "# Verification / small checks (same as before)\n",
        "# -----------------------\n",
        "diag = {\"loaded_from\": src_pkl, \"time_start\": time.time(), \"n_rows\": int(len(df_meta)), \"n_cols\": int(df_meta.shape[1]), \"checks\": {}}\n",
        "diag[\"dropped_vector_like_initial\"] = {\"found\": list(_initial_bad.keys()) if _initial_bad else [], \"backups\": initial_backups}\n",
        "diag[\"dropped_vector_like_postmeta\"] = {\"found\": list(bad_meta.keys()) if bad_meta else [], \"backups\": meta_backups}\n",
        "diag[\"preventive_drops\"] = preventive_drops\n",
        "\n",
        "_leak_re = re.compile(r'(^tb_)|(^_meta)|(_meta_)|(^|_)proba(_|$)|(^target$)|(_label$)')\n",
        "leak_cols = [c for c in df_meta.columns if (isinstance(c, str) and _leak_re.search(c))]\n",
        "diag[\"checks\"][\"leak_cols_count\"] = len(leak_cols)\n",
        "diag[\"checks\"][\"leak_cols_sample\"] = leak_cols[:20]\n",
        "assert not leak_cols, f\"LEAKAGE COLUMNS FOUND — remove before training: {leak_cols[:10]}\"\n",
        "\n",
        "orig_cols = set(df_in.columns)\n",
        "new_cols = [c for c in df_meta.columns if c not in orig_cols]\n",
        "diag[\"new_cols_count\"] = len(new_cols)\n",
        "diag[\"new_cols_sample\"] = new_cols[:50]\n",
        "\n",
        "nan_rates = {c: float(df_meta[c].isna().mean()) for c in new_cols} if new_cols else {}\n",
        "diag[\"checks\"][\"new_cols_nan_rates_max\"] = max(nan_rates.values()) if nan_rates else 0.0\n",
        "diag[\"checks\"][\"new_cols_nan_high\"] = [c for c, v in nan_rates.items() if v > MAX_NAN_RATE_NEWCOL][:20]\n",
        "assert not [c for c, v in nan_rates.items() if v > MAX_NAN_RATE_NEWCOL], \\\n",
        "    f\"High NaN rate in new cols: {[c for c, v in nan_rates.items() if v > MAX_NAN_RATE_NEWCOL][:8]}\"\n",
        "\n",
        "z_cols = [c for c in df_meta.columns if isinstance(c, str) and c.endswith(\"_z\")]\n",
        "diag[\"z_cols_count\"] = len(z_cols)\n",
        "if z_cols:\n",
        "    samp = df_meta[z_cols].sample(n=min(SAMPLE_CAP, len(df_meta)), random_state=RNG_SEED) if len(df_meta) > SAMPLE_CAP else df_meta[z_cols]\n",
        "    mu_abs = float(samp.mean().abs().mean())\n",
        "    sd_mean = float(samp.std().mean())\n",
        "    diag[\"checks\"][\"z_mu_abs\"] = mu_abs\n",
        "    diag[\"checks\"][\"z_std_mean\"] = sd_mean\n",
        "    if len(df_meta) >= MIN_ROWS_FOR_Z_ASSERT:\n",
        "        assert mu_abs < Z_MEAN_ABS_MAX and (Z_STD_MEAN_RANGE[0] < sd_mean < Z_STD_MEAN_RANGE[1]), \\\n",
        "            f\"Z-features distribution suspicious: mean(abs(mean))={mu_abs:.3f}, mean(std)={sd_mean:.3f}\"\n",
        "    else:\n",
        "        warnings.warn(f\"Z-features distribution check skipped (len={len(df_meta)} < {MIN_ROWS_FOR_Z_ASSERT}).\")\n",
        "\n",
        "diag[\"time_end\"] = time.time()\n",
        "\n",
        "# Index checks\n",
        "idx_in = _ensure_dt_index(df_in.index, name=\"df_in.index\")\n",
        "idx_meta = _ensure_dt_index(df_meta.index, name=\"df_meta.index\")\n",
        "assert idx_meta.equals(idx_in), \"Index mismatch: df_meta and df_in indices must be identical (tz-normalized).\"\n",
        "assert df_meta.index.is_monotonic_increasing, \"Index is not monotonic increasing\"\n",
        "assert df_meta.index.is_unique, \"Index contains duplicates\"\n",
        "\n",
        "# Coerce remaining object/category columns (if any) as before\n",
        "obj_remaining = df_meta.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "if obj_remaining:\n",
        "    diag.setdefault('category_mappings', {})\n",
        "    coercion_attempts = {}\n",
        "    for col in list(obj_remaining):\n",
        "        s = df_meta[col]\n",
        "        converted = False\n",
        "        try:\n",
        "            name_low = col.lower()\n",
        "            vals = s.dropna().unique()[:20]\n",
        "            vals_str = set([str(v).strip().lower() for v in vals if v is not None])\n",
        "            bool_like_set = {'true', 'false', '1', '0', 'yes', 'no', 't', 'f'}\n",
        "            if name_low.endswith('_flag') or name_low.startswith('is_') or name_low.startswith('has_') or vals_str.issubset(bool_like_set):\n",
        "                def _to_flag(v):\n",
        "                    if pd.isna(v): return np.nan\n",
        "                    vs = str(v).strip().lower()\n",
        "                    if vs in ('1', 'true', 't', 'yes', 'y'): return 1\n",
        "                    if vs in ('0', 'false', 'f', 'no', 'n'): return 0\n",
        "                    try: nv = float(v); return 1 if nv != 0.0 else 0\n",
        "                    except Exception: return 0\n",
        "                df_meta[col] = s.map(_to_flag).astype('float32')\n",
        "                coercion_attempts[col] = 'flag->float32'; converted = True\n",
        "            if not converted:\n",
        "                conv = pd.to_numeric(s, errors='coerce')\n",
        "                if conv.notna().any():\n",
        "                    df_meta[col] = conv; coercion_attempts[col] = 'to_numeric'; converted = True\n",
        "            if not converted:\n",
        "                cat = s.astype('category'); codes = cat.cat.codes\n",
        "                mapping = {int(i): str(v) for i, v in enumerate(cat.cat.categories)}\n",
        "                diag['category_mappings'][col] = mapping\n",
        "                df_meta[col] = codes.astype('int16')\n",
        "                coercion_attempts[col] = f'category_codes({len(mapping)})'; converted = True\n",
        "        except Exception as e:\n",
        "            log.warning(\"Failed coercion attempt on column %s: %s\", col, e); converted = False\n",
        "    obj_remaining = df_meta.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    if obj_remaining:\n",
        "        raise AssertionError(f\"Object/category dtype columns remain in df_meta after coercion attempts: {obj_remaining[:20]}; attempts: {coercion_attempts}\")\n",
        "    else:\n",
        "        log.info(\"Converted object/category columns to numeric (coercion_attempts=%s)\", {k: v for k, v in coercion_attempts.items()})\n",
        "\n",
        "# Final non-scalar check (should be none)\n",
        "_check_sample_n = min(2000, len(df_meta))\n",
        "half = max(1, _check_sample_n // 2)\n",
        "for col in df_meta.columns:\n",
        "    if len(df_meta) <= _check_sample_n:\n",
        "        s_sample = df_meta[col]\n",
        "    else:\n",
        "        head = df_meta[col].iloc[:half]\n",
        "        tail = df_meta[col].iloc[-half:]\n",
        "        s_sample = pd.concat([head, tail])\n",
        "    if s_sample.apply(lambda v: _is_vector_like_cell_full(v)).any():\n",
        "        raise AssertionError(f\"Non-scalar cells detected in column '{col}' — should have been dropped/backed-up earlier.\")\n",
        "\n",
        "# -----------------------\n",
        "# Save artifacts\n",
        "# -----------------------\n",
        "log.info(\" - saving outputs ...\")\n",
        "ts = int(time.time())\n",
        "OUT_PKL = OUT_PKL_BASE.replace(\".pkl\", f\".v{ts}.pkl\")\n",
        "PARQUET_PATH = OUT_PKL_BASE.replace(\".pkl\", f\".v{ts}.parquet\")\n",
        "saved_files = []\n",
        "try:\n",
        "    df_meta.to_parquet(PARQUET_PATH, index=True)\n",
        "    saved_files.append(PARQUET_PATH)\n",
        "except Exception as e:\n",
        "    warnings.warn(f\"Parquet save failed ({e}), falling back to pickle+csv.\")\n",
        "    try: df_meta.to_pickle(OUT_PKL); saved_files.append(OUT_PKL)\n",
        "    except Exception as e2: warnings.warn(f\"Pickle save failed ({e2}) in fallback.\")\n",
        "    try: df_meta.to_csv(OUT_CSV); saved_files.append(OUT_CSV)\n",
        "    except Exception as e3: warnings.warn(f\"CSV save failed ({e3}) in fallback.\")\n",
        "else:\n",
        "    try: df_meta.to_pickle(OUT_PKL); saved_files.append(OUT_PKL)\n",
        "    except Exception as e: warnings.warn(f\"Pickle save failed ({e}) after parquet save.\")\n",
        "    try: df_meta.to_csv(OUT_CSV); saved_files.append(OUT_CSV)\n",
        "    except Exception as e: warnings.warn(f\"CSV save failed ({e}) after parquet save.\")\n",
        "\n",
        "with open(DIAG_JSON, \"w\") as f:\n",
        "    json.dump(diag, f, indent=2, default=lambda x: str(x))\n",
        "saved_files.append(DIAG_JSON)\n",
        "pd.DataFrame([{\"feature\": c, \"nan_rate\": nan_rates.get(c, 0.0)} for c in new_cols] if new_cols else []).to_csv(DIAG_CSV, index=False)\n",
        "saved_files.append(DIAG_CSV)\n",
        "if z_cols:\n",
        "    z_stats = pd.DataFrame({\"mean\": df_meta[z_cols].mean(), \"std\": df_meta[z_cols].std(), \"nan_rate\": df_meta[z_cols].isna().mean()}).sort_values(\"std\", ascending=False)\n",
        "    z_stats.reset_index(inplace=True); z_stats.columns = [\"feature\", \"mean\", \"std\", \"nan_rate\"]\n",
        "    z_stats.to_csv(DIAG_TOP_Z, index=False); saved_files.append(DIAG_TOP_Z)\n",
        "\n",
        "log.info(\"Saved: %s\", \", \".join(saved_files) if saved_files else \"none\")\n",
        "log.info(\"STEP04 complete | rows: %d cols: %d time_s: %.2f\", len(df_meta), df_meta.shape[1], round(time.time() - t0, 2))\n",
        "\n",
        "# -----------------------\n",
        "# Print compact diagnostics summary\n",
        "# -----------------------\n",
        "print(\"\\n=== DIAG SUMMARY (written to diag_step04.json) ===\")\n",
        "print(\"Loaded from:\", diag.get(\"loaded_from\"))\n",
        "print(\"Rows,Cols:\", diag.get(\"n_rows\"), \"x\", diag.get(\"n_cols\"))\n",
        "checks = diag.get(\"checks\", {})\n",
        "print(\"Checks summary keys:\", list(checks.keys()))\n",
        "print(\"Leak cols found:\", checks.get(\"leak_cols_count\", 0))\n",
        "print(\"New cols count:\", diag.get(\"new_cols_count\", 0))\n",
        "if diag.get(\"new_cols_sample\"):\n",
        "    print(\"New cols sample (first 30):\", diag.get(\"new_cols_sample\")[:30])\n",
        "if \"z_mu_abs\" in checks:\n",
        "    print(\"Z-features mean(abs(mean)):\", checks[\"z_mu_abs\"])\n",
        "    print(\"Z-features mean(std):\", checks[\"z_std_mean\"])\n",
        "print(\"Dropped initial vector-like (backups):\", diag.get(\"dropped_vector_like_initial\"))\n",
        "print(\"Dropped post-meta vector-like (backups):\", diag.get(\"dropped_vector_like_postmeta\"))\n",
        "print(\"Preventive column drops (skip patterns):\", preventive_drops)\n",
        "print(\"Diagnostics written to:\", DIAG_JSON, DIAG_CSV)\n",
        "if z_cols:\n",
        "    print(\"Top Z-stats written to:\", DIAG_TOP_Z)\n",
        "print(\"Quick verification PASS (no leakage asserted).\")\n",
        "\n",
        "# -----------------------\n",
        "# Lightweight unit-tests (if executed directly)\n",
        "# -----------------------\n",
        "def _test_index_coercion():\n",
        "    rng = pd.date_range(\"2025-01-01\", periods=3, freq=\"1H\")\n",
        "    df = pd.DataFrame({\"close\": [1.0, 2.0, 3.0]}, index=rng)\n",
        "    idx = _ensure_dt_index(df.index, name=\"test_idx\")\n",
        "    assert isinstance(idx, pd.DatetimeIndex) and idx.tz is not None\n",
        "\n",
        "def _test_non_scalar_detection():\n",
        "    idx = pd.date_range(\"2025-01-01\", periods=4, freq=\"1H\", tz=\"UTC\")\n",
        "    df = pd.DataFrame({\"a\": pd.Series([1,2,3,4], index=idx, dtype=object)})\n",
        "    df.loc[idx[2], \"a\"] = [1, 2]\n",
        "    sample = df.copy()\n",
        "    col = sample.columns[0]\n",
        "    assert _is_vector_like_cell_full(sample[col].iloc[2]) is True\n",
        "    s = sample[col].apply(lambda v: _is_vector_like_cell_full(v))\n",
        "    assert s.any()\n",
        "\n",
        "def _test_fracdiff_small():\n",
        "    s = pd.Series(np.arange(10.0), index=pd.date_range(\"2025-01-01\", periods=10, freq=\"1H\", tz=\"UTC\"))\n",
        "    fd0 = fracdiff_series(s, d=0.0, max_len=50)\n",
        "    assert np.allclose(fd0.fillna(0).values, s.fillna(0).values, atol=1e-9)\n",
        "\n",
        "def _test_fracdiff_max_len():\n",
        "    s = pd.Series(np.linspace(1,2,200), index=pd.date_range(\"2025-01-01\", periods=200, freq=\"1H\", tz=\"UTC\"))\n",
        "    fd = fracdiff_series(s, d=0.4, max_len=10)\n",
        "    assert isinstance(fd, pd.Series) and len(fd) == len(s)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running lightweight unit-tests...\")\n",
        "    _test_index_coercion()\n",
        "    _test_non_scalar_detection()\n",
        "    _test_fracdiff_small()\n",
        "    _test_fracdiff_max_len()\n",
        "    print(\"All tests passed (lightweight).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9hOWTMm-usg"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# B0_07_purge_utils — exposure intervals + purged CV helpers (patched)\n",
        "# - Robust t_break normalization (utc=True)\n",
        "# - Vectorized exposure->pos mapping (searchsorted)\n",
        "# - drop_unmapped=True option (default)\n",
        "# - Per-event embargo union when embargo is Timedelta\n",
        "# ============================\n",
        "import os\n",
        "import math\n",
        "from typing import List, Tuple, Optional, Iterable, Dict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --------------------------------\n",
        "# CONFIG (tweak if needed)\n",
        "# --------------------------------\n",
        "HORIZON_DEFAULT = pd.Timedelta(hours=12)\n",
        "EMBARGO_DEFAULT = pd.Timedelta(hours=1)\n",
        "MIN_EVENTS_FOR_SPLIT = 10\n",
        "\n",
        "# --------------------------------\n",
        "# Dataclass for an exposure interval (inclusive start, inclusive end semantics handled when mapping to pos)\n",
        "# --------------------------------\n",
        "@dataclass\n",
        "class ExposureInterval:\n",
        "    start_ts: pd.Timestamp\n",
        "    end_ts: pd.Timestamp\n",
        "\n",
        "    def to_tuple(self):\n",
        "        return (self.start_ts, self.end_ts)\n",
        "\n",
        "# --------------------------------\n",
        "# Utility: ensure index tz-normalized UTC\n",
        "# --------------------------------\n",
        "def _ensure_index_utc(idx: pd.Index) -> pd.DatetimeIndex:\n",
        "    if not isinstance(idx, pd.DatetimeIndex):\n",
        "        idx = pd.to_datetime(idx, utc=True)\n",
        "    if getattr(idx, \"tz\", None) is None:\n",
        "        idx = idx.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        idx = idx.tz_convert(\"UTC\")\n",
        "    return idx\n",
        "\n",
        "# --------------------------------\n",
        "# Compute exposure intervals (robust)\n",
        "# --------------------------------\n",
        "def compute_exposure_intervals(index: pd.DatetimeIndex,\n",
        "                               tbreak: pd.Series,\n",
        "                               horizon_fallback: Optional[pd.Timedelta] = None,\n",
        "                               last_index: Optional[pd.Timestamp] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Deterministic exposure intervals.\n",
        "    - Forces tz-aware t_break via pd.to_datetime(..., utc=True).\n",
        "    - If t_break is NaT -> fallback to event + horizon_fallback (clipped to last_index).\n",
        "    - Returns DataFrame indexed by event time with columns ['t_event','t_break'] (both tz-aware UTC).\n",
        "    \"\"\"\n",
        "    if horizon_fallback is None:\n",
        "        horizon_fallback = HORIZON_DEFAULT\n",
        "\n",
        "    idx = _ensure_index_utc(index)\n",
        "\n",
        "    # deterministic normalization: preserves NaT\n",
        "    tb = pd.to_datetime(tbreak, utc=True).reindex(idx)\n",
        "\n",
        "    # ensure last index tz-aware\n",
        "    if last_index is not None:\n",
        "        last_idx = pd.to_datetime(last_index, utc=True)\n",
        "        last_idx = last_idx.tz_convert(\"UTC\")\n",
        "    else:\n",
        "        last_idx = idx[-1] if len(idx) else None\n",
        "\n",
        "    # vectorized building of ends (but keep simple loop for NaT logic clarity and correctness)\n",
        "    starts = idx.values  # numpy datetime64[ns]\n",
        "    ends = []\n",
        "    for ev_ts, tb_ts in zip(idx, tb.values):\n",
        "        if pd.isna(tb_ts):\n",
        "            end_ts = ev_ts + horizon_fallback\n",
        "            if (last_idx is not None) and (end_ts > last_idx):\n",
        "                end_ts = last_idx\n",
        "            ends.append(end_ts)\n",
        "        else:\n",
        "            # tb_ts already tz-aware due to utc=True; ensure UTC timezone object\n",
        "            tbt = pd.to_datetime(tb_ts).tz_convert(\"UTC\") if getattr(tb_ts, \"tz\", None) is not None else pd.to_datetime(tb_ts).tz_localize(\"UTC\")\n",
        "            # enforce non-decreasing\n",
        "            if tbt < ev_ts:\n",
        "                # confidence-preserving correction\n",
        "                ends.append(ev_ts + pd.Timedelta(microseconds=1))\n",
        "            else:\n",
        "                ends.append(tbt)\n",
        "    out = pd.DataFrame({\"t_event\": idx, \"t_break\": ends}, index=idx)\n",
        "    return out\n",
        "\n",
        "# --------------------------------\n",
        "# Vectorized exposure -> positional intervals\n",
        "# --------------------------------\n",
        "def exposure_to_pos_intervals(exposure_df: pd.DataFrame, index: pd.DatetimeIndex) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert t_event/t_break timestamps into integer positions in provided index.\n",
        "    - pos_start: index.get_indexer(t_event)  (exact match expected, -1 if no exact mapping)\n",
        "    - pos_end: rightmost position <= t_break (searchsorted(..., side='right') - 1), -1 if NaT or before first index\n",
        "    \"\"\"\n",
        "    idx = _ensure_index_utc(index)\n",
        "    idx_vals = idx.values  # numpy datetime64[ns]\n",
        "\n",
        "    # starts and ends as arrays aligned to exposure_df.index\n",
        "    starts = exposure_df[\"t_event\"].to_numpy()\n",
        "    ends = exposure_df[\"t_break\"].to_numpy()\n",
        "\n",
        "    # pos_start: exact mapping, -1 if not found\n",
        "    pos_start = idx.get_indexer(starts).astype(int)\n",
        "\n",
        "    # pos_end: vectorized searchsorted, but handle NaT explicitly\n",
        "    # build numpy datetime64 array for ends; NaT preserved as numpy.datetime64('NaT')\n",
        "    ends_arr = np.array([np.datetime64(x) if not pd.isna(x) else np.datetime64(\"NaT\") for x in ends], dtype='datetime64[ns]')\n",
        "    # searchsorted yields insertion idx; rightmost <= end is insertion_idx - 1\n",
        "    pos_end = np.searchsorted(idx_vals, ends_arr, side='right') - 1\n",
        "    # where ends_arr is NaT, set -1\n",
        "    nan_mask = np.isnat(ends_arr)\n",
        "    if nan_mask.any():\n",
        "        pos_end[nan_mask] = -1\n",
        "    # clamp pos_end to -1..len(idx)-1\n",
        "    pos_end = pos_end.astype(int)\n",
        "    pos_end[pos_end < -1] = -1\n",
        "    pos_end[pos_end >= len(idx)] = len(idx) - 1\n",
        "\n",
        "    return pd.DataFrame({\"pos_start\": pos_start, \"pos_end\": pos_end}, index=exposure_df.index)\n",
        "\n",
        "# --------------------------------\n",
        "# Purged CV splits (with drop_unmapped + per-event embargo union)\n",
        "# --------------------------------\n",
        "def purged_cv_splits(exposure_pos_df: pd.DataFrame,\n",
        "                     n_splits: int = 5,\n",
        "                     embargo: Optional[pd.Timedelta] = None,\n",
        "                     index: Optional[pd.DatetimeIndex] = None,\n",
        "                     drop_unmapped: bool = True,\n",
        "                     random_state: Optional[int] = None) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Generator of (train_pos_array, test_pos_array) where positions are integer positional indices into the exposure_pos_df (0..N-1).\n",
        "    - drop_unmapped: if True, events with pos_start==-1 or pos_end==-1 will be excluded from splitting (recommended).\n",
        "    - embargo: pd.Timedelta or int number of bars.\n",
        "      If Timedelta, index MUST be provided and per-event embargo union will be computed.\n",
        "      If int, interpreted as number of positions to embargo on both sides of test block.\n",
        "    \"\"\"\n",
        "    N_full = len(exposure_pos_df)\n",
        "    if N_full == 0:\n",
        "        raise ValueError(\"Empty exposure_pos_df.\")\n",
        "\n",
        "    # make a working copy to potentially drop unmapped\n",
        "    exp = exposure_pos_df.copy().reset_index(drop=True)\n",
        "    pos_s = exp[\"pos_start\"].to_numpy(dtype=int)\n",
        "    pos_e = exp[\"pos_end\"].to_numpy(dtype=int)\n",
        "    indices = np.arange(len(exp), dtype=int)\n",
        "\n",
        "    # handle unmapped events\n",
        "    unmapped_mask = (pos_s == -1) | (pos_e == -1)\n",
        "    if drop_unmapped:\n",
        "        if unmapped_mask.any():\n",
        "            keep_mask = ~unmapped_mask\n",
        "            exp = exp.loc[keep_mask].reset_index(drop=True)\n",
        "            pos_s = exp[\"pos_start\"].to_numpy(dtype=int)\n",
        "            pos_e = exp[\"pos_end\"].to_numpy(dtype=int)\n",
        "            indices = np.arange(len(exp), dtype=int)\n",
        "    else:\n",
        "        # if not dropped, leave them — but downstream logic treats -1 carefully\n",
        "        pass\n",
        "\n",
        "    N = len(exp)\n",
        "    if N == 0:\n",
        "        raise ValueError(\"No valid mapped events remain after drop_unmapped=True.\")\n",
        "\n",
        "    # compute contiguous time-based folds (default) — time-contiguous avoids lookahead blending\n",
        "    # partition indices 0..N-1 into contiguous blocks\n",
        "    fold_sizes = np.full(n_splits, N // n_splits, dtype=int)\n",
        "    fold_sizes[: N % n_splits] += 1\n",
        "    test_starts = np.concatenate([[0], np.cumsum(fold_sizes)[:-1]])\n",
        "    test_ends = np.cumsum(fold_sizes) - 1\n",
        "\n",
        "    # map embargo\n",
        "    if embargo is None:\n",
        "        embargo = EMBARGO_DEFAULT\n",
        "\n",
        "    is_timedelta = isinstance(embargo, pd.Timedelta)\n",
        "    if is_timedelta and index is None:\n",
        "        raise ValueError(\"index must be provided when embargo is a Timedelta.\")\n",
        "\n",
        "    # pre-extract index numpy values if needed\n",
        "    if is_timedelta:\n",
        "        idx = _ensure_index_utc(index)\n",
        "        idx_vals = idx.values\n",
        "\n",
        "    # loop folds\n",
        "    for tstart, tend in zip(test_starts, test_ends):\n",
        "        test_idx = indices[tstart:tend+1]\n",
        "\n",
        "        # start with all observations as train candidate (relative to exp)\n",
        "        train_mask = np.ones(N, dtype=bool)\n",
        "        train_mask[test_idx] = False  # remove test samples\n",
        "\n",
        "        # --- PURGE: remove any train sample overlapping any test sample exposure (vectorized + per-test union) ---\n",
        "        # Compute union of test intervals (pos ranges) first: we will compute a conservative union span and then refine\n",
        "        # Build list of (ts, te) for test members (skip -1)\n",
        "        t_s = pos_s[test_idx]\n",
        "        t_e = pos_e[test_idx]\n",
        "        valid_test_mask = (t_s != -1) & (t_e != -1)\n",
        "        if valid_test_mask.any():\n",
        "            t_intervals = np.vstack([t_s[valid_test_mask], t_e[valid_test_mask]]).T\n",
        "            # merge intervals (sort by start)\n",
        "            order = np.argsort(t_intervals[:, 0])\n",
        "            merged = []\n",
        "            for i in order:\n",
        "                s_i, e_i = int(t_intervals[i,0]), int(t_intervals[i,1])\n",
        "                if not merged:\n",
        "                    merged.append([s_i, e_i])\n",
        "                else:\n",
        "                    if s_i <= merged[-1][1] + 1:\n",
        "                        merged[-1][1] = max(merged[-1][1], e_i)\n",
        "                    else:\n",
        "                        merged.append([s_i, e_i])\n",
        "            # For each merged test interval, mark train samples whose intervals overlap it\n",
        "            forbidden = np.zeros(N, dtype=bool)\n",
        "            for (ms, me) in merged:\n",
        "                # vectorized overlap check for this merged interval\n",
        "                overlap_mask = (pos_s <= me) & (pos_e >= ms)\n",
        "                forbidden |= overlap_mask\n",
        "            # remove forbidden from train\n",
        "            train_mask &= ~forbidden\n",
        "\n",
        "        # --- EMBARGO ---\n",
        "        if is_timedelta:\n",
        "            # Corrected embargo handling:\n",
        "            # compute per-test-event embargo bar-intervals (start_bar, end_bar),\n",
        "            # merge them and mark any event whose [pos_s,pos_e] overlaps any embargo bar interval.\n",
        "            emb_intervals = []\n",
        "            # prepare idx values (bar DatetimeIndex) and pos arrays\n",
        "            # pos_s/pos_e are bar positions already\n",
        "            for tj in test_idx:\n",
        "                s_pos = int(pos_s[tj])\n",
        "                e_pos = int(pos_e[tj])\n",
        "                if s_pos == -1 or e_pos == -1:\n",
        "                    continue\n",
        "                # map bar positions to bar timestamps using idx_vals\n",
        "                # clamp positions\n",
        "                s_pos_clamped = max(0, min(len(idx_vals)-1, s_pos))\n",
        "                e_pos_clamped = max(0, min(len(idx_vals)-1, e_pos))\n",
        "                test_start_ts = pd.Timestamp(idx_vals[s_pos_clamped], tz=\"UTC\")\n",
        "                test_end_ts = pd.Timestamp(idx_vals[e_pos_clamped], tz=\"UTC\")\n",
        "                emb_start_ts = test_start_ts - embargo\n",
        "                emb_end_ts = test_end_ts + embargo\n",
        "                emb_start_bar = np.searchsorted(idx_vals, np.datetime64(emb_start_ts), side='left')\n",
        "                emb_end_bar = np.searchsorted(idx_vals, np.datetime64(emb_end_ts), side='right') - 1\n",
        "                emb_start_bar = max(0, emb_start_bar)\n",
        "                emb_end_bar = min(len(idx_vals)-1, emb_end_bar)\n",
        "                if emb_start_bar <= emb_end_bar:\n",
        "                    emb_intervals.append([emb_start_bar, emb_end_bar])\n",
        "            # merge embargo bar-intervals\n",
        "            if emb_intervals:\n",
        "                arr = np.array(sorted(emb_intervals, key=lambda x: x[0]), dtype=int)\n",
        "                merged_emb = [list(arr[0])]\n",
        "                for s,e in arr[1:]:\n",
        "                    if s <= merged_emb[-1][1] + 1:\n",
        "                        merged_emb[-1][1] = max(merged_emb[-1][1], e)\n",
        "                    else:\n",
        "                        merged_emb.append([s,e])\n",
        "            else:\n",
        "                merged_emb = []\n",
        "            # mark events whose pos interval overlaps any merged_emb interval\n",
        "            embargo_event_mask = np.zeros(N, dtype=bool)\n",
        "            if merged_emb:\n",
        "                for (ms, me) in merged_emb:\n",
        "                    overlap_mask = (pos_s <= me) & (pos_e >= ms)\n",
        "                    embargo_event_mask |= overlap_mask\n",
        "            # never remove test events\n",
        "            embargo_event_mask[test_idx] = False\n",
        "            train_mask &= ~embargo_event_mask\n",
        "        else:\n",
        "            # embargo treated as int number of positions\n",
        "            epos = int(embargo)\n",
        "            embargo_left = max(0, tstart - epos)\n",
        "            embargo_right = min(N-1, tend + epos)\n",
        "            emb_mask = np.zeros(N, dtype=bool)\n",
        "            emb_mask[embargo_left:embargo_right+1] = True\n",
        "            emb_mask[test_idx] = False\n",
        "            train_mask &= ~emb_mask\n",
        "\n",
        "        train_idx = np.where(train_mask)[0]\n",
        "        yield (train_idx, test_idx)\n",
        "\n",
        "# --------------------------------\n",
        "# Convenience wrapper: from df + tb series produce purged splits\n",
        "# --------------------------------\n",
        "def make_purged_splits_from_df(df_index: pd.DatetimeIndex,\n",
        "                               tbreak_series: pd.Series,\n",
        "                               n_splits: int = 5,\n",
        "                               embargo: Optional[pd.Timedelta] = None,\n",
        "                               horizon_fallback: Optional[pd.Timedelta] = None,\n",
        "                               drop_unmapped: bool = True,\n",
        "                               random_state: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    High-level convenience wrapper. Returns generator of purged splits.\n",
        "    \"\"\"\n",
        "    exp = compute_exposure_intervals(df_index, tbreak_series, horizon_fallback=horizon_fallback, last_index=df_index[-1] if len(df_index)>0 else None)\n",
        "    pos = exposure_to_pos_intervals(exp, df_index)\n",
        "    return purged_cv_splits(pos, n_splits=n_splits, embargo=embargo or EMBARGO_DEFAULT, index=df_index, drop_unmapped=drop_unmapped, random_state=random_state)\n",
        "\n",
        "# --------------------------------\n",
        "# Sanity check (excludes unmapped)\n",
        "# --------------------------------\n",
        "def sanity_check_purged_split(train_pos: np.ndarray, test_pos: np.ndarray, exposure_pos_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Assert no train sample exposure overlaps any test sample exposure.\n",
        "    Unmapped events (pos_start==-1 or pos_end==-1) are excluded from check.\n",
        "    \"\"\"\n",
        "    ps = exposure_pos_df.reset_index(drop=True)\n",
        "    pos_s = ps[\"pos_start\"].to_numpy(dtype=int)\n",
        "    pos_e = ps[\"pos_end\"].to_numpy(dtype=int)\n",
        "\n",
        "    # exclude unmapped\n",
        "    mapped_train = train_pos[(pos_s[train_pos] != -1) & (pos_e[train_pos] != -1)]\n",
        "    mapped_test = test_pos[(pos_s[test_pos] != -1) & (pos_e[test_pos] != -1)]\n",
        "\n",
        "    s_train = pos_s[mapped_train] if mapped_train.size > 0 else np.array([], dtype=int)\n",
        "    e_train = pos_e[mapped_train] if mapped_train.size > 0 else np.array([], dtype=int)\n",
        "    s_test = pos_s[mapped_test] if mapped_test.size > 0 else np.array([], dtype=int)\n",
        "    e_test = pos_e[mapped_test] if mapped_test.size > 0 else np.array([], dtype=int)\n",
        "\n",
        "    for st, et in zip(s_test, e_test):\n",
        "        if st == -1 or et == -1:\n",
        "            continue\n",
        "        overlap = (s_train <= et) & (e_train >= st)\n",
        "        if overlap.any():\n",
        "            raise AssertionError(\"Purge failed: some train samples overlap test exposure interval.\")\n",
        "\n",
        "# --------------------------------\n",
        "# Example snippet (paste-run)\n",
        "# --------------------------------\n",
        "EXAMPLE_SNIPPET = \"\"\"\n",
        "# df: your feature df with tz-aware index\n",
        "# tbreak_series: df['tb_t_break_h8'] or similar\n",
        "splits = list(make_purged_splits_from_df(df.index, df['tb_t_break_h8'], n_splits=5, embargo=pd.Timedelta(hours=1), drop_unmapped=True))\n",
        "for i, (train_pos, test_pos) in enumerate(splits):\n",
        "    print(i, \"train_len\", len(train_pos), \"test_len\", len(test_pos))\n",
        "    # run sanity check:\n",
        "    exp = compute_exposure_intervals(df.index, df['tb_t_break_h8'])\n",
        "    pos = exposure_to_pos_intervals(exp, df.index)\n",
        "    sanity_check_purged_split(train_pos, test_pos, pos)\n",
        "\"\"\"\n",
        "\n",
        "# expose\n",
        "__all__ = [\n",
        "    \"ExposureInterval\",\n",
        "    \"compute_exposure_intervals\",\n",
        "    \"exposure_to_pos_intervals\",\n",
        "    \"purged_cv_splits\",\n",
        "    \"make_purged_splits_from_df\",\n",
        "    \"sanity_check_purged_split\",\n",
        "    \"HORIZON_DEFAULT\",\n",
        "    \"EMBARGO_DEFAULT\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD-VWKxLDePS",
        "outputId": "5e0e41a5-bf3e-4678-a68c-10366140255d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: compute_exposure_intervals basic mapping\n",
            "OK: exposure_to_pos_intervals exact mapping\n",
            "OK: embargo mapping consistency & purge sanity (small synthetic)\n",
            "OK: drop_unmapped behavior (artificial unmapped)\n",
            "OK: drop_unmapped=False path ok\n",
            "OK: no train/test overlap after purge (randomized larger test)\n",
            "\n",
            "=== UNIT TESTS SUMMARY ===\n",
            "ALL TESTS PASSED\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# UNIT TESTS — B0_07_purge_utils (Google Colab friendly)\n",
        "# Run this cell to execute lightweight tests (no pytest dependency).\n",
        "# ============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import traceback\n",
        "from datetime import timedelta\n",
        "\n",
        "# Helper to report\n",
        "def _ok(msg):\n",
        "    print(\"OK:\", msg)\n",
        "\n",
        "def _fail(msg):\n",
        "    print(\"FAIL:\", msg)\n",
        "\n",
        "errors = []\n",
        "\n",
        "try:\n",
        "    # Basic setup: create hourly index\n",
        "    idx = pd.date_range(\"2025-01-01 00:00\", periods=24, freq=\"1H\", tz=\"UTC\")\n",
        "\n",
        "    # 1) test_compute_exposure_intervals_basic\n",
        "    try:\n",
        "        tbreak = pd.Series(idx + pd.Timedelta(hours=2), index=idx)\n",
        "        exp = compute_exposure_intervals(idx, tbreak, horizon_fallback=pd.Timedelta(hours=12))\n",
        "        # t_break should equal event + 2h\n",
        "        diffs = (exp[\"t_break\"] - exp[\"t_event\"]).dt.total_seconds() / 3600.0\n",
        "        assert np.allclose(diffs.values, 2.0, atol=1e-9)\n",
        "        _ok(\"compute_exposure_intervals basic mapping\")\n",
        "    except Exception as e:\n",
        "        errors.append((\"compute_exposure_intervals_basic\", e))\n",
        "        _fail(\"compute_exposure_intervals basic mapping failed: \" + str(e))\n",
        "\n",
        "    # 2) test_exposure_to_pos_intervals_exact\n",
        "    try:\n",
        "        pos = exposure_to_pos_intervals(exp, idx)\n",
        "        # pos_start should be 0..23, pos_end should be pos_start + 2 (clipped)\n",
        "        expected_start = np.arange(len(idx))\n",
        "        assert (pos[\"pos_start\"].to_numpy() == expected_start).all()\n",
        "        # pos_end should be start+2 (capped at last index)\n",
        "        expected_end = np.minimum(expected_start + 2, len(idx)-1)\n",
        "        assert (pos[\"pos_end\"].to_numpy() == expected_end).all()\n",
        "        _ok(\"exposure_to_pos_intervals exact mapping\")\n",
        "    except Exception as e:\n",
        "        errors.append((\"exposure_to_pos_intervals_exact\", e))\n",
        "        _fail(\"exposure_to_pos_intervals exact mapping failed: \" + str(e))\n",
        "\n",
        "    # 3) test_embargo_mapping_consistency + purge invariant (no overlap)\n",
        "    try:\n",
        "        # events at hours 0,4,8,12,16\n",
        "        ev_idx = idx[[0,4,8,12,16]]\n",
        "        tb = pd.Series(ev_idx + pd.Timedelta(hours=2), index=ev_idx)\n",
        "        exp2 = compute_exposure_intervals(idx, tb, horizon_fallback=pd.Timedelta(hours=12))\n",
        "        pos2 = exposure_to_pos_intervals(exp2, idx)\n",
        "        splits = list(make_purged_splits_from_df(idx, tb, n_splits=2, embargo=pd.Timedelta(hours=1), drop_unmapped=True))\n",
        "        assert len(splits) == 2\n",
        "        for (train_pos, test_pos) in splits:\n",
        "            # sanity_check should not raise\n",
        "            sanity_check_purged_split(train_pos, test_pos, pos2)\n",
        "        _ok(\"embargo mapping consistency & purge sanity (small synthetic)\")\n",
        "    except Exception as e:\n",
        "        errors.append((\"embargo_mapping_consistency\", e))\n",
        "        _fail(\"embargo mapping / purge sanity failed: \" + str(e))\n",
        "\n",
        "    # 4) test_drop_unmapped_behavior\n",
        "    try:\n",
        "        # Create a pos df but inject unmapped (-1) artificially then ensure drop_unmapped removes it\n",
        "        tbreak3 = pd.Series(idx + pd.Timedelta(hours=1), index=idx)\n",
        "        exp3 = compute_exposure_intervals(idx, tbreak3)\n",
        "        pos3 = exposure_to_pos_intervals(exp3, idx)\n",
        "        pos3_mod = pos3.copy().reset_index(drop=True)\n",
        "        # artificially mark the 3rd event as unmapped\n",
        "        pos3_mod.loc[pos3_mod.index[3], \"pos_start\"] = -1\n",
        "        pos3_mod.loc[pos3_mod.index[3], \"pos_end\"] = -1\n",
        "        # run purged_cv_splits with drop_unmapped=True: should not raise and should return splits over fewer events\n",
        "        splits_with_drop = list(purged_cv_splits(pos3_mod, n_splits=3, embargo=pd.Timedelta(hours=1), index=idx, drop_unmapped=True))\n",
        "        # ensure number of events used < original\n",
        "        used_total = sum(len(t[1]) for t in splits_with_drop)\n",
        "        assert used_total <= len(pos3_mod)\n",
        "        _ok(\"drop_unmapped behavior (artificial unmapped)\")\n",
        "\n",
        "        # also test with drop_unmapped=False (should not raise)\n",
        "        splits_no_drop = list(purged_cv_splits(pos3_mod, n_splits=3, embargo=pd.Timedelta(hours=1), index=idx, drop_unmapped=False))\n",
        "        assert len(splits_no_drop) == 3\n",
        "        _ok(\"drop_unmapped=False path ok\")\n",
        "    except Exception as e:\n",
        "        errors.append((\"drop_unmapped_behavior\", e))\n",
        "        _fail(\"drop_unmapped behavior failed: \" + str(e))\n",
        "\n",
        "    # 5) test_no_train_test_overlap_after_purge (bigger random test)\n",
        "    try:\n",
        "        # random-ish tbreaks: horizon 3-6 hours\n",
        "        rng_idx = pd.date_range(\"2025-02-01 00:00\", periods=200, freq=\"H\", tz=\"UTC\")\n",
        "        # simulate tbreaks where each event has tbreak = event + U(2,6) hours, some NaT\n",
        "        import random\n",
        "        random.seed(42)\n",
        "        hours = [random.randint(2,6) for _ in range(len(rng_idx))]\n",
        "        tb_series = pd.Series([ts + pd.Timedelta(hours=h) for ts,h in zip(rng_idx, hours)], index=rng_idx)\n",
        "        # inject some NaT\n",
        "        tb_series.iloc[5] = pd.NaT\n",
        "        tb_series.iloc[50] = pd.NaT\n",
        "        exp_big = compute_exposure_intervals(rng_idx, tb_series)\n",
        "        pos_big = exposure_to_pos_intervals(exp_big, rng_idx)\n",
        "        splits_big = list(make_purged_splits_from_df(rng_idx, tb_series, n_splits=5, embargo=pd.Timedelta(hours=1), drop_unmapped=True))\n",
        "        for train_pos, test_pos in splits_big:\n",
        "            # sanity check should pass\n",
        "            sanity_check_purged_split(train_pos, test_pos, pos_big)\n",
        "        _ok(\"no train/test overlap after purge (randomized larger test)\")\n",
        "    except Exception as e:\n",
        "        errors.append((\"no_train_test_overlap_after_purge\", e))\n",
        "        _fail(\"no_train_test_overlap_after_purge failed: \" + str(e))\n",
        "\n",
        "except Exception as e_outer:\n",
        "    errors.append((\"setup_error\", e_outer))\n",
        "    print(\"Critical test harness error:\", e_outer)\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Summary\n",
        "print(\"\\n=== UNIT TESTS SUMMARY ===\")\n",
        "if not errors:\n",
        "    print(\"ALL TESTS PASSED\")\n",
        "else:\n",
        "    print(f\"{len(errors)} TEST(S) FAILED:\")\n",
        "    for name, err in errors:\n",
        "        print(\"-\", name, \"error:\", repr(err))\n",
        "    raise AssertionError(\"Some unit tests failed (see summary above).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kzpYW2TWEL3",
        "outputId": "ef69c663-58fe-44b0-f89e-ede9770ca392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created module shim 'b0_07_purge_utils'. Attached: ['ExposureInterval', 'compute_exposure_intervals', 'exposure_to_pos_intervals', 'purged_cv_splits', 'make_purged_splits_from_df', 'sanity_check_purged_split', 'HORIZON_DEFAULT', 'EMBARGO_DEFAULT']\n"
          ]
        }
      ],
      "source": [
        "# --- Run this cell once in the same notebook (after the cell where you defined purge utils) ---\n",
        "import sys, types, inspect\n",
        "\n",
        "MODULE_NAME = \"b0_07_purge_utils\"\n",
        "\n",
        "# if module already exists, remove so we rebind fresh\n",
        "if MODULE_NAME in sys.modules:\n",
        "    del sys.modules[MODULE_NAME]\n",
        "\n",
        "mod = types.ModuleType(MODULE_NAME)\n",
        "# prefer to use __all__ if defined in your cell; otherwise pick sensible names\n",
        "gl = globals()\n",
        "candidate_names = []\n",
        "if \"__all__\" in gl:\n",
        "    candidate_names = list(gl[\"__all__\"])\n",
        "else:\n",
        "    # common symbols we expect (edit if your names differ)\n",
        "    candidate_names = [\n",
        "        \"ExposureInterval\",\n",
        "        \"compute_exposure_intervals\",\n",
        "        \"exposure_to_pos_intervals\",\n",
        "        \"purged_cv_splits\",\n",
        "        \"make_purged_splits_from_df\",\n",
        "        \"sanity_check_purged_split\",\n",
        "        \"HORIZON_DEFAULT\",\n",
        "        \"EMBARGO_DEFAULT\"\n",
        "    ]\n",
        "\n",
        "# attach available symbols from globals into the module\n",
        "attached = []\n",
        "missing = []\n",
        "for name in candidate_names:\n",
        "    if name in gl:\n",
        "        setattr(mod, name, gl[name])\n",
        "        attached.append(name)\n",
        "    else:\n",
        "        missing.append(name)\n",
        "\n",
        "# add module to sys.modules so normal `import b0_07_purge_utils` works\n",
        "sys.modules[MODULE_NAME] = mod\n",
        "\n",
        "print(f\"Created module shim '{MODULE_NAME}'. Attached: {attached}\")\n",
        "if missing:\n",
        "    print(\"Note: these names were not found in globals (maybe you used different names):\", missing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoFyKixyO7_f"
      },
      "outputs": [],
      "source": [
        "# ===== B1_08_cpcv_grid (CPCV + Purge+Embargo) — PATCHED =====\n",
        "# Requirements: numpy, pandas, sklearn, xgboost, imblearn (optional)\n",
        "# Assumes purged utilities are importable:\n",
        "# from b0_07_purge_utils import make_purged_splits_from_df, compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits, sanity_check_purged_split\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception:\n",
        "    SMOTE = None\n",
        "\n",
        "# defaults\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# NOTE: removed unused placeholder/_fit_and_predict_xgb to avoid confusion\n",
        "\n",
        "# --- Core CPCV grid runner (patched) ---\n",
        "def run_cpcv_grid(df_features: pd.DataFrame,\n",
        "                  tbreak_series: pd.Series,\n",
        "                  feature_cols: list,\n",
        "                  side: str = \"long\",                    # \"long\" or \"short\"\n",
        "                  label_col: str = \"tb_label_h8\",        # triple barrier label column\n",
        "                  n_outer: int = 5,\n",
        "                  n_inner: int = 3,\n",
        "                  embargo: pd.Timedelta = pd.Timedelta(\"1H\"),\n",
        "                  drop_unmapped: bool = True,\n",
        "                  random_state: Optional[int] = 42,\n",
        "                  scaler_type = StandardScaler,\n",
        "                  use_smote: bool = False,               # default False: prefer sample_weight for time-series\n",
        "                  grid: Dict[str, list] = None,\n",
        "                  xgb_common: Dict[str, Any] = None,\n",
        "                  out_dir: str = OUT_DIR,\n",
        "                  max_inner_budget: int = 200):\n",
        "    \"\"\"\n",
        "    CPCV grid runner (patched):\n",
        "      - inner_gen -> inner_splits = list(...) to avoid generator exhaustion\n",
        "      - XGBClassifier seeded via random_state\n",
        "      - SMOTE seeded via random_state (if used)\n",
        "      - preference: sample_weight (balanced) for time-series; SMOTE optional with warning\n",
        "      - grid/fold budget capped (deterministic sampling of grid if budget too large)\n",
        "    Returns dict with OOF preds, grid results, and saved models.\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    out = {\"meta\": {}, \"grid_results\": [], \"oof\": None, \"models\": []}\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # label transform (per-side)\n",
        "    assert side in (\"long\",\"short\")\n",
        "    side_val = 1 if side == \"long\" else -1\n",
        "    y_all = (df_features[label_col] == side_val).astype(int).reindex(df_features.index).fillna(0).astype(int)\n",
        "\n",
        "    X_all = df_features[feature_cols].copy()\n",
        "    idx = df_features.index\n",
        "\n",
        "    # exposure pos dataframe (used for purged splits)\n",
        "    exp = compute_exposure_intervals(idx, tbreak_series, horizon_fallback=None, last_index=idx[-1])\n",
        "    pos = exposure_to_pos_intervals(exp, idx)\n",
        "\n",
        "    # prepare OOF container\n",
        "    oof_preds = pd.Series(index=idx, dtype=float)\n",
        "    oof_fold = pd.Series(index=idx, dtype=int).fillna(-1)\n",
        "\n",
        "    # default grid\n",
        "    if grid is None:\n",
        "        grid = {\n",
        "            \"max_depth\": [3,5],\n",
        "            \"learning_rate\": [0.05, 0.1],\n",
        "            \"subsample\": [0.8],\n",
        "            \"colsample_bytree\": [0.8],\n",
        "            \"n_estimators\": [200],\n",
        "        }\n",
        "    if xgb_common is None:\n",
        "        xgb_common = {\"verbosity\": 0}\n",
        "\n",
        "    # compute full config list\n",
        "    all_cfgs = list(ParameterGrid(grid))\n",
        "    # budget check: total inner trainings = len(cfgs) * n_inner\n",
        "    total_inner_runs = len(all_cfgs) * max(1, n_inner)\n",
        "    if total_inner_runs > max_inner_budget:\n",
        "        # deterministic sampling of configs to respect budget\n",
        "        rng = np.random.RandomState(random_state)\n",
        "        max_cfgs_allowed = max(1, max_inner_budget // max(1, n_inner))\n",
        "        # sample without replacement deterministically\n",
        "        idxs = rng.choice(len(all_cfgs), size=max_cfgs_allowed, replace=False)\n",
        "        sampled_cfgs = [all_cfgs[i] for i in sorted(idxs)]\n",
        "        print(f\"[WARN] Grid x inner budget {total_inner_runs} > {max_inner_budget}. Sampling {len(sampled_cfgs)} configs deterministically (random_state={random_state}).\")\n",
        "        all_cfgs = sampled_cfgs\n",
        "\n",
        "    # outer CPCV - use purged_cv_splits for time-contiguous outer folds\n",
        "    outer_gen = purged_cv_splits(pos, n_splits=n_outer, embargo=embargo, index=idx, drop_unmapped=drop_unmapped, random_state=random_state)\n",
        "\n",
        "    fold_id = 0\n",
        "    for train_pos_outer, test_pos_outer in outer_gen:\n",
        "        fold_id += 1\n",
        "        test_idx = pos.index[test_pos_outer]              # event timestamps (index into pos DataFrame)\n",
        "        # Train indices as timestamps\n",
        "        train_pos_outer = np.array(train_pos_outer, dtype=int)\n",
        "        train_idx_timestamps = pos.index[train_pos_outer]\n",
        "\n",
        "        X_train_outer = X_all.loc[train_idx_timestamps]\n",
        "        y_train_outer = y_all.loc[train_idx_timestamps]\n",
        "        X_test_outer = X_all.loc[test_idx]\n",
        "        y_test_outer = y_all.loc[test_idx]\n",
        "\n",
        "        # inner CPCV on X_train_outer for hyperparam tuning\n",
        "        # build exposure/pos for the TRAIN partition only (must reflect original pos coordinates)\n",
        "        exp_train = compute_exposure_intervals(train_idx_timestamps, tbreak_series.reindex(train_idx_timestamps), horizon_fallback=None, last_index=idx[-1])\n",
        "        pos_train = exposure_to_pos_intervals(exp_train, idx)\n",
        "        # important: create inner_splits as list to avoid generator exhaustion\n",
        "        inner_splits = list(purged_cv_splits(pos_train, n_splits=n_inner, embargo=embargo, index=idx, drop_unmapped=True, random_state=random_state))\n",
        "        if len(inner_splits) == 0:\n",
        "            # fallback: single split (train only)\n",
        "            inner_splits = [(np.arange(len(pos_train)), np.array([], dtype=int))]\n",
        "\n",
        "        best_cfg = None\n",
        "        best_score = -np.inf\n",
        "        cfg_results = []\n",
        "\n",
        "        # iterate deterministic set of configs (all_cfgs possibly sampled above)\n",
        "        for cfg in all_cfgs:\n",
        "            # per-cfg inner CV average score\n",
        "            inner_scores = []\n",
        "            for train_pos_inner, val_pos_inner in inner_splits:\n",
        "                # map positions -> timestamps\n",
        "                train_ts_inner = pos_train.index[train_pos_inner]\n",
        "                val_ts_inner = pos_train.index[val_pos_inner]\n",
        "\n",
        "                X_tr = X_all.loc[train_ts_inner]\n",
        "                y_tr = y_all.loc[train_ts_inner]\n",
        "                X_val = X_all.loc[val_ts_inner]\n",
        "                y_val = y_all.loc[val_ts_inner]\n",
        "\n",
        "                # scale (fit only on train)\n",
        "                scaler = scaler_type()\n",
        "                X_tr_s = scaler.fit_transform(X_tr)\n",
        "                X_val_s = scaler.transform(X_val)\n",
        "\n",
        "                # sample weight preferred for time-series; compute balanced weights\n",
        "                sw = compute_sample_weight(class_weight='balanced', y=y_tr)\n",
        "\n",
        "                # SMOTE: optional and explicit; prefer sample_weight, but honor flag with warning\n",
        "                if use_smote and (SMOTE is not None):\n",
        "                    print(\"[WARN] use_smote=True chosen. SMOTE can produce time-order-violating synthetic samples; prefer sample_weight for time-series.\")\n",
        "                    sm = SMOTE(random_state=random_state)\n",
        "                    try:\n",
        "                        X_tr_s, y_tr_res = sm.fit_resample(X_tr_s, y_tr)\n",
        "                        # recompute sample weights on resampled set\n",
        "                        sw = compute_sample_weight(class_weight='balanced', y=y_tr_res)\n",
        "                    except Exception as e:\n",
        "                        print(f\"[WARN] SMOTE failed for inner split: {e}. Falling back to sample_weight only.\")\n",
        "                # assemble xgb params\n",
        "                xgb_params = deepcopy(cfg)\n",
        "                xgb_params.update(xgb_common)\n",
        "                # ensure reproducibility\n",
        "                if random_state is not None:\n",
        "                    xgb_params['random_state'] = random_state\n",
        "\n",
        "                # fit simple xgb\n",
        "                model = XGBClassifier(**xgb_params, use_label_encoder=False, eval_metric='logloss')\n",
        "                try:\n",
        "                    model.fit(X_tr_s, y_tr, sample_weight=sw, verbose=False)\n",
        "                except TypeError:\n",
        "                    # older xgboost versions may not accept sample_weight as kw for fit in same way; fallback to without sample_weight\n",
        "                    model.fit(X_tr_s, y_tr, verbose=False)\n",
        "\n",
        "                # predict on val\n",
        "                p_val = model.predict_proba(X_val_s)[:,1] if len(X_val_s) > 0 else np.array([])\n",
        "                auc = roc_auc_score(y_val, p_val) if (len(p_val) > 0 and len(np.unique(y_val))>1) else 0.5\n",
        "                inner_scores.append(auc)\n",
        "\n",
        "            mean_inner = float(np.mean(inner_scores)) if inner_scores else 0.0\n",
        "            cfg_results.append({\"cfg\": cfg, \"mean_inner_auc\": mean_inner})\n",
        "            if mean_inner > best_score:\n",
        "                best_score = mean_inner\n",
        "                best_cfg = deepcopy(cfg)\n",
        "\n",
        "        # train final model on outer train with best_cfg\n",
        "        scaler = scaler_type()\n",
        "        X_tr_full = scaler.fit_transform(X_train_outer)\n",
        "        y_tr_full = y_train_outer.values.copy()\n",
        "        # final sample weights on full outer train\n",
        "        sw_full = compute_sample_weight(class_weight='balanced', y=y_tr_full)\n",
        "\n",
        "        if use_smote and (SMOTE is not None):\n",
        "            print(\"[WARN] use_smote=True chosen at final train. Prefer sample_weight for time-series; using SMOTE will create synthetic points.\")\n",
        "            sm = SMOTE(random_state=random_state)\n",
        "            try:\n",
        "                X_tr_full, y_tr_full = sm.fit_resample(X_tr_full, y_tr_full)\n",
        "                sw_full = compute_sample_weight(class_weight='balanced', y=y_tr_full)\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] SMOTE failed at final train: {e}. Continuing with sample_weight only.\")\n",
        "\n",
        "        final_params = deepcopy(best_cfg) if best_cfg is not None else {}\n",
        "        final_params.update(xgb_common or {})\n",
        "        if random_state is not None:\n",
        "            final_params['random_state'] = random_state\n",
        "\n",
        "        model_final = XGBClassifier(**final_params, use_label_encoder=False, eval_metric='logloss')\n",
        "        try:\n",
        "            model_final.fit(X_tr_full, y_tr_full, sample_weight=sw_full, verbose=False)\n",
        "        except TypeError:\n",
        "            model_final.fit(X_tr_full, y_tr_full, verbose=False)\n",
        "\n",
        "        # predict on outer test\n",
        "        X_test_s = scaler.transform(X_test_outer)\n",
        "        p_test = model_final.predict_proba(X_test_s)[:,1]\n",
        "\n",
        "        # store OOF\n",
        "        oof_preds.loc[X_test_outer.index] = p_test\n",
        "        oof_fold.loc[X_test_outer.index] = fold_id\n",
        "\n",
        "        # store model & fold metadata\n",
        "        out['models'].append({\"fold\": fold_id, \"side\": side, \"model\": model_final, \"scaler\": scaler, \"cfg\": best_cfg})\n",
        "\n",
        "        # record grid results for this fold\n",
        "        out['grid_results'].append({\"fold\": fold_id, \"best_cfg\": best_cfg, \"best_score\": best_score, \"cfg_summary\": cfg_results})\n",
        "\n",
        "        # small save per-fold\n",
        "        fold_path = os.path.join(out_dir, f\"cpcv_{side}_fold{fold_id}.pkl\")\n",
        "        try:\n",
        "            pd.to_pickle({\"model\": model_final, \"scaler\": scaler, \"cfg\": best_cfg}, fold_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # final packaging\n",
        "    out[\"oof\"] = oof_preds\n",
        "    out[\"oof_fold\"] = oof_fold\n",
        "    out[\"meta\"][\"time_s\"] = time.time() - t0\n",
        "    out[\"meta\"][\"n_splits_outer\"] = n_outer\n",
        "    out[\"meta\"][\"n_splits_inner\"] = n_inner\n",
        "    out[\"meta\"][\"drop_unmapped\"] = drop_unmapped\n",
        "    out[\"meta\"][\"use_smote\"] = bool(use_smote)\n",
        "    # save OOF to disk\n",
        "    oof_path = os.path.join(out_dir, f\"oof_{side}.csv\")\n",
        "    oof_preds.to_csv(oof_path, index=True)\n",
        "    # save summary\n",
        "    json_path = os.path.join(out_dir, f\"cpcv_{side}_summary.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump({\"meta\": out[\"meta\"], \"n_models\": len(out[\"models\"])}, f, default=str)\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# Usage example:\n",
        "# -------------------------\n",
        "# features_df = pd.read_pickle(\"/content/drive/.../df_meta_step04.vXXX.pkl\")  # df_meta from Step04\n",
        "# tbreak = features_df[\"tb_t_break_h8\"]\n",
        "# feat_cols = [c for c in features_df.columns if not c.startswith(\"tb_\") and not c.endswith(\"_z\")]  # example\n",
        "# res_long = run_cpcv_grid(features_df, tbreak, feat_cols, side=\"long\", n_outer=5, n_inner=3, embargo=pd.Timedelta(\"1H\"), use_smote=False)\n",
        "# res_short = run_cpcv_grid(features_df, tbreak, feat_cols, side=\"short\", n_outer=5, n_inner=3, embargo=pd.Timedelta(\"1H\"), use_smote=False)\n",
        "# -------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQTLQm9zPn3-",
        "outputId": "2023fb8f-9795-49cd-f529-8dd09b5a2e85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using meta: df_meta_step04.v1764900161.pkl\n",
            "[INFO] Using TB  : df_step03_tb_multi_20251205T015857Z.pkl\n",
            "[INFO] Loaded df_meta: (17521, 229), df_tb: (17521, 17)  (load time: 0.03s)\n",
            "[OK] Saved merged features+TB -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl  shape=(17521, 238)\n",
            "[INFO] Example TB columns: ['tb_label_h4', 'tb_t_break_h4', 'tb_ret_at_break_h4', 'tb_label_h8', 'tb_t_break_h8', 'tb_ret_at_break_h8'] ...\n",
            "[INFO] Using label column: tb_label_h8, t_break col: tb_t_break_h8\n",
            "[INFO] Using 156 feature cols (example first 10): ['open', 'high', 'low', 'close', 'volume', 'is_imputed', 'volume_imputed_flag', 'hl2', 'hlc3', 'ohlc4']\n",
            "\n",
            "[INFO] Starting smoke-run CPCV (long)...\n",
            "[OK] Long smoke-run done in 16.3s | models: 3\n",
            "\n",
            "[INFO] Starting smoke-run CPCV (short)...\n",
            "[OK] Short smoke-run done in 17.6s | models: 3\n",
            "\n",
            "=== DIAG: LONG ===\n",
            "models_saved: 3\n",
            "OOF non-nan preds: 17521  / total rows: 17521\n",
            "TB counts (merged): {'long': 6531, 'short': 10976, 'neutral': 14}\n",
            " outer_fold 1: train_len=11677 test_len=5841\n",
            "   sanity_check: PASS\n",
            " outer_fold 2: train_len=11670 test_len=5840\n",
            "   sanity_check: PASS\n",
            " outer_fold 3: train_len=11678 test_len=5840\n",
            "   sanity_check: PASS\n",
            "\n",
            "=== DIAG: SHORT ===\n",
            "models_saved: 3\n",
            "OOF non-nan preds: 17521  / total rows: 17521\n",
            "TB counts (merged): {'long': 6531, 'short': 10976, 'neutral': 14}\n",
            " outer_fold 1: train_len=11677 test_len=5841\n",
            "   sanity_check: PASS\n",
            " outer_fold 2: train_len=11670 test_len=5840\n",
            "   sanity_check: PASS\n",
            " outer_fold 3: train_len=11678 test_len=5840\n",
            "   sanity_check: PASS\n",
            "\n",
            "[ALL DONE] If diagnostics look OK, re-run run_cpcv_grid with your full grid/budgets.\n"
          ]
        }
      ],
      "source": [
        "# === ONE-CLICK: merge TB -> meta, smoke-run CPCV, diagnostics & sanity checks ===\n",
        "# Run this in Colab after you have executed previous cells that define:\n",
        "#   - purged utils (compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits, sanity_check_purged_split)\n",
        "#   - run_cpcv_grid (patched version)\n",
        "# If run_cpcv_grid is not defined in the notebook, this cell will stop and explain.\n",
        "\n",
        "import os, glob, time, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) find latest meta and tb artifacts\n",
        "meta_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"df_meta_step04*.pkl\")), key=os.path.getmtime)\n",
        "tb_candidates   = sorted(glob.glob(os.path.join(OUT_DIR, \"df_step03_tb_multi_*.pkl\")), key=os.path.getmtime)\n",
        "\n",
        "if not meta_candidates:\n",
        "    raise FileNotFoundError(f\"No df_meta_step04*.pkl found in {OUT_DIR}. Run Step04 first.\")\n",
        "if not tb_candidates:\n",
        "    raise FileNotFoundError(f\"No df_step03_tb_multi_*.pkl found in {OUT_DIR}. Generate TB (Step03) first.\")\n",
        "\n",
        "meta_path = meta_candidates[-1]\n",
        "tb_path   = tb_candidates[-1]\n",
        "\n",
        "print(f\"[INFO] Using meta: {os.path.basename(meta_path)}\")\n",
        "print(f\"[INFO] Using TB  : {os.path.basename(tb_path)}\")\n",
        "\n",
        "# 2) load\n",
        "t0 = time.time()\n",
        "df_meta = pd.read_pickle(meta_path)\n",
        "df_tb   = pd.read_pickle(tb_path)\n",
        "print(f\"[INFO] Loaded df_meta: {df_meta.shape}, df_tb: {df_tb.shape}  (load time: {time.time()-t0:.2f}s)\")\n",
        "\n",
        "# 3) canonicalize indices (UTC)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "df_tb.index   = pd.to_datetime(df_tb.index, utc=True)\n",
        "\n",
        "# 4) select TB columns to merge (conservative)\n",
        "tb_cols = [c for c in df_tb.columns if c.startswith(\"tb_label_\") or c.startswith(\"tb_t_break_\") or c.startswith(\"tb_ret_at_break_\")]\n",
        "if not tb_cols:\n",
        "    raise RuntimeError(\"No TB columns found in TB artifact (expected tb_label_*, tb_t_break_*, tb_ret_at_break_*).\")\n",
        "\n",
        "# 5) merge (left join on meta index)\n",
        "df_merged = df_meta.join(df_tb[tb_cols], how=\"left\")\n",
        "merged_path = os.path.join(OUT_DIR, \"df_features_with_tb.pkl\")\n",
        "df_merged.to_pickle(merged_path)\n",
        "print(f\"[OK] Saved merged features+TB -> {merged_path}  shape={df_merged.shape}\")\n",
        "\n",
        "# 6) quick sanity of TB presence (example horizon 8)\n",
        "example_h = None\n",
        "for s in tb_cols:\n",
        "    if s.startswith(\"tb_label_h\"):\n",
        "        example_h = s.split(\"tb_label_h\")[-1]\n",
        "        break\n",
        "if example_h is None:\n",
        "    # pick first label name\n",
        "    example_h = tb_cols[0]\n",
        "print(f\"[INFO] Example TB columns: {tb_cols[:6]} ...\")\n",
        "\n",
        "# 7) prepare for CPCV smoke-run\n",
        "# check run_cpcv_grid availability\n",
        "if 'run_cpcv_grid' not in globals():\n",
        "    raise RuntimeError(\"run_cpcv_grid not present in notebook. Paste/run the patched run_cpcv_grid cell before executing this orchestration cell.\")\n",
        "\n",
        "# pick TB column for horizon 8 if present, else use first tb_label_*\n",
        "label_cols = [c for c in df_merged.columns if c.startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"Merged DF has no tb_label_* columns.\")\n",
        "# prefer h8 if exists\n",
        "tb_label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tb_tbreak_col = tb_label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "print(f\"[INFO] Using label column: {tb_label_col}, t_break col: {tb_tbreak_col}\")\n",
        "\n",
        "# 8) build feature column list (exclude tb_ and _z to avoid leakage and meta-only z-features if desired)\n",
        "feat_cols = [c for c in df_merged.columns if not c.startswith(\"tb_\") and not c.endswith(\"_z\")]\n",
        "print(f\"[INFO] Using {len(feat_cols)} feature cols (example first 10): {feat_cols[:10]}\")\n",
        "\n",
        "# 9) short smoke-run params (fast): small grid & budget to keep test quick\n",
        "grid_small = {\n",
        "    \"max_depth\": [3],\n",
        "    \"learning_rate\": [0.05],\n",
        "    \"subsample\": [0.8],\n",
        "    \"colsample_bytree\": [0.8],\n",
        "    \"n_estimators\": [50]\n",
        "}\n",
        "# call run_cpcv_grid for both sides using small budget (smoke-run)\n",
        "smoke_kwargs = dict(\n",
        "    df_features=df_merged,\n",
        "    tbreak_series=df_merged[tb_tbreak_col],\n",
        "    feature_cols=feat_cols,\n",
        "    n_outer=3,\n",
        "    n_inner=2,\n",
        "    embargo=pd.Timedelta(\"1H\"),\n",
        "    drop_unmapped=True,\n",
        "    random_state=42,\n",
        "    use_smote=False,\n",
        "    grid=grid_small,\n",
        "    xgb_common={\"verbosity\":0},\n",
        "    max_inner_budget=100,\n",
        "    out_dir=OUT_DIR\n",
        ")\n",
        "\n",
        "print(\"\\n[INFO] Starting smoke-run CPCV (long)...\")\n",
        "t0 = time.time()\n",
        "res_long = run_cpcv_grid(side=\"long\", **smoke_kwargs)\n",
        "print(f\"[OK] Long smoke-run done in {time.time()-t0:.1f}s | models: {len(res_long['models'])}\")\n",
        "\n",
        "print(\"\\n[INFO] Starting smoke-run CPCV (short)...\")\n",
        "t0 = time.time()\n",
        "res_short = run_cpcv_grid(side=\"short\", **smoke_kwargs)\n",
        "print(f\"[OK] Short smoke-run done in {time.time()-t0:.1f}s | models: {len(res_short['models'])}\")\n",
        "\n",
        "# 10) Diagnostics summary\n",
        "def print_diagnostics(res, name):\n",
        "    oof = res.get(\"oof\")\n",
        "    oof_fold = res.get(\"oof_fold\")\n",
        "    models = res.get(\"models\", [])\n",
        "    print(f\"\\n=== DIAG: {name} ===\")\n",
        "    print(\"models_saved:\", len(models))\n",
        "    if oof is None:\n",
        "        print(\"OOF: None\")\n",
        "    else:\n",
        "        n_oof_nonan = int(oof.notna().sum())\n",
        "        print(\"OOF non-nan preds:\", n_oof_nonan, \" / total rows:\", len(oof))\n",
        "    # mapped TB counts\n",
        "    tb_label = df_merged[tb_label_col].fillna(0).astype(int)\n",
        "    pos_counts = { \"long\": int((tb_label==1).sum()), \"short\": int((tb_label==-1).sum()), \"neutral\": int((tb_label==0).sum()) }\n",
        "    print(\"TB counts (merged):\", pos_counts)\n",
        "    # per-fold sample sizes if we can reproduce outer splits\n",
        "    try:\n",
        "        from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits\n",
        "        exp = compute_exposure_intervals(df_merged.index, df_merged[tb_tbreak_col], horizon_fallback=None, last_index=df_merged.index[-1])\n",
        "        pos = exposure_to_pos_intervals(exp, df_merged.index)\n",
        "        outer = list(purged_cv_splits(pos, n_splits=3, embargo=pd.Timedelta(\"1H\"), index=df_merged.index, drop_unmapped=True, random_state=42))\n",
        "        for i,(tr,te) in enumerate(outer,1):\n",
        "            print(f\" outer_fold {i}: train_len={len(tr)} test_len={len(te)}\")\n",
        "            # run sanity check on this fold (should not raise)\n",
        "            sanity_check_purged_split(tr, te, pos)\n",
        "            print(\"   sanity_check: PASS\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not run per-fold diagnostics (purged utils missing or error):\", e)\n",
        "\n",
        "print_diagnostics(res_long, \"LONG\")\n",
        "print_diagnostics(res_short, \"SHORT\")\n",
        "\n",
        "print(\"\\n[ALL DONE] If diagnostics look OK, re-run run_cpcv_grid with your full grid/budgets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc_ulmM9WqUg",
        "outputId": "c27b983b-da48-4e54-c524-736b0ede5b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "START: load latest df_meta_step04.v*.pkl and TB artifact (df_step03_tb_multi_*.pkl).\n",
            "meta: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_step04.v1764900161.pkl\n",
            "tb: /content/drive/MyDrive/quant_pipeline/mtb_out/df_step03_tb_multi_20251205T015857Z.pkl\n",
            "Using label: tb_label_h8 t_break: tb_t_break_h8 features: 156\n",
            "TB counts: {'long': 6531, 'short': 10976, 'neu': 14}\n",
            "\n",
            "RUN CPCV (LONG)...\n",
            "DONE LONG time_s: 29.62208080291748 models: 3\n",
            "\n",
            "RUN CPCV (SHORT)...\n",
            "DONE SHORT time_s: 27.607036590576172 models: 3\n",
            "\n",
            "=== DIAG LONG ===\n",
            "models: 3\n",
            "OOF non-null: 17521 total: 17521\n",
            "OOF AUC (long): 0.5103974688365935\n",
            "OOF AUC (short): 0.4898418690296155\n",
            " fold 1 train_len 11678 test_len 5841\n",
            "  sanity: PASS\n",
            " fold 2 train_len 11671 test_len 5840\n",
            "  sanity: PASS\n",
            " fold 3 train_len 11678 test_len 5840\n",
            "  sanity: PASS\n",
            "\n",
            "=== DIAG SHORT ===\n",
            "models: 3\n",
            "OOF non-null: 17521 total: 17521\n",
            "OOF AUC (long): 0.48887703482892325\n",
            "OOF AUC (short): 0.5110023299672373\n",
            " fold 1 train_len 11678 test_len 5841\n",
            "  sanity: PASS\n",
            " fold 2 train_len 11671 test_len 5840\n",
            "  sanity: PASS\n",
            " fold 3 train_len 11678 test_len 5840\n",
            "  sanity: PASS\n"
          ]
        }
      ],
      "source": [
        "# === SELF-CONTAINED COLAB CELL (FIXED embargo indexing) ===\n",
        "# Paste & run. This fixes AttributeError: 'DatetimeIndex' object has no attribute 'iloc'\n",
        "import os, time, glob, json\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "from typing import Iterable, Tuple, Optional, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# CONFIG\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Minimal patched purge utils (fixed embargo indexing)\n",
        "def _ensure_index_utc(idx):\n",
        "    if not isinstance(idx, pd.DatetimeIndex):\n",
        "        idx = pd.to_datetime(idx, utc=True)\n",
        "    if getattr(idx, \"tz\", None) is None:\n",
        "        idx = idx.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        idx = idx.tz_convert(\"UTC\")\n",
        "    return idx\n",
        "\n",
        "def compute_exposure_intervals(index: pd.DatetimeIndex,\n",
        "                               tbreak: pd.Series,\n",
        "                               horizon_fallback: Optional[pd.Timedelta] = None,\n",
        "                               last_index: Optional[pd.Timestamp] = None) -> pd.DataFrame:\n",
        "    if horizon_fallback is None:\n",
        "        horizon_fallback = pd.Timedelta(hours=12)\n",
        "    idx = _ensure_index_utc(index)\n",
        "    tb = pd.to_datetime(tbreak, utc=True).reindex(idx)\n",
        "    if last_index is not None:\n",
        "        last_idx = pd.to_datetime(last_index, utc=True)\n",
        "        last_idx = last_idx.tz_convert(\"UTC\")\n",
        "    else:\n",
        "        last_idx = idx[-1] if len(idx) else None\n",
        "    ends = []\n",
        "    for ev_ts, tb_ts in zip(idx, tb.values):\n",
        "        if pd.isna(tb_ts):\n",
        "            end_ts = ev_ts + horizon_fallback\n",
        "            if (last_idx is not None) and (end_ts > last_idx):\n",
        "                end_ts = last_idx\n",
        "            ends.append(end_ts)\n",
        "        else:\n",
        "            tbt = pd.to_datetime(tb_ts)\n",
        "            if getattr(tbt, \"tz\", None) is None:\n",
        "                tbt = tbt.tz_localize(\"UTC\")\n",
        "            else:\n",
        "                tbt = tbt.tz_convert(\"UTC\")\n",
        "            if tbt < ev_ts:\n",
        "                ends.append(ev_ts + pd.Timedelta(microseconds=1))\n",
        "            else:\n",
        "                ends.append(tbt)\n",
        "    out = pd.DataFrame({\"t_event\": idx, \"t_break\": ends}, index=idx)\n",
        "    return out\n",
        "\n",
        "def exposure_to_pos_intervals(exposure_df: pd.DataFrame, index: pd.DatetimeIndex) -> pd.DataFrame:\n",
        "    idx = _ensure_index_utc(index)\n",
        "    idx_vals = idx.values\n",
        "    starts = exposure_df[\"t_event\"].to_numpy()\n",
        "    ends = exposure_df[\"t_break\"].to_numpy()\n",
        "    pos_start = idx.get_indexer(starts).astype(int)\n",
        "    ends_arr = np.array([np.datetime64(x) if not pd.isna(x) else np.datetime64(\"NaT\") for x in ends], dtype='datetime64[ns]')\n",
        "    pos_end = np.searchsorted(idx_vals, ends_arr, side='right') - 1\n",
        "    nan_mask = np.isnat(ends_arr)\n",
        "    if nan_mask.any():\n",
        "        pos_end[nan_mask] = -1\n",
        "    pos_end = pos_end.astype(int)\n",
        "    pos_end[pos_end < -1] = -1\n",
        "    pos_end[pos_end >= len(idx)] = len(idx) - 1\n",
        "    return pd.DataFrame({\"pos_start\": pos_start, \"pos_end\": pos_end}, index=exposure_df.index)\n",
        "\n",
        "def purged_cv_splits(exposure_pos_df: pd.DataFrame,\n",
        "                     n_splits: int = 5,\n",
        "                     embargo: Optional[pd.Timedelta] = None,\n",
        "                     index: Optional[pd.DatetimeIndex] = None,\n",
        "                     drop_unmapped: bool = True,\n",
        "                     random_state: Optional[int] = None) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "    if len(exposure_pos_df) == 0:\n",
        "        raise ValueError(\"Empty exposure_pos_df.\")\n",
        "    exp = exposure_pos_df.copy().reset_index(drop=True)\n",
        "    pos_s = exp[\"pos_start\"].to_numpy(dtype=int)\n",
        "    pos_e = exp[\"pos_end\"].to_numpy(dtype=int)\n",
        "    unmapped_mask = (pos_s == -1) | (pos_e == -1)\n",
        "    if drop_unmapped and unmapped_mask.any():\n",
        "        keep_mask = ~unmapped_mask\n",
        "        exp = exp.loc[keep_mask].reset_index(drop=True)\n",
        "        pos_s = exp[\"pos_start\"].to_numpy(dtype=int)\n",
        "        pos_e = exp[\"pos_end\"].to_numpy(dtype=int)\n",
        "    N = len(exp)\n",
        "    if N == 0:\n",
        "        raise ValueError(\"No valid mapped events remain after drop_unmapped=True.\")\n",
        "    fold_sizes = np.full(n_splits, N // n_splits, dtype=int)\n",
        "    fold_sizes[: N % n_splits] += 1\n",
        "    test_starts = np.concatenate([[0], np.cumsum(fold_sizes)[:-1]])\n",
        "    test_ends = np.cumsum(fold_sizes) - 1\n",
        "    if embargo is None:\n",
        "        embargo = pd.Timedelta(hours=1)\n",
        "    is_timedelta = isinstance(embargo, pd.Timedelta)\n",
        "    if is_timedelta and index is None:\n",
        "        raise ValueError(\"index must be provided when embargo is a Timedelta.\")\n",
        "    if is_timedelta:\n",
        "        idx = _ensure_index_utc(index)\n",
        "        idx_vals = idx.values\n",
        "    indices = np.arange(N, dtype=int)\n",
        "    for tstart, tend in zip(test_starts, test_ends):\n",
        "        test_idx = indices[tstart:tend+1]\n",
        "        train_mask = np.ones(N, dtype=bool)\n",
        "        train_mask[test_idx] = False\n",
        "        # PURGE: merge test pos intervals then remove overlapping train\n",
        "        t_s = pos_s[test_idx]; t_e = pos_e[test_idx]\n",
        "        valid_test_mask = (t_s != -1) & (t_e != -1)\n",
        "        if valid_test_mask.any():\n",
        "            t_intervals = np.vstack([t_s[valid_test_mask], t_e[valid_test_mask]]).T\n",
        "            order = np.argsort(t_intervals[:, 0])\n",
        "            merged = []\n",
        "            for i in order:\n",
        "                s_i, e_i = int(t_intervals[i,0]), int(t_intervals[i,1])\n",
        "                if not merged:\n",
        "                    merged.append([s_i, e_i])\n",
        "                else:\n",
        "                    if s_i <= merged[-1][1] + 1:\n",
        "                        merged[-1][1] = max(merged[-1][1], e_i)\n",
        "                    else:\n",
        "                        merged.append([s_i, e_i])\n",
        "            forbidden = np.zeros(N, dtype=bool)\n",
        "            for (ms, me) in merged:\n",
        "                overlap_mask = (pos_s <= me) & (pos_e >= ms)\n",
        "                forbidden |= overlap_mask\n",
        "            train_mask &= ~forbidden\n",
        "        # EMBARGO (FIXED indexing)\n",
        "        if is_timedelta:\n",
        "            embargo_masks = np.zeros(N, dtype=bool)\n",
        "            # keep event_times as DatetimeIndex (index positions correspond to exp.index entries)\n",
        "            event_times = pd.to_datetime(exp.index)\n",
        "            # use integer indexing against DatetimeIndex directly (no .iloc)\n",
        "            for tj in test_idx:\n",
        "                s_pos = int(pos_s[tj]); e_pos = int(pos_e[tj])\n",
        "                if s_pos == -1 or e_pos == -1:\n",
        "                    continue\n",
        "                # safe accesses: event_times[...] supports integer position indexing; use -1 for fallback\n",
        "                test_end_ts = event_times[e_pos] if (0 <= e_pos < len(event_times)) else event_times[-1]\n",
        "                embargo_end_ts = test_end_ts + embargo\n",
        "                embargo_end_pos = np.searchsorted(idx_vals, np.datetime64(embargo_end_ts), side='right') - 1\n",
        "                embargo_start_ts = event_times[s_pos] - embargo if (0 <= s_pos < len(event_times)) else event_times[0] - embargo\n",
        "                embargo_start_pos = np.searchsorted(idx_vals, np.datetime64(embargo_start_ts), side='left')\n",
        "                left = max(0, int(embargo_start_pos))\n",
        "                right = min(len(idx_vals) - 1, int(embargo_end_pos)) if embargo_end_pos >= 0 else -1\n",
        "                if right >= left:\n",
        "                    embargo_masks[left:right+1] = True\n",
        "            embargo_masks[test_idx] = False\n",
        "            train_mask &= ~embargo_masks\n",
        "        else:\n",
        "            epos = int(embargo)\n",
        "            embargo_left = max(0, tstart - epos)\n",
        "            embargo_right = min(N-1, tend + epos)\n",
        "            emb_mask = np.zeros(N, dtype=bool)\n",
        "            emb_mask[embargo_left:embargo_right+1] = True\n",
        "            emb_mask[test_idx] = False\n",
        "            train_mask &= ~emb_mask\n",
        "        train_idx = np.where(train_mask)[0]\n",
        "        yield (train_idx, test_idx)\n",
        "\n",
        "def sanity_check_purged_split(train_pos: np.ndarray, test_pos: np.ndarray, exposure_pos_df: pd.DataFrame):\n",
        "    ps = exposure_pos_df.reset_index(drop=True)\n",
        "    pos_s = ps[\"pos_start\"].to_numpy(dtype=int)\n",
        "    pos_e = ps[\"pos_end\"].to_numpy(dtype=int)\n",
        "    mapped_train = train_pos[(pos_s[train_pos] != -1) & (pos_e[train_pos] != -1)]\n",
        "    mapped_test = test_pos[(pos_s[test_pos] != -1) & (pos_e[test_pos] != -1)]\n",
        "    s_train = pos_s[mapped_train] if mapped_train.size > 0 else np.array([], dtype=int)\n",
        "    e_train = pos_e[mapped_train] if mapped_train.size > 0 else np.array([], dtype=int)\n",
        "    s_test = pos_s[mapped_test] if mapped_test.size > 0 else np.array([], dtype=int)\n",
        "    e_test = pos_e[mapped_test] if mapped_test.size > 0 else np.array([], dtype=int)\n",
        "    for st, et in zip(s_test, e_test):\n",
        "        if st == -1 or et == -1:\n",
        "            continue\n",
        "        overlap = (s_train <= et) & (e_train >= st)\n",
        "        if overlap.any():\n",
        "            raise AssertionError(\"Purge failed: some train samples overlap test exposure interval.\")\n",
        "\n",
        "# patched run_cpcv_grid (unchanged logic except using the fixed purged functions)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception:\n",
        "    SMOTE = None\n",
        "\n",
        "def run_cpcv_grid(df_features: pd.DataFrame,\n",
        "                  tbreak_series: pd.Series,\n",
        "                  feature_cols: list,\n",
        "                  side: str = \"long\",\n",
        "                  label_col: str = \"tb_label_h8\",\n",
        "                  n_outer: int = 5,\n",
        "                  n_inner: int = 3,\n",
        "                  embargo: pd.Timedelta = pd.Timedelta(\"1H\"),\n",
        "                  drop_unmapped: bool = True,\n",
        "                  random_state: Optional[int] = RANDOM_STATE,\n",
        "                  scaler_type = StandardScaler,\n",
        "                  use_smote: bool = False,\n",
        "                  grid: Dict[str, list] = None,\n",
        "                  xgb_common: Dict[str, Any] = None,\n",
        "                  out_dir: str = OUT_DIR,\n",
        "                  max_inner_budget: int = 200):\n",
        "    t0 = time.time()\n",
        "    out = {\"meta\": {}, \"grid_results\": [], \"oof\": None, \"models\": []}\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "    assert side in (\"long\",\"short\")\n",
        "    side_val = 1 if side == \"long\" else -1\n",
        "    y_all = (df_features[label_col] == side_val).astype(int).reindex(df_features.index).fillna(0).astype(int)\n",
        "    X_all = df_features[feature_cols].copy()\n",
        "    idx = df_features.index\n",
        "    exp = compute_exposure_intervals(idx, tbreak_series, horizon_fallback=None, last_index=idx[-1])\n",
        "    pos = exposure_to_pos_intervals(exp, idx)\n",
        "    oof_preds = pd.Series(index=idx, dtype=float)\n",
        "    oof_fold = pd.Series(index=idx, dtype=int).fillna(-1)\n",
        "    if grid is None:\n",
        "        grid = {\"max_depth\":[3,5],\"learning_rate\":[0.05,0.1],\"subsample\":[0.8],\"colsample_bytree\":[0.8],\"n_estimators\":[200]}\n",
        "    if xgb_common is None:\n",
        "        xgb_common = {\"verbosity\": 0}\n",
        "    all_cfgs = list(ParameterGrid(grid))\n",
        "    total_inner_runs = len(all_cfgs) * max(1, n_inner)\n",
        "    if total_inner_runs > max_inner_budget:\n",
        "        rng = np.random.RandomState(random_state)\n",
        "        max_cfgs_allowed = max(1, max_inner_budget // max(1, n_inner))\n",
        "        idxs = rng.choice(len(all_cfgs), size=max_cfgs_allowed, replace=False)\n",
        "        sampled_cfgs = [all_cfgs[i] for i in sorted(idxs)]\n",
        "        print(f\"[WARN] sampling {len(sampled_cfgs)} configs out of {len(all_cfgs)} to respect budget.\")\n",
        "        all_cfgs = sampled_cfgs\n",
        "    outer_gen = purged_cv_splits(pos, n_splits=n_outer, embargo=embargo, index=idx, drop_unmapped=drop_unmapped, random_state=random_state)\n",
        "    fold_id = 0\n",
        "    for train_pos_outer, test_pos_outer in outer_gen:\n",
        "        fold_id += 1\n",
        "        test_idx = pos.index[test_pos_outer]\n",
        "        train_pos_outer = np.array(train_pos_outer, dtype=int)\n",
        "        train_idx_timestamps = pos.index[train_pos_outer]\n",
        "        X_train_outer = X_all.loc[train_idx_timestamps]\n",
        "        y_train_outer = y_all.loc[train_idx_timestamps]\n",
        "        X_test_outer = X_all.loc[test_idx]\n",
        "        y_test_outer = y_all.loc[test_idx]\n",
        "        exp_train = compute_exposure_intervals(train_idx_timestamps, tbreak_series.reindex(train_idx_timestamps), horizon_fallback=None, last_index=idx[-1])\n",
        "        pos_train = exposure_to_pos_intervals(exp_train, idx)\n",
        "        inner_splits = list(purged_cv_splits(pos_train, n_splits=n_inner, embargo=embargo, index=idx, drop_unmapped=True, random_state=random_state))\n",
        "        if len(inner_splits) == 0:\n",
        "            inner_splits = [(np.arange(len(pos_train)), np.array([], dtype=int))]\n",
        "        best_cfg = None\n",
        "        best_score = -np.inf\n",
        "        cfg_results = []\n",
        "        for cfg in all_cfgs:\n",
        "            inner_scores = []\n",
        "            for train_pos_inner, val_pos_inner in inner_splits:\n",
        "                train_ts_inner = pos_train.index[train_pos_inner]\n",
        "                val_ts_inner = pos_train.index[val_pos_inner]\n",
        "                X_tr = X_all.loc[train_ts_inner]\n",
        "                y_tr = y_all.loc[train_ts_inner]\n",
        "                X_val = X_all.loc[val_ts_inner]\n",
        "                y_val = y_all.loc[val_ts_inner]\n",
        "                scaler = scaler_type()\n",
        "                X_tr_s = scaler.fit_transform(X_tr)\n",
        "                X_val_s = scaler.transform(X_val)\n",
        "                sw = compute_sample_weight(class_weight='balanced', y=y_tr)\n",
        "                if use_smote and (SMOTE is not None):\n",
        "                    print(\"[WARN] SMOTE enabled; this can introduce time-order leakage. Prefer sample_weight.\")\n",
        "                    sm = SMOTE(random_state=random_state)\n",
        "                    try:\n",
        "                        X_tr_s, y_tr_res = sm.fit_resample(X_tr_s, y_tr)\n",
        "                        sw = compute_sample_weight(class_weight='balanced', y=y_tr_res)\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] SMOTE failed:\", e)\n",
        "                xgb_params = deepcopy(cfg)\n",
        "                xgb_params.update(xgb_common)\n",
        "                if random_state is not None:\n",
        "                    xgb_params['random_state'] = int(random_state)\n",
        "                model = XGBClassifier(**xgb_params, use_label_encoder=False, eval_metric='logloss')\n",
        "                try:\n",
        "                    model.fit(X_tr_s, y_tr, sample_weight=sw, verbose=False)\n",
        "                except TypeError:\n",
        "                    model.fit(X_tr_s, y_tr, verbose=False)\n",
        "                p_val = model.predict_proba(X_val_s)[:,1] if len(X_val_s) > 0 else np.array([])\n",
        "                auc = roc_auc_score(y_val, p_val) if (len(p_val) > 0 and len(np.unique(y_val))>1) else 0.5\n",
        "                inner_scores.append(auc)\n",
        "            mean_inner = float(np.mean(inner_scores)) if inner_scores else 0.0\n",
        "            cfg_results.append({\"cfg\": cfg, \"mean_inner_auc\": mean_inner})\n",
        "            if mean_inner > best_score:\n",
        "                best_score = mean_inner\n",
        "                best_cfg = deepcopy(cfg)\n",
        "        scaler = scaler_type()\n",
        "        X_tr_full = scaler.fit_transform(X_train_outer)\n",
        "        y_tr_full = y_train_outer.values.copy()\n",
        "        sw_full = compute_sample_weight(class_weight='balanced', y=y_tr_full)\n",
        "        if use_smote and (SMOTE is not None):\n",
        "            print(\"[WARN] SMOTE on final train enabled; prefer sample_weight.\")\n",
        "            sm = SMOTE(random_state=random_state)\n",
        "            try:\n",
        "                X_tr_full, y_tr_full = sm.fit_resample(X_tr_full, y_tr_full)\n",
        "                sw_full = compute_sample_weight(class_weight='balanced', y=y_tr_full)\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] SMOTE final failed:\", e)\n",
        "        final_params = deepcopy(best_cfg) if best_cfg is not None else {}\n",
        "        final_params.update(xgb_common or {})\n",
        "        if random_state is not None:\n",
        "            final_params['random_state'] = int(random_state)\n",
        "        model_final = XGBClassifier(**final_params, use_label_encoder=False, eval_metric='logloss')\n",
        "        try:\n",
        "            model_final.fit(X_tr_full, y_tr_full, sample_weight=sw_full, verbose=False)\n",
        "        except TypeError:\n",
        "            model_final.fit(X_tr_full, y_tr_full, verbose=False)\n",
        "        X_test_s = scaler.transform(X_test_outer)\n",
        "        p_test = model_final.predict_proba(X_test_s)[:,1]\n",
        "        oof_preds.loc[X_test_outer.index] = p_test\n",
        "        oof_fold.loc[X_test_outer.index] = fold_id\n",
        "        out['models'].append({\"fold\": fold_id, \"side\": side, \"model\": model_final, \"scaler\": scaler, \"cfg\": best_cfg})\n",
        "        out['grid_results'].append({\"fold\": fold_id, \"best_cfg\": best_cfg, \"best_score\": best_score, \"cfg_summary\": cfg_results})\n",
        "        fold_path = os.path.join(out_dir, f\"cpcv_{side}_fold{fold_id}.pkl\")\n",
        "        try:\n",
        "            pd.to_pickle({\"model\": model_final, \"scaler\": scaler, \"cfg\": best_cfg}, fold_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    out[\"oof\"] = oof_preds\n",
        "    out[\"oof_fold\"] = oof_fold\n",
        "    out[\"meta\"][\"time_s\"] = time.time() - t0\n",
        "    out[\"meta\"][\"n_splits_outer\"] = n_outer\n",
        "    out[\"meta\"][\"n_splits_inner\"] = n_inner\n",
        "    out[\"meta\"][\"drop_unmapped\"] = drop_unmapped\n",
        "    out[\"meta\"][\"use_smote\"] = bool(use_smote)\n",
        "    oof_path = os.path.join(out_dir, f\"oof_{side}.csv\")\n",
        "    try:\n",
        "        out[\"oof\"].to_csv(oof_path, index=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    json_path = os.path.join(out_dir, f\"cpcv_{side}_summary.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump({\"meta\": out[\"meta\"], \"n_models\": len(out[\"models\"])}, f, default=str)\n",
        "    return out\n",
        "\n",
        "# Orchestration (same as previously used)\n",
        "print(\"START: load latest df_meta_step04.v*.pkl and TB artifact (df_step03_tb_multi_*.pkl).\")\n",
        "meta_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"df_meta_step04.v*.pkl\")), key=os.path.getmtime)\n",
        "tb_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"df_step03_tb_multi_*.pkl\")), key=os.path.getmtime)\n",
        "if not meta_candidates:\n",
        "    raise FileNotFoundError(\"No df_meta_step04.v*.pkl found in OUT_DIR.\")\n",
        "if not tb_candidates:\n",
        "    raise FileNotFoundError(\"No df_step03_tb_multi_*.pkl found in OUT_DIR.\")\n",
        "meta_path = meta_candidates[-1]; tb_path = tb_candidates[-1]\n",
        "print(\"meta:\", meta_path); print(\"tb:\", tb_path)\n",
        "df_meta = pd.read_pickle(meta_path); df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "df_tb   = pd.read_pickle(tb_path);   df_tb.index   = pd.to_datetime(df_tb.index, utc=True)\n",
        "tb_cols = [c for c in df_tb.columns if c.startswith(\"tb_label_\") or c.startswith(\"tb_t_break_\") or c.startswith(\"tb_ret_at_break_\")]\n",
        "if not tb_cols:\n",
        "    raise RuntimeError(\"No TB columns in TB artifact.\")\n",
        "df = df_meta.join(df_tb[tb_cols], how=\"left\")\n",
        "label_cols = [c for c in df.columns if c.startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"No tb_label_* after merge.\")\n",
        "tb_label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tb_tbreak_col = tb_label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "feat_cols = [c for c in df.columns if not c.startswith(\"tb_\") and not c.endswith(\"_z\")]\n",
        "print(\"Using label:\", tb_label_col, \"t_break:\", tb_tbreak_col, \"features:\", len(feat_cols))\n",
        "tb_label = df[tb_label_col].fillna(0).astype(int)\n",
        "print(\"TB counts:\", {\"long\": int((tb_label==1).sum()), \"short\": int((tb_label==-1).sum()), \"neu\": int((tb_label==0).sum())})\n",
        "\n",
        "# Quick run (small grid)\n",
        "grid = {\"max_depth\":[3],\"learning_rate\":[0.05],\"subsample\":[0.8],\"colsample_bytree\":[0.8],\"n_estimators\":[50]}\n",
        "run_kwargs = dict(df_features=df, tbreak_series=df[tb_tbreak_col], feature_cols=feat_cols,\n",
        "                  n_outer=3, n_inner=2, embargo=pd.Timedelta(\"1H\"), drop_unmapped=True,\n",
        "                  random_state=RANDOM_STATE, use_smote=False, grid=grid, xgb_common={\"verbosity\":0}, max_inner_budget=200, out_dir=OUT_DIR)\n",
        "print(\"\\nRUN CPCV (LONG)...\")\n",
        "t0 = time.time()\n",
        "res_long = run_cpcv_grid(side=\"long\", **run_kwargs)\n",
        "print(\"DONE LONG time_s:\", time.time()-t0, \"models:\", len(res_long['models']))\n",
        "print(\"\\nRUN CPCV (SHORT)...\")\n",
        "t0 = time.time()\n",
        "res_short = run_cpcv_grid(side=\"short\", **run_kwargs)\n",
        "print(\"DONE SHORT time_s:\", time.time()-t0, \"models:\", len(res_short['models']))\n",
        "\n",
        "# Diagnostics (sanity_check per outer fold)\n",
        "def summarize(res, name):\n",
        "    print(f\"\\n=== DIAG {name} ===\")\n",
        "    print(\"models:\", len(res.get(\"models\", [])))\n",
        "    oof = res.get(\"oof\")\n",
        "    if oof is not None:\n",
        "        print(\"OOF non-null:\", int(oof.notna().sum()), \"total:\", len(oof))\n",
        "    try:\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "        preds = oof.fillna(0.5).astype(float)\n",
        "        lab_long = (df[tb_label_col]==1).astype(int)\n",
        "        lab_short = (df[tb_label_col]==-1).astype(int)\n",
        "        if lab_long.nunique() > 1:\n",
        "            print(\"OOF AUC (long):\", roc_auc_score(lab_long, preds))\n",
        "        if lab_short.nunique() > 1:\n",
        "            print(\"OOF AUC (short):\", roc_auc_score(lab_short, preds))\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        pos_exp = compute_exposure_intervals(df.index, df[tb_tbreak_col], horizon_fallback=None, last_index=df.index[-1])\n",
        "        pos = exposure_to_pos_intervals(pos_exp, df.index)\n",
        "        outer = list(purged_cv_splits(pos, n_splits=run_kwargs['n_outer'], embargo=run_kwargs['embargo'], index=df.index, drop_unmapped=True, random_state=run_kwargs['random_state']))\n",
        "        for i, (tr,te) in enumerate(outer,1):\n",
        "            print(\" fold\", i, \"train_len\", len(tr), \"test_len\", len(te))\n",
        "            sanity_check_purged_split(tr, te, pos)\n",
        "            print(\"  sanity: PASS\")\n",
        "    except Exception as e:\n",
        "        print(\"fold sanity skipped:\", e)\n",
        "\n",
        "summarize(res_long, \"LONG\")\n",
        "summarize(res_short, \"SHORT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIyxRX4_eDuP",
        "outputId": "c4f8b460-58b3-450a-8640-785941aa1f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading meta: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl\n",
            "[INFO] df shape: (17521, 238)\n",
            "[INFO] candidate features: 155\n",
            "[INFO] label: tb_label_h8\n",
            "[INFO] models found -> long: 3  short: 3\n",
            "[INFO] processing cpcv_long_fold1.pkl\n",
            "[WARN] model does not expose feature_names; assuming global candidates list.\n",
            "[WARN] permutation failed for cpcv_long_fold1.pkl : multi_class must be in ('ovo', 'ovr')\n",
            "[INFO] processing cpcv_long_fold2.pkl\n",
            "[WARN] model does not expose feature_names; assuming global candidates list.\n",
            "[WARN] permutation failed for cpcv_long_fold2.pkl : multi_class must be in ('ovo', 'ovr')\n",
            "[INFO] processing cpcv_long_fold3.pkl\n",
            "[WARN] model does not expose feature_names; assuming global candidates list.\n",
            "[WARN] permutation failed for cpcv_long_fold3.pkl : multi_class must be in ('ovo', 'ovr')\n",
            "[INFO] processing cpcv_short_fold1.pkl\n",
            "[WARN] model does not expose feature_names; assuming global candidates list.\n",
            "[WARN] permutation failed for cpcv_short_fold1.pkl : multi_class must be in ('ovo', 'ovr')\n",
            "[INFO] processing cpcv_short_fold2.pkl\n",
            "[WARN] model does not expose feature_names; assuming global candidates list.\n",
            "[WARN] permutation failed for cpcv_short_fold2.pkl : multi_class must be in ('ovo', 'ovr')\n",
            "[INFO] processing cpcv_short_fold3.pkl\n",
            "[WARN] model does not expose feature_names; assuming global candidates list.\n",
            "[WARN] permutation failed for cpcv_short_fold3.pkl : multi_class must be in ('ovo', 'ovr')\n",
            "[OK] saved shortlist -> /content/drive/MyDrive/quant_pipeline/mtb_out/shortlist_selected_top30_inter_1764900275.pkl\n",
            "\n",
            "=== DONE ===\n",
            "Top long shortlist (n=30): ['cs_spread_2_z_small', 'rsi_14_z_small', 'vol_compression_ratio', 'kyle_lambda_50', 'macd', 'ema_100', 'ret_1_abs', 'ema_100_z_small', 'ret_mean_168_z_small', 'ret_skew_24', 'ret_skew_72_z_small', 'ret_kurt_72', 'ret_skew_72', 'ret_mean_72_z_small', 'keltner_h_z_small', 'pvo', 'ret_mean_24_z_small', 'midprice_z_small', 'kyle_lambda_50_z_small', 'ret_kurt_168_z_small', 'keltner_l_z_small', 'ret_skew_24_z_small', 'tick_sign_z_small', 'ret_std_168_z_small', 'bb_low_z_small', 'ret_kurt_72_z_small', 'ret_skew_168_z_small', 'gk_vol_50_z_small', 'high_z_small', 'atr_rel_z_small']\n",
            "Top short shortlist (n=30): ['rsi_14_z_small', 'ret_skew_24', 'kama_10', 'ret_1_abs', 'kyle_lambda_50', 'ret_mean_168_z_small', 'ret_skew_72_z_small', 'keltner_h_z_small', 'ema_100_z_small', 'macd', 'ret_skew_72', 'midprice_z_small', 'vpin_proxy_rolling_z_small', 'ret_kurt_72', 'ret_skew_168_z_small', 'ppo_z_small', 'kyle_lambda_50_z_small', 'tick_sign_z_small', 'low_z_small', 'ret_kurt_168_z_small', 'gk_vol_50_z_small', 'pvo', 'ret_std_168_z_small', 'atr_rel_z_small', 'bb_low_z_small', 'high_z_small', 'ret_kurt_72_z_small', 'hl2_z_small', 'ret_mean_72_z_small', 'keltner_l_z_small']\n",
            "CSVs + per-fold CSVs saved in OUT_DIR.\n"
          ]
        }
      ],
      "source": [
        "# === FEATURE IMPORTANCE (FINAL PATCHED) ===\n",
        "# Robust per-fold feature-name alignment + SHAP/permutation fallback\n",
        "import os, glob, time, json, pickle, math\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "TS = int(time.time())\n",
        "\n",
        "# Tunables\n",
        "TOP_K = 30\n",
        "SHAP_SUBSAMPLE = 2000\n",
        "PERM_SUBSAMPLE = 5000\n",
        "PERM_N_REPEATS = 6\n",
        "PERM_RANDOM_STATE = 42\n",
        "\n",
        "def latest(path_glob):\n",
        "    lst = sorted(glob.glob(path_glob), key=os.path.getmtime)\n",
        "    return lst[-1] if lst else None\n",
        "\n",
        "# load merged features\n",
        "meta_path = latest(os.path.join(OUT_DIR, \"df_features_with_tb*.pkl\")) or latest(os.path.join(OUT_DIR, \"df_meta_step04*.pkl\"))\n",
        "if meta_path is None:\n",
        "    raise FileNotFoundError(\"No df_features_with_tb*.pkl or df_meta_step04*.pkl in OUT_DIR.\")\n",
        "print(\"[INFO] loading meta:\", meta_path)\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "print(\"[INFO] df shape:\", df.shape)\n",
        "\n",
        "# candidate numeric features (exclude tb_ and _z)\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "candidates = [c for c in num_cols if (not str(c).startswith(\"tb_\")) and (not str(c).endswith(\"_z\"))]\n",
        "print(\"[INFO] candidate features:\", len(candidates))\n",
        "\n",
        "# label\n",
        "label_cols = [c for c in df.columns if str(c).startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"No tb_label_* found.\")\n",
        "label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "print(\"[INFO] label:\", label_col)\n",
        "\n",
        "# model files\n",
        "models_long = sorted(glob.glob(os.path.join(OUT_DIR, \"cpcv_long_fold*.pkl\")))\n",
        "models_short = sorted(glob.glob(os.path.join(OUT_DIR, \"cpcv_short_fold*.pkl\")))\n",
        "print(\"[INFO] models found -> long:\", len(models_long), \" short:\", len(models_short))\n",
        "if not (models_long or models_short):\n",
        "    raise FileNotFoundError(\"No CPCV fold model files found (cpcv_*_fold*.pkl). Run CPCV first.\")\n",
        "\n",
        "# load model helper\n",
        "def load_model_obj(path):\n",
        "    try:\n",
        "        obj = pd.read_pickle(path)\n",
        "    except Exception:\n",
        "        with open(path, \"rb\") as f:\n",
        "            obj = pickle.load(f)\n",
        "    if isinstance(obj, dict) and \"model\" in obj:\n",
        "        return obj[\"model\"]\n",
        "    return obj\n",
        "\n",
        "# try shap\n",
        "try:\n",
        "    import shap\n",
        "except Exception as e:\n",
        "    shap = None\n",
        "    print(\"[WARN] shap unavailable:\", e)\n",
        "\n",
        "# predictable pred_proba that accepts feature alignment\n",
        "def pred_proba_safe(model, X_df, feature_names):\n",
        "    X_sub = X_df.reindex(columns=feature_names, fill_value=0.0)\n",
        "    X_arr = np.asarray(X_sub)\n",
        "    # try predict_proba\n",
        "    try:\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            p = model.predict_proba(X_arr)\n",
        "            if p.ndim == 1:\n",
        "                p = np.vstack([1-p, p]).T\n",
        "            return p[:,1]\n",
        "    except Exception:\n",
        "        pass\n",
        "    # try xgboost Booster / DMatrix path\n",
        "    try:\n",
        "        import xgboost as xgb\n",
        "        if hasattr(model, \"get_booster\") or hasattr(model, \"booster_\"):\n",
        "            # many xgb wrappers expose predict_proba\n",
        "            try:\n",
        "                p = model.predict_proba(X_arr)\n",
        "                if p.ndim == 1:\n",
        "                    p = np.vstack([1-p, p]).T\n",
        "                return p[:,1]\n",
        "            except Exception:\n",
        "                d = xgb.DMatrix(X_arr, feature_names=feature_names)\n",
        "                raw = model.get_booster().predict(d) if hasattr(model, \"get_booster\") else model.predict(d)\n",
        "                if raw.ndim == 1:\n",
        "                    return raw\n",
        "                return raw[:, -1]\n",
        "    except Exception:\n",
        "        pass\n",
        "    # fallback to predict\n",
        "    try:\n",
        "        p0 = model.predict(X_arr)\n",
        "        return (np.asarray(p0) == 1).astype(float)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Model has no usable predict_proba/predict: {e}\")\n",
        "\n",
        "# manual permutation importance aligned to model feature names\n",
        "def manual_permutation_importance(model, X_df, y_ser, feature_names, n_repeats=6, subsample=5000, random_state=42):\n",
        "    rng = check_random_state(random_state)\n",
        "    n = len(X_df)\n",
        "    if n > subsample:\n",
        "        idx = rng.choice(n, size=subsample, replace=False)\n",
        "        Xs = X_df.iloc[idx].reset_index(drop=True)\n",
        "        ys = y_ser.iloc[idx].reset_index(drop=True)\n",
        "    else:\n",
        "        Xs = X_df.reset_index(drop=True)\n",
        "        ys = y_ser.reset_index(drop=True)\n",
        "    # base predict on aligned features\n",
        "    base_p = pred_proba_safe(model, Xs, feature_names)\n",
        "    base_auc = np.nan if len(np.unique(ys)) < 2 else roc_auc_score(ys, base_p)\n",
        "    imps = {}\n",
        "    col_vals = {f: Xs[f].values.copy() if f in Xs.columns else np.zeros(len(Xs)) for f in feature_names}\n",
        "    for feat in feature_names:\n",
        "        drops = []\n",
        "        vals = col_vals[feat]\n",
        "        for _ in range(n_repeats):\n",
        "            perm = rng.permutation(vals)\n",
        "            Xp = Xs.copy()\n",
        "            Xp[feat] = perm\n",
        "            try:\n",
        "                p = pred_proba_safe(model, Xp, feature_names)\n",
        "                auc_p = np.nan if len(np.unique(ys)) < 2 else roc_auc_score(ys, p)\n",
        "                drops.append(base_auc - auc_p if (not math.isnan(base_auc) and not math.isnan(auc_p)) else 0.0)\n",
        "            except Exception:\n",
        "                drops.append(0.0)\n",
        "        imps[feat] = float(np.mean(drops))\n",
        "    return pd.Series(imps)\n",
        "\n",
        "# process one side with per-fold feature alignment\n",
        "def process_side(model_files, side_name):\n",
        "    X_full = df[candidates].astype(float).fillna(0.0)\n",
        "    y_full = df[label_col].fillna(0).astype(int)\n",
        "    folds_gain = []; folds_shap = []; folds_perm = []\n",
        "    for p in model_files:\n",
        "        print(\"[INFO] processing\", os.path.basename(p))\n",
        "        model = load_model_obj(p)\n",
        "        # determine feature names used by model\n",
        "        model_feats = None\n",
        "        if hasattr(model, \"feature_names_in_\"):\n",
        "            model_feats = list(model.feature_names_in_)\n",
        "        else:\n",
        "            # try booster feature names\n",
        "            try:\n",
        "                booster = model.get_booster() if hasattr(model, \"get_booster\") else getattr(model, \"booster_\", None)\n",
        "                if booster is not None and hasattr(booster, \"feature_names\"):\n",
        "                    model_feats = list(booster.feature_names)\n",
        "            except Exception:\n",
        "                model_feats = None\n",
        "        if model_feats is None:\n",
        "            # fallback: assume candidates order at training time — warn\n",
        "            print(\"[WARN] model does not expose feature_names; assuming global candidates list.\")\n",
        "            model_feats = list(candidates)\n",
        "        # Log mismatches\n",
        "        missing = [c for c in model_feats if c not in X_full.columns]\n",
        "        extra = [c for c in X_full.columns if c not in model_feats]\n",
        "        if missing:\n",
        "            print(f\"[WARN] model expects {len(missing)} missing columns -> will fill zeros: {missing[:5]}\")\n",
        "        # align X to model features (fill missing with 0, drop extras)\n",
        "        X_aligned = X_full.reindex(columns=model_feats, fill_value=0.0)\n",
        "\n",
        "        # --- Gain importance ---\n",
        "        try:\n",
        "            if hasattr(model, \"get_booster\"):\n",
        "                booster = model.get_booster()\n",
        "            elif hasattr(model, \"booster_\"):\n",
        "                booster = model.booster_\n",
        "            else:\n",
        "                booster = None\n",
        "            if booster is not None:\n",
        "                gain_map = booster.get_score(importance_type=\"gain\") or {}\n",
        "                # map to model_feats (booster keys often 'f0'..)\n",
        "                if all(k.startswith(\"f\") for k in gain_map.keys()):\n",
        "                    gain_ser = pd.Series({feat: float(gain_map.get(f\"f{i}\", 0.0)) for i, feat in enumerate(model_feats)}, index=model_feats)\n",
        "                else:\n",
        "                    # assume names present\n",
        "                    gain_ser = pd.Series({feat: float(gain_map.get(feat, 0.0)) for feat in model_feats}, index=model_feats)\n",
        "            elif hasattr(model, \"feature_importances_\"):\n",
        "                gain_ser = pd.Series(model.feature_importances_, index=model_feats).reindex(model_feats).fillna(0.0)\n",
        "            else:\n",
        "                gain_ser = pd.Series(0.0, index=model_feats)\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] gain extraction failed:\", e)\n",
        "            gain_ser = pd.Series(0.0, index=model_feats)\n",
        "        folds_gain.append(gain_ser.reindex(model_feats).fillna(0.0))\n",
        "        gain_ser.reindex(model_feats).sort_values(ascending=False).to_csv(os.path.join(OUT_DIR, f\"feature_gain_{side_name}_{os.path.basename(p)}.csv\"))\n",
        "\n",
        "        # --- SHAP (best-effort) ---\n",
        "        shap_ser = pd.Series(0.0, index=model_feats)\n",
        "        if shap is not None:\n",
        "            try:\n",
        "                nsub = min(SHAP_SUBSAMPLE, len(X_aligned))\n",
        "                rng = check_random_state(PERM_RANDOM_STATE)\n",
        "                subs_idx = rng.choice(len(X_aligned), size=nsub, replace=False) if nsub < len(X_aligned) else np.arange(len(X_aligned))\n",
        "                Xs = X_aligned.iloc[subs_idx]\n",
        "                expl = None\n",
        "                try:\n",
        "                    expl = shap.Explainer(model, Xs)\n",
        "                    sv = expl(Xs)\n",
        "                    if hasattr(sv, \"values\"):\n",
        "                        arr = np.asarray(sv.values)\n",
        "                    else:\n",
        "                        arr = np.asarray(sv)\n",
        "                    if arr.ndim == 3:\n",
        "                        arr = arr.mean(axis=0)\n",
        "                    mean_abs = np.nanmean(np.abs(arr), axis=0)\n",
        "                    shap_ser = pd.Series(mean_abs, index=Xs.columns).reindex(model_feats).fillna(0.0)\n",
        "                    shap_ser.sort_values(ascending=False).to_csv(os.path.join(OUT_DIR, f\"feature_shap_{side_name}_{os.path.basename(p)}.csv\"))\n",
        "                except Exception as e:\n",
        "                    print(\"[WARN] SHAP explainer/predict failed for\", os.path.basename(p), \":\", e)\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] SHAP top-level failure for\", os.path.basename(p), \":\", e)\n",
        "        else:\n",
        "            print(\"[WARN] shap not installed; skipping SHAP for\", os.path.basename(p))\n",
        "        folds_shap.append(shap_ser.reindex(model_feats).fillna(0.0))\n",
        "\n",
        "        # --- Permutation importance (manual) ---\n",
        "        try:\n",
        "            perm_ser = manual_permutation_importance(model, X_full, y_full, model_feats,\n",
        "                                                    n_repeats=PERM_N_REPEATS, subsample=PERM_SUBSAMPLE, random_state=PERM_RANDOM_STATE)\n",
        "            perm_ser = perm_ser.reindex(model_feats).fillna(0.0)\n",
        "            perm_ser.sort_values(ascending=False).to_csv(os.path.join(OUT_DIR, f\"feature_perm_{side_name}_{os.path.basename(p)}.csv\"))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] permutation failed for\", os.path.basename(p), \":\", e)\n",
        "            perm_ser = pd.Series(0.0, index=model_feats)\n",
        "        folds_perm.append(perm_ser.reindex(model_feats).fillna(0.0))\n",
        "\n",
        "    # align across folds to union of all model_feats seen\n",
        "    all_feats = sorted(set().union(*[s.index.tolist() for s in folds_gain]))\n",
        "    df_gain = pd.concat([s.reindex(all_feats).fillna(0.0) for s in folds_gain], axis=1) if folds_gain else pd.DataFrame(columns=all_feats)\n",
        "    df_shap = pd.concat([s.reindex(all_feats).fillna(0.0) for s in folds_shap], axis=1) if folds_shap else pd.DataFrame(columns=all_feats)\n",
        "    df_perm = pd.concat([s.reindex(all_feats).fillna(0.0) for s in folds_perm], axis=1) if folds_perm else pd.DataFrame(columns=all_feats)\n",
        "\n",
        "    mean_gain = df_gain.mean(axis=1) if not df_gain.empty else pd.Series(0.0, index=all_feats)\n",
        "    mean_shap = df_shap.mean(axis=1) if not df_shap.empty else pd.Series(0.0, index=all_feats)\n",
        "    mean_perm = df_perm.mean(axis=1) if not df_perm.empty else pd.Series(0.0, index=all_feats)\n",
        "\n",
        "    return {\"df_gain\": df_gain, \"df_shap\": df_shap, \"df_perm\": df_perm,\n",
        "            \"mean_gain\": mean_gain, \"mean_shap\": mean_shap, \"mean_perm\": mean_perm}\n",
        "\n",
        "res_long = process_side(models_long, \"long\") if models_long else None\n",
        "res_short = process_side(models_short, \"short\") if models_short else None\n",
        "\n",
        "# combine summary and shortlist\n",
        "def make_summary(mean_gain, mean_shap, mean_perm):\n",
        "    dfm = pd.DataFrame({\"gain\": mean_gain, \"shap\": mean_shap, \"perm\": mean_perm}).fillna(0.0)\n",
        "    for c in [\"gain\", \"shap\", \"perm\"]:\n",
        "        vals = dfm[c].values\n",
        "        mn, mx = np.nanmin(vals), np.nanmax(vals)\n",
        "        dfm[c+\"_r\"] = 0.0 if mx == mn else (vals - mn) / (mx - mn)\n",
        "    rank_cols = [c for c in dfm.columns if c.endswith(\"_r\")]\n",
        "    dfm[\"score\"] = dfm[rank_cols].mean(axis=1)\n",
        "    return dfm.sort_values(\"score\", ascending=False)\n",
        "\n",
        "summary_long = make_summary(res_long[\"mean_gain\"], res_long[\"mean_shap\"], res_long[\"mean_perm\"]) if res_long else None\n",
        "summary_short = make_summary(res_short[\"mean_gain\"], res_short[\"mean_shap\"], res_short[\"mean_perm\"]) if res_short else None\n",
        "\n",
        "if summary_long is not None:\n",
        "    summary_long.to_csv(os.path.join(OUT_DIR, f\"feature_importance_summary_long_{TS}.csv\"))\n",
        "if summary_short is not None:\n",
        "    summary_short.to_csv(os.path.join(OUT_DIR, f\"feature_importance_summary_short_{TS}.csv\"))\n",
        "\n",
        "def shortlist(summary_df, top_k=TOP_K):\n",
        "    if summary_df is None: return {\"intersect\": [], \"union\": []}\n",
        "    top_gain = summary_df.sort_values(\"gain\", ascending=False).head(top_k).index.tolist()\n",
        "    top_shap = summary_df.sort_values(\"shap\", ascending=False).head(top_k).index.tolist()\n",
        "    top_perm = summary_df.sort_values(\"perm\", ascending=False).head(top_k).index.tolist()\n",
        "    from collections import Counter\n",
        "    cnt = Counter(top_gain + top_shap + top_perm)\n",
        "    intersect = [f for f,c in cnt.items() if c >= 2]\n",
        "    union = list(dict.fromkeys(top_gain + top_shap + top_perm))\n",
        "    return {\"top_gain\": top_gain, \"top_shap\": top_shap, \"top_perm\": top_perm, \"intersect\": intersect, \"union\": union}\n",
        "\n",
        "sl_long = shortlist(summary_long)\n",
        "sl_short = shortlist(summary_short)\n",
        "\n",
        "shortlist_obj = {\n",
        "    \"long\": sl_long[\"intersect\"],\n",
        "    \"short\": sl_short[\"intersect\"],\n",
        "    \"union_long\": sl_long[\"union\"],\n",
        "    \"union_short\": sl_short[\"union\"],\n",
        "    \"meta_path\": meta_path,\n",
        "    \"timestamp\": TS\n",
        "}\n",
        "short_pkl = os.path.join(OUT_DIR, f\"shortlist_selected_top{TOP_K}_inter_{TS}.pkl\")\n",
        "with open(short_pkl, \"wb\") as f:\n",
        "    pickle.dump(shortlist_obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(\"[OK] saved shortlist ->\", short_pkl)\n",
        "\n",
        "# JSON summary\n",
        "with open(os.path.join(OUT_DIR, f\"feature_importance_summary_{TS}.json\"), \"w\") as f:\n",
        "    json.dump({\"meta_path\": meta_path, \"n_candidates\": len(candidates),\n",
        "               \"models\": {\"long\": len(models_long), \"short\": len(models_short)},\n",
        "               \"shortlist\": short_pkl, \"timestamp\": TS}, f, indent=2)\n",
        "\n",
        "print(\"\\n=== DONE ===\")\n",
        "if sl_long[\"intersect\"]:\n",
        "    print(\"Top long shortlist (n=%d):\" % len(sl_long[\"intersect\"]), sl_long[\"intersect\"][:TOP_K])\n",
        "if sl_short[\"intersect\"]:\n",
        "    print(\"Top short shortlist (n=%d):\" % len(sl_short[\"intersect\"]), sl_short[\"intersect\"][:TOP_K])\n",
        "print(\"CSVs + per-fold CSVs saved in OUT_DIR.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGgcE2i3fS4H",
        "outputId": "23597535-4007-4de0-b9f0-6a79252acdc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loaded merged features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl shape: (17521, 238)\n",
            "[INFO] Using label: tb_label_h8 t_break: tb_t_break_h8\n",
            "[INFO] Candidate feature count: 229\n",
            "[INFO] Found fold model files -> 6\n",
            "[INFO] processing cpcv_long_fold1 -> XGBClassifier\n",
            "[INFO] processing cpcv_long_fold2 -> XGBClassifier\n",
            "[INFO] processing cpcv_long_fold3 -> XGBClassifier\n",
            "[INFO] processing cpcv_short_fold1 -> XGBClassifier\n",
            "[INFO] processing cpcv_short_fold2 -> XGBClassifier\n",
            "[INFO] processing cpcv_short_fold3 -> XGBClassifier\n",
            "[DEBUG] agg sizes -> gain: 139 shap: 0 perm: 0\n",
            "[OK] Summary CSV -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_summary_1764900291.csv\n",
            "[OK] Shortlist pkl -> /content/drive/MyDrive/quant_pipeline/mtb_out/shortlist_selected_top30_inter_1764900291.pkl\n",
            "\n",
            "[WARNINGS] (first 20):\n",
            " - cpcv_long_fold1: no feature_names; SHAP/perm may be skipped.\n",
            " - cpcv_long_fold1: permutation failed: Feature shape mismatch, expected: 156, got 229\n",
            " - cpcv_long_fold2: no feature_names; SHAP/perm may be skipped.\n",
            " - cpcv_long_fold2: permutation failed: Feature shape mismatch, expected: 156, got 229\n",
            " - cpcv_long_fold3: no feature_names; SHAP/perm may be skipped.\n",
            " - cpcv_long_fold3: permutation failed: Feature shape mismatch, expected: 156, got 229\n",
            " - cpcv_short_fold1: no feature_names; SHAP/perm may be skipped.\n",
            " - cpcv_short_fold1: permutation failed: Feature shape mismatch, expected: 156, got 229\n",
            " - cpcv_short_fold2: no feature_names; SHAP/perm may be skipped.\n",
            " - cpcv_short_fold2: permutation failed: Feature shape mismatch, expected: 156, got 229\n",
            " - cpcv_short_fold3: no feature_names; SHAP/perm may be skipped.\n",
            " - cpcv_short_fold3: permutation failed: Feature shape mismatch, expected: 156, got 229\n",
            "[DIAG] intersection with previous shortlist: 30 sample: ['f44', 'f128', 'f129', 'f71', 'f29', 'f114', 'f142', 'f49', 'f74', 'f28', 'f66', 'f151', 'f108', 'f150', 'f34', 'f102', 'f105', 'f42', 'f120', 'f131']\n"
          ]
        }
      ],
      "source": [
        "# CELL: Robust feature importance (fixed empty-table KeyError + hardening)\n",
        "# Paste into Colab. Assumes per-fold pkls exist and df_features_with_tb.pkl present.\n",
        "import os, glob, time, pickle, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Settings\n",
        "SHAP_SUBSAMPLE = 2000\n",
        "PERM_REPEATS = 8\n",
        "PERM_SAMPLE_FRAC = 0.5\n",
        "RANDOM_STATE = 42\n",
        "TOP_K = 30\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def extract_model_and_features(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        model = obj.get(\"model\", None)\n",
        "        feat = obj.get(\"feature_names\", None)\n",
        "        # fallback to find model-like entry\n",
        "        if model is None:\n",
        "            for v in obj.values():\n",
        "                if hasattr(v, \"predict_proba\") or hasattr(v, \"get_booster\"):\n",
        "                    model = v; break\n",
        "    else:\n",
        "        model = obj; feat = None\n",
        "    feat_names = None\n",
        "    try:\n",
        "        feat_names = getattr(obj, \"feature_names\", None) or getattr(obj, \"feature_names_in_\", None)\n",
        "    except Exception:\n",
        "        feat_names = None\n",
        "    if isinstance(obj, dict) and obj.get(\"feature_names\"):\n",
        "        feat_names = obj.get(\"feature_names\")\n",
        "    # try booster feature names\n",
        "    try:\n",
        "        booster = model.get_booster() if hasattr(model, \"get_booster\") else None\n",
        "        if booster is not None:\n",
        "            bfn = getattr(booster, \"feature_names\", None)\n",
        "            if bfn: feat_names = list(bfn)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return model, feat_names\n",
        "\n",
        "def predict_proba_safe(model, X):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model.predict_proba(X)[:,1]\n",
        "    try:\n",
        "        import xgboost as xgb\n",
        "        booster = model if isinstance(model, xgb.Booster) else (model.get_booster() if hasattr(model, \"get_booster\") else None)\n",
        "        if booster is None:\n",
        "            raise RuntimeError(\"No predict_proba and no booster\")\n",
        "        dm = xgb.DMatrix(X)\n",
        "        out = booster.predict(dm)\n",
        "        return out if out.ndim == 1 else out[:,1]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"predict_proba_safe failed: {e}\")\n",
        "\n",
        "def manual_permutation_importance(model, X_val_df, y_val, metric=roc_auc_score, n_repeats=PERM_REPEATS, frac=PERM_SAMPLE_FRAC, random_state=RANDOM_STATE):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    baseline_proba = predict_proba_safe(model, X_val_df)\n",
        "    baseline_score = metric(y_val, baseline_proba) if len(np.unique(y_val))>1 else 0.5\n",
        "    feat_scores = {}\n",
        "    n = len(X_val_df)\n",
        "    nsub = max(10, int(frac * n))\n",
        "    cols = X_val_df.columns.tolist()\n",
        "    for col in cols:\n",
        "        scores = []\n",
        "        for r in range(n_repeats):\n",
        "            idx = rng.choice(n, size=nsub, replace=False)\n",
        "            Xv = X_val_df.copy()\n",
        "            perm_vals = Xv.iloc[idx][col].values.copy()\n",
        "            rng.shuffle(perm_vals)\n",
        "            Xv.iloc[idx, Xv.columns.get_loc(col)] = perm_vals\n",
        "            try:\n",
        "                p = predict_proba_safe(model, Xv)\n",
        "                s = metric(y_val, p) if len(np.unique(y_val))>1 else 0.5\n",
        "            except Exception:\n",
        "                s = baseline_score\n",
        "            scores.append(baseline_score - s)\n",
        "        feat_scores[col] = float(np.mean(scores))\n",
        "    return feat_scores, baseline_score\n",
        "\n",
        "# 1) load merged df\n",
        "meta_files = sorted(glob.glob(os.path.join(OUT_DIR, \"df_features_with_tb*.pkl\")), key=os.path.getmtime)\n",
        "if not meta_files:\n",
        "    raise FileNotFoundError(\"df_features_with_tb*.pkl not found in OUT_DIR\")\n",
        "df = pd.read_pickle(meta_files[-1])\n",
        "print(\"[INFO] Loaded merged features:\", meta_files[-1], \"shape:\", df.shape)\n",
        "\n",
        "# choose label/t_break\n",
        "label_cols = [c for c in df.columns if c.startswith(\"tb_label_\")]\n",
        "tbreak_cols = [c for c in df.columns if c.startswith(\"tb_t_break_\")]\n",
        "if not label_cols or not tbreak_cols:\n",
        "    raise RuntimeError(\"Missing TB columns\")\n",
        "label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tbreak_col = label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "print(\"[INFO] Using label:\", label_col, \"t_break:\", tbreak_col)\n",
        "\n",
        "candidates = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "print(\"[INFO] Candidate feature count:\", len(candidates))\n",
        "\n",
        "# 2) find fold pkls\n",
        "fold_files = sorted(glob.glob(os.path.join(OUT_DIR, \"cpcv_*_fold*.pkl\")), key=os.path.getmtime)\n",
        "if not fold_files:\n",
        "    raise FileNotFoundError(\"No cpcv_*_fold*.pkl found. Run CPCV first.\")\n",
        "print(\"[INFO] Found fold model files ->\", len(fold_files))\n",
        "\n",
        "# containers\n",
        "gain_tables = []\n",
        "shap_tables = []\n",
        "perm_tables = []\n",
        "warnings_list = []\n",
        "\n",
        "for f in fold_files:\n",
        "    fold_name = os.path.basename(f).replace(\".pkl\",\"\")\n",
        "    try:\n",
        "        obj = load_pickle(f)\n",
        "    except Exception as e:\n",
        "        warnings_list.append(f\"{fold_name}: load failed: {e}\")\n",
        "        continue\n",
        "\n",
        "    model, feat_names = extract_model_and_features(obj)\n",
        "    print(\"[INFO] processing\", fold_name, \"->\", type(model).__name__)\n",
        "\n",
        "    if feat_names is None:\n",
        "        # try to infer from model.feature_importances_ length\n",
        "        try:\n",
        "            fi = getattr(model, \"feature_importances_\", None)\n",
        "            if fi is not None and len(fi) == len(candidates):\n",
        "                feat_names = candidates.copy()\n",
        "                warnings_list.append(f\"{fold_name}: inferred feature_names from global candidates (len match).\")\n",
        "            else:\n",
        "                warnings_list.append(f\"{fold_name}: no feature_names; SHAP/perm may be skipped.\")\n",
        "        except Exception as e:\n",
        "            warnings_list.append(f\"{fold_name}: feature name inference error: {e}\")\n",
        "\n",
        "    # GAIN\n",
        "    gain_dict = {}\n",
        "    try:\n",
        "        booster = model.get_booster() if hasattr(model, \"get_booster\") else (model if 'Booster' in type(model).__name__ else None)\n",
        "        if booster is not None:\n",
        "            score = booster.get_score(importance_type='gain')\n",
        "            # map keys to feature names if possible\n",
        "            if feat_names:\n",
        "                mapped = {}\n",
        "                for k,v in score.items():\n",
        "                    if k.startswith(\"f\") and k[1:].isdigit():\n",
        "                        idx = int(k[1:])\n",
        "                        if idx < len(feat_names): mapped[feat_names[idx]] = v\n",
        "                        else: mapped[k] = v\n",
        "                    else:\n",
        "                        mapped[k] = v\n",
        "                gain_dict = mapped\n",
        "            else:\n",
        "                gain_dict = score\n",
        "        else:\n",
        "            fi = getattr(model, \"feature_importances_\", None)\n",
        "            if fi is not None and feat_names:\n",
        "                gain_dict = {fn: float(fi[i]) for i,fn in enumerate(feat_names) if i < len(fi)}\n",
        "    except Exception as e:\n",
        "        warnings_list.append(f\"{fold_name}: gain extraction failed: {e}\")\n",
        "\n",
        "    if gain_dict:\n",
        "        gain_tbl = pd.DataFrame.from_dict(gain_dict, orient='index', columns=[\"gain\"]).reset_index().rename(columns={\"index\":\"feature\"})\n",
        "        gain_tbl[\"fold\"] = fold_name\n",
        "        gain_tables.append(gain_tbl)\n",
        "\n",
        "    # SHAP try (only if feat_names available)\n",
        "    if feat_names:\n",
        "        try:\n",
        "            import shap\n",
        "            X_for_shap = df.reindex(columns=feat_names).fillna(0.0)\n",
        "            Xs = X_for_shap.sample(n=min(len(X_for_shap), SHAP_SUBSAMPLE), random_state=RANDOM_STATE)\n",
        "            expl = shap.TreeExplainer(model)\n",
        "            shap_vals = expl.shap_values(Xs)\n",
        "            if isinstance(shap_vals, list):\n",
        "                arr = np.abs(shap_vals[0]) + np.abs(shap_vals[1])\n",
        "            else:\n",
        "                arr = np.abs(shap_vals)\n",
        "            mean_abs = np.mean(arr, axis=0)\n",
        "            shap_tbl = pd.DataFrame({\"feature\": Xs.columns.tolist(), \"shap_mean_abs\": mean_abs})\n",
        "            shap_tbl[\"fold\"] = fold_name\n",
        "            shap_tables.append(shap_tbl)\n",
        "        except Exception as e:\n",
        "            warnings_list.append(f\"{fold_name}: SHAP failed: {e}\")\n",
        "\n",
        "    # permutation (manual) attempt: use last 20% as validation\n",
        "    try:\n",
        "        n = len(df)\n",
        "        start = max(0, int(0.8 * n))\n",
        "        X_val = df.iloc[start:].reindex(columns=feat_names if feat_names is not None else candidates)\n",
        "        y_val = df[label_col].iloc[start:].fillna(0).astype(int)\n",
        "        if len(np.unique(y_val)) > 1:\n",
        "            perm_scores, base = manual_permutation_importance(model, X_val.fillna(0.0), y_val.values)\n",
        "            perm_tbl = pd.DataFrame.from_dict(perm_scores, orient='index', columns=[\"perm_drop_auc\"]).reset_index().rename(columns={\"index\":\"feature\"})\n",
        "            perm_tbl[\"fold\"] = fold_name\n",
        "            perm_tables.append(perm_tbl)\n",
        "        else:\n",
        "            warnings_list.append(f\"{fold_name}: validation label constant; skip perm.\")\n",
        "    except Exception as e:\n",
        "        warnings_list.append(f\"{fold_name}: permutation failed: {e}\")\n",
        "\n",
        "# Aggregate safely\n",
        "def agg_safe(tables, col):\n",
        "    if not tables:\n",
        "        return pd.DataFrame(columns=[\"feature\", col])\n",
        "    df_all = pd.concat(tables, ignore_index=True)\n",
        "    if \"feature\" not in df_all.columns:\n",
        "        return pd.DataFrame(columns=[\"feature\", col])\n",
        "    agg = df_all.groupby(\"feature\")[col].mean().reset_index().sort_values(col, ascending=False)\n",
        "    return agg\n",
        "\n",
        "gain_agg = agg_safe(gain_tables, \"gain\")\n",
        "shap_agg = agg_safe(shap_tables, \"shap_mean_abs\")\n",
        "perm_agg = agg_safe(perm_tables, \"perm_drop_auc\")\n",
        "\n",
        "print(\"[DEBUG] agg sizes -> gain:\", len(gain_agg), \"shap:\", len(shap_agg), \"perm:\", len(perm_agg))\n",
        "\n",
        "# union of features across available aggs\n",
        "all_feats = set()\n",
        "for df_ in (gain_agg, shap_agg, perm_agg):\n",
        "    if not df_.empty:\n",
        "        all_feats.update(df_[\"feature\"].tolist())\n",
        "all_feats = sorted(all_feats)\n",
        "summary = pd.DataFrame({\"feature\": all_feats})\n",
        "\n",
        "# merge only if non-empty\n",
        "if not gain_agg.empty:\n",
        "    summary = summary.merge(gain_agg, on=\"feature\", how=\"left\")\n",
        "if not shap_agg.empty:\n",
        "    summary = summary.merge(shap_agg, on=\"feature\", how=\"left\")\n",
        "if not perm_agg.empty:\n",
        "    summary = summary.merge(perm_agg, on=\"feature\", how=\"left\")\n",
        "\n",
        "# compute ranks where available\n",
        "if \"gain\" in summary.columns:\n",
        "    summary[\"gain_rank\"] = summary[\"gain\"].rank(method=\"average\", ascending=False)\n",
        "if \"shap_mean_abs\" in summary.columns:\n",
        "    summary[\"shap_rank\"] = summary[\"shap_mean_abs\"].rank(method=\"average\", ascending=False)\n",
        "if \"perm_drop_auc\" in summary.columns:\n",
        "    # larger drop == more important -> rank descending\n",
        "    summary[\"perm_rank\"] = summary[\"perm_drop_auc\"].rank(method=\"average\", ascending=False)\n",
        "\n",
        "rank_cols = [c for c in [\"gain_rank\",\"shap_rank\",\"perm_rank\"] if c in summary.columns]\n",
        "if rank_cols:\n",
        "    summary[\"mean_rank\"] = summary[rank_cols].mean(axis=1)\n",
        "    summary = summary.sort_values(\"mean_rank\")\n",
        "\n",
        "shortlist = summary.head(TOP_K)[\"feature\"].tolist()\n",
        "ts = int(time.time())\n",
        "summary_path = os.path.join(OUT_DIR, f\"feature_importance_summary_{ts}.csv\")\n",
        "short_pkl = os.path.join(OUT_DIR, f\"shortlist_selected_top{TOP_K}_inter_{ts}.pkl\")\n",
        "summary.to_csv(summary_path, index=False)\n",
        "with open(short_pkl, \"wb\") as f:\n",
        "    pickle.dump({\"summary\": summary, \"shortlist\": shortlist}, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"[OK] Summary CSV ->\", summary_path)\n",
        "print(\"[OK] Shortlist pkl ->\", short_pkl)\n",
        "if warnings_list:\n",
        "    print(\"\\n[WARNINGS] (first 20):\")\n",
        "    for w in warnings_list[:20]:\n",
        "        print(\" -\", w)\n",
        "else:\n",
        "    print(\"\\n[No warnings]\")\n",
        "\n",
        "# quick intersection diag with previous shortlist if exists\n",
        "prevs = sorted(glob.glob(os.path.join(OUT_DIR, \"shortlist_selected_top*_inter_*.pkl\")), key=os.path.getmtime)\n",
        "if prevs:\n",
        "    prev = load_pickle(prevs[-1])\n",
        "    prev_list = prev.get(\"shortlist\", []) if isinstance(prev, dict) else []\n",
        "    inter = set(prev_list).intersection(set(shortlist))\n",
        "    print(\"[DIAG] intersection with previous shortlist:\", len(inter), \"sample:\", list(inter)[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL0WVEVyf-Up",
        "outputId": "5814e7a7-d6ee-40b6-e48f-16fedcd589c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded merged features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl shape: (17521, 238)\n",
            "[INFO] candidate feature count: 229\n",
            "[INFO] found fold files: 6\n",
            "[INFO] patched 6 fold files. sample: [('cpcv_long_fold1.pkl', 'trim_to_expected=156', ['feature_names', 'booster.feature_names']), ('cpcv_long_fold2.pkl', 'trim_to_expected=156', ['feature_names', 'booster.feature_names']), ('cpcv_long_fold3.pkl', 'trim_to_expected=156', ['feature_names', 'booster.feature_names'])]\n",
            "[INFO] using label: tb_label_h8 t_break: tb_t_break_h8\n",
            "[INFO] collected counts gain=589 shap=936 perm=0 warnings=6\n",
            "[WARN] cpcv_long_fold1.pkl: permutation skipped or result-mismatch\n",
            "[WARN] cpcv_long_fold2.pkl: permutation skipped or result-mismatch\n",
            "[WARN] cpcv_long_fold3.pkl: permutation skipped or result-mismatch\n",
            "[WARN] cpcv_short_fold1.pkl: permutation skipped or result-mismatch\n",
            "[WARN] cpcv_short_fold2.pkl: permutation skipped or result-mismatch\n",
            "[WARN] cpcv_short_fold3.pkl: permutation skipped or result-mismatch\n",
            "\n",
            "[RESULT] summary saved: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_summary_1764900292.csv\n",
            "[RESULT] shortlist saved: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_summary_1764900292_shortlist.json\n",
            "[STATS] gain_rows=589 shap_rows=936 perm_rows=0 warnings=6\n",
            "[TOP] combined top-20:\n",
            "['ret_mean_24', 'squeeze_score', 'ret_48h_z_small', 'atr_14_z', 'lower_wick_z', 'ret_24h_z_small', 'rsi_14_z_small', 'ret_24h', 'ret_std_24', 'bb_mid_z_small', 'keltner_width', 'amihud_50', 'body_pct_z', 'ema_21_z', 'weekday', 'bb_mid_z', 'atr_14', 'bb_width', 'ret_12h_z', 'awesome_osc']\n",
            "\n",
            "[SHORTLIST] intersection_all (must be present in all methods): []\n",
            "[SHORTLIST] intersection_atleast2 (present in >=2 methods): ['atr_14', 'bb_mid_z_small', 'ema_100_z_small', 'ema_200', 'ema_200_z_small', 'hl2', 'keltner_width', 'weekday']\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: patch fold pkls + run feature-importance (gain / shap / permutation) -> shortlist top-K\n",
        "# Paste-run in Colab (assumes OUT_DIR has df_features_with_tb.pkl and cpcv_*_fold*.pkl)\n",
        "import os, glob, pickle, shutil, json, time\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "TS = int(time.time())\n",
        "\n",
        "# ------- helpers -------\n",
        "def load_pkl(p):\n",
        "    with open(p, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "def save_pkl(obj, p):\n",
        "    with open(p, \"wb\") as f:\n",
        "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# 0) locate latest merged features (required)\n",
        "meta_files = sorted(glob.glob(os.path.join(OUT_DIR, \"df_features_with_tb*.pkl\")), key=os.path.getmtime)\n",
        "if not meta_files:\n",
        "    raise FileNotFoundError(f\"No df_features_with_tb*.pkl in {OUT_DIR}\")\n",
        "meta_path = meta_files[-1]\n",
        "df = pd.read_pickle(meta_path)\n",
        "print(\"[INFO] loaded merged features:\", meta_path, \"shape:\", df.shape)\n",
        "\n",
        "# derive candidate feature list (exclude TB columns)\n",
        "candidate_cols = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "print(\"[INFO] candidate feature count:\", len(candidate_cols))\n",
        "\n",
        "# find fold pkls\n",
        "fold_files = sorted(glob.glob(os.path.join(OUT_DIR, \"cpcv_*_fold*.pkl\")), key=os.path.getmtime)\n",
        "if not fold_files:\n",
        "    raise FileNotFoundError(f\"No cpcv_*_fold*.pkl found in {OUT_DIR}\")\n",
        "print(\"[INFO] found fold files:\", len(fold_files))\n",
        "\n",
        "# ------- 1) patch fold pkls to include feature names (idempotent, backup .bak) -------\n",
        "patched = []\n",
        "for p in fold_files:\n",
        "    base = os.path.basename(p)\n",
        "    bak = p + \".bak\"\n",
        "    if not os.path.exists(bak):\n",
        "        shutil.copy2(p, bak)\n",
        "    try:\n",
        "        payload = load_pkl(p)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] load {base}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # locate model object\n",
        "    model = None\n",
        "    if isinstance(payload, dict):\n",
        "        model = payload.get(\"model\", None)\n",
        "        if model is None:\n",
        "            # pick first object that looks like a model\n",
        "            for v in payload.values():\n",
        "                if hasattr(v, \"predict_proba\") or hasattr(v, \"get_booster\") or hasattr(v, \"feature_importances_\"):\n",
        "                    model = v\n",
        "                    break\n",
        "    else:\n",
        "        model = payload\n",
        "\n",
        "    if model is None:\n",
        "        print(f\"[WARN] {base}: no model found -> skipping patch\")\n",
        "        continue\n",
        "\n",
        "    # determine expected feature count if model exposes it\n",
        "    expected = None\n",
        "    try:\n",
        "        if hasattr(model, \"n_features_in_\"):\n",
        "            expected = int(getattr(model, \"n_features_in_\"))\n",
        "    except Exception:\n",
        "        expected = None\n",
        "\n",
        "    # pick feature_names to attach: if expected <= candidates, trim; else use full candidates\n",
        "    if expected is not None and 0 < expected <= len(candidate_cols):\n",
        "        feat_names = candidate_cols[:expected]\n",
        "        reason = f\"trim_to_expected={expected}\"\n",
        "    else:\n",
        "        feat_names = candidate_cols.copy()\n",
        "        reason = f\"use_full_candidates={len(candidate_cols)}\"\n",
        "\n",
        "    # set attributes on model (best-effort)\n",
        "    ops = []\n",
        "    try:\n",
        "        try:\n",
        "            model.feature_names_in_ = np.asarray(feat_names, dtype=object); ops.append(\"feature_names_in_\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            model.feature_names = list(feat_names); ops.append(\"feature_names\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            if hasattr(model, \"get_booster\"):\n",
        "                booster = model.get_booster()\n",
        "                try:\n",
        "                    booster.feature_names = list(feat_names); ops.append(\"booster.feature_names\")\n",
        "                except Exception:\n",
        "                    setattr(booster, \"feature_names\", list(feat_names)); ops.append(\"booster.feature_names_set\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {base}: failed setting attrs: {e}\")\n",
        "\n",
        "    # write back payload with 'feature_names' key for convenience\n",
        "    if isinstance(payload, dict):\n",
        "        payload[\"feature_names\"] = feat_names\n",
        "    else:\n",
        "        payload = {\"model\": model, \"feature_names\": feat_names}\n",
        "\n",
        "    try:\n",
        "        save_pkl(payload, p)\n",
        "        patched.append((base, reason, ops))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] saving patched {base}: {e}\")\n",
        "\n",
        "print(f\"[INFO] patched {len(patched)} fold files. sample: {patched[:3]}\")\n",
        "\n",
        "# ------- 2) collect importances -------\n",
        "gain_rows = []\n",
        "shap_rows = []\n",
        "perm_rows = []\n",
        "warnings = []\n",
        "\n",
        "# prepare X/y sample for permutation and shap inputs\n",
        "# Use df merged; label prefer tb_label_h8 if exists else first tb_label_*\n",
        "label_cols = [c for c in df.columns if c.startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"No tb_label_* found in merged df\")\n",
        "label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tbreak_col = label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "print(\"[INFO] using label:\", label_col, \"t_break:\", tbreak_col)\n",
        "\n",
        "X_all = df[candidate_cols].ffill().fillna(0.0)\n",
        "y_all = df[label_col].fillna(0).astype(int)\n",
        "\n",
        "# subsample for permutation/SHAP to avoid OOM\n",
        "subsamp_n = min(2000, len(df))\n",
        "rng = np.random.RandomState(42)\n",
        "idx_sub = rng.choice(len(df), size=subsamp_n, replace=False)\n",
        "X_sub = X_all.iloc[idx_sub].copy()\n",
        "y_sub = y_all.iloc[idx_sub].copy()\n",
        "\n",
        "# helper: sklearn-like wrapper for permutation_importance\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "class ModelWrap(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model):\n",
        "        self._m = model\n",
        "    def fit(self, X, y=None):\n",
        "        # no-op (permutation_importance expects estimator to implement fit)\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        try:\n",
        "            return np.asarray(self._m.predict(X))\n",
        "        except Exception:\n",
        "            # try booster raw predict\n",
        "            try:\n",
        "                b = self._m.get_booster()\n",
        "                return np.asarray(b.predict(xgb.DMatrix(X)))\n",
        "            except Exception:\n",
        "                raise\n",
        "    def predict_proba(self, X):\n",
        "        try:\n",
        "            return np.asarray(self._m.predict_proba(X))\n",
        "        except Exception:\n",
        "            # try booster + transform to two-column prob (if binary)\n",
        "            try:\n",
        "                b = self._m.get_booster()\n",
        "                p = np.asarray(b.predict(xgb.DMatrix(X)))\n",
        "                if p.ndim == 1:\n",
        "                    return np.vstack([1 - p, p]).T\n",
        "                return p\n",
        "            except Exception as e:\n",
        "                raise\n",
        "\n",
        "# import xgboost only if needed by SHAP/booster fallback\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "# permutation_importance function (safe)\n",
        "from sklearn.inspection import permutation_importance\n",
        "def safe_permutation_est(model, X, y, n_repeats=8, random_state=42):\n",
        "    try:\n",
        "        wrap = ModelWrap(model)\n",
        "        res = permutation_importance(wrap, X, y, n_repeats=n_repeats, random_state=random_state, scoring='roc_auc' if len(np.unique(y))>1 else 'accuracy', n_jobs=1)\n",
        "        importances = res.importances_mean\n",
        "        return importances\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# try SHAP if available\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "except Exception:\n",
        "    HAS_SHAP = False\n",
        "\n",
        "# process each fold file\n",
        "for p in fold_files:\n",
        "    base = os.path.basename(p)\n",
        "    try:\n",
        "        payload = load_pkl(p)\n",
        "    except Exception as e:\n",
        "        warnings.append(f\"{base}: load failed {e}\")\n",
        "        continue\n",
        "\n",
        "    # extract model and feature names\n",
        "    model = None; feat_names = None\n",
        "    if isinstance(payload, dict):\n",
        "        feat_names = payload.get(\"feature_names\", None)\n",
        "        model = payload.get(\"model\", None)\n",
        "        if model is None:\n",
        "            # fallback: detect first plausible model object\n",
        "            for v in payload.values():\n",
        "                if hasattr(v, \"predict_proba\") or hasattr(v, \"get_booster\") or hasattr(v, \"feature_importances_\"):\n",
        "                    model = v; break\n",
        "    else:\n",
        "        model = payload\n",
        "\n",
        "    if model is None:\n",
        "        warnings.append(f\"{base}: model not found in payload\")\n",
        "        continue\n",
        "\n",
        "    # ensure feat_names exists (fallback to candidate_cols)\n",
        "    if feat_names is None:\n",
        "        try:\n",
        "            if hasattr(model, \"feature_names_in_\"):\n",
        "                feat_names = list(getattr(model, \"feature_names_in_\"))\n",
        "            elif hasattr(model, \"feature_names\"):\n",
        "                feat_names = list(getattr(model, \"feature_names\"))\n",
        "            else:\n",
        "                feat_names = candidate_cols.copy()\n",
        "        except Exception:\n",
        "            feat_names = candidate_cols.copy()\n",
        "\n",
        "    # --- GAIN importances via booster (preferred) ---\n",
        "    try:\n",
        "        # try XGBoost booster get_score\n",
        "        if hasattr(model, \"get_booster\"):\n",
        "            booster = model.get_booster()\n",
        "            score_dict = booster.get_score(importance_type=\"gain\") or {}\n",
        "            # map names to values (if booster feature_names exist, they match)\n",
        "            for fn, val in score_dict.items():\n",
        "                gain_rows.append({\"fold\": base, \"feature\": fn, \"gain\": float(val)})\n",
        "            # if score_dict empty but model has feature_importances_, use that as proxy\n",
        "            if not score_dict and hasattr(model, \"feature_importances_\"):\n",
        "                vals = np.asarray(model.feature_importances_)\n",
        "                for i, v in enumerate(vals):\n",
        "                    fn = feat_names[i] if i < len(feat_names) else f\"f_{i}\"\n",
        "                    gain_rows.append({\"fold\": base, \"feature\": fn, \"gain\": float(v)})\n",
        "        else:\n",
        "            # fallback: sklearn feature_importances_\n",
        "            if hasattr(model, \"feature_importances_\"):\n",
        "                vals = np.asarray(model.feature_importances_)\n",
        "                for i, v in enumerate(vals):\n",
        "                    fn = feat_names[i] if i < len(feat_names) else f\"f_{i}\"\n",
        "                    gain_rows.append({\"fold\": base, \"feature\": fn, \"gain\": float(v)})\n",
        "    except Exception as e:\n",
        "        warnings.append(f\"{base}: gain extraction failed: {e}\")\n",
        "\n",
        "    # --- SHAP (try-catch; subsample X_sub; align cols) ---\n",
        "    if HAS_SHAP:\n",
        "        try:\n",
        "            # align X_sub to feat_names if possible\n",
        "            X_shap = X_sub.reindex(columns=feat_names, fill_value=0.0)\n",
        "            expl = None\n",
        "            try:\n",
        "                expl = shap.TreeExplainer(model)\n",
        "                shap_vals = expl.shap_values(X_shap)\n",
        "            except Exception:\n",
        "                # fallback: if booster available, use TreeExplainer on booster\n",
        "                if hasattr(model, \"get_booster\"):\n",
        "                    booster = model.get_booster()\n",
        "                    expl = shap.TreeExplainer(booster)\n",
        "                    shap_vals = expl.shap_values(X_shap)\n",
        "                else:\n",
        "                    raise\n",
        "            # shap_vals: for binary classifier might be list of arrays or single array\n",
        "            if isinstance(shap_vals, list):\n",
        "                # use second-class contribution if exists else sum abs across classes\n",
        "                arr = np.asarray(shap_vals[-1])\n",
        "            else:\n",
        "                arr = np.asarray(shap_vals)\n",
        "            # arr shape (n_samples, n_features)\n",
        "            mean_abs = np.nanmean(np.abs(arr), axis=0)\n",
        "            for fn, v in zip(X_shap.columns.tolist(), mean_abs.tolist()):\n",
        "                shap_rows.append({\"fold\": base, \"feature\": fn, \"shap\": float(v)})\n",
        "        except Exception as e:\n",
        "            warnings.append(f\"{base}: SHAP failed: {e}\")\n",
        "\n",
        "    # --- Permutation importance (safe) on small subsample ---\n",
        "    try:\n",
        "        # align X_sub to feat_names\n",
        "        X_perm = X_sub.reindex(columns=feat_names, fill_value=0.0)\n",
        "        y_perm = y_sub.copy()\n",
        "        importances = safe_permutation_est(model, X_perm, y_perm, n_repeats=8, random_state=42)\n",
        "        if importances is not None and len(importances) == X_perm.shape[1]:\n",
        "            for fn, v in zip(X_perm.columns.tolist(), importances.tolist()):\n",
        "                perm_rows.append({\"fold\": base, \"feature\": fn, \"perm\": float(v)})\n",
        "        else:\n",
        "            warnings.append(f\"{base}: permutation skipped or result-mismatch\")\n",
        "    except Exception as e:\n",
        "        warnings.append(f\"{base}: permutation failed: {e}\")\n",
        "\n",
        "print(f\"[INFO] collected counts gain={len(gain_rows)} shap={len(shap_rows)} perm={len(perm_rows)} warnings={len(warnings)}\")\n",
        "for w in warnings[:10]:\n",
        "    print(\"[WARN]\", w)\n",
        "\n",
        "# ------- 3) aggregate + ranking -------\n",
        "gain_df = pd.DataFrame(gain_rows)\n",
        "shap_df = pd.DataFrame(shap_rows)\n",
        "perm_df = pd.DataFrame(perm_rows)\n",
        "\n",
        "# Normalize per-fold then average ranks\n",
        "def rank_agg(df, value_col, score_name):\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(columns=[\"feature\", score_name])\n",
        "    # pivot: compute fold-normalized rank (higher = better)\n",
        "    piv = df.groupby(\"fold\").apply(lambda g: g.assign(r=g[value_col].rank(ascending=False, method=\"average\"))).reset_index(drop=True)\n",
        "    # convert rank -> percentile (1..0)\n",
        "    piv = piv.merge(df[[\"fold\",\"feature\"]].drop_duplicates(), on=[\"fold\",\"feature\"], how=\"right\")\n",
        "    # average rank per feature across folds\n",
        "    agg = piv.groupby(\"feature\")[value_col].mean().reset_index(name=score_name+\"_mean\")\n",
        "    # if value_col is meaningful magnitude, also compute mean magnitude\n",
        "    mag = df.groupby(\"feature\")[value_col].mean().reset_index(name=score_name+\"_mag\")\n",
        "    out = agg.merge(mag, on=\"feature\", how=\"left\")\n",
        "    return out.sort_values(score_name+\"_mean\", ascending=False)\n",
        "\n",
        "gain_agg = rank_agg(gain_df, \"gain\", \"gain\")\n",
        "shap_agg = rank_agg(shap_df, \"shap\", \"shap\")\n",
        "perm_agg = rank_agg(perm_df, \"perm\", \"perm\")\n",
        "\n",
        "# build unified summary (union of all features seen)\n",
        "all_feats = sorted(set(list(gain_agg[\"feature\"]) + list(shap_agg[\"feature\"]) + list(perm_agg[\"feature\"])))\n",
        "summary = pd.DataFrame({\"feature\": all_feats})\n",
        "if not gain_agg.empty:\n",
        "    summary = summary.merge(gain_agg.rename(columns={\"gain_mean\":\"gain_rank_mean\",\"gain_mag\":\"gain_mean_mag\"}), on=\"feature\", how=\"left\")\n",
        "if not shap_agg.empty:\n",
        "    summary = summary.merge(shap_agg.rename(columns={\"shap_mean\":\"shap_rank_mean\",\"shap_mag\":\"shap_mean_mag\"}), on=\"feature\", how=\"left\")\n",
        "if not perm_agg.empty:\n",
        "    summary = summary.merge(perm_agg.rename(columns={\"perm_mean\":\"perm_rank_mean\",\"perm_mag\":\"perm_mean_mag\"}), on=\"feature\", how=\"left\")\n",
        "\n",
        "# compute combined score (mean of available rank_means; na ignored)\n",
        "rank_cols = [c for c in summary.columns if c.endswith(\"_rank_mean\")]\n",
        "summary[\"rank_mean_combined\"] = summary[rank_cols].mean(axis=1, skipna=True)\n",
        "summary = summary.sort_values(\"rank_mean_combined\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# select top-K intersection logic\n",
        "TOPK = 30\n",
        "top_gain = list(gain_agg[\"feature\"].head(TOPK)) if not gain_agg.empty else []\n",
        "top_shap = list(shap_agg[\"feature\"].head(TOPK)) if not shap_agg.empty else []\n",
        "top_perm = list(perm_agg[\"feature\"].head(TOPK)) if not perm_agg.empty else []\n",
        "\n",
        "# intersection/union\n",
        "intersect = sorted(list(set(top_gain) & set(top_shap) & set(top_perm))) if (top_gain and top_shap and top_perm) else []\n",
        "union = sorted(list(set(top_gain) | set(top_shap) | set(top_perm)))\n",
        "\n",
        "# fallback: if SHAP or PERM missing, use intersection of available methods (>=2 methods)\n",
        "available_sets = []\n",
        "if top_gain: available_sets.append(set(top_gain))\n",
        "if top_shap: available_sets.append(set(top_shap))\n",
        "if top_perm: available_sets.append(set(top_perm))\n",
        "intersect_atleast2 = []\n",
        "if len(available_sets) >= 2:\n",
        "    # intersection across any two methods: choose features that appear in at least 2 sets\n",
        "    from collections import Counter\n",
        "    cnt = Counter()\n",
        "    for s in available_sets:\n",
        "        for f in s:\n",
        "            cnt[f] += 1\n",
        "    intersect_atleast2 = sorted([f for f,c in cnt.items() if c >= 2])\n",
        "\n",
        "# ------- 4) save outputs -------\n",
        "out_prefix = os.path.join(OUT_DIR, f\"feature_importance_summary_{TS}\")\n",
        "summary.to_csv(out_prefix + \".csv\", index=False)\n",
        "if not gain_df.empty: gain_df.to_csv(out_prefix + \"_gain_by_fold.csv\", index=False)\n",
        "if not shap_df.empty: shap_df.to_csv(out_prefix + \"_shap_by_fold.csv\", index=False)\n",
        "if not perm_df.empty: perm_df.to_csv(out_prefix + \"_perm_by_fold.csv\", index=False)\n",
        "\n",
        "shortlist = {\n",
        "    \"timestamp\": TS,\n",
        "    \"top_gain\": top_gain,\n",
        "    \"top_shap\": top_shap,\n",
        "    \"top_perm\": top_perm,\n",
        "    \"intersection_all\": intersect,\n",
        "    \"intersection_atleast2\": intersect_atleast2,\n",
        "    \"union\": union\n",
        "}\n",
        "with open(out_prefix + \"_shortlist.json\", \"w\") as f:\n",
        "    json.dump(shortlist, f, indent=2)\n",
        "save_pkl(shortlist, out_prefix + \"_shortlist.pkl\")\n",
        "\n",
        "print(\"\\n[RESULT] summary saved:\", out_prefix + \".csv\")\n",
        "print(\"[RESULT] shortlist saved:\", out_prefix + \"_shortlist.json\")\n",
        "print(f\"[STATS] gain_rows={len(gain_rows)} shap_rows={len(shap_rows)} perm_rows={len(perm_rows)} warnings={len(warnings)}\")\n",
        "print(\"[TOP] combined top-20:\")\n",
        "print(summary[\"feature\"].head(20).tolist())\n",
        "\n",
        "# print intersection results\n",
        "print(\"\\n[SHORTLIST] intersection_all (must be present in all methods):\", intersect)\n",
        "print(\"[SHORTLIST] intersection_atleast2 (present in >=2 methods):\", intersect_atleast2[:TOPK])\n",
        "\n",
        "# done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f36aVXRRgmXf",
        "outputId": "1a559c29-53cb-4336-c618-766a0b1fe53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded merged features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl n_expected_features: 229\n",
            "[INFO] found model files: 6 -> sample: ['/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_long_fold1.pkl', '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_long_fold2.pkl', '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_long_fold3.pkl', '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_short_fold1.pkl', '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_short_fold2.pkl', '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_short_fold3.pkl']\n",
            "[OK] patched cpcv_long_fold1.pkl -> cpcv_long_fold1_patched.pkl | n_features=156 | warnings=0\n",
            "[OK] patched cpcv_long_fold2.pkl -> cpcv_long_fold2_patched.pkl | n_features=156 | warnings=0\n",
            "[OK] patched cpcv_long_fold3.pkl -> cpcv_long_fold3_patched.pkl | n_features=156 | warnings=0\n",
            "[OK] patched cpcv_short_fold1.pkl -> cpcv_short_fold1_patched.pkl | n_features=156 | warnings=0\n",
            "[OK] patched cpcv_short_fold2.pkl -> cpcv_short_fold2_patched.pkl | n_features=156 | warnings=0\n",
            "[OK] patched cpcv_short_fold3.pkl -> cpcv_short_fold3_patched.pkl | n_features=156 | warnings=0\n",
            "\n",
            "=== PATCH SUMMARY ===\n",
            "cpcv_long_fold1.pkl PATCHED {'patched_path': '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_long_fold1_patched.pkl', 'n_features': 156, 'warnings': [], 'time_s': 0.02}\n",
            "cpcv_long_fold2.pkl PATCHED {'patched_path': '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_long_fold2_patched.pkl', 'n_features': 156, 'warnings': [], 'time_s': 0.02}\n",
            "cpcv_long_fold3.pkl PATCHED {'patched_path': '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_long_fold3_patched.pkl', 'n_features': 156, 'warnings': [], 'time_s': 0.02}\n",
            "cpcv_short_fold1.pkl PATCHED {'patched_path': '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_short_fold1_patched.pkl', 'n_features': 156, 'warnings': [], 'time_s': 0.02}\n",
            "cpcv_short_fold2.pkl PATCHED {'patched_path': '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_short_fold2_patched.pkl', 'n_features': 156, 'warnings': [], 'time_s': 0.02}\n",
            "cpcv_short_fold3.pkl PATCHED {'patched_path': '/content/drive/MyDrive/quant_pipeline/mtb_out/cpcv_short_fold3_patched.pkl', 'n_features': 156, 'warnings': [], 'time_s': 0.03}\n",
            "=== Done. Patched models saved as *_patched.pkl in OUT_DIR. ===\n",
            "Next: re-run your feature-importance cell but point it at *_patched.pkl models (or overwrite original names if desired).\n"
          ]
        }
      ],
      "source": [
        "# Cell: patch_cpcv_models_for_importance.py\n",
        "# Run in Colab after run_cpcv_grid produced cpcv_*_fold*.pkl and df_features_with_tb.pkl exists.\n",
        "import os, glob, time, pickle, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Helper wrapper so sklearn permutation_importance and our code can call predict/predict_proba\n",
        "class SklearnWrap:\n",
        "    \"\"\"\n",
        "    Light wrapper to present a scikit-learn compatible estimator interface around\n",
        "    a fitted XGBClassifier-like object. Implements:\n",
        "      - fit(X, y): no-op (model already trained)\n",
        "      - predict(X)\n",
        "      - predict_proba(X)\n",
        "      - feature_names_in_, n_features_in_\n",
        "    This wrapper is intentionally minimal; it delegates to underlying `model`.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, feature_names):\n",
        "        self.model = model\n",
        "        self.feature_names_in_ = np.asarray(list(feature_names))\n",
        "        self.n_features_in_ = len(self.feature_names_in_)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # no-op: wrapper for sklearn compatibility only\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Accept DataFrame or 2D-array. If df, reorder/trim columns to feature_names_in_\n",
        "        import numpy as _np\n",
        "        if hasattr(X, \"loc\") and getattr(X, \"columns\", None) is not None:\n",
        "            # DataFrame: select in same order\n",
        "            # if columns missing, raise informative error\n",
        "            miss = [c for c in self.feature_names_in_ if c not in X.columns]\n",
        "            if miss:\n",
        "                raise ValueError(f\"Input DataFrame missing columns required by model: {miss}\")\n",
        "            X_in = X.loc[:, self.feature_names_in_].to_numpy()\n",
        "        else:\n",
        "            X_in = _np.asarray(X)\n",
        "            if X_in.shape[1] != self.n_features_in_:\n",
        "                raise ValueError(f\"Input ndarray has shape {X_in.shape}, model expects {self.n_features_in_} features\")\n",
        "        # delegate\n",
        "        if hasattr(self.model, \"predict_proba\"):\n",
        "            proba = self.model.predict_proba(X_in)\n",
        "            # binary/class: return argmax\n",
        "            return np.argmax(proba, axis=1)\n",
        "        else:\n",
        "            return self.model.predict(X_in)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        import numpy as _np\n",
        "        if hasattr(X, \"loc\") and getattr(X, \"columns\", None) is not None:\n",
        "            miss = [c for c in self.feature_names_in_ if c not in X.columns]\n",
        "            if miss:\n",
        "                raise ValueError(f\"Input DataFrame missing columns required by model: {miss}\")\n",
        "            X_in = X.loc[:, self.feature_names_in_].to_numpy()\n",
        "        else:\n",
        "            X_in = _np.asarray(X)\n",
        "            if X_in.shape[1] != self.n_features_in_:\n",
        "                raise ValueError(f\"Input ndarray has shape {X_in.shape}, model expects {self.n_features_in_} features\")\n",
        "        if hasattr(self.model, \"predict_proba\"):\n",
        "            return self.model.predict_proba(X_in)\n",
        "        # fallback: try predict and convert to one-hot-ish probabilities (not ideal)\n",
        "        pred = self.model.predict(X_in)\n",
        "        # return shape (n, n_classes) approximate\n",
        "        if pred.ndim == 1:\n",
        "            # binary -> provide [1-p, p]\n",
        "            eps = 1e-6\n",
        "            p = (pred.astype(float) + eps) / (1.0 + 2*eps)\n",
        "            p = np.clip(p, eps, 1-eps)\n",
        "            return np.vstack([1-p, p]).T\n",
        "        return pred\n",
        "\n",
        "# 1) load df and expected features\n",
        "meta_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"df_features_with_tb*.pkl\")), key=os.path.getmtime)\n",
        "if not meta_candidates:\n",
        "    raise FileNotFoundError(\"df_features_with_tb*.pkl not found in OUT_DIR; run merge-TB step first.\")\n",
        "meta_path = meta_candidates[-1]\n",
        "df = pd.read_pickle(meta_path)\n",
        "# prefer numeric feature candidates: exclude tb_ columns and _z suffix if you used that earlier\n",
        "expected_features = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "print(\"[INFO] loaded merged features:\", meta_path, \"n_expected_features:\", len(expected_features))\n",
        "\n",
        "# 2) find fold model pkls\n",
        "model_files = sorted(glob.glob(os.path.join(OUT_DIR, \"cpcv_*_fold*.pkl\")))\n",
        "if not model_files:\n",
        "    raise FileNotFoundError(\"No cpcv_*_fold*.pkl found in OUT_DIR. Run run_cpcv_grid first.\")\n",
        "print(f\"[INFO] found model files: {len(model_files)} -> sample: {model_files[:6]}\")\n",
        "\n",
        "summary = []\n",
        "for mf in model_files:\n",
        "    t0 = time.time()\n",
        "    base = os.path.basename(mf)\n",
        "    patched_path = mf.replace(\".pkl\", \"_patched.pkl\")\n",
        "    warn_msgs = []\n",
        "    try:\n",
        "        with open(mf, \"rb\") as fh:\n",
        "            payload = pickle.load(fh)\n",
        "    except Exception as e:\n",
        "        summary.append((base, \"ERROR_LOAD\", str(e)))\n",
        "        print(f\"[ERROR] cannot load {base}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # payload may be dict with model inside\n",
        "    # try common keys\n",
        "    model = None\n",
        "    if isinstance(payload, dict):\n",
        "        # look for 'model' or 'estimator' or direct model\n",
        "        for k in (\"model\",\"estimator\",\"clf\",\"booster\"):\n",
        "            if k in payload and payload[k] is not None:\n",
        "                model = payload[k]; break\n",
        "        # else if a sklearn object stored as whole dict, try first value that looks like estimator\n",
        "        if model is None:\n",
        "            for v in payload.values():\n",
        "                # dumb check: has predict or predict_proba\n",
        "                if hasattr(v, \"predict\") or hasattr(v, \"predict_proba\"):\n",
        "                    model = v; break\n",
        "    else:\n",
        "        model = payload\n",
        "\n",
        "    if model is None:\n",
        "        summary.append((base, \"NO_MODEL\", \"no estimator found in pkl\"))\n",
        "        print(f\"[WARN] {base}: no estimator found inside pickle; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Attempt to extract original model feature names if present\n",
        "    trained_feats = None\n",
        "    # common: model.feature_names_in_ (sklearn >=1.0)\n",
        "    if hasattr(model, \"feature_names_in_\"):\n",
        "        try:\n",
        "            trained_feats = list(getattr(model, \"feature_names_in_\"))\n",
        "        except Exception:\n",
        "            trained_feats = None\n",
        "    # XGBoost scikit-learn wrapper: model.get_booster().feature_names may exist\n",
        "    try:\n",
        "        booster = None\n",
        "        if hasattr(model, \"get_booster\"):\n",
        "            booster = model.get_booster()\n",
        "            # booster.feature_names might be list or None\n",
        "            bf = getattr(booster, \"feature_names\", None)\n",
        "            if bf is not None:\n",
        "                trained_feats = list(bf)\n",
        "    except Exception:\n",
        "        booster = None\n",
        "\n",
        "    # If no trained feature list, attempt to infer from model 'feature_names' attribute\n",
        "    if trained_feats is None:\n",
        "        if hasattr(model, \"feature_names\"):\n",
        "            try:\n",
        "                trained_feats = list(getattr(model, \"feature_names\"))\n",
        "            except Exception:\n",
        "                trained_feats = None\n",
        "\n",
        "    final_feat_list = None\n",
        "    note = \"\"\n",
        "    # If trained feats present -> use them (but ensure they are subset of expected_features)\n",
        "    if trained_feats:\n",
        "        # if mismatch in set, warn\n",
        "        set_tr = set(trained_feats); set_exp = set(expected_features)\n",
        "        if not set_tr.issubset(set_exp):\n",
        "            # trained features contain names not in current expected set -> keep trained_feats but log\n",
        "            warn_msgs.append(f\"trained_feats contain unknown cols (n={len(trained_feats)}). Will keep trained_feats as model input order.\")\n",
        "            final_feat_list = trained_feats\n",
        "        else:\n",
        "            # map to expected order\n",
        "            final_feat_list = [f for f in expected_features if f in trained_feats]\n",
        "            # If ordering changed, prefer the original trained order to avoid misalignment\n",
        "            if final_feat_list != trained_feats:\n",
        "                # preserve trained order\n",
        "                final_feat_list = trained_feats\n",
        "    else:\n",
        "        # No trained-feats metadata. Try to detect numeric feature count the model expects.\n",
        "        nfeat_expected = len(expected_features)\n",
        "        nfeat_model = None\n",
        "        # try reading model attribute that hints n_features\n",
        "        if hasattr(model, \"n_features_in_\"):\n",
        "            try:\n",
        "                nfeat_model = int(getattr(model, \"n_features_in_\"))\n",
        "            except Exception:\n",
        "                nfeat_model = None\n",
        "        # try booster.num_feature()\n",
        "        if nfeat_model is None and booster is not None:\n",
        "            try:\n",
        "                nfeat_model = int(booster.num_feature())\n",
        "            except Exception:\n",
        "                nfeat_model = None\n",
        "\n",
        "        if nfeat_model is None:\n",
        "            # fallback: assume model used same features as current expected list\n",
        "            final_feat_list = list(expected_features)\n",
        "            warn_msgs.append(\"no trained feature-list found; assuming current expected features (best-effort).\")\n",
        "        else:\n",
        "            if nfeat_model <= len(expected_features):\n",
        "                final_feat_list = list(expected_features)[:nfeat_model]\n",
        "                warn_msgs.append(f\"model expects {nfeat_model} features -> trimming expected features to first {nfeat_model}.\")\n",
        "            else:\n",
        "                # model expects more features than we have -> cannot safely fix\n",
        "                summary.append((base, \"INCOMPATIBLE_SHAPE\", f\"model expects {nfeat_model} features but current expected {len(expected_features)}\"))\n",
        "                print(f\"[ERROR] {base}: model expects {nfeat_model} features but you have only {len(expected_features)} expected features. Skipping.\")\n",
        "                continue\n",
        "\n",
        "    # Ensure booster.feature_names is set to final_feat_list if possible\n",
        "    try:\n",
        "        if hasattr(model, \"get_booster\") and model.get_booster() is not None:\n",
        "            b = model.get_booster()\n",
        "            try:\n",
        "                # For safety: convert to list of str\n",
        "                b.feature_names = list(map(str, final_feat_list))\n",
        "                # Also set model.feature_names attribute if exists\n",
        "                if hasattr(model, \"feature_names\"):\n",
        "                    try:\n",
        "                        model.feature_names = list(map(str, final_feat_list))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                # Also set model.feature_names_in_ / n_features_in_\n",
        "                try:\n",
        "                    model.feature_names_in_ = np.asarray(list(final_feat_list))\n",
        "                    model.n_features_in_ = len(final_feat_list)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            except Exception as e:\n",
        "                warn_msgs.append(f\"failed to set booster.feature_names: {e}\")\n",
        "    except Exception as e:\n",
        "        warn_msgs.append(f\"cannot access booster: {e}\")\n",
        "\n",
        "    # wrap estimator so sklearn.permutation_importance and our code can call fit/predict_proba\n",
        "    wrapped = SklearnWrap(model, final_feat_list)\n",
        "\n",
        "    # Save patched payload: keep original payload structure but replace model with wrapper\n",
        "    new_payload = payload if isinstance(payload, dict) else {\"model\": payload}\n",
        "    # try to keep same top-level structure: if dict had 'model' key replace it; else create 'model'\n",
        "    if isinstance(payload, dict):\n",
        "        # replace model-like entries with wrapper\n",
        "        found_key = None\n",
        "        for k in (\"model\",\"estimator\",\"clf\",\"booster\"):\n",
        "            if k in new_payload:\n",
        "                new_payload[k] = wrapped\n",
        "                found_key = k\n",
        "                break\n",
        "        if found_key is None:\n",
        "            # put into 'model' key\n",
        "            new_payload[\"model\"] = wrapped\n",
        "    else:\n",
        "        new_payload = {\"model\": wrapped}\n",
        "\n",
        "    # write patched pkl\n",
        "    try:\n",
        "        with open(patched_path, \"wb\") as fh:\n",
        "            pickle.dump(new_payload, fh)\n",
        "    except Exception as e:\n",
        "        summary.append((base, \"ERROR_SAVE\", str(e)))\n",
        "        print(f\"[ERROR] failed to save patched {patched_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    summary.append((base, \"PATCHED\", {\"patched_path\": patched_path, \"n_features\": len(final_feat_list), \"warnings\": warn_msgs, \"time_s\": round(elapsed,2)}))\n",
        "    print(f\"[OK] patched {base} -> {os.path.basename(patched_path)} | n_features={len(final_feat_list)} | warnings={len(warn_msgs)}\")\n",
        "\n",
        "# 3) summary\n",
        "print(\"\\n=== PATCH SUMMARY ===\")\n",
        "for row in summary:\n",
        "    name, status, info = row\n",
        "    print(name, status, info)\n",
        "print(\"=== Done. Patched models saved as *_patched.pkl in OUT_DIR. ===\")\n",
        "print(\"Next: re-run your feature-importance cell but point it at *_patched.pkl models (or overwrite original names if desired).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5NLU51Cg4Zc",
        "outputId": "35729a46-3b35-4513-999a-8137974be1e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded merged features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl shape: (17521, 238)\n",
            "[INFO] candidate feature count: 229\n",
            "[INFO] found fold files -> 6\n",
            "[INFO] processing cpcv_long_fold1_patched.pkl\n",
            "  [WARN] SHAP failed for cpcv_long_fold1_patched.pkl: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            "[INFO] processing cpcv_long_fold2_patched.pkl\n",
            "  [WARN] SHAP failed for cpcv_long_fold2_patched.pkl: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            "[INFO] processing cpcv_long_fold3_patched.pkl\n",
            "  [WARN] SHAP failed for cpcv_long_fold3_patched.pkl: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            "[INFO] processing cpcv_short_fold1_patched.pkl\n",
            "  [WARN] SHAP failed for cpcv_short_fold1_patched.pkl: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            "[INFO] processing cpcv_short_fold2_patched.pkl\n",
            "  [WARN] SHAP failed for cpcv_short_fold2_patched.pkl: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            "[INFO] processing cpcv_short_fold3_patched.pkl\n",
            "  [WARN] SHAP failed for cpcv_short_fold3_patched.pkl: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            "[OK] Summary CSV -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_summary_1764900294.csv\n",
            "[OK] Shortlist JSON -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_shortlist_1764900294.json\n",
            "[OK] Shortlist PKL -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_shortlist_1764900294.pkl\n",
            "\n",
            "=== TOP RESULTS ===\n",
            "top_gain (sample 20): ['amihud_50', 'amihud_50_z', 'amihud_50_z_small', 'atr_14', 'atr_14_z', 'atr_14_z_small', 'atr_rel', 'atr_rel_z', 'atr_rel_z_small', 'awesome_osc', 'awesome_osc_z', 'awesome_osc_z_small', 'bb_high', 'bb_high_z', 'bb_high_z_small', 'bb_low', 'bb_low_z', 'bb_low_z_small', 'bb_mid', 'bb_mid_z']\n",
            "top_shap (sample 20): ['amihud_50', 'amihud_50_z', 'amihud_50_z_small', 'atr_14', 'atr_14_z', 'atr_14_z_small', 'atr_rel', 'atr_rel_z', 'atr_rel_z_small', 'awesome_osc', 'awesome_osc_z', 'awesome_osc_z_small', 'bb_high', 'bb_high_z', 'bb_high_z_small', 'bb_low', 'bb_low_z', 'bb_low_z_small', 'bb_mid', 'bb_mid_z']\n",
            "intersection (n): 30 sample: ['bb_high_z', 'candle_body_signed', 'body_pct_z', 'atr_14_z', 'bb_mid_z_small', 'awesome_osc_z_small', 'bb_mid', 'bb_width_z', 'bb_high_z_small', 'atr_rel_z_small', 'amihud_50_z', 'bb_mid_z', 'bb_low_z', 'awesome_osc_z', 'bb_low_z_small', 'candle_body_signed_z', 'amihud_50', 'atr_rel_z', 'atr_rel', 'bb_width_z_small']\n",
            "union (n): 30 sample: ['amihud_50', 'amihud_50_z', 'amihud_50_z_small', 'atr_14', 'atr_14_z', 'atr_14_z_small', 'atr_rel', 'atr_rel_z', 'atr_rel_z_small', 'awesome_osc', 'awesome_osc_z', 'awesome_osc_z_small', 'bb_high', 'bb_high_z', 'bb_high_z_small', 'bb_low', 'bb_low_z', 'bb_low_z_small', 'bb_mid', 'bb_mid_z']\n",
            "\n",
            "[WARNINGS] (first 20):\n",
            " - cpcv_long_fold1_patched.pkl: model has no get_booster; skipping gain importances.\n",
            " - cpcv_long_fold1_patched.pkl: SHAP failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_long_fold2_patched.pkl: model has no get_booster; skipping gain importances.\n",
            " - cpcv_long_fold2_patched.pkl: SHAP failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_long_fold3_patched.pkl: model has no get_booster; skipping gain importances.\n",
            " - cpcv_long_fold3_patched.pkl: SHAP failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_short_fold1_patched.pkl: model has no get_booster; skipping gain importances.\n",
            " - cpcv_short_fold1_patched.pkl: SHAP failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_short_fold2_patched.pkl: model has no get_booster; skipping gain importances.\n",
            " - cpcv_short_fold2_patched.pkl: SHAP failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_short_fold3_patched.pkl: model has no get_booster; skipping gain importances.\n",
            " - cpcv_short_fold3_patched.pkl: SHAP failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            "\n",
            "Done in 0.2s\n",
            "Files written to OUT_DIR: /content/drive/MyDrive/quant_pipeline/mtb_out\n"
          ]
        }
      ],
      "source": [
        "# Cell: feature_importance_final.py\n",
        "# One-shot feature-importance runner (gain + SHAP, robust fallbacks).\n",
        "import os, glob, time, json, pickle, warnings\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# SETTINGS\n",
        "N_TOP = 30                # shortlist size\n",
        "SHAP_SUBSAMPLE = 2000     # rows for SHAP (None = full)\n",
        "SHAP_SEED = 42\n",
        "SAVE_PREFIX = \"feature_importance\"\n",
        "\n",
        "# Helpers\n",
        "def load_latest_meta(out_dir=OUT_DIR):\n",
        "    cand = sorted(glob.glob(os.path.join(out_dir, \"df_features_with_tb*.pkl\")), key=os.path.getmtime)\n",
        "    if not cand:\n",
        "        raise FileNotFoundError(\"No df_features_with_tb*.pkl found in OUT_DIR.\")\n",
        "    path = cand[-1]\n",
        "    df = pd.read_pickle(path)\n",
        "    return path, df\n",
        "\n",
        "def find_fold_files(out_dir=OUT_DIR):\n",
        "    # prefer patched\n",
        "    patched = sorted(glob.glob(os.path.join(out_dir, \"cpcv_*_fold*_patched.pkl\")), key=os.path.getmtime)\n",
        "    if patched:\n",
        "        return patched\n",
        "    orig = sorted(glob.glob(os.path.join(out_dir, \"cpcv_*_fold*.pkl\")), key=os.path.getmtime)\n",
        "    return orig\n",
        "\n",
        "def extract_model_from_payload(payload):\n",
        "    # payload may be dict or estimator\n",
        "    model = None\n",
        "    if isinstance(payload, dict):\n",
        "        # common keys\n",
        "        for k in (\"model\",\"estimator\",\"clf\",\"booster\"):\n",
        "            if k in payload and payload[k] is not None:\n",
        "                model = payload[k]; break\n",
        "        if model is None:\n",
        "            # pick first estimator-like value\n",
        "            for v in payload.values():\n",
        "                if hasattr(v, \"predict\") or hasattr(v, \"predict_proba\"):\n",
        "                    model = v; break\n",
        "    else:\n",
        "        model = payload\n",
        "    return model\n",
        "\n",
        "# Main\n",
        "tstart = time.time()\n",
        "meta_path, df = load_latest_meta()\n",
        "print(\"[INFO] loaded merged features:\", meta_path, \"shape:\", df.shape)\n",
        "\n",
        "# candidate feature list (exclude tb_ prefix)\n",
        "candidate_features = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "print(\"[INFO] candidate feature count:\", len(candidate_features))\n",
        "\n",
        "fold_files = find_fold_files()\n",
        "if not fold_files:\n",
        "    raise FileNotFoundError(\"No fold model files found (patched or original). Run run_cpcv_grid first.\")\n",
        "print(\"[INFO] found fold files ->\", len(fold_files))\n",
        "\n",
        "# containers for importances\n",
        "gain_rows = []\n",
        "shap_rows = []\n",
        "warnings_list = []\n",
        "\n",
        "# try to import shap lazily\n",
        "try:\n",
        "    import shap\n",
        "    HAVE_SHAP = True\n",
        "except Exception:\n",
        "    HAVE_SHAP = False\n",
        "    warnings_list.append(\"shap not available; SHAP importances will be skipped.\")\n",
        "\n",
        "# process each fold file\n",
        "for mf in fold_files:\n",
        "    name = os.path.basename(mf)\n",
        "    print(f\"[INFO] processing {name}\")\n",
        "    try:\n",
        "        with open(mf, \"rb\") as fh:\n",
        "            payload = pickle.load(fh)\n",
        "    except Exception as e:\n",
        "        warnings_list.append(f\"{name}: cannot load pkl: {e}\")\n",
        "        print(\"  [WARN] load failed:\", e)\n",
        "        continue\n",
        "\n",
        "    model = extract_model_from_payload(payload)\n",
        "    if model is None:\n",
        "        warnings_list.append(f\"{name}: no estimator inside pickle.\")\n",
        "        print(\"  [WARN] no estimator found in payload.\")\n",
        "        continue\n",
        "\n",
        "    # Attempt to get final feature order used by this model\n",
        "    # Prefer: model.feature_names_in_, model.feature_names, model.get_booster().feature_names\n",
        "    feat_order = None\n",
        "    try:\n",
        "        if hasattr(model, \"feature_names_in_\"):\n",
        "            feat_order = list(model.feature_names_in_)\n",
        "    except Exception:\n",
        "        feat_order = None\n",
        "    if feat_order is None:\n",
        "        try:\n",
        "            if hasattr(model, \"feature_names\"):\n",
        "                feat_order = list(model.feature_names)\n",
        "        except Exception:\n",
        "            feat_order = None\n",
        "    if feat_order is None:\n",
        "        try:\n",
        "            if hasattr(model, \"get_booster\"):\n",
        "                booster = model.get_booster()\n",
        "                bf = getattr(booster, \"feature_names\", None)\n",
        "                if bf:\n",
        "                    feat_order = list(bf)\n",
        "        except Exception:\n",
        "            feat_order = None\n",
        "\n",
        "    # If still None, fallback to candidate_features trimmed to model.n_features_in_ if available\n",
        "    if feat_order is None:\n",
        "        nreq = None\n",
        "        if hasattr(model, \"n_features_in_\"):\n",
        "            try: nreq = int(model.n_features_in_)\n",
        "            except Exception: nreq = None\n",
        "        if nreq is not None:\n",
        "            if nreq <= len(candidate_features):\n",
        "                feat_order = candidate_features[:nreq]\n",
        "                warnings_list.append(f\"{name}: no saved feature list; assumed first {nreq} candidate_features.\")\n",
        "            else:\n",
        "                warnings_list.append(f\"{name}: model expects {nreq} features but candidate_features length {len(candidate_features)}; using all candidate_features.\")\n",
        "                feat_order = list(candidate_features)\n",
        "        else:\n",
        "            feat_order = list(candidate_features)\n",
        "            warnings_list.append(f\"{name}: no feature metadata; assumed current candidate_features order.\")\n",
        "\n",
        "    # --- GAIN importances (XGBoost booster) ---\n",
        "    try:\n",
        "        # try to access booster\n",
        "        if hasattr(model, \"get_booster\"):\n",
        "            booster = model.get_booster()\n",
        "            score = booster.get_score(importance_type=\"gain\") or {}\n",
        "            # XG returns keys like 'f0','f1' when trained from array; if so try map via booster.feature_names\n",
        "            bfn = getattr(booster, \"feature_names\", None)\n",
        "            if bfn and any(k.startswith(\"f\") for k in score.keys()):\n",
        "                # map f{i} -> name if possible\n",
        "                mapped = {}\n",
        "                for k,v in score.items():\n",
        "                    if k.startswith(\"f\"):\n",
        "                        try:\n",
        "                            idx = int(k[1:])\n",
        "                            namek = bfn[idx] if idx < len(bfn) else f\"f{idx}\"\n",
        "                        except Exception:\n",
        "                            namek = k\n",
        "                    else:\n",
        "                        namek = k\n",
        "                    mapped[namek] = float(v)\n",
        "                score = mapped\n",
        "            # normalize to per-feature float\n",
        "            for feat, val in score.items():\n",
        "                gain_rows.append({\"fold_file\": name, \"feature\": str(feat), \"gain\": float(val)})\n",
        "        else:\n",
        "            warnings_list.append(f\"{name}: model has no get_booster; skipping gain importances.\")\n",
        "    except Exception as e:\n",
        "        warnings_list.append(f\"{name}: gain extraction failed: {e}\")\n",
        "\n",
        "    # --- SHAP (TreeExplainer) ---\n",
        "    if HAVE_SHAP:\n",
        "        try:\n",
        "            # prepare sample: DataFrame with feat_order columns\n",
        "            # ensure columns exist in df; if missing, fill zeros with warning\n",
        "            miss = [c for c in feat_order if c not in df.columns]\n",
        "            if miss:\n",
        "                # fill missing with zeros in a copy\n",
        "                X = df.reindex(columns=feat_order).fillna(0.0)\n",
        "                warnings_list.append(f\"{name}: SHAP - model expects {len(miss)} missing cols; filling zeros for SHAP: {miss[:5]}{'...' if len(miss)>5 else ''}\")\n",
        "            else:\n",
        "                X = df[feat_order]\n",
        "            # subsample rows for speed\n",
        "            nrows = X.shape[0]\n",
        "            if SHAP_SUBSAMPLE is None or SHAP_SUBSAMPLE >= nrows:\n",
        "                Xs = X\n",
        "            else:\n",
        "                rng = np.random.RandomState(SHAP_SEED)\n",
        "                sel = rng.choice(nrows, size=min(SHAP_SUBSAMPLE, nrows), replace=False)\n",
        "                Xs = X.iloc[sel]\n",
        "            # TreeExplainer expects booster or object supporting predict; wrap if necessary\n",
        "            expl = shap.TreeExplainer(model, feature_perturbation=\"tree_path_dependent\")\n",
        "            shap_vals = expl.shap_values(Xs)\n",
        "            # shap_values returns list per class for classifier; handle binary/class\n",
        "            if isinstance(shap_vals, list):\n",
        "                # choose the class-1 at index 1 if available, else use first\n",
        "                if len(shap_vals) >= 2:\n",
        "                    sv = np.array(shap_vals[1])\n",
        "                else:\n",
        "                    sv = np.array(shap_vals[0])\n",
        "            else:\n",
        "                sv = np.array(shap_vals)\n",
        "            # compute mean absolute shap per feature\n",
        "            mean_abs = np.mean(np.abs(sv), axis=0)\n",
        "            for feat, v in zip(Xs.columns, mean_abs):\n",
        "                shap_rows.append({\"fold_file\": name, \"feature\": str(feat), \"shap_mean_abs\": float(v)})\n",
        "            print(f\"  [DEBUG] SHAP done for {name}\")\n",
        "        except Exception as e:\n",
        "            warnings_list.append(f\"{name}: SHAP failed: {e}\")\n",
        "            print(f\"  [WARN] SHAP failed for {name}: {e}\")\n",
        "    else:\n",
        "        # shap not installed\n",
        "        pass\n",
        "\n",
        "# Convert to DataFrames\n",
        "gain_df = pd.DataFrame(gain_rows)\n",
        "shap_df = pd.DataFrame(shap_rows)\n",
        "\n",
        "# save raw per-fold CSVs\n",
        "if not gain_df.empty:\n",
        "    gain_out = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_gain_per_fold.csv\")\n",
        "    gain_df.to_csv(gain_out, index=False)\n",
        "    print(\"[INFO] Gain importances saved ->\", gain_out)\n",
        "if not shap_df.empty:\n",
        "    shap_out = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_shap_per_fold.csv\")\n",
        "    shap_df.to_csv(shap_out, index=False)\n",
        "    print(\"[INFO] SHAP importances saved ->\", shap_out)\n",
        "\n",
        "# Aggregate: compute mean rank and mean value\n",
        "summary_rows = []\n",
        "all_feats = sorted(set(list(gain_df['feature']) if not gain_df.empty else []) | set(list(shap_df['feature']) if not shap_df.empty else []) | set(candidate_features))\n",
        "for feat in all_feats:\n",
        "    gvals = gain_df[gain_df['feature']==feat]['gain'].values if not gain_df.empty else np.array([])\n",
        "    svals = shap_df[shap_df['feature']==feat]['shap_mean_abs'].values if not shap_df.empty else np.array([])\n",
        "    gain_mean = float(np.nanmean(gvals)) if gvals.size else np.nan\n",
        "    shap_mean = float(np.nanmean(svals)) if svals.size else np.nan\n",
        "    # rank: lower rank value is better (we invert by negating values when ranking)\n",
        "    summary_rows.append({\"feature\": feat, \"gain_mean\": gain_mean, \"shap_mean_abs\": shap_mean})\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "# compute ranks (handle NaN by putting them last)\n",
        "if 'gain_mean' in summary_df.columns:\n",
        "    summary_df['gain_rank'] = summary_df['gain_mean'].rank(ascending=False, method='min', na_option='bottom')\n",
        "if 'shap_mean_abs' in summary_df.columns:\n",
        "    summary_df['shap_rank'] = summary_df['shap_mean_abs'].rank(ascending=False, method='min', na_option='bottom')\n",
        "# combined rank (mean of available ranks)\n",
        "rank_cols = [c for c in ('gain_rank','shap_rank') if c in summary_df.columns]\n",
        "if rank_cols:\n",
        "    summary_df['mean_rank'] = summary_df[rank_cols].mean(axis=1)\n",
        "else:\n",
        "    summary_df['mean_rank'] = np.nan\n",
        "\n",
        "summary_df = summary_df.sort_values('mean_rank').reset_index(drop=True)\n",
        "\n",
        "# Shortlists\n",
        "top_gain = summary_df.sort_values('gain_rank').head(N_TOP)['feature'].tolist() if 'gain_rank' in summary_df.columns else []\n",
        "top_shap = summary_df.sort_values('shap_rank').head(N_TOP)['feature'].tolist() if 'shap_rank' in summary_df.columns else []\n",
        "intersect_atleast2 = list(set(top_gain).intersection(set(top_shap))) if top_gain and top_shap else list(set(top_gain + top_shap)[:N_TOP])\n",
        "union_top = list(dict.fromkeys(top_gain + top_shap))[:N_TOP]\n",
        "\n",
        "# Save outputs\n",
        "ts = int(time.time())\n",
        "summary_csv = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_summary_{ts}.csv\")\n",
        "summary_df.to_csv(summary_csv, index=False)\n",
        "shortlist_json = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_shortlist_{ts}.json\")\n",
        "with open(shortlist_json, \"w\") as f:\n",
        "    json.dump({\"top_gain\": top_gain, \"top_shap\": top_shap, \"intersect_atleast2\": intersect_atleast2, \"union_top\": union_top}, f, indent=2)\n",
        "shortlist_pkl = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_shortlist_{ts}.pkl\")\n",
        "pd.Series(intersect_atleast2).to_pickle(shortlist_pkl)\n",
        "\n",
        "print(\"[OK] Summary CSV ->\", summary_csv)\n",
        "print(\"[OK] Shortlist JSON ->\", shortlist_json)\n",
        "print(\"[OK] Shortlist PKL ->\", shortlist_pkl)\n",
        "\n",
        "# Print brief diagnostics\n",
        "print(\"\\n=== TOP RESULTS ===\")\n",
        "print(\"top_gain (sample 20):\", top_gain[:20])\n",
        "print(\"top_shap (sample 20):\", top_shap[:20])\n",
        "print(\"intersection (n):\", len(intersect_atleast2), \"sample:\", intersect_atleast2[:20])\n",
        "print(\"union (n):\", len(union_top), \"sample:\", union_top[:20])\n",
        "\n",
        "if warnings_list:\n",
        "    print(\"\\n[WARNINGS] (first 20):\")\n",
        "    for w in warnings_list[:20]:\n",
        "        print(\" -\", w)\n",
        "\n",
        "print(\"\\nDone in %.1fs\" % (time.time() - tstart))\n",
        "print(\"Files written to OUT_DIR:\", OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCjHR_xohQEB",
        "outputId": "a4489b0e-3110-476b-9bd0-2325a0dd4f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded merged features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl shape: (17521, 238)\n",
            "[INFO] candidate feature count: 229\n",
            "[INFO] found fold files -> 6\n",
            "processing cpcv_long_fold1_patched.pkl\n",
            "  [DEBUG] estimator type: XGBClassifier trace: payload['model']->attr_model->direct:XGBClassifier\n",
            "processing cpcv_long_fold2_patched.pkl\n",
            "  [DEBUG] estimator type: XGBClassifier trace: payload['model']->attr_model->direct:XGBClassifier\n",
            "processing cpcv_long_fold3_patched.pkl\n",
            "  [DEBUG] estimator type: XGBClassifier trace: payload['model']->attr_model->direct:XGBClassifier\n",
            "processing cpcv_short_fold1_patched.pkl\n",
            "  [DEBUG] estimator type: XGBClassifier trace: payload['model']->attr_model->direct:XGBClassifier\n",
            "processing cpcv_short_fold2_patched.pkl\n",
            "  [DEBUG] estimator type: XGBClassifier trace: payload['model']->attr_model->direct:XGBClassifier\n",
            "processing cpcv_short_fold3_patched.pkl\n",
            "  [DEBUG] estimator type: XGBClassifier trace: payload['model']->attr_model->direct:XGBClassifier\n",
            "[INFO] saved gain csv -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_gain_per_fold.csv\n",
            "[INFO] saved shap csv -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_shap_per_fold.csv\n",
            "[OK] summary saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_summary_1764900296.csv\n",
            "[OK] shortlist json -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_shortlist_1764900296.json\n",
            "[OK] shortlist pkl -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_shortlist_1764900296.pkl\n",
            "\n",
            "=== TOP RESULTS ===\n",
            "top_gain (sample): ['ret_mean_24', 'squeeze_score', 'ret_48h_z_small', 'atr_14_z', 'lower_wick_z', 'ret_24h_z_small', 'rsi_14_z_small', 'ret_24h', 'ret_std_24', 'bb_mid_z_small', 'keltner_width', 'amihud_50', 'body_pct_z', 'ema_21_z', 'weekday', 'bb_mid_z', 'atr_14', 'bb_width', 'ret_12h_z', 'awesome_osc']\n",
            "top_shap (sample): ['bb_mid_z_small', 'close', 'ret_kurt_72', 'kyle_lambda_50', 'ema_200_z_small', 'ema_200', 'weekday', 'midprice', 'ema_100', 'pvo', 'hl2', 'ret_kurt_168', 'mfi_14', 'obv', 'ema_100_z_small', 'ema_50', 'rsi_7_z_small', 'high_z_small', 'macd_z_small', 'roll_spread_50']\n",
            "intersection (n): 8 sample: ['keltner_width', 'hl2', 'bb_mid_z_small', 'ema_200_z_small', 'ema_200', 'ema_100_z_small', 'weekday', 'atr_14']\n",
            "union (n): 30 sample: ['ret_mean_24', 'squeeze_score', 'ret_48h_z_small', 'atr_14_z', 'lower_wick_z', 'ret_24h_z_small', 'rsi_14_z_small', 'ret_24h', 'ret_std_24', 'bb_mid_z_small', 'keltner_width', 'amihud_50', 'body_pct_z', 'ema_21_z', 'weekday', 'bb_mid_z', 'atr_14', 'bb_width', 'ret_12h_z', 'awesome_osc']\n",
            "\n",
            "Done in 2.0s\n"
          ]
        }
      ],
      "source": [
        "# Cell: feature_importance_fixed_unwrap.py\n",
        "# Robust feature-importance runner that unwraps common wrappers (SklearnWrap etc.)\n",
        "import os, glob, time, pickle, json, warnings\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Config\n",
        "N_TOP = 30\n",
        "SHAP_SUBSAMPLE = 2000\n",
        "SHAP_SEED = 42\n",
        "SAVE_PREFIX = \"feature_importance_fixed\"\n",
        "\n",
        "# Optional: try import shap (if available)\n",
        "try:\n",
        "    import shap\n",
        "    HAVE_SHAP = True\n",
        "except Exception:\n",
        "    HAVE_SHAP = False\n",
        "\n",
        "# Helpers: load latest merged features\n",
        "def load_latest_meta(out_dir=OUT_DIR):\n",
        "    cand = sorted(glob.glob(os.path.join(out_dir, \"df_features_with_tb*.pkl\")), key=os.path.getmtime)\n",
        "    if not cand:\n",
        "        raise FileNotFoundError(\"No df_features_with_tb*.pkl found in OUT_DIR.\")\n",
        "    p = cand[-1]\n",
        "    return p, pd.read_pickle(p)\n",
        "\n",
        "# Helper: find fold files (prefer patched)\n",
        "def find_fold_files(out_dir=OUT_DIR):\n",
        "    patched = sorted(glob.glob(os.path.join(out_dir, \"cpcv_*_fold*_patched.pkl\")), key=os.path.getmtime)\n",
        "    if patched:\n",
        "        return patched\n",
        "    return sorted(glob.glob(os.path.join(out_dir, \"cpcv_*_fold*.pkl\")), key=os.path.getmtime)\n",
        "\n",
        "# Unwrap: try to find an xgboost/XGBClassifier/Booster or sklearn estimator inside wrapper\n",
        "def unwrap_estimator(obj, depth=0):\n",
        "    \"\"\"\n",
        "    Recursively inspect object to find a usable estimator.\n",
        "    Returns (estimator_obj, trace_str)\n",
        "    \"\"\"\n",
        "    if depth > 4 or obj is None:\n",
        "        return None, \"max-depth\"\n",
        "    # common direct types detection without importing heavy libs\n",
        "    t = type(obj)\n",
        "    name = t.__name__\n",
        "    # quick checks by attributes\n",
        "    # if object looks like xgboost booster or XGBClassifier: accept\n",
        "    if name in (\"Booster\", \"XGBClassifier\", \"XGBRegressor\"):\n",
        "        return obj, f\"direct:{name}\"\n",
        "    # duck-type: has get_booster -> likely XGB scikit wrapper\n",
        "    if hasattr(obj, \"get_booster\") and callable(getattr(obj, \"get_booster\")):\n",
        "        try:\n",
        "            # prefer returning original object (XGBClassifier) for fit/predict usage\n",
        "            return obj, \"has_get_booster\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    # if object exposes feature_importances_ or coef_ -> scikit estimator\n",
        "    if hasattr(obj, \"feature_importances_\") or hasattr(obj, \"coef_\"):\n",
        "        return obj, \"sklearn_like\"\n",
        "    # common container keys to explore\n",
        "    candidates = []\n",
        "    if isinstance(obj, dict):\n",
        "        # prioritized keys\n",
        "        for k in (\"model\",\"estimator\",\"clf\",\"classifier\",\"wrapped\",\"inner\",\"base_estimator\",\"booster\"):\n",
        "            if k in obj:\n",
        "                candidates.append((\"key_\"+k, obj[k]))\n",
        "        # also try any dict value\n",
        "        for k,v in obj.items():\n",
        "            if not k.startswith(\"_\"):\n",
        "                candidates.append((f\"key_{k}\", v))\n",
        "    else:\n",
        "        # inspect attributes\n",
        "        for attr in (\"estimator_\",\"best_estimator_\",\"wrapped_estimator\",\"model\",\"clf\",\"booster\",\"_booster\",\"estimator\",\"sklearn_estimator\"):\n",
        "            if hasattr(obj, attr):\n",
        "                try:\n",
        "                    candidates.append((f\"attr_{attr}\", getattr(obj, attr)))\n",
        "                except Exception:\n",
        "                    pass\n",
        "        # also try scanning object __dict__ for estimator-like values\n",
        "        try:\n",
        "            for k,v in getattr(obj, \"__dict__\", {}).items():\n",
        "                if k.startswith(\"_\"): continue\n",
        "                candidates.append((f\"attr_{k}\", v))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Recursively probe candidates\n",
        "    for tag, cand in candidates:\n",
        "        est, trace = unwrap_estimator(cand, depth+1)\n",
        "        if est is not None:\n",
        "            return est, f\"{tag}->{trace}\"\n",
        "    return None, \"not-found\"\n",
        "\n",
        "# Compute gain importances from Booster/XGB or fallback to sklearn importances\n",
        "def compute_gain_from_model(model, feat_order):\n",
        "    \"\"\"\n",
        "    Return dict(feature->value) or {} on failure.\n",
        "    \"\"\"\n",
        "    # try booster\n",
        "    try:\n",
        "        if hasattr(model, \"get_booster\"):\n",
        "            booster = model.get_booster()\n",
        "            score = booster.get_score(importance_type=\"gain\") or {}\n",
        "            # map f0.. -> booster.feature_names\n",
        "            bfn = getattr(booster, \"feature_names\", None)\n",
        "            mapped = {}\n",
        "            for k,v in score.items():\n",
        "                if k.startswith(\"f\") and bfn:\n",
        "                    try:\n",
        "                        i = int(k[1:]); mapped[bfn[i]] = float(v)\n",
        "                    except Exception:\n",
        "                        mapped[k] = float(v)\n",
        "                else:\n",
        "                    mapped[k] = float(v)\n",
        "            return mapped\n",
        "        # direct booster object (xgboost.core.Booster)\n",
        "        if type(model).__name__ == \"Booster\":\n",
        "            score = model.get_score(importance_type=\"gain\") or {}\n",
        "            bfn = getattr(model, \"feature_names\", None)\n",
        "            mapped = {}\n",
        "            for k,v in score.items():\n",
        "                if k.startswith(\"f\") and bfn:\n",
        "                    try:\n",
        "                        i = int(k[1:]); mapped[bfn[i]] = float(v)\n",
        "                    except Exception:\n",
        "                        mapped[k] = float(v)\n",
        "                else:\n",
        "                    mapped[k] = float(v)\n",
        "            return mapped\n",
        "    except Exception as e:\n",
        "        # continue to fallback\n",
        "        pass\n",
        "\n",
        "    # sklearn-style fat importances\n",
        "    try:\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            vals = getattr(model, \"feature_importances_\")\n",
        "            if len(vals) == len(feat_order):\n",
        "                return {f: float(v) for f,v in zip(feat_order, vals)}\n",
        "            else:\n",
        "                # try partial mapping by length\n",
        "                L = min(len(vals), len(feat_order))\n",
        "                return {f: float(v) for f,v in zip(feat_order[:L], vals[:L])}\n",
        "        if hasattr(model, \"coef_\"):\n",
        "            coef = getattr(model, \"coef_\")\n",
        "            # coef may be (n_classes, n_features) or (n_features,)\n",
        "            arr = np.array(coef)\n",
        "            if arr.ndim == 2:\n",
        "                # if binary classifier, pick row 0 or absolute-mean\n",
        "                vals = np.mean(np.abs(arr), axis=0)\n",
        "            else:\n",
        "                vals = np.abs(arr)\n",
        "            if len(vals) == len(feat_order):\n",
        "                return {f: float(v) for f,v in zip(feat_order, vals)}\n",
        "            else:\n",
        "                L = min(len(vals), len(feat_order))\n",
        "                return {f: float(v) for f,v in zip(feat_order[:L], vals[:L])}\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return {}\n",
        "\n",
        "# SHAP wrapper: only works with booster or XGBClassifier-like; handle exceptions\n",
        "def compute_shap_for_model(model, feat_order, df, subsample=SHAP_SUBSAMPLE):\n",
        "    if not HAVE_SHAP:\n",
        "        return {}\n",
        "    # prepare X (ensure columns exist)\n",
        "    miss = [c for c in feat_order if c not in df.columns]\n",
        "    if miss:\n",
        "        X = df.reindex(columns=feat_order).fillna(0.0)\n",
        "    else:\n",
        "        X = df[feat_order].fillna(0.0)\n",
        "    nrows = X.shape[0]\n",
        "    if subsample is None or subsample >= nrows:\n",
        "        Xs = X\n",
        "    else:\n",
        "        rng = np.random.RandomState(SHAP_SEED)\n",
        "        sel = rng.choice(nrows, size=min(subsample, nrows), replace=False)\n",
        "        Xs = X.iloc[sel]\n",
        "    # attempt to pass booster or estimator\n",
        "    try:\n",
        "        # if model supports get_booster, pass model directly\n",
        "        if hasattr(model, \"get_booster\"):\n",
        "            expl = shap.TreeExplainer(model)\n",
        "        else:\n",
        "            # maybe the model is a Booster directly\n",
        "            if type(model).__name__ == \"Booster\":\n",
        "                expl = shap.TreeExplainer(model)\n",
        "            else:\n",
        "                # fallback: try TreeExplainer on wrapped object (may fail)\n",
        "                expl = shap.TreeExplainer(model)\n",
        "        shap_vals = expl.shap_values(Xs)\n",
        "        if isinstance(shap_vals, list):\n",
        "            if len(shap_vals) >= 2:\n",
        "                arr = np.array(shap_vals[1])\n",
        "            else:\n",
        "                arr = np.array(shap_vals[0])\n",
        "        else:\n",
        "            arr = np.array(shap_vals)\n",
        "        mean_abs = np.mean(np.abs(arr), axis=0)\n",
        "        return {f: float(v) for f,v in zip(Xs.columns, mean_abs)}\n",
        "    except Exception as e:\n",
        "        return {\"__error\": str(e)}\n",
        "\n",
        "# -----------------------\n",
        "# Main execution\n",
        "# -----------------------\n",
        "t0 = time.time()\n",
        "meta_path, df = load_latest_meta(OUT_DIR)\n",
        "print(\"[INFO] loaded merged features:\", meta_path, \"shape:\", df.shape)\n",
        "\n",
        "candidate_features = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "print(\"[INFO] candidate feature count:\", len(candidate_features))\n",
        "\n",
        "fold_files = find_fold_files(OUT_DIR)\n",
        "if not fold_files:\n",
        "    raise FileNotFoundError(\"No fold model files found - run run_cpcv_grid first.\")\n",
        "print(\"[INFO] found fold files ->\", len(fold_files))\n",
        "\n",
        "gain_rows = []\n",
        "shap_rows = []\n",
        "warns = []\n",
        "\n",
        "for fp in fold_files:\n",
        "    name = os.path.basename(fp)\n",
        "    print(\"processing\", name)\n",
        "    try:\n",
        "        with open(fp, \"rb\") as fh:\n",
        "            payload = pickle.load(fh)\n",
        "    except Exception as e:\n",
        "        warns.append(f\"{name}: failed to load pickle: {e}\")\n",
        "        print(\"  [WARN] load failed:\", e)\n",
        "        continue\n",
        "\n",
        "    # attempt to extract payload['model'] or estimator directly\n",
        "    estimator = None\n",
        "    trace = \"\"\n",
        "    # if payload is dict or has 'model' key\n",
        "    if isinstance(payload, dict):\n",
        "        if \"model\" in payload and payload[\"model\"] is not None:\n",
        "            estimator = payload[\"model\"]\n",
        "            trace = \"payload['model']\"\n",
        "        elif \"estimator\" in payload and payload[\"estimator\"] is not None:\n",
        "            estimator = payload[\"estimator\"]\n",
        "            trace = \"payload['estimator']\"\n",
        "        else:\n",
        "            # fallback to first estimator-like value\n",
        "            for v in payload.values():\n",
        "                if hasattr(v, \"predict\") or hasattr(v, \"predict_proba\") or hasattr(v, \"get_booster\"):\n",
        "                    estimator = v\n",
        "                    trace = \"payload[first-estimator-like]\"\n",
        "                    break\n",
        "            if estimator is None:\n",
        "                # pass payload itself to unwrap\n",
        "                estimator, trace = unwrap_estimator(payload)\n",
        "    else:\n",
        "        estimator = payload\n",
        "        trace = \"payload-direct\"\n",
        "\n",
        "    # If estimator looks like wrapper, try deeper unwrap\n",
        "    if estimator is not None:\n",
        "        est2, tr2 = unwrap_estimator(estimator)\n",
        "        if est2 is not None:\n",
        "            estimator = est2\n",
        "            trace = trace + \"->\" + tr2\n",
        "\n",
        "    if estimator is None:\n",
        "        warns.append(f\"{name}: no estimator found inside payload (trace={trace})\")\n",
        "        print(\"  [WARN] no estimator found.\")\n",
        "        continue\n",
        "\n",
        "    print(\"  [DEBUG] estimator type:\", type(estimator).__name__, \"trace:\", trace)\n",
        "\n",
        "    # Determine feature order to use for this model\n",
        "    feat_order = None\n",
        "    for attr in (\"feature_names_in_\", \"feature_names\", \"feature_names_in\", \"columns\"):\n",
        "        if hasattr(estimator, attr):\n",
        "            try:\n",
        "                feat_order = list(getattr(estimator, attr))\n",
        "                break\n",
        "            except Exception:\n",
        "                pass\n",
        "    if feat_order is None:\n",
        "        # if booster exposes feature_names\n",
        "        try:\n",
        "            if hasattr(estimator, \"get_booster\"):\n",
        "                b = estimator.get_booster()\n",
        "                if hasattr(b, \"feature_names\") and b.feature_names:\n",
        "                    feat_order = list(b.feature_names)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if feat_order is None:\n",
        "        # fallback to candidate_features or length hint\n",
        "        nreq = getattr(estimator, \"n_features_in_\", None)\n",
        "        if nreq is not None and nreq <= len(candidate_features):\n",
        "            feat_order = candidate_features[:nreq]\n",
        "            warns.append(f\"{name}: inferred feat_order from candidate_features first {nreq}\")\n",
        "        else:\n",
        "            feat_order = list(candidate_features)\n",
        "            warns.append(f\"{name}: using global candidate_features as feat_order\")\n",
        "\n",
        "    # GAIN\n",
        "    gdict = compute_gain_from_model(estimator, feat_order)\n",
        "    if not gdict:\n",
        "        warns.append(f\"{name}: gain importances unavailable for estimator type {type(estimator).__name__}\")\n",
        "    else:\n",
        "        for f,v in gdict.items():\n",
        "            gain_rows.append({\"fold_file\": name, \"feature\": f, \"gain\": v})\n",
        "\n",
        "    # SHAP\n",
        "    shap_res = {}\n",
        "    if HAVE_SHAP:\n",
        "        shap_res = compute_shap_for_model(estimator, feat_order, df, subsample=SHAP_SUBSAMPLE)\n",
        "        if shap_res and \"__error\" in shap_res:\n",
        "            warns.append(f\"{name}: SHAP error -> {shap_res['__error']}\")\n",
        "            # do not store error as rows\n",
        "        else:\n",
        "            for f,v in shap_res.items():\n",
        "                shap_rows.append({\"fold_file\": name, \"feature\": f, \"shap_mean_abs\": v})\n",
        "    else:\n",
        "        warns.append(f\"{name}: shap not installed; skipping SHAP\")\n",
        "\n",
        "# Save per-fold raw outputs\n",
        "gain_df = pd.DataFrame(gain_rows)\n",
        "shap_df = pd.DataFrame(shap_rows)\n",
        "if not gain_df.empty:\n",
        "    out_gain = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_gain_per_fold.csv\")\n",
        "    gain_df.to_csv(out_gain, index=False)\n",
        "    print(\"[INFO] saved gain csv ->\", out_gain)\n",
        "if not shap_df.empty:\n",
        "    out_shap = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_shap_per_fold.csv\")\n",
        "    shap_df.to_csv(out_shap, index=False)\n",
        "    print(\"[INFO] saved shap csv ->\", out_shap)\n",
        "\n",
        "# Aggregate\n",
        "all_feats = sorted(set(list(gain_df['feature']) if not gain_df.empty else []) | set(list(shap_df['feature']) if not shap_df.empty else []) | set(candidate_features))\n",
        "rows = []\n",
        "for f in all_feats:\n",
        "    gvals = gain_df[gain_df['feature']==f]['gain'].values if not gain_df.empty else np.array([])\n",
        "    svals = shap_df[shap_df['feature']==f]['shap_mean_abs'].values if not shap_df.empty else np.array([])\n",
        "    rows.append({\n",
        "        \"feature\": f,\n",
        "        \"gain_mean\": float(np.nanmean(gvals)) if gvals.size else np.nan,\n",
        "        \"shap_mean_abs\": float(np.nanmean(svals)) if svals.size else np.nan,\n",
        "    })\n",
        "summary = pd.DataFrame(rows)\n",
        "if 'gain_mean' in summary.columns:\n",
        "    summary['gain_rank'] = summary['gain_mean'].rank(ascending=False, method='min', na_option='bottom')\n",
        "if 'shap_mean_abs' in summary.columns:\n",
        "    summary['shap_rank'] = summary['shap_mean_abs'].rank(ascending=False, method='min', na_option='bottom')\n",
        "rank_cols = [c for c in ('gain_rank','shap_rank') if c in summary.columns]\n",
        "if rank_cols:\n",
        "    summary['mean_rank'] = summary[rank_cols].mean(axis=1)\n",
        "else:\n",
        "    summary['mean_rank'] = np.nan\n",
        "summary = summary.sort_values('mean_rank').reset_index(drop=True)\n",
        "\n",
        "# Shortlists\n",
        "top_gain = summary.sort_values('gain_rank').head(N_TOP)['feature'].tolist() if 'gain_rank' in summary.columns else []\n",
        "top_shap = summary.sort_values('shap_rank').head(N_TOP)['feature'].tolist() if 'shap_rank' in summary.columns else []\n",
        "intersect_atleast2 = list(set(top_gain).intersection(set(top_shap))) if top_gain and top_shap else list(set(top_gain + top_shap)[:N_TOP])\n",
        "union_top = list(dict.fromkeys(top_gain + top_shap))[:N_TOP]\n",
        "\n",
        "# Save summary + shortlist\n",
        "ts = int(time.time())\n",
        "summary_csv = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_summary_{ts}.csv\")\n",
        "summary.to_csv(summary_csv, index=False)\n",
        "short_json = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_shortlist_{ts}.json\")\n",
        "with open(short_json, \"w\") as f:\n",
        "    json.dump({\"top_gain\": top_gain, \"top_shap\": top_shap, \"intersect_atleast2\": intersect_atleast2, \"union_top\": union_top}, f, indent=2)\n",
        "short_pkl = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_shortlist_{ts}.pkl\")\n",
        "pd.Series(intersect_atleast2).to_pickle(short_pkl)\n",
        "\n",
        "print(\"[OK] summary saved ->\", summary_csv)\n",
        "print(\"[OK] shortlist json ->\", short_json)\n",
        "print(\"[OK] shortlist pkl ->\", short_pkl)\n",
        "print(\"\\n=== TOP RESULTS ===\")\n",
        "print(\"top_gain (sample):\", top_gain[:20])\n",
        "print(\"top_shap (sample):\", top_shap[:20])\n",
        "print(\"intersection (n):\", len(intersect_atleast2), \"sample:\", intersect_atleast2[:20])\n",
        "print(\"union (n):\", len(union_top), \"sample:\", union_top[:20])\n",
        "\n",
        "if warns:\n",
        "    print(\"\\n[WARNINGS] (first 20):\")\n",
        "    for w in warns[:20]:\n",
        "        print(\" -\", w)\n",
        "\n",
        "print(\"\\nDone in %.1fs\" % (time.time() - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eWgbq0DxiZG7",
        "outputId": "284433aa-361d-4d65-ad44-3a4e2198a76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] df loaded: (17521, 238)\n",
            "[OK] correlation table saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/correlation_z_vs_zsmall.csv\n",
            "[TOP] highest correlations:\n",
            "                       base                  feat_a                    feat_b  \\\n",
            "186               tick_rule               tick_rule               tick_rule_z   \n",
            "174               tick_sign               tick_sign               tick_sign_z   \n",
            "176               tick_sign             tick_sign_z         tick_sign_z_small   \n",
            "188               tick_rule             tick_rule_z         tick_rule_z_small   \n",
            "175               tick_sign               tick_sign         tick_sign_z_small   \n",
            "187               tick_rule               tick_rule         tick_rule_z_small   \n",
            "93                    rsi_7                   rsi_7                   rsi_7_z   \n",
            "135                     pvo                     pvo                     pvo_z   \n",
            "123   vol_compression_ratio   vol_compression_ratio   vol_compression_ratio_z   \n",
            "132                  mfi_14                  mfi_14                  mfi_14_z   \n",
            "90                   rsi_14                  rsi_14                  rsi_14_z   \n",
            "144             ret_skew_24             ret_skew_24             ret_skew_24_z   \n",
            "126           squeeze_score           squeeze_score           squeeze_score_z   \n",
            "204      vpin_proxy_rolling      vpin_proxy_rolling      vpin_proxy_rolling_z   \n",
            "24                    ret_1                   ret_1                   ret_1_z   \n",
            "27                 logret_1                logret_1                logret_1_z   \n",
            "137                     pvo                   pvo_z               pvo_z_small   \n",
            "207  vpin_proxy_rolling_sma  vpin_proxy_rolling_sma  vpin_proxy_rolling_sma_z   \n",
            "30                   ret_3h                  ret_3h                  ret_3h_z   \n",
            "33                   ret_6h                  ret_6h                  ret_6h_z   \n",
            "\n",
            "         corr  \n",
            "186  0.999843  \n",
            "174  0.999835  \n",
            "176  0.998623  \n",
            "188  0.998595  \n",
            "175  0.998352  \n",
            "187  0.998326  \n",
            "93   0.996329  \n",
            "135  0.995668  \n",
            "123  0.991881  \n",
            "132  0.991834  \n",
            "90   0.991770  \n",
            "144  0.989548  \n",
            "126  0.988963  \n",
            "204  0.987333  \n",
            "24   0.983808  \n",
            "27   0.983765  \n",
            "137  0.983034  \n",
            "207  0.982481  \n",
            "30   0.982435  \n",
            "33   0.981997  \n",
            "\n",
            "[SUMMARY] corr >= 0.9: 89\n",
            "                       base                  feat_a                    feat_b  \\\n",
            "186               tick_rule               tick_rule               tick_rule_z   \n",
            "174               tick_sign               tick_sign               tick_sign_z   \n",
            "176               tick_sign             tick_sign_z         tick_sign_z_small   \n",
            "188               tick_rule             tick_rule_z         tick_rule_z_small   \n",
            "175               tick_sign               tick_sign         tick_sign_z_small   \n",
            "187               tick_rule               tick_rule         tick_rule_z_small   \n",
            "93                    rsi_7                   rsi_7                   rsi_7_z   \n",
            "135                     pvo                     pvo                     pvo_z   \n",
            "123   vol_compression_ratio   vol_compression_ratio   vol_compression_ratio_z   \n",
            "132                  mfi_14                  mfi_14                  mfi_14_z   \n",
            "90                   rsi_14                  rsi_14                  rsi_14_z   \n",
            "144             ret_skew_24             ret_skew_24             ret_skew_24_z   \n",
            "126           squeeze_score           squeeze_score           squeeze_score_z   \n",
            "204      vpin_proxy_rolling      vpin_proxy_rolling      vpin_proxy_rolling_z   \n",
            "24                    ret_1                   ret_1                   ret_1_z   \n",
            "27                 logret_1                logret_1                logret_1_z   \n",
            "137                     pvo                   pvo_z               pvo_z_small   \n",
            "207  vpin_proxy_rolling_sma  vpin_proxy_rolling_sma  vpin_proxy_rolling_sma_z   \n",
            "30                   ret_3h                  ret_3h                  ret_3h_z   \n",
            "33                   ret_6h                  ret_6h                  ret_6h_z   \n",
            "\n",
            "         corr  \n",
            "186  0.999843  \n",
            "174  0.999835  \n",
            "176  0.998623  \n",
            "188  0.998595  \n",
            "175  0.998352  \n",
            "187  0.998326  \n",
            "93   0.996329  \n",
            "135  0.995668  \n",
            "123  0.991881  \n",
            "132  0.991834  \n",
            "90   0.991770  \n",
            "144  0.989548  \n",
            "126  0.988963  \n",
            "204  0.987333  \n",
            "24   0.983808  \n",
            "27   0.983765  \n",
            "137  0.983034  \n",
            "207  0.982481  \n",
            "30   0.982435  \n",
            "33   0.981997  \n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 400x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "\n",
        "DF_PATH = \"/content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "THRESH = 0.90\n",
        "\n",
        "df = pd.read_pickle(DF_PATH).sort_index()\n",
        "cols = df.columns.tolist()\n",
        "\n",
        "print(\"[INFO] df loaded:\", df.shape)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Build families safely (detect exact suffixes)\n",
        "# --------------------------------------------------------------\n",
        "families = {}\n",
        "\n",
        "for c in cols:\n",
        "    if c.endswith(\"_z_small\"):\n",
        "        base = c[:-8]\n",
        "        families.setdefault(base, {\"plain\": None, \"z\": None, \"z_small\": None})\n",
        "        families[base][\"z_small\"] = c\n",
        "\n",
        "    elif c.endswith(\"_z\"):\n",
        "        base = c[:-2]\n",
        "        families.setdefault(base, {\"plain\": None, \"z\": None, \"z_small\": None})\n",
        "        families[base][\"z\"] = c\n",
        "\n",
        "    else:\n",
        "        base = c\n",
        "        families.setdefault(base, {\"plain\": None, \"z\": None, \"z_small\": None})\n",
        "        families[base][\"plain\"] = c\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Compute correlations per trio\n",
        "# --------------------------------------------------------------\n",
        "rows = []\n",
        "\n",
        "for base, trio in families.items():\n",
        "    subset = [x for x in trio.values() if x is not None]\n",
        "\n",
        "    if len(subset) < 2:\n",
        "        continue\n",
        "\n",
        "    subcorr = df[subset].corr()\n",
        "\n",
        "    for i in range(len(subset)):\n",
        "        for j in range(i + 1, len(subset)):\n",
        "            a, b = subset[i], subset[j]\n",
        "            rows.append({\n",
        "                \"base\": base,\n",
        "                \"feat_a\": a,\n",
        "                \"feat_b\": b,\n",
        "                \"corr\": subcorr.loc[a, b]\n",
        "            })\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Handle empty case safely\n",
        "# --------------------------------------------------------------\n",
        "if results_df.empty:\n",
        "    print(\"[INFO] No valid feature trios (plain/z/z_small) found.\")\n",
        "    out_csv = f\"{OUT_DIR}/correlation_z_vs_zsmall_EMPTY.csv\"\n",
        "    results_df.to_csv(out_csv, index=False)\n",
        "    print(f\"[OK] Saved empty CSV -> {out_csv}\")\n",
        "    sys.exit(0)   # <-- SAFE for Colab\n",
        "\n",
        "# Continue if non-empty\n",
        "results_df = results_df.sort_values(\"corr\", ascending=False)\n",
        "\n",
        "csv_path = f\"{OUT_DIR}/correlation_z_vs_zsmall.csv\"\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"[OK] correlation table saved -> {csv_path}\")\n",
        "print(\"[TOP] highest correlations:\")\n",
        "print(results_df.head(20))\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# High-correlation summary\n",
        "# --------------------------------------------------------------\n",
        "high_corr = results_df[results_df[\"corr\"] >= THRESH]\n",
        "\n",
        "print(f\"\\n[SUMMARY] corr >= {THRESH}: {len(high_corr)}\")\n",
        "if len(high_corr):\n",
        "    print(high_corr.head(20))\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Heatmap for top correlated base\n",
        "# --------------------------------------------------------------\n",
        "if len(high_corr):\n",
        "    fam = high_corr.iloc[0][\"base\"]\n",
        "    subset = [c for c in families[fam].values() if c is not None]\n",
        "\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.heatmap(df[subset].corr(), annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "    plt.title(f\"Correlation Heatmap: {fam}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJG3x_VojWnD",
        "outputId": "8581f988-05ee-4b58-e18e-c4736356571e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl shape: (17521, 238)\n",
            "[INFO] candidate feature count: 229 (example: ['open', 'high', 'low', 'close', 'volume', 'is_imputed', 'volume_imputed_flag', 'hl2', 'hlc3', 'ohlc4'])\n",
            "[INFO] shap rows: 0 | gain rows: 589\n",
            "\n",
            "[RESULT] shortlist saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_variant_shortlist_by_shap.json\n",
            "[RESULT] CSV -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_variant_shortlist_by_shap.csv | PKL -> /content/drive/MyDrive/quant_pipeline/mtb_out/feature_variant_shortlist_by_shap.pkl\n",
            "[STATS] total base families: 83 | selected variants: 83 | top30 saved\n",
            "\n",
            "Top 30 chosen variants (feature, score):\n",
            "01. ret_mean_24                              score=0.002875\n",
            "02. squeeze_score                            score=0.002706\n",
            "03. ret_48h_z_small                          score=0.002644\n",
            "04. atr_14_z                                 score=0.002623\n",
            "05. lower_wick_z                             score=0.002583\n",
            "06. ret_24h_z_small                          score=0.002450\n",
            "07. rsi_14_z_small                           score=0.002404\n",
            "08. ret_std_24                               score=0.002250\n",
            "09. bb_mid_z_small                           score=0.002239\n",
            "10. keltner_width                            score=0.002234\n",
            "11. amihud_50                                score=0.002218\n",
            "12. body_pct_z                               score=0.002212\n",
            "13. ema_21_z                                 score=0.002201\n",
            "14. weekday                                  score=0.002179\n",
            "15. bb_width                                 score=0.002158\n",
            "16. ret_12h_z                                score=0.002146\n",
            "17. awesome_osc                              score=0.002145\n",
            "18. ema_200_z_small                          score=0.002138\n",
            "19. macd                                     score=0.002117\n",
            "20. lower_wick_pct_z                         score=0.002116\n",
            "21. ema_100_z_small                          score=0.002102\n",
            "22. candle_body_z                            score=0.002082\n",
            "23. hl2                                      score=0.002040\n",
            "24. vpin_proxy_rolling_sma                   score=0.002034\n",
            "25. ppo                                      score=0.002025\n",
            "26. ema_10                                   score=0.002013\n",
            "27. ret_std_72                               score=0.001994\n",
            "28. gk_vol_50                                score=0.001979\n",
            "29. ret_skew_72                              score=0.001974\n",
            "30. ohlc4_z                                  score=0.001963\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: select best variant per base feature by SHAP (fallback gain) — Colab-ready\n",
        "# Output: shortlist JSON/CSV/PKL in OUT_DIR\n",
        "import os, glob, json, pickle, math, re, warnings\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- helper utils ----\n",
        "def find_latest(path_glob):\n",
        "    hits = sorted(glob.glob(path_glob), key=os.path.getmtime)\n",
        "    return hits[-1] if hits else None\n",
        "\n",
        "def load_df_features():\n",
        "    # prefer merged features with TB\n",
        "    cand = find_latest(os.path.join(OUT_DIR, \"df_features_with_tb*.pkl\")) or \\\n",
        "           find_latest(os.path.join(OUT_DIR, \"df_features_with_tb.pkl\")) or \\\n",
        "           find_latest(os.path.join(OUT_DIR, \"df_meta_step04*.pkl\")) or \\\n",
        "           find_latest(os.path.join(OUT_DIR, \"df_features*.pkl\"))\n",
        "    if cand is None:\n",
        "        raise FileNotFoundError(f\"No features found in {OUT_DIR}\")\n",
        "    df = pd.read_pickle(cand)\n",
        "    print(\"[INFO] loaded features:\", cand, \"shape:\", df.shape)\n",
        "    return df, cand\n",
        "\n",
        "# canonical base name extraction for variant families\n",
        "SUFFIXES = [\"_z_small\",\"_z_med\",\"_z_large\",\"_z\",\"_z1\",\"_z2\"]  # common variants\n",
        "_suffix_re = re.compile(\"|\".join(re.escape(s) + r\"$\" for s in SUFFIXES))\n",
        "def base_name(feat):\n",
        "    b = feat\n",
        "    # remove suffix repeatedly (defensive)\n",
        "    for _ in range(3):\n",
        "        new = _suffix_re.sub(\"\", b)\n",
        "        if new == b:\n",
        "            break\n",
        "        b = new\n",
        "    return b\n",
        "\n",
        "def aggregate_importances_from_csvs(patterns):\n",
        "    \"\"\"Find CSVs matching patterns and concat feature/importances.\n",
        "       Returns df with columns ['feature','importance','fold','method']\"\"\"\n",
        "    rows = []\n",
        "    for p,method in patterns:\n",
        "        for f in sorted(glob.glob(os.path.join(OUT_DIR, p))):\n",
        "            try:\n",
        "                df = pd.read_csv(f)\n",
        "            except Exception:\n",
        "                continue\n",
        "            # heuristics to find relevant columns\n",
        "            if \"feature\" in df.columns and (\"importance\" in df.columns or \"gain\" in df.columns or \"shap\" in df.columns):\n",
        "                imp_col = \"importance\" if \"importance\" in df.columns else (\"shap\" if \"shap\" in df.columns else \"gain\")\n",
        "                for _,r in df.iterrows():\n",
        "                    feat = str(r[\"feature\"])\n",
        "                    val = float(r.get(imp_col, np.nan)) if not pd.isna(r.get(imp_col, np.nan)) else np.nan\n",
        "                    rows.append({\"feature\": feat, \"importance\": val, \"fold\": os.path.basename(f), \"method\": method})\n",
        "            elif df.shape[1] >= 2:\n",
        "                # fallback: assume first col is feature, second is importance\n",
        "                feat = df.columns[0]\n",
        "                imp = df.columns[1]\n",
        "                try:\n",
        "                    for _,r in df.iterrows():\n",
        "                        rows.append({\"feature\": str(r[feat]), \"importance\": float(r[imp]), \"fold\": os.path.basename(f), \"method\": method})\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ---- load features ----\n",
        "df, features_path = load_df_features()\n",
        "\n",
        "# exclude TB/meta columns from candidate features\n",
        "candidate_cols = [c for c in df.columns if not c.startswith(\"tb_\") and not c.startswith(\"y_meta\") and c != \"index\"]\n",
        "if not candidate_cols:\n",
        "    raise RuntimeError(\"No candidate feature columns found after excluding tb_*\")\n",
        "print(f\"[INFO] candidate feature count: {len(candidate_cols)} (example: {candidate_cols[:10]})\")\n",
        "\n",
        "# ---- collect importance data ----\n",
        "# priority: SHAP per-fold CSVs -> gain per-fold CSVs -> attempt to compute from model files (skip heavy)\n",
        "shap_patterns = [(\"feature_importance_shap*.csv\",\"shap\"),\n",
        "                 (\"feature_importance_fixed_shap_per_fold*.csv\",\"shap\"),\n",
        "                 (\"feature_importance_fixed_shap*.csv\",\"shap\"),\n",
        "                 (\"feature_importance_shap_per_fold*.csv\",\"shap\")]\n",
        "gain_patterns = [(\"feature_importance_gain*.csv\",\"gain\"),\n",
        "                 (\"feature_importance_fixed_gain_per_fold*.csv\",\"gain\"),\n",
        "                 (\"feature_importance_gain_per_fold*.csv\",\"gain\"),\n",
        "                 (\"feature_importance_gain_*.csv\",\"gain\")]\n",
        "\n",
        "shap_df = aggregate_importances_from_csvs(shap_patterns)\n",
        "gain_df = aggregate_importances_from_csvs(gain_patterns)\n",
        "\n",
        "print(f\"[INFO] shap rows: {len(shap_df)} | gain rows: {len(gain_df)}\")\n",
        "\n",
        "# If no shap/gain CSVs found, attempt to read per-fold model files and extract feature_gain via model.get_booster()\n",
        "def extract_gain_from_models():\n",
        "    rows = []\n",
        "    model_files = sorted(glob.glob(os.path.join(OUT_DIR, \"cpcv_*_fold*.pkl\")) + glob.glob(os.path.join(OUT_DIR, \"cpcv_*_fold*.pkl\")))\n",
        "    model_files = [m for m in model_files if \"patched\" in m or \"cpcv_\" in os.path.basename(m)]\n",
        "    for mf in model_files:\n",
        "        try:\n",
        "            payload = pd.read_pickle(mf)\n",
        "        except Exception:\n",
        "            continue\n",
        "        # payload may be dict with 'model' or direct objects\n",
        "        model = None\n",
        "        if isinstance(payload, dict) and \"model\" in payload:\n",
        "            model = payload[\"model\"]\n",
        "        else:\n",
        "            # try common payload shapes\n",
        "            if hasattr(payload, \"get\"):\n",
        "                model = payload.get(\"model\", None)\n",
        "            if model is None:\n",
        "                # if it's a direct model object\n",
        "                model = payload\n",
        "        # try to extract booster/get_score\n",
        "        try:\n",
        "            if hasattr(model, \"get_booster\"):\n",
        "                booster = model.get_booster()\n",
        "                sc = booster.get_score(importance_type=\"gain\")\n",
        "                for feat, val in sc.items():\n",
        "                    # xgboost keys like 'f0','f1' -> map later; prefer only when feature names available\n",
        "                    rows.append({\"feature\": feat, \"importance\": float(val), \"fold\": os.path.basename(mf), \"method\":\"gain_model\"})\n",
        "            elif hasattr(model, \"booster_\"):\n",
        "                booster = model.booster_\n",
        "                sc = booster.get_score(importance_type=\"gain\")\n",
        "                for feat, val in sc.items():\n",
        "                    rows.append({\"feature\": feat, \"importance\": float(val), \"fold\": os.path.basename(mf), \"method\":\"gain_model\"})\n",
        "        except Exception as e:\n",
        "            # skip if any failure (keep process robust)\n",
        "            continue\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "if shap_df.empty and gain_df.empty:\n",
        "    print(\"[WARN] No shap/gain CSVs found in OUT_DIR — attempting to extract gain from model pickles (may be fast).\")\n",
        "    model_gain_df = extract_gain_from_models()\n",
        "    if not model_gain_df.empty:\n",
        "        gain_df = model_gain_df\n",
        "        print(f\"[INFO] extracted gain rows from models: {len(gain_df)}\")\n",
        "    else:\n",
        "        print(\"[WARN] No model gain could be extracted. Feature-importance data missing — aborting.\")\n",
        "        raise RuntimeError(\"No feature importance sources found. Run feature-importance pipeline first or ensure shap/gain CSVs exist in OUT_DIR.\")\n",
        "\n",
        "# ---- normalize & aggregate per-feature importance across folds & methods ----\n",
        "def normalise_series(s):\n",
        "    if s.isnull().all():\n",
        "        return s.fillna(0.0)\n",
        "    # positive-only normalisation\n",
        "    s = s.fillna(0.0)\n",
        "    total = s.abs().sum()\n",
        "    return s if total == 0 else s / total\n",
        "\n",
        "# prefer SHAP if available; combine both but weight shap higher\n",
        "agg = defaultdict(list)\n",
        "if not shap_df.empty:\n",
        "    # drop rows not in candidate_cols (defensive)\n",
        "    shap_df = shap_df[shap_df[\"feature\"].isin(candidate_cols) | shap_df[\"feature\"].str.contains(\"|\".join(candidate_cols))]\n",
        "    # per-fold normalize then mean\n",
        "    for fold, g in shap_df.groupby(\"fold\"):\n",
        "        ser = pd.Series(g.importance.values, index=g.feature.values)\n",
        "        ser = normalise_series(ser)\n",
        "        for feat, val in ser.items():\n",
        "            agg[feat].append((\"shap\", val))\n",
        "if not gain_df.empty:\n",
        "    # similar process for gains, but weight smaller\n",
        "    for fold, g in gain_df.groupby(\"fold\"):\n",
        "        ser = pd.Series(g.importance.values, index=g.feature.values)\n",
        "        ser = normalise_series(ser)\n",
        "        for feat, val in ser.items():\n",
        "            agg[feat].append((\"gain\", val))\n",
        "\n",
        "# compute combined score: shap weight 0.75, gain 0.25 (if both exist)\n",
        "combined = {}\n",
        "for feat, entries in agg.items():\n",
        "    shap_vals = [v for m,v in entries if m==\"shap\"]\n",
        "    gain_vals = [v for m,v in entries if m==\"gain\" or m==\"gain_model\"]\n",
        "    score = 0.0\n",
        "    if shap_vals:\n",
        "        score += 0.75 * (float(np.nanmean(shap_vals)))\n",
        "    if gain_vals:\n",
        "        score += 0.25 * (float(np.nanmean(gain_vals)))\n",
        "    # if only gain present, use gain\n",
        "    if (not shap_vals) and gain_vals:\n",
        "        score = float(np.nanmean(gain_vals))\n",
        "    combined[feat] = float(score)\n",
        "\n",
        "if not combined:\n",
        "    raise RuntimeError(\"No combined importances computed (empty).\")\n",
        "\n",
        "# ensure all candidate cols are represented (missing -> 0)\n",
        "for c in candidate_cols:\n",
        "    if c not in combined:\n",
        "        combined[c] = 0.0\n",
        "\n",
        "# ---- group by base and pick top variant per base ----\n",
        "by_base = defaultdict(list)\n",
        "for feat,score in combined.items():\n",
        "    b = base_name(feat)\n",
        "    by_base[b].append((feat, score))\n",
        "\n",
        "selected = {}\n",
        "for b, items in by_base.items():\n",
        "    # pick variant with max score; tie-breaker: prefer _z_small then _z then plain\n",
        "    items_sorted = sorted(items, key=lambda x: (x[1], 1 if x[0].endswith(\"_z_small\") else (2 if x[0].endswith(\"_z\") else 0)), reverse=True)\n",
        "    chosen_feat = items_sorted[0][0]\n",
        "    selected[b] = {\"chosen\": chosen_feat, \"score\": items_sorted[0][1], \"candidates\": items_sorted}\n",
        "\n",
        "# prepare final shortlist (ordered)\n",
        "final_list = [v[\"chosen\"] for k,v in sorted(selected.items(), key=lambda kv: kv[1][\"score\"], reverse=True)]\n",
        "\n",
        "# optionally limit top-K (user requested top-K earlier -> we keep full but also save top30)\n",
        "TOP_K = 30\n",
        "topK = final_list[:TOP_K]\n",
        "\n",
        "# ---- save outputs ----\n",
        "out_json = os.path.join(OUT_DIR, f\"feature_variant_shortlist_by_shap.json\")\n",
        "out_csv  = os.path.join(OUT_DIR, f\"feature_variant_shortlist_by_shap.csv\")\n",
        "out_pkl  = os.path.join(OUT_DIR, f\"feature_variant_shortlist_by_shap.pkl\")\n",
        "\n",
        "# save JSON with metadata\n",
        "with open(out_json, \"w\") as f:\n",
        "    json.dump({\"selected\": final_list, \"topK\": topK, \"per_base\": {b:{\"chosen\":v[\"chosen\"], \"score\":v[\"score\"], \"candidates\":v[\"candidates\"]} for b,v in selected.items()}}, f, indent=2)\n",
        "pd.DataFrame([{\"rank\": i+1, \"feature\": feat, \"base\": base_name(feat), \"score\": combined.get(feat,0.0)} for i,feat in enumerate(final_list)]).to_csv(out_csv, index=False)\n",
        "with open(out_pkl, \"wb\") as f:\n",
        "    pickle.dump({\"selected\": final_list, \"topK\": topK, \"selected_per_base\": selected, \"combined_scores\": combined}, f)\n",
        "\n",
        "# ---- quick diagnostics print ----\n",
        "print(\"\\n[RESULT] shortlist saved ->\", out_json)\n",
        "print(\"[RESULT] CSV ->\", out_csv, \"| PKL ->\", out_pkl)\n",
        "print(f\"[STATS] total base families: {len(by_base)} | selected variants: {len(final_list)} | top{TOP_K} saved\")\n",
        "\n",
        "# print top 30\n",
        "print(\"\\nTop 30 chosen variants (feature, score):\")\n",
        "for i,feat in enumerate(topK,1):\n",
        "    print(f\"{i:02d}. {feat:40} score={combined.get(feat,0):.6f}\")\n",
        "\n",
        "# ---- safety checks ----\n",
        "# warn if any chosen features are TB or contain direct target leakage patterns\n",
        "leak_candidates = [f for f in final_list if (\"tb_\" in f) or (\"t_break\" in f) or (\"ret_at_break\" in f)]\n",
        "if leak_candidates:\n",
        "    warnings.warn(f\"Potential leakage features selected (tb_/ret_at_break/t_break). Remove them before training: {leak_candidates}\")\n",
        "\n",
        "# done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hakKGJyKkmVW",
        "outputId": "f05c3a5a-46b3-4ad3-eaa7-eeec4663c37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl shape: (17521, 238)\n",
            "[INFO] using label: tb_label_h8  t_break: tb_t_break_h8\n",
            "[INFO] candidate feature count: 229\n",
            "[INFO] found model files -> 12\n",
            "[INFO] processing cpcv_long_fold1.pkl\n",
            "  estimator type: XGBClassifier\n",
            "   [OK] Gain importances: 99\n",
            "   [OK] SHAP computed: 156\n",
            "[INFO] processing cpcv_long_fold1_patched.pkl\n",
            "  estimator type: SklearnWrap\n",
            "[INFO] processing cpcv_long_fold2.pkl\n",
            "  estimator type: XGBClassifier\n",
            "   [OK] Gain importances: 96\n",
            "   [OK] SHAP computed: 156\n",
            "[INFO] processing cpcv_long_fold2_patched.pkl\n",
            "  estimator type: SklearnWrap\n",
            "[INFO] processing cpcv_long_fold3.pkl\n",
            "  estimator type: XGBClassifier\n",
            "   [OK] Gain importances: 102\n",
            "   [OK] SHAP computed: 156\n",
            "[INFO] processing cpcv_long_fold3_patched.pkl\n",
            "  estimator type: SklearnWrap\n",
            "[INFO] processing cpcv_short_fold1.pkl\n",
            "  estimator type: XGBClassifier\n",
            "   [OK] Gain importances: 99\n",
            "   [OK] SHAP computed: 156\n",
            "[INFO] processing cpcv_short_fold1_patched.pkl\n",
            "  estimator type: SklearnWrap\n",
            "[INFO] processing cpcv_short_fold2.pkl\n",
            "  estimator type: XGBClassifier\n",
            "   [OK] Gain importances: 95\n",
            "   [OK] SHAP computed: 156\n",
            "[INFO] processing cpcv_short_fold2_patched.pkl\n",
            "  estimator type: SklearnWrap\n",
            "[INFO] processing cpcv_short_fold3.pkl\n",
            "  estimator type: XGBClassifier\n",
            "   [OK] Gain importances: 98\n",
            "   [OK] SHAP computed: 156\n",
            "[INFO] processing cpcv_short_fold3_patched.pkl\n",
            "  estimator type: SklearnWrap\n",
            "\n",
            "[OK] Feature importance finished.\n",
            "  summary csv: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_1764900298_summary.csv\n",
            "  gain csv: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_1764900298_gain_per_fold.csv\n",
            "  shap csv: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_1764900298_shap_per_fold.csv\n",
            "  shortlist json: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_1764900298_shortlist.json\n",
            "  shortlist pkl: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_1764900298_shortlist.pkl\n",
            "\n",
            "[TOP] ['bb_mid_z_small', 'ema_200_z_small', 'ema_200', 'ema_100_z_small', 'amihud_50', 'ema_100', 'ret_48h_z_small', 'ret_12h_z', 'weekday', 'close', 'kyle_lambda_50', 'keltner_width', 'ret_std_24', 'mfi_14', 'hl2', 'vpin_proxy_rolling_sma', 'atr_14', 'obv', 'bb_high_z_small', 'macd_signal_z', 'macd_z_small', 'atr_14_z', 'ret_skew_72', 'roll_spread_50', 'pvo', 'ret_kurt_168', 'ema_50', 'squeeze_score', 'macd_z', 'bb_width']\n",
            "\n",
            "[WARNINGS] (first 20):\n",
            " - cpcv_long_fold1.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_long_fold1_patched.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_long_fold1_patched.pkl: no booster.get_score available -> skipping gain\n",
            " - cpcv_long_fold1_patched.pkl: shap.TreeExplainer init failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_long_fold1_patched.pkl: SHAP explainer not available; skipped shap for this fold.\n",
            " - cpcv_long_fold2.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_long_fold2_patched.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_long_fold2_patched.pkl: no booster.get_score available -> skipping gain\n",
            " - cpcv_long_fold2_patched.pkl: shap.TreeExplainer init failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_long_fold2_patched.pkl: SHAP explainer not available; skipped shap for this fold.\n",
            " - cpcv_long_fold3.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_long_fold3_patched.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_long_fold3_patched.pkl: no booster.get_score available -> skipping gain\n",
            " - cpcv_long_fold3_patched.pkl: shap.TreeExplainer init failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_long_fold3_patched.pkl: SHAP explainer not available; skipped shap for this fold.\n",
            " - cpcv_short_fold1.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_short_fold1_patched.pkl: model-feature-count (156) != global (229). Will attempt trim/pad.\n",
            " - cpcv_short_fold1_patched.pkl: no booster.get_score available -> skipping gain\n",
            " - cpcv_short_fold1_patched.pkl: shap.TreeExplainer init failed: Model type not yet supported by TreeExplainer: <class '__main__.SklearnWrap'>\n",
            " - cpcv_short_fold1_patched.pkl: SHAP explainer not available; skipped shap for this fold.\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: robust feature-importance (gain + SHAP if available) + shortlist\n",
        "# Paste & run in Colab. Requires: pandas, numpy, xgboost, shap (optional).\n",
        "import os, glob, json, time\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#---- helpers -----------------------------------------------------------------\n",
        "def load_pickle(path):\n",
        "    return pd.read_pickle(path)\n",
        "\n",
        "def find_model_files(out_dir):\n",
        "    files = sorted(glob.glob(os.path.join(out_dir, \"cpcv_*_fold*.pkl\")))\n",
        "    return files\n",
        "\n",
        "def try_unwrap_model(payload):\n",
        "    \"\"\"\n",
        "    payload may be a dict saved earlier {\"model\": model, \"scaler\":..., \"cfg\":...}\n",
        "    or the raw model object; return (estimator, scaler_or_None, meta_dict)\n",
        "    \"\"\"\n",
        "    scaler = None\n",
        "    meta = {}\n",
        "    est = payload\n",
        "    if isinstance(payload, dict):\n",
        "        est = payload.get(\"model\", payload.get(\"estimator\", payload.get(\"clf\", None)))\n",
        "        scaler = payload.get(\"scaler\", None)\n",
        "        meta = {k:v for k,v in payload.items() if k not in (\"model\",\"scaler\",\"estimator\",\"clf\")}\n",
        "    return est, scaler, meta\n",
        "\n",
        "def feature_list_from_df(df):\n",
        "    # candidate features: not tb_ columns\n",
        "    return [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "\n",
        "def save_csv(df, path):\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "#---- load merged features+TB -------------------------------------------------\n",
        "meta_path = os.path.join(OUT_DIR, \"df_features_with_tb.pkl\")\n",
        "if not os.path.exists(meta_path):\n",
        "    raise FileNotFoundError(f\"{meta_path} not found. Run merge TB -> meta step first.\")\n",
        "df = load_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "print(\"[INFO] loaded features:\", meta_path, \"shape:\", df.shape)\n",
        "\n",
        "# find TB label & t_break\n",
        "label_cols = [c for c in df.columns if c.startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"No tb_label_* columns in merged features.\")\n",
        "tb_label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tb_tbreak_col = tb_label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "print(\"[INFO] using label:\", tb_label_col, \" t_break:\", tb_tbreak_col)\n",
        "\n",
        "# global candidate features (the set you'll reference)\n",
        "global_feats = feature_list_from_df(df)\n",
        "n_expected = len(global_feats)\n",
        "print(\"[INFO] candidate feature count:\", n_expected)\n",
        "\n",
        "#---- find model files -------------------------------------------------------\n",
        "model_files = find_model_files(OUT_DIR)\n",
        "if len(model_files) == 0:\n",
        "    raise FileNotFoundError(\"No cpcv_*_fold*.pkl models found in OUT_DIR. Run CPCV first.\")\n",
        "print(\"[INFO] found model files ->\", len(model_files))\n",
        "\n",
        "#---- attempt to extract gain importances & SHAP per fold ---------------------\n",
        "# We'll build two tables: gain_rows, shap_rows\n",
        "gain_rows = []\n",
        "shap_rows = []\n",
        "warnings = []\n",
        "\n",
        "# optional imports\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    shap = None\n",
        "    SHAP_AVAILABLE = False\n",
        "    warnings.append(f\"shap unavailable: {e}\")\n",
        "\n",
        "from xgboost import XGBClassifier, Booster\n",
        "\n",
        "for mf in model_files:\n",
        "    basename = os.path.basename(mf)\n",
        "    print(\"[INFO] processing\", basename)\n",
        "    try:\n",
        "        payload = pd.read_pickle(mf)\n",
        "    except Exception as e:\n",
        "        warnings.append(f\"{basename}: failed to load pickle: {e}\")\n",
        "        continue\n",
        "\n",
        "    est, scaler, meta = try_unwrap_model(payload)\n",
        "    est_type = type(est).__name__\n",
        "    print(\"  estimator type:\", est_type)\n",
        "\n",
        "    # try to recover feature names used when training:\n",
        "    feat_names = None\n",
        "    # check if estimator has feature_names_in_ (sklearn)\n",
        "    if hasattr(est, \"feature_names_in_\"):\n",
        "        feat_names = list(getattr(est, \"feature_names_in_\"))\n",
        "    # check xgboost booster structure\n",
        "    if feat_names is None:\n",
        "        # if est is sklearn wrapper around xgb (XGBClassifier), try estimator.get_booster().feature_names\n",
        "        try:\n",
        "            booster = est.get_booster()\n",
        "            # booster.feature_names may exist\n",
        "            if hasattr(booster, \"feature_names\") and booster.feature_names is not None:\n",
        "                feat_names = list(booster.feature_names)\n",
        "        except Exception:\n",
        "            booster = None\n",
        "    else:\n",
        "        booster = None\n",
        "\n",
        "    # fallback: if scaler present and scaler had feature_names_in_, use it\n",
        "    if feat_names is None and scaler is not None and hasattr(scaler, \"feature_names_in_\"):\n",
        "        feat_names = list(getattr(scaler, \"feature_names_in_\"))\n",
        "\n",
        "    # final fallback: assume global list but we must ensure shape compatibility\n",
        "    if feat_names is None:\n",
        "        feat_names = list(global_feats)\n",
        "\n",
        "    # If model expects fewer features than global, trim; if expects more, warn\n",
        "    exp_n = len(feat_names)\n",
        "    if exp_n != n_expected:\n",
        "        warnings.append(f\"{basename}: model-feature-count ({exp_n}) != global ({n_expected}). Will attempt trim/pad.\")\n",
        "    # Prepare X sample for SHAP/predictions (use first 2048 rows or full if small)\n",
        "    sample_X = df[global_feats].iloc[:min(len(df), 2048)].copy()\n",
        "    # If feature names != global_feats, try to rearrange columns for predict/shap\n",
        "    try:\n",
        "        # if model was trained on a subset, pick those columns\n",
        "        X_for_model = sample_X.loc[:, feat_names] if set(feat_names).issubset(sample_X.columns) else sample_X.iloc[:, :exp_n]\n",
        "    except Exception:\n",
        "        X_for_model = sample_X.iloc[:, :exp_n]\n",
        "\n",
        "    # GAS: Gain importances\n",
        "    try:\n",
        "        # prefer booster if available\n",
        "        if booster is None:\n",
        "            try:\n",
        "                booster = est.get_booster()\n",
        "            except Exception:\n",
        "                booster = None\n",
        "        if booster is not None and hasattr(booster, \"get_score\"):\n",
        "            gdict = booster.get_score(importance_type=\"gain\")\n",
        "            # gdict keys are feature names or \"f0\"..; convert using feat_names mapping when needed\n",
        "            # normalize keys: if keys like 'f0' then map to feat_names[int(idx[1:])]\n",
        "            rows = []\n",
        "            for k,v in gdict.items():\n",
        "                if k.startswith(\"f\") and k[1:].isdigit() and int(k[1:]) < len(feat_names):\n",
        "                    fname = feat_names[int(k[1:])]\n",
        "                else:\n",
        "                    fname = k\n",
        "                rows.append({\"fold_file\": basename, \"method\":\"gain\", \"feature\":fname, \"importance\": float(v)})\n",
        "            gain_rows.extend(rows)\n",
        "            print(\"   [OK] Gain importances:\", len(rows))\n",
        "        else:\n",
        "            warnings.append(f\"{basename}: no booster.get_score available -> skipping gain\")\n",
        "    except Exception as e:\n",
        "        warnings.append(f\"{basename}: gain extraction failed: {e}\")\n",
        "\n",
        "    # SHAP (best-effort)\n",
        "    if SHAP_AVAILABLE:\n",
        "        try:\n",
        "            # prefer TreeExplainer on booster if possible\n",
        "            explainer = None\n",
        "            try:\n",
        "                if booster is not None:\n",
        "                    explainer = shap.TreeExplainer(booster)\n",
        "                else:\n",
        "                    explainer = shap.TreeExplainer(est)\n",
        "            except Exception as e:\n",
        "                explainer = None\n",
        "                warnings.append(f\"{basename}: shap.TreeExplainer init failed: {e}\")\n",
        "\n",
        "            if explainer is not None:\n",
        "                # compute shap values (use small sample to speed)\n",
        "                X_eval = X_for_model.iloc[:min(256, len(X_for_model))]\n",
        "                shap_vals = explainer.shap_values(X_eval)\n",
        "                # shap_values shape varies (list or array). reduce to mean(abs) per feature\n",
        "                if isinstance(shap_vals, list):\n",
        "                    # multi-output; average across outputs then across rows\n",
        "                    arr = np.mean(np.abs(np.vstack([np.array(s) for s in shap_vals])), axis=0)\n",
        "                else:\n",
        "                    arr = np.mean(np.abs(shap_vals), axis=0)\n",
        "                # ensure mapping length == n_features\n",
        "                n_feats_local = X_eval.shape[1]\n",
        "                for i in range(min(n_feats_local, len(arr))):\n",
        "                    fname = X_eval.columns[i]\n",
        "                    shap_rows.append({\"fold_file\": basename, \"method\":\"shap\", \"feature\": fname, \"importance\": float(arr[i])})\n",
        "                print(\"   [OK] SHAP computed:\", min(n_feats_local, len(arr)))\n",
        "            else:\n",
        "                warnings.append(f\"{basename}: SHAP explainer not available; skipped shap for this fold.\")\n",
        "        except Exception as e:\n",
        "            warnings.append(f\"{basename}: SHAP computation failed: {e}\")\n",
        "    else:\n",
        "        # shap not installed\n",
        "        pass\n",
        "\n",
        "#---- aggregate results ------------------------------------------------------\n",
        "gain_df = pd.DataFrame(gain_rows) if gain_rows else pd.DataFrame(columns=[\"fold_file\",\"method\",\"feature\",\"importance\"])\n",
        "shap_df = pd.DataFrame(shap_rows) if shap_rows else pd.DataFrame(columns=[\"fold_file\",\"method\",\"feature\",\"importance\"])\n",
        "\n",
        "# normalize per-fold ranks -> mean across folds\n",
        "def agg_rank(df_in, method_name):\n",
        "    if df_in.empty:\n",
        "        return pd.DataFrame(columns=[\"feature\",\"mean_rank\",\"mean_raw\"])\n",
        "    # for each fold, compute rank (descending importance -> rank 1 = highest)\n",
        "    df = df_in.copy()\n",
        "    df[\"importance\"] = pd.to_numeric(df[\"importance\"], errors=\"coerce\").fillna(0.0)\n",
        "    ranks = []\n",
        "    for f,sub in df.groupby(\"fold_file\"):\n",
        "        sub = sub.copy()\n",
        "        sub[\"rank\"] = sub[\"importance\"].rank(ascending=False, method=\"min\")\n",
        "        ranks.append(sub[[\"feature\",\"rank\",\"importance\"]])\n",
        "    if not ranks:\n",
        "        return pd.DataFrame(columns=[\"feature\",\"mean_rank\",\"mean_raw\"])\n",
        "    ranks_df = pd.concat(ranks)\n",
        "    agg = ranks_df.groupby(\"feature\").agg(mean_rank=(\"rank\",\"mean\"), mean_raw=(\"importance\",\"mean\"), count=(\"rank\",\"size\")).reset_index()\n",
        "    agg = agg.sort_values(\"mean_rank\")\n",
        "    agg[\"method\"] = method_name\n",
        "    return agg\n",
        "\n",
        "gain_agg = agg_rank(gain_df, \"gain\")\n",
        "shap_agg = agg_rank(shap_df, \"shap\")\n",
        "\n",
        "# Merge methods\n",
        "all_features = sorted(set(list(gain_agg[\"feature\"]) + list(shap_agg[\"feature\"])))\n",
        "summary = pd.DataFrame({\"feature\": all_features})\n",
        "if not gain_agg.empty:\n",
        "    summary = summary.merge(gain_agg[[\"feature\",\"mean_rank\",\"mean_raw\"]].rename(columns={\"mean_rank\":\"gain_mean_rank\",\"mean_raw\":\"gain_mean\"}), on=\"feature\", how=\"left\")\n",
        "if not shap_agg.empty:\n",
        "    summary = summary.merge(shap_agg[[\"feature\",\"mean_rank\",\"mean_raw\"]].rename(columns={\"mean_rank\":\"shap_mean_rank\",\"mean_raw\":\"shap_mean\"}), on=\"feature\", how=\"left\")\n",
        "\n",
        "# compute combined score (simple): inverse mean_rank if available\n",
        "def compute_score(row):\n",
        "    ranks = []\n",
        "    if not pd.isna(row.get(\"gain_mean_rank\")):\n",
        "        ranks.append(row[\"gain_mean_rank\"])\n",
        "    if not pd.isna(row.get(\"shap_mean_rank\")):\n",
        "        ranks.append(row[\"shap_mean_rank\"])\n",
        "    if len(ranks)==0:\n",
        "        return 0.0\n",
        "    return 1.0 / (np.mean(ranks) + 1e-9)\n",
        "\n",
        "summary[\"score\"] = summary.apply(compute_score, axis=1)\n",
        "summary = summary.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "#---- shortlist selection -----------------------------------------------------\n",
        "TOP_K = 30\n",
        "shortlist = list(summary[\"feature\"].iloc[:TOP_K])\n",
        "\n",
        "#---- save outputs -----------------------------------------------------------\n",
        "ts = int(time.time())\n",
        "out_prefix = os.path.join(OUT_DIR, f\"feature_importance_fixed_{ts}\")\n",
        "summary_path = out_prefix + \"_summary.csv\"\n",
        "gain_path = out_prefix + \"_gain_per_fold.csv\"\n",
        "shap_path = out_prefix + \"_shap_per_fold.csv\"\n",
        "shortlist_json = out_prefix + \"_shortlist.json\"\n",
        "shortlist_pkl = out_prefix + \"_shortlist.pkl\"\n",
        "\n",
        "summary.to_csv(summary_path, index=False)\n",
        "if not gain_df.empty:\n",
        "    gain_df.to_csv(gain_path, index=False)\n",
        "if not shap_df.empty:\n",
        "    shap_df.to_csv(shap_path, index=False)\n",
        "\n",
        "with open(shortlist_json, \"w\") as f:\n",
        "    json.dump({\"shortlist\":shortlist, \"meta\":{\"n_features\":n_expected, \"model_files\": model_files}}, f, indent=2)\n",
        "pd.to_pickle(shortlist, shortlist_pkl)\n",
        "\n",
        "#---- print short summary ----------------------------------------------------\n",
        "print(\"\\n[OK] Feature importance finished.\")\n",
        "print(\"  summary csv:\", summary_path)\n",
        "print(\"  gain csv:\", gain_path if not gain_df.empty else \"(none)\")\n",
        "print(\"  shap csv:\", shap_path if not shap_df.empty else \"(none)\")\n",
        "print(\"  shortlist json:\", shortlist_json)\n",
        "print(\"  shortlist pkl:\", shortlist_pkl)\n",
        "print(\"\\n[TOP]\",\n",
        "      list(summary[\"feature\"].iloc[:min(30,len(summary))]))\n",
        "if warnings:\n",
        "    print(\"\\n[WARNINGS] (first 20):\")\n",
        "    for w in warnings[:20]:\n",
        "        print(\" -\", w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623,
          "referenced_widgets": [
            "addd303bd705460c9cbbb0ad99fa8760",
            "628ac448548d436eb1a2220fbeb6af77",
            "aa40a33531414b30ac3f7a24617ebd86",
            "8f274cb107464eadb18cf23762709ac3",
            "20466e108c72449dbf379b5507c93c54",
            "08eb86bd76f44e26b5a261dffc59c89d",
            "7abe8de868884741aecfc532a1791c47",
            "7ef1715563ab4e489596e50e7d064e85",
            "88812425d8ef4cd680ff87bfe090d6e2",
            "0e9e40b7d6b243969f786ca239b46f23",
            "96e53fa4fafc407183b4da867aa6fd68"
          ]
        },
        "id": "T9g3AXhClB5k",
        "outputId": "6e906455-fe98-4697-c882-815486e39cb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:04:58,460] A new study created in memory with name: cpcv_opt_1764900298\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] using shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/feature_importance_fixed_1764900298_shortlist.pkl n: 30\n",
            "[INFO] label: tb_label_h8 tbreak: tb_t_break_h8\n",
            "[INFO] using features: 30\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "addd303bd705460c9cbbb0ad99fa8760",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:05:06,873] Trial 0 finished with value: 0.5093478167886648 and parameters: {'max_depth': 3, 'learning_rate': 0.1593589717717672, 'subsample': 0.75283245254877, 'colsample_bytree': 0.9771927549053999, 'reg_lambda': 2.70630327081388, 'reg_alpha': 9.015549846121168, 'n_estimators': 100}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:05:24,644] Trial 1 finished with value: 0.5050799093676425 and parameters: {'max_depth': 5, 'learning_rate': 0.06657699535652682, 'subsample': 0.6161085503162154, 'colsample_bytree': 0.6867942542057872, 'reg_lambda': 1.1713842321319456, 'reg_alpha': 0.010210198332825715, 'n_estimators': 200}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:05:58,114] Trial 2 finished with value: 0.5026312390727278 and parameters: {'max_depth': 8, 'learning_rate': 0.1030595830922356, 'subsample': 0.7581906905421529, 'colsample_bytree': 0.5033554451831086, 'reg_lambda': 0.0022979731073536577, 'reg_alpha': 0.007911866814189544, 'n_estimators': 300}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:06:08,984] Trial 3 finished with value: 0.5064668622482069 and parameters: {'max_depth': 3, 'learning_rate': 0.029564859958929456, 'subsample': 0.7425727390728005, 'colsample_bytree': 0.6182218006085893, 'reg_lambda': 0.10049179537606914, 'reg_alpha': 0.0028040423384694925, 'n_estimators': 200}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:06:21,815] Trial 4 finished with value: 0.508126156641615 and parameters: {'max_depth': 3, 'learning_rate': 0.16685956817522604, 'subsample': 0.8819769072233019, 'colsample_bytree': 0.7413889913745535, 'reg_lambda': 0.443136314078559, 'reg_alpha': 0.010856765378945952, 'n_estimators': 300}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:06:39,009] Trial 5 finished with value: 0.5011860561702717 and parameters: {'max_depth': 5, 'learning_rate': 0.10028294314758747, 'subsample': 0.9928819410720509, 'colsample_bytree': 0.9794345810012788, 'reg_lambda': 0.13688616774177614, 'reg_alpha': 0.10782192847602112, 'n_estimators': 200}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:07:08,699] Trial 6 finished with value: 0.49645012120398985 and parameters: {'max_depth': 7, 'learning_rate': 0.12613806957231616, 'subsample': 0.71165207628231, 'colsample_bytree': 0.6675713978675185, 'reg_lambda': 0.02097763308126914, 'reg_alpha': 0.0016544757623071843, 'n_estimators': 300}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:07:30,566] Trial 7 finished with value: 0.506789931242737 and parameters: {'max_depth': 6, 'learning_rate': 0.02579982623594606, 'subsample': 0.7610087089909734, 'colsample_bytree': 0.6867183713381529, 'reg_lambda': 0.20373322437252603, 'reg_alpha': 0.037945596747998744, 'n_estimators': 200}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:07:42,233] Trial 8 finished with value: 0.5055184352808032 and parameters: {'max_depth': 3, 'learning_rate': 0.013906851255987365, 'subsample': 0.8000440256093164, 'colsample_bytree': 0.7256406249273641, 'reg_lambda': 1.6218533583730972, 'reg_alpha': 0.013164267572591912, 'n_estimators': 300}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:08:05,005] Trial 9 finished with value: 0.5067640589731705 and parameters: {'max_depth': 5, 'learning_rate': 0.011787524838351469, 'subsample': 0.8500889678406252, 'colsample_bytree': 0.8760595812229768, 'reg_lambda': 0.02646965155123491, 'reg_alpha': 0.013338206094095513, 'n_estimators': 300}. Best is trial 0 with value: 0.5093478167886648.\n",
            "[I 2025-12-05 02:08:29,231] Trial 10 finished with value: 0.5176762285392171 and parameters: {'max_depth': 4, 'learning_rate': 0.05220060203176875, 'subsample': 0.6330871161675649, 'colsample_bytree': 0.8669438899311978, 'reg_lambda': 6.3486263302285835, 'reg_alpha': 7.459182888144092, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:08:50,417] Trial 11 finished with value: 0.5130692299858072 and parameters: {'max_depth': 4, 'learning_rate': 0.05986299729434195, 'subsample': 0.6334468672543575, 'colsample_bytree': 0.8744299614999202, 'reg_lambda': 9.776370637863444, 'reg_alpha': 8.52828340999416, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:09:07,771] Trial 12 finished with value: 0.5126838975703333 and parameters: {'max_depth': 4, 'learning_rate': 0.04823002801300787, 'subsample': 0.6077117295339693, 'colsample_bytree': 0.8581320855449037, 'reg_lambda': 8.79641838342508, 'reg_alpha': 9.88341079597318, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:09:18,462] Trial 13 finished with value: 0.5036682127333084 and parameters: {'max_depth': 4, 'learning_rate': 0.06005730427018726, 'subsample': 0.6807645631896031, 'colsample_bytree': 0.8442255205149836, 'reg_lambda': 8.682199618959874, 'reg_alpha': 1.5605420979862352, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:09:29,060] Trial 14 finished with value: 0.5088400195107842 and parameters: {'max_depth': 4, 'learning_rate': 0.02745087040737496, 'subsample': 0.6827992035147795, 'colsample_bytree': 0.9149204200883859, 'reg_lambda': 3.3858500009164354, 'reg_alpha': 1.0802476052075483, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:09:37,838] Trial 15 finished with value: 0.5100023838154674 and parameters: {'max_depth': 4, 'learning_rate': 0.03872757624107114, 'subsample': 0.6636896683679959, 'colsample_bytree': 0.7898826497470722, 'reg_lambda': 0.6197998535006164, 'reg_alpha': 1.342415642801756, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:09:52,974] Trial 16 finished with value: 0.5163173283321971 and parameters: {'max_depth': 6, 'learning_rate': 0.07612201992797679, 'subsample': 0.6419866617378255, 'colsample_bytree': 0.795062408871968, 'reg_lambda': 9.541649409666563, 'reg_alpha': 0.3613438500213801, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:10:06,973] Trial 17 finished with value: 0.5032029577145131 and parameters: {'max_depth': 6, 'learning_rate': 0.0813462327910453, 'subsample': 0.8185582138572903, 'colsample_bytree': 0.780524440833057, 'reg_lambda': 0.022724321982540836, 'reg_alpha': 0.2591662288892997, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:10:25,923] Trial 18 finished with value: 0.5047565129642083 and parameters: {'max_depth': 7, 'learning_rate': 0.017581486015588205, 'subsample': 0.9057611303065094, 'colsample_bytree': 0.8036019233967698, 'reg_lambda': 0.001575005989888177, 'reg_alpha': 0.4241350198677822, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:10:46,279] Trial 19 finished with value: 0.5172607257415429 and parameters: {'max_depth': 7, 'learning_rate': 0.03993205928662422, 'subsample': 0.654950074743651, 'colsample_bytree': 0.9285518679468592, 'reg_lambda': 3.6086145616528054, 'reg_alpha': 3.4164645991156943, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:11:13,654] Trial 20 finished with value: 0.5104697147460373 and parameters: {'max_depth': 8, 'learning_rate': 0.03983117557221442, 'subsample': 0.7107664185772745, 'colsample_bytree': 0.9109586923173448, 'reg_lambda': 0.4475989496756605, 'reg_alpha': 2.3396004518041686, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:11:35,798] Trial 21 finished with value: 0.5153799427076216 and parameters: {'max_depth': 7, 'learning_rate': 0.04879194714755891, 'subsample': 0.6474678201492994, 'colsample_bytree': 0.9328395994022043, 'reg_lambda': 3.496265956321801, 'reg_alpha': 3.7451713162821942, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:11:50,456] Trial 22 finished with value: 0.5118102382575493 and parameters: {'max_depth': 6, 'learning_rate': 0.079280833190867, 'subsample': 0.6457482326544878, 'colsample_bytree': 0.8173781113750684, 'reg_lambda': 4.120039879892939, 'reg_alpha': 0.4210086284098968, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[I 2025-12-05 02:12:11,074] Trial 23 finished with value: 0.5095749689066034 and parameters: {'max_depth': 7, 'learning_rate': 0.033390057870792855, 'subsample': 0.6045400724999245, 'colsample_bytree': 0.9361186250512425, 'reg_lambda': 1.4379117202125826, 'reg_alpha': 3.959723325603294, 'n_estimators': 100}. Best is trial 10 with value: 0.5176762285392171.\n",
            "[OK] Optuna finished. best auc: 0.5176762285392171\n",
            "  saved: /content/drive/MyDrive/quant_pipeline/mtb_out/optuna_result_1764900731.json\n",
            "  best params: {'max_depth': 4, 'learning_rate': 0.05220060203176875, 'subsample': 0.6330871161675649, 'colsample_bytree': 0.8669438899311978, 'reg_lambda': 6.3486263302285835, 'reg_alpha': 7.459182888144092, 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: Optuna tuning using CPCV + shortlist\n",
        "# Requirements: optuna, pandas, numpy; assumes run_cpcv_grid is defined in the notebook.\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# load merged features + shortlist\n",
        "meta_path = os.path.join(OUT_DIR, \"df_features_with_tb.pkl\")\n",
        "if not os.path.exists(meta_path):\n",
        "    raise FileNotFoundError(\"df_features_with_tb.pkl not found in OUT_DIR. Merge TB -> meta first.\")\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "\n",
        "# load shortlist (pick the most recent fixed shortlist if present)\n",
        "cands = sorted([p for p in os.listdir(OUT_DIR) if p.startswith(\"feature_importance_fixed_\") and p.endswith(\"_shortlist.pkl\")])\n",
        "if not cands:\n",
        "    # fallback: generic shortlist file names you produced\n",
        "    cands = sorted([p for p in os.listdir(OUT_DIR) if \"shortlist\" in p and p.endswith(\".pkl\")])\n",
        "if not cands:\n",
        "    raise FileNotFoundError(\"No shortlist pkl in OUT_DIR. Run feature-importance cell first.\")\n",
        "shortlist_pkl = os.path.join(OUT_DIR, cands[-1])\n",
        "shortlist = pd.read_pickle(shortlist_pkl)\n",
        "print(\"[INFO] using shortlist:\", shortlist_pkl, \"n:\", len(shortlist))\n",
        "\n",
        "# choose label/tbreak\n",
        "label_cols = [c for c in df.columns if c.startswith(\"tb_label_\")]\n",
        "tb_label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tb_tbreak_col = tb_label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "print(\"[INFO] label:\", tb_label_col, \"tbreak:\", tb_tbreak_col)\n",
        "\n",
        "# create feature set: shortlisted features intersect df columns\n",
        "feat_cols = [f for f in shortlist if f in df.columns]\n",
        "if len(feat_cols) == 0:\n",
        "    raise RuntimeError(\"No shortlist features present in df columns.\")\n",
        "print(\"[INFO] using features:\", len(feat_cols))\n",
        "\n",
        "# ensure run_cpcv_grid exists\n",
        "if 'run_cpcv_grid' not in globals():\n",
        "    raise RuntimeError(\"run_cpcv_grid not defined in notebook. Paste/run the CPCV cell before this.\")\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Optuna objective: quick CPCV with small grid per trial (deterministic mapping)\n",
        "def objective(trial):\n",
        "    # suggest params\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
        "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
        "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-3, 10.0),\n",
        "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-3, 10.0),\n",
        "        # keep n_estimators moderate for objective evaluations\n",
        "        \"n_estimators\": int(trial.suggest_categorical(\"n_estimators\", [100, 200, 300])),\n",
        "    }\n",
        "    # xgb_common passed-through\n",
        "    xgb_common = {\"verbosity\": 0}\n",
        "    # small CPCV budget for trials (fast)\n",
        "    try:\n",
        "        res = run_cpcv_grid(\n",
        "            df_features = df,\n",
        "            tbreak_series = df[tb_tbreak_col],\n",
        "            feature_cols = feat_cols,\n",
        "            side = \"long\",\n",
        "            label_col = tb_label_col,\n",
        "            n_outer = 3,\n",
        "            n_inner = 2,\n",
        "            embargo = pd.Timedelta(\"1H\"),\n",
        "            drop_unmapped = True,\n",
        "            random_state = 42,\n",
        "            scaler_type = None if False else __import__('sklearn').preprocessing.StandardScaler,\n",
        "            use_smote = False,\n",
        "            grid = {k: [v] for k,v in params.items() if k != \"n_estimators\"}, # run_cpcv_grid expects grid of lists\n",
        "            xgb_common = {**xgb_common, **{\"n_estimators\": params[\"n_estimators\"]}},\n",
        "            out_dir = OUT_DIR,\n",
        "            max_inner_budget = 200\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # if run fails, return bad score\n",
        "        print(\"run_cpcv_grid failed in trial:\", e)\n",
        "        return 0.5\n",
        "\n",
        "    # compute OOF AUC on long (use stored oof)\n",
        "    oof = res.get(\"oof\")\n",
        "    if oof is None or oof.isna().all():\n",
        "        return 0.5\n",
        "    y_true = (df[tb_label_col] == 1).astype(int)\n",
        "    # align\n",
        "    mask = oof.notna()\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true.loc[mask], oof.loc[mask])\n",
        "    except Exception:\n",
        "        auc = 0.5\n",
        "    # Optuna tries to maximize, so return auc\n",
        "    return float(auc)\n",
        "\n",
        "# run study\n",
        "study_name = f\"cpcv_opt_{int(time.time())}\"\n",
        "storage = None  # no RDB by default\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=study_name)\n",
        "N_TRIALS = 24  # quick budget; increase to 100+ later\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "# save results\n",
        "best = study.best_trial\n",
        "outp = {\n",
        "    \"best_value\": best.value,\n",
        "    \"best_params\": best.params,\n",
        "    \"n_trials\": len(study.trials)\n",
        "}\n",
        "json_path = os.path.join(OUT_DIR, f\"optuna_result_{int(time.time())}.json\")\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(outp, f, indent=2)\n",
        "print(\"[OK] Optuna finished. best auc:\", best.value)\n",
        "print(\"  saved:\", json_path)\n",
        "print(\"  best params:\", best.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlNh96qPnrK-",
        "outputId": "cbfcb5fe-57d9-48e5-88bd-b6b0523258a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_with_tb.pkl shape=(17521, 238)\n",
            "[INFO] using shortlist file: feature_importance_fixed_1764900298_shortlist.json\n",
            "[DEBUG] raw shortlist type: <class 'dict'>\n",
            "[INFO] extracted 43 raw candidate names (sample 30):\n",
            "['bb_mid_z_small', 'ema_200_z_small', 'ema_200', 'ema_100_z_small', 'amihud_50', 'ema_100', 'ret_48h_z_small', 'ret_12h_z', 'weekday', 'close', 'kyle_lambda_50', 'keltner_width', 'ret_std_24', 'mfi_14', 'hl2', 'vpin_proxy_rolling_sma', 'atr_14', 'obv', 'bb_high_z_small', 'macd_signal_z', 'macd_z_small', 'atr_14_z', 'ret_skew_72', 'roll_spread_50', 'pvo', 'ret_kurt_168', 'ema_50', 'squeeze_score', 'macd_z', 'bb_width']\n",
            "[INFO] mapping summary: mapped=42, unmapped=1\n",
            "[SAMPLE mappings (orig -> mapped, reason)]:\n",
            "  bb_mid_z_small                           -> bb_mid_z_small                             (exact_variant)\n",
            "  ema_200_z_small                          -> ema_200_z_small                            (exact_variant)\n",
            "  ema_200                                  -> ema_200                                    (exact_variant)\n",
            "  ema_100_z_small                          -> ema_100_z_small                            (exact_variant)\n",
            "  amihud_50                                -> amihud_50                                  (exact_variant)\n",
            "  ema_100                                  -> ema_100                                    (exact_variant)\n",
            "  ret_48h_z_small                          -> ret_48h_z_small                            (exact_variant)\n",
            "  ret_12h_z                                -> ret_12h_z                                  (exact_variant)\n",
            "  weekday                                  -> weekday                                    (exact_variant)\n",
            "  close                                    -> close                                      (exact_variant)\n",
            "  kyle_lambda_50                           -> kyle_lambda_50                             (exact_variant)\n",
            "  keltner_width                            -> keltner_width                              (exact_variant)\n",
            "  ret_std_24                               -> ret_std_24                                 (exact_variant)\n",
            "  mfi_14                                   -> mfi_14                                     (exact_variant)\n",
            "  hl2                                      -> hl2                                        (exact_variant)\n",
            "  vpin_proxy_rolling_sma                   -> vpin_proxy_rolling_sma                     (exact_variant)\n",
            "  atr_14                                   -> atr_14                                     (exact_variant)\n",
            "  obv                                      -> obv                                        (exact_variant)\n",
            "  bb_high_z_small                          -> bb_high_z_small                            (exact_variant)\n",
            "  macd_signal_z                            -> macd_signal_z                              (exact_variant)\n",
            "  macd_z_small                             -> macd_z_small                               (exact_variant)\n",
            "  atr_14_z                                 -> atr_14_z                                   (exact_variant)\n",
            "  ret_skew_72                              -> ret_skew_72                                (exact_variant)\n",
            "  roll_spread_50                           -> roll_spread_50                             (exact_variant)\n",
            "  pvo                                      -> pvo                                        (exact_variant)\n",
            "  ret_kurt_168                             -> ret_kurt_168                               (exact_variant)\n",
            "  ema_50                                   -> ema_50                                     (exact_variant)\n",
            "  squeeze_score                            -> squeeze_score                              (exact_variant)\n",
            "  macd_z                                   -> macd_z                                     (exact_variant)\n",
            "  bb_width                                 -> bb_width                                   (exact_variant)\n",
            "\n",
            "[UNMAPPED sample up to 30]:\n",
            "['229']\n",
            "\n",
            "[HINT] available df columns sample (first 80):\n",
            "['open', 'high', 'low', 'close', 'volume', 'is_imputed', 'volume_imputed_flag', 'hl2', 'hlc3', 'ohlc4', 'ret_1', 'logret_1', 'ret_3h', 'ret_6h', 'ret_12h', 'ret_24h', 'ret_48h', 'candle_body', 'candle_body_signed', 'upper_wick', 'lower_wick', 'body_pct', 'upper_wick_pct', 'lower_wick_pct', 'ema_10', 'ema_21', 'ema_50', 'ema_100', 'ema_200', 'macd', 'macd_signal', 'macd_hist', 'rsi_14', 'rsi_7', 'atr_14', 'atr_rel', 'bb_high', 'bb_low', 'bb_mid', 'bb_width', 'keltner_h', 'keltner_l', 'keltner_width', 'vol_compression_ratio', 'squeeze_score', 'obv', 'mfi_14', 'pvo', 'vwap', 'ret_mean_24', 'ret_std_24', 'ret_skew_24', 'ret_kurt_24', 'ret_mean_72', 'ret_std_72', 'ret_skew_72', 'ret_kurt_72', 'ret_mean_168', 'ret_std_168', 'ret_skew_168', 'ret_kurt_168', 'tick_sign', 'hour', 'weekday', 'kama_10', 'awesome_osc', 'ppo', 'tick_rule', 'roll_spread_50', 'gk_vol_50', 'cs_spread_2', 'amihud_50', 'kyle_lambda_50', 'vpin_proxy_rolling', 'vpin_proxy_rolling_sma', 'midprice', 'high_low_spread', 'regime_flag', 'open_z', 'open_z_small']\n",
            "\n",
            "[INFO] final selected features (mapped, n=32), sample 30:\n",
            "['bb_mid_z_small', 'ema_200_z_small', 'ema_200', 'ema_100_z_small', 'amihud_50', 'ema_100', 'ret_48h_z_small', 'ret_12h_z', 'weekday', 'close', 'kyle_lambda_50', 'keltner_width', 'ret_std_24', 'mfi_14', 'hl2', 'vpin_proxy_rolling_sma', 'atr_14', 'obv', 'bb_high_z_small', 'macd_signal_z', 'macd_z_small', 'atr_14_z', 'ret_skew_72', 'roll_spread_50', 'pvo', 'ret_kurt_168', 'ema_50', 'squeeze_score', 'macd_z', 'bb_width']\n",
            "[OK] saved df_meta_shortlist pkl -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n",
            "[OK] saved cols csv -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist_cols.v1764900731.csv\n",
            "[INFO] mapping diag saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist_map_diag_1764900731.json\n",
            "\n",
            "[DONE] df_meta_shortlist ready.\n",
            "  n_features_selected: 32\n",
            "  tb_cols_kept: 9\n",
            "  pkl: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n",
            "  csv: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist_cols.v1764900731.csv\n",
            "\n",
            "[NOTICE] Some shortlist names could not be mapped automatically. Inspect diag JSON and adjust shortlist or column names accordingly.\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: robust loader for shortlist_shap_per_side.json -> df_meta_shortlist (Colab)\n",
        "# - handles many shortlist shapes (list, dict, per-side dict, nested dict)\n",
        "# - best-effort name matching (strip/add _z/_z_small, substring)\n",
        "# - saves df_meta_shortlist.v{ts}.pkl and cols CSV\n",
        "import os, glob, json, pickle, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CONFIG\n",
        "PREFERRED_SHORTLIST = None   # e.g. \"shortlist_shap_per_side.json\" or None to auto-pick latest shortlist_*.*\n",
        "KEEP_TB_COLS = True\n",
        "WRITE_CSV = True\n",
        "VERBOSE = True\n",
        "MATCH_SUBSTRING_THRESHOLD = 0.75  # not used as fuzzy, kept for readability\n",
        "\n",
        "# 1) load merged features\n",
        "meta_path = os.path.join(OUT_DIR, \"df_features_with_tb.pkl\")\n",
        "if not os.path.exists(meta_path):\n",
        "    raise FileNotFoundError(f\"{meta_path} not found. Create merged df_features_with_tb.pkl first.\")\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "all_cols = list(df.columns)\n",
        "print(f\"[INFO] loaded features: {meta_path} shape={df.shape}\")\n",
        "\n",
        "# 2) locate shortlist file (prefer user-specified)\n",
        "if PREFERRED_SHORTLIST:\n",
        "    shortlist_candidates = [os.path.join(OUT_DIR, PREFERRED_SHORTLIST)]\n",
        "else:\n",
        "    shortlist_candidates = sorted(\n",
        "        glob.glob(os.path.join(OUT_DIR, \"shortlist*.pkl\")) +\n",
        "        glob.glob(os.path.join(OUT_DIR, \"shortlist*.json\")) +\n",
        "        glob.glob(os.path.join(OUT_DIR, \"shortlist*.csv\")) +\n",
        "        glob.glob(os.path.join(OUT_DIR, \"*shortlist*.pkl\")) +\n",
        "        glob.glob(os.path.join(OUT_DIR, \"*shortlist*.json\")) +\n",
        "        glob.glob(os.path.join(OUT_DIR, \"*shortlist*.csv\")),\n",
        "        key=os.path.getmtime\n",
        "    )\n",
        "if not shortlist_candidates:\n",
        "    raise FileNotFoundError(f\"No shortlist file found in {OUT_DIR}. Run feature-importance pipeline first.\")\n",
        "shortlist_path = shortlist_candidates[-1]\n",
        "print(f\"[INFO] using shortlist file: {os.path.basename(shortlist_path)}\")\n",
        "\n",
        "# 3) load raw shortlist object\n",
        "def _load_raw(path):\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext == \".pkl\":\n",
        "        with open(path,\"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    if ext == \".json\":\n",
        "        with open(path,\"r\") as f:\n",
        "            return json.load(f)\n",
        "    if ext == \".csv\":\n",
        "        return pd.read_csv(path)\n",
        "    raise RuntimeError(\"Unsupported shortlist extension: \" + ext)\n",
        "\n",
        "raw = _load_raw(shortlist_path)\n",
        "if VERBOSE:\n",
        "    print(f\"[DEBUG] raw shortlist type: {type(raw)}\")\n",
        "\n",
        "# 4) extractor: handle many shapes and return union of candidate names (strings)\n",
        "def extract_candidate_names(obj):\n",
        "    names = []\n",
        "\n",
        "    # flatten DataFrame -> first column or 'feature' column\n",
        "    if isinstance(obj, pd.DataFrame):\n",
        "        if \"feature\" in obj.columns:\n",
        "            return [str(x) for x in obj[\"feature\"].tolist()]\n",
        "        else:\n",
        "            return [str(x) for x in obj.iloc[:,0].astype(str).tolist()]\n",
        "\n",
        "    # list-like: names or (name,score)\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        for v in obj:\n",
        "            if isinstance(v, (list, tuple)) and len(v) >= 1:\n",
        "                names.append(str(v[0]))\n",
        "            else:\n",
        "                names.append(str(v))\n",
        "        return names\n",
        "\n",
        "    # dict-like: many possible options\n",
        "    if isinstance(obj, dict):\n",
        "        # common: per-side dict: { \"long\": [...], \"short\": [...] }\n",
        "        if all(k in (\"long\",\"short\") for k in obj.keys()):\n",
        "            sub = []\n",
        "            for side in (\"long\",\"short\"):\n",
        "                subobj = obj.get(side)\n",
        "                if subobj is not None:\n",
        "                    sub += extract_candidate_names(subobj)\n",
        "            return list(dict.fromkeys(sub))  # preserve order unique\n",
        "\n",
        "        # common: nested methods: {\"long\": {\"shap\": [...], \"gain\":[...]} , \"short\": {...}}\n",
        "        if any(isinstance(v, dict) for v in obj.values()):\n",
        "            sub = []\n",
        "            for v in obj.values():\n",
        "                if isinstance(v, dict):\n",
        "                    for v2 in v.values():\n",
        "                        sub += extract_candidate_names(v2)\n",
        "                else:\n",
        "                    sub += extract_candidate_names(v)\n",
        "            return list(dict.fromkeys(sub))\n",
        "\n",
        "        # fallback: dict of feature->score or wrapper with \"features\"\n",
        "        if \"features\" in obj and isinstance(obj[\"features\"], (list,tuple)):\n",
        "            return [str(x) for x in obj[\"features\"]]\n",
        "        # treat keys as feature names if values numeric\n",
        "        if all(isinstance(k, str) for k in obj.keys()) and all(isinstance(v,(int,float)) for v in obj.values()):\n",
        "            return [str(k) for k in obj.keys()]\n",
        "        # generic: keys as features\n",
        "        return [str(k) for k in obj.keys()]\n",
        "\n",
        "    # any other object -> try str()\n",
        "    try:\n",
        "        return [str(obj)]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "cand_raw = extract_candidate_names(raw)\n",
        "cand_raw = [c for c in cand_raw if isinstance(c, str) and c.strip() != \"\"]\n",
        "if VERBOSE:\n",
        "    print(f\"[INFO] extracted {len(cand_raw)} raw candidate names (sample 30):\")\n",
        "    print(cand_raw[:30])\n",
        "\n",
        "if len(cand_raw) == 0:\n",
        "    # helpful diagnostic and abort\n",
        "    raise RuntimeError(f\"Shortlist file parsed to zero candidate names. Inspect {shortlist_path} (top-level keys: {list(raw.keys()) if isinstance(raw,dict) else type(raw)}).\")\n",
        "\n",
        "# 5) best-effort name matching to df.columns\n",
        "# heuristics: exact -> exact (case-sensitive)\n",
        "# then: try variants: add/remove '_z', '_z_small', try base stripping suffixes, substring matches\n",
        "cols_set = set(all_cols)\n",
        "\n",
        "def try_variants(name, cols_set, cols_list):\n",
        "    variants = []\n",
        "    name = name.strip()\n",
        "    variants.append(name)\n",
        "    # strip common suffixes\n",
        "    for suf in [\"_z_small\",\"_z\",\"_small\"]:\n",
        "        if name.endswith(suf):\n",
        "            variants.append(name[:-len(suf)])\n",
        "    # add common z variants\n",
        "    for suf in [\"_z_small\",\"_z\"]:\n",
        "        variants.append(name + suf)\n",
        "    # replace '-',' ' with '_'\n",
        "    variants.append(name.replace('-', '_'))\n",
        "    variants.append(name.replace(' ', '_'))\n",
        "    # lower/upper attempts\n",
        "    variants.append(name.lower())\n",
        "    variants.append(name.upper())\n",
        "\n",
        "    # now check for exact membership in cols\n",
        "    found = []\n",
        "    for v in variants:\n",
        "        if v in cols_set:\n",
        "            found.append(v)\n",
        "    if found:\n",
        "        return found[0], \"exact_variant\"\n",
        "\n",
        "    # substring match: find column that contains name or name contains col token\n",
        "    # prefer longest common substring heuristic: pick col where name is substring or vice versa\n",
        "    for col in cols_list:\n",
        "        if name in col:\n",
        "            return col, \"col_contains_name\"\n",
        "    for col in cols_list:\n",
        "        if col in name:\n",
        "            return col, \"name_contains_col\"\n",
        "\n",
        "    # try more relaxed: token overlap\n",
        "    toks = set(name.replace('-', '_').replace(' ', '_').split('_'))\n",
        "    best = None\n",
        "    best_common = 0\n",
        "    for col in cols_list:\n",
        "        ctoks = set(col.replace('-', '_').replace(' ', '_').split('_'))\n",
        "        common = len(toks & ctoks)\n",
        "        if common > best_common:\n",
        "            best_common = common\n",
        "            best = col\n",
        "    if best and best_common >= 1:\n",
        "        return best, f\"token_match({best_common})\"\n",
        "\n",
        "    return None, None\n",
        "\n",
        "mapped = {}\n",
        "unmapped = []\n",
        "cols_list = list(all_cols)\n",
        "for name in cand_raw:\n",
        "    m, why = try_variants(name, cols_set, cols_list)\n",
        "    if m is not None:\n",
        "        mapped[name] = {\"mapped_to\": m, \"why\": why}\n",
        "    else:\n",
        "        unmapped.append(name)\n",
        "\n",
        "# 6) summary diagnostics\n",
        "n_map = len(mapped)\n",
        "n_un = len(unmapped)\n",
        "print(f\"[INFO] mapping summary: mapped={n_map}, unmapped={n_un}\")\n",
        "if n_map:\n",
        "    # show sample mappings\n",
        "    sample_map = list(mapped.items())[:30]\n",
        "    print(\"[SAMPLE mappings (orig -> mapped, reason)]:\")\n",
        "    for orig, info in sample_map:\n",
        "        print(f\"  {orig:40s} -> {info['mapped_to']:40s}   ({info['why']})\")\n",
        "if n_un:\n",
        "    print(\"\\n[UNMAPPED sample up to 30]:\")\n",
        "    print(unmapped[:30])\n",
        "    # helpful hint: show top 40 columns to inspect\n",
        "    print(\"\\n[HINT] available df columns sample (first 80):\")\n",
        "    print(all_cols[:80])\n",
        "    # do not abort if there are some mapped features; but if none, abort with clear message\n",
        "if n_map == 0:\n",
        "    raise RuntimeError(f\"No candidate names could be mapped to df columns. Inspect shortlist file {shortlist_path} and df columns. Unmapped sample: {unmapped[:30]}\")\n",
        "\n",
        "# 7) build final selected feature list = unique mapped targets preserving ranking order of cand_raw\n",
        "selected_mapped = []\n",
        "seen = set()\n",
        "for name in cand_raw:\n",
        "    if name in mapped:\n",
        "        target = mapped[name][\"mapped_to\"]\n",
        "        if target not in seen:\n",
        "            selected_mapped.append(target)\n",
        "            seen.add(target)\n",
        "\n",
        "print(f\"\\n[INFO] final selected features (mapped, n={len(selected_mapped)}), sample 30:\")\n",
        "print(selected_mapped[:30])\n",
        "\n",
        "# 8) prepare df_meta_shortlist (add tb_ columns)\n",
        "tb_cols = [c for c in df.columns if c.startswith(\"tb_\")] if KEEP_TB_COLS else []\n",
        "cols_to_keep = [c for c in selected_mapped if c in df.columns] + tb_cols\n",
        "df_meta_shortlist = df[cols_to_keep].copy()\n",
        "ts = int(time.time())\n",
        "pkl_out = os.path.join(OUT_DIR, f\"df_meta_shortlist.v{ts}.pkl\")\n",
        "csv_out = os.path.join(OUT_DIR, f\"df_meta_shortlist_cols.v{ts}.csv\")\n",
        "try:\n",
        "    df_meta_shortlist.to_pickle(pkl_out)\n",
        "    print(f\"[OK] saved df_meta_shortlist pkl -> {pkl_out}\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] saving pkl failed:\", e)\n",
        "if WRITE_CSV:\n",
        "    pd.DataFrame({\"feature\": cols_to_keep}).to_csv(csv_out, index=False)\n",
        "    print(f\"[OK] saved cols csv -> {csv_out}\")\n",
        "\n",
        "# 9) write mapping diagnostics file\n",
        "diag = {\n",
        "    \"timestamp\": ts,\n",
        "    \"shortlist_source\": os.path.basename(shortlist_path),\n",
        "    \"n_raw_candidates\": len(cand_raw),\n",
        "    \"n_mapped\": n_map,\n",
        "    \"n_unmapped\": n_un,\n",
        "    \"mapped_sample\": {k:v for k,v in list(mapped.items())[:50]},\n",
        "    \"unmapped_sample\": unmapped[:50],\n",
        "    \"pkl_out\": os.path.basename(pkl_out),\n",
        "    \"csv_out\": os.path.basename(csv_out)\n",
        "}\n",
        "diag_path = os.path.join(OUT_DIR, f\"df_meta_shortlist_map_diag_{ts}.json\")\n",
        "with open(diag_path, \"w\") as f:\n",
        "    json.dump(diag, f, indent=2)\n",
        "print(f\"[INFO] mapping diag saved -> {diag_path}\")\n",
        "\n",
        "# 10) final checks\n",
        "print(\"\\n[DONE] df_meta_shortlist ready.\")\n",
        "print(\"  n_features_selected:\", len(selected_mapped))\n",
        "print(\"  tb_cols_kept:\", len(tb_cols))\n",
        "print(\"  pkl:\", pkl_out)\n",
        "print(\"  csv:\", csv_out)\n",
        "if n_un:\n",
        "    print(\"\\n[NOTICE] Some shortlist names could not be mapped automatically. Inspect diag JSON and adjust shortlist or column names accordingly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIZM5wASn-gK",
        "outputId": "513531df-c3c4-47b3-ef9e-8f212cbb205b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n",
            "[INFO] df shape: (17521, 41)\n",
            "[INFO] tb cols found: ['tb_label_h4', 'tb_t_break_h4', 'tb_ret_at_break_h4', 'tb_label_h8', 'tb_t_break_h8', 'tb_ret_at_break_h8', 'tb_label_h12', 'tb_t_break_h12', 'tb_ret_at_break_h12']\n",
            "[INFO] feature_cols count: 32 sample: ['bb_mid_z_small', 'ema_200_z_small', 'ema_200', 'ema_100_z_small', 'amihud_50', 'ema_100', 'ret_48h_z_small', 'ret_12h_z', 'weekday', 'close', 'kyle_lambda_50', 'keltner_width', 'ret_std_24', 'mfi_14', 'hl2', 'vpin_proxy_rolling_sma', 'atr_14', 'obv', 'bb_high_z_small', 'macd_signal_z']\n",
            "[INFO] using label: tb_label_h8 t_break: tb_t_break_h8\n",
            "\n",
            "[INFO] Starting smoke-run CPCV (long) ...\n",
            "[OK] long done in 8.6s | models: 3\n",
            " OOF non-nan preds: 17521 / total rows: 17521\n",
            "  OOF AUC (long): 0.5080158337732454\n",
            "  OOF AUC (short): 0.49235928601496254\n",
            " outer_fold 1: train_len=11677 test_len=5841\n",
            "   sanity_check: PASS\n",
            " outer_fold 2: train_len=11670 test_len=5840\n",
            "   sanity_check: PASS\n",
            " outer_fold 3: train_len=11678 test_len=5840\n",
            "   sanity_check: PASS\n",
            "\n",
            "[INFO] Starting smoke-run CPCV (short) ...\n",
            "[OK] short done in 8.5s | models: 3\n",
            " OOF non-nan preds: 17521 / total rows: 17521\n",
            "  OOF AUC (long): 0.4916330236602393\n",
            "  OOF AUC (short): 0.5081327521732255\n",
            " outer_fold 1: train_len=11677 test_len=5841\n",
            "   sanity_check: PASS\n",
            " outer_fold 2: train_len=11670 test_len=5840\n",
            "   sanity_check: PASS\n",
            " outer_fold 3: train_len=11678 test_len=5840\n",
            "   sanity_check: PASS\n",
            "\n",
            "[ALL DONE] If diagnostics OK, run full grid/Optuna next.\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: smoke-run CPCV on df_meta_shortlist (Colab)\n",
        "import os, time, json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- load shortlist meta\n",
        "pkl_candidates = sorted([p for p in Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")], key=os.path.getmtime)\n",
        "if not pkl_candidates:\n",
        "    raise FileNotFoundError(\"No df_meta_shortlist.v*.pkl found in OUT_DIR. Run the shortlist mapping cell first.\")\n",
        "meta_path = str(pkl_candidates[-1])\n",
        "print(\"[INFO] loading df_meta_shortlist:\", meta_path)\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "print(\"[INFO] df shape:\", df.shape)\n",
        "\n",
        "# find tb cols\n",
        "tb_cols = [c for c in df.columns if c.startswith(\"tb_\")]\n",
        "print(\"[INFO] tb cols found:\", tb_cols)\n",
        "\n",
        "# build feature_cols (explicitly exclude tb_ and any index-like stray)\n",
        "feature_cols = [c for c in df.columns if (not c.startswith(\"tb_\")) and (c != \"index\") and (c != \"Unnamed: 0\")]\n",
        "print(f\"[INFO] feature_cols count: {len(feature_cols)} sample:\", feature_cols[:20])\n",
        "\n",
        "# quick sanity: ensure labels exist in df (we'll prefer tb_label_h8)\n",
        "label_cols = [c for c in df.columns if c.startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"No tb_label_* found in df_meta_shortlist. Merge TB first.\")\n",
        "tb_label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tb_tbreak_col = tb_label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "print(\"[INFO] using label:\", tb_label_col, \"t_break:\", tb_tbreak_col)\n",
        "\n",
        "# prepare run_cpcv_grid args (smoke-run small grid)\n",
        "grid_small = {\n",
        "    \"max_depth\":[3],\n",
        "    \"learning_rate\":[0.05],\n",
        "    \"subsample\":[0.8],\n",
        "    \"colsample_bytree\":[0.8],\n",
        "    \"n_estimators\":[50]\n",
        "}\n",
        "smoke_kwargs = dict(\n",
        "    df_features=df,\n",
        "    tbreak_series=df[tb_tbreak_col],\n",
        "    feature_cols=feature_cols,\n",
        "    n_outer=3,\n",
        "    n_inner=2,\n",
        "    embargo=pd.Timedelta(\"1H\"),\n",
        "    drop_unmapped=True,\n",
        "    random_state=42,\n",
        "    use_smote=False,\n",
        "    grid=grid_small,\n",
        "    xgb_common={\"verbosity\":0},\n",
        "    max_inner_budget=100,\n",
        "    out_dir=OUT_DIR\n",
        ")\n",
        "\n",
        "# check run_cpcv_grid present\n",
        "if 'run_cpcv_grid' not in globals():\n",
        "    raise RuntimeError(\"run_cpcv_grid not defined in this notebook. Paste/run patched run_cpcv_grid cell before executing.\")\n",
        "\n",
        "# run smoke-run for long and short\n",
        "from sklearn.metrics import roc_auc_score\n",
        "def run_and_report(side):\n",
        "    print(f\"\\n[INFO] Starting smoke-run CPCV ({side}) ...\")\n",
        "    t0 = time.time()\n",
        "    res = run_cpcv_grid(side=side, **smoke_kwargs)\n",
        "    print(f\"[OK] {side} done in {time.time()-t0:.1f}s | models: {len(res.get('models',[]))}\")\n",
        "    # diagnostics\n",
        "    oof = res.get(\"oof\")\n",
        "    print(f\" OOF non-nan preds: {int(oof.notna().sum()) if oof is not None else 0} / total rows: {len(df)}\")\n",
        "    # basic OOF AUC against both long and short labels (if present)\n",
        "    try:\n",
        "        preds = oof.fillna(0.5).astype(float)\n",
        "        lab_long = (df[tb_label_col]==1).astype(int)\n",
        "        lab_short = (df[tb_label_col]==-1).astype(int)\n",
        "        if lab_long.nunique()>1:\n",
        "            print(\"  OOF AUC (long):\", roc_auc_score(lab_long, preds))\n",
        "        if lab_short.nunique()>1:\n",
        "            print(\"  OOF AUC (short):\", roc_auc_score(lab_short, preds))\n",
        "    except Exception as e:\n",
        "        print(\"  OOF AUC calc skipped:\", e)\n",
        "    # per-fold train/test sizes + sanity (if purge utils present)\n",
        "    try:\n",
        "        from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits, sanity_check_purged_split\n",
        "        exp = compute_exposure_intervals(df.index, df[tb_tbreak_col], horizon_fallback=None, last_index=df.index[-1])\n",
        "        pos = exposure_to_pos_intervals(exp, df.index)\n",
        "        outer = list(purged_cv_splits(pos, n_splits=smoke_kwargs['n_outer'], embargo=smoke_kwargs['embargo'], index=df.index, drop_unmapped=True, random_state=smoke_kwargs['random_state']))\n",
        "        for i,(tr,te) in enumerate(outer,1):\n",
        "            print(f\" outer_fold {i}: train_len={len(tr)} test_len={len(te)}\")\n",
        "            try:\n",
        "                sanity_check_purged_split(tr, te, pos)\n",
        "                print(\"   sanity_check: PASS\")\n",
        "            except AssertionError as ae:\n",
        "                print(\"   sanity_check: FAIL ->\", ae)\n",
        "    except Exception as e:\n",
        "        print(\" Per-fold diagnostics skipped (purge utils missing or error):\", e)\n",
        "    return res\n",
        "\n",
        "res_long = run_and_report(\"long\")\n",
        "res_short = run_and_report(\"short\")\n",
        "print(\"\\n[ALL DONE] If diagnostics OK, run full grid/Optuna next.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899,
          "referenced_widgets": [
            "b160f83dc7b9404baf8be9b569082289",
            "f036160f5af540b6a51653e7e8bad0d0",
            "9e1bb7b5c0d0407dbc29445e6ed56613",
            "e57756ef5e984a61ada1672b0e574fec",
            "caf47bf1ddb540beb86c3e6d6dedc993",
            "bcc78bca73c44805bb2abdad1ab207bd",
            "faa4c132ac2e49a781bfd014690be257",
            "bbaf77978bb748679710568ff16273bf",
            "899a4cab4b6943d89c315821db1e29e6",
            "bdeeb52fd7e84068aedfa1482a11be85",
            "5ae1bd5996f04298bce98d9b15588503"
          ]
        },
        "id": "h8X70fOGoXFJ",
        "outputId": "2608885f-d072-4d43-d67e-eaffaf2538d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n",
            "[INFO] df shape: (17521, 41)\n",
            "[INFO] features: 32 | label: tb_label_h8 | tbreak: tb_t_break_h8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:12:34,925] A new study created in memory with name: no-name-0ca11a2e-9c1a-427c-908e-2cd7fbb213e3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] starting Optuna tuning (long). trials: 40\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b160f83dc7b9404baf8be9b569082289",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:12:53,636] Trial 0 finished with value: 0.506599668693587 and parameters: {'max_depth': 5, 'learning_rate': 0.17254716573280354, 'subsample': 0.892797576724562, 'colsample_bytree': 0.8394633936788146, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07, 'n_estimators': 105}. Best is trial 0 with value: 0.506599668693587.\n",
            "[I 2025-12-05 02:14:13,766] Trial 1 finished with value: 0.5173224014669622 and parameters: {'max_depth': 8, 'learning_rate': 0.06054365855469246, 'subsample': 0.8832290311184181, 'colsample_bytree': 0.608233797718321, 'reg_alpha': 5.360294728728285, 'reg_lambda': 0.31044435499483225, 'n_estimators': 251}. Best is trial 1 with value: 0.5173224014669622.\n",
            "[I 2025-12-05 02:15:00,079] Trial 2 finished with value: 0.512273568347578 and parameters: {'max_depth': 4, 'learning_rate': 0.017322667470546258, 'subsample': 0.7216968971838151, 'colsample_bytree': 0.8099025726528951, 'reg_alpha': 7.71800699380605e-05, 'reg_lambda': 4.17890272377219e-06, 'n_estimators': 631}. Best is trial 1 with value: 0.5173224014669622.\n",
            "[I 2025-12-05 02:15:32,720] Trial 3 finished with value: 0.5145268473515189 and parameters: {'max_depth': 3, 'learning_rate': 0.023993242906812727, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'reg_alpha': 0.1165691561324743, 'reg_lambda': 6.267062696005991e-07, 'n_estimators': 539}. Best is trial 1 with value: 0.5173224014669622.\n",
            "[I 2025-12-05 02:18:00,020] Trial 4 finished with value: 0.5169375431284766 and parameters: {'max_depth': 6, 'learning_rate': 0.011492999300221412, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.6682096494749166, 'reg_alpha': 3.850031979199519e-08, 'reg_lambda': 3.4671276804481113, 'n_estimators': 968}. Best is trial 1 with value: 0.5173224014669622.\n",
            "[I 2025-12-05 02:19:55,368] Trial 5 finished with value: 0.5088794713239778 and parameters: {'max_depth': 7, 'learning_rate': 0.0249064396938244, 'subsample': 0.6390688456025535, 'colsample_bytree': 0.8736932106048627, 'reg_alpha': 9.148975058772307e-05, 'reg_lambda': 1.254134495897175e-07, 'n_estimators': 520}. Best is trial 1 with value: 0.5173224014669622.\n",
            "[W 2025-12-05 02:20:22,681] Trial 6 failed with parameters: {'max_depth': 3, 'learning_rate': 0.15242391728466367, 'subsample': 0.7035119926400067, 'colsample_bytree': 0.8650089137415928, 'reg_alpha': 6.388511557344611e-06, 'reg_lambda': 0.0004793052550782129, 'n_estimators': 569} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2152097453.py\", line 83, in objective\n",
            "    inner_splits = list(purged_cv_splits(pos_train, n_splits=n_inner, embargo=embargo, index=idx, drop_unmapped=True, random_state=random_state))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4217872810.py\", line 252, in purged_cv_splits\n",
            "    emb_start_bar = np.searchsorted(idx_vals, np.datetime64(emb_start_ts), side='left')\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\", line 1402, in searchsorted\n",
            "    @array_function_dispatch(_searchsorted_dispatcher)\n",
            "    \n",
            "KeyboardInterrupt\n",
            "[W 2025-12-05 02:20:22,689] Trial 6 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2152097453.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mobj_long\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_INNER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBARGO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_STATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEARLY_STOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mstudy_long\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_TRIALS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[OK] Optuna long done in %.1fs | best_value=%.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_long\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     ):\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2152097453.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mexp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_exposure_intervals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtbreak_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mpos_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexposure_to_pos_intervals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0minner_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpurged_cv_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membargo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membargo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_unmapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_splits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m# fallback: single split (train only) -> return neutral score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4217872810.py\u001b[0m in \u001b[0;36mpurged_cv_splits\u001b[0;34m(exposure_pos_df, n_splits, embargo, index, drop_unmapped, random_state)\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0memb_start_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_start_ts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0membargo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0memb_end_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_end_ts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membargo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0memb_start_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_start_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0memb_end_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_end_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0memb_start_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_start_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_searchsorted_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# ONE-CELL: Optuna tune XGB (purged CV) + train final CPCV models per outer fold\n",
        "# Paste & run in Colab (requires purge utils present in notebook)\n",
        "import os, time, json, pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from copy import deepcopy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# USER CONFIG\n",
        "N_TRIALS = 40               # reduce if compute limited\n",
        "N_OUTER = 5\n",
        "N_INNER = 3\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "RANDOM_STATE = 42\n",
        "LABEL_H = \"h8\"              # target horizon suffix used in tb_label_h8\n",
        "EARLY_STOP = 20\n",
        "VERBOSE = True\n",
        "\n",
        "# 1) load df_meta_shortlist\n",
        "pkl_candidates = sorted([p for p in Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")], key=os.path.getmtime)\n",
        "if not pkl_candidates:\n",
        "    raise FileNotFoundError(\"No df_meta_shortlist.v*.pkl found in OUT_DIR. Create shortlist first.\")\n",
        "meta_path = str(pkl_candidates[-1])\n",
        "print(\"[INFO] loading df_meta_shortlist:\", meta_path)\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "print(\"[INFO] df shape:\", df.shape)\n",
        "\n",
        "# 2) define label and feature columns\n",
        "label_col = f\"tb_label_{LABEL_H}\"\n",
        "tbreak_col = f\"tb_t_break_{LABEL_H}\"\n",
        "if label_col not in df.columns or tbreak_col not in df.columns:\n",
        "    raise RuntimeError(f\"{label_col} or {tbreak_col} missing in df.\")\n",
        "feature_cols = [c for c in df.columns if (not c.startswith(\"tb_\")) and (c not in (\"index\",\"Unnamed: 0\"))]\n",
        "print(f\"[INFO] features: {len(feature_cols)} | label: {label_col} | tbreak: {tbreak_col}\")\n",
        "\n",
        "# 3) ensure purge utils available (try import)\n",
        "try:\n",
        "    from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits, sanity_check_purged_split\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Purge utils not available in notebook. Paste patched purge utils before running Optuna.\") from e\n",
        "\n",
        "# 4) prepare exposure & pos mapping once\n",
        "idx = df.index\n",
        "exp_all = compute_exposure_intervals(idx, df[tbreak_col], horizon_fallback=None, last_index=idx[-1])\n",
        "pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "# pre-check outer splits\n",
        "outer_splits = list(purged_cv_splits(pos_all, n_splits=N_OUTER, embargo=EMBARGO, index=idx, drop_unmapped=True, random_state=RANDOM_STATE))\n",
        "if len(outer_splits) < N_OUTER:\n",
        "    print(\"[WARN] fewer outer splits produced than requested:\", len(outer_splits))\n",
        "\n",
        "# 5) objective for Optuna: evaluate candidate params by inner purged CV averaged across OUTER folds\n",
        "def make_objective(df, idx, pos_all, feature_cols, label_col, n_inner, embargo, random_state, early_stop):\n",
        "    y_all = (df[label_col] == 1).astype(int)  # we'll tune for 'long' objective; run separately for short later if desired\n",
        "    X_all = df[feature_cols].astype(float)\n",
        "    def objective(trial):\n",
        "        # sample hyperparams\n",
        "        params = {\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
        "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
        "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.6, 1.0),\n",
        "            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
        "            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
        "        }\n",
        "        # evaluation: average inner CV AUC across outer folds\n",
        "        outer_scores = []\n",
        "        # iterate outer folds (use precomputed split positions)\n",
        "        for fold_id, (train_pos_outer, test_pos_outer) in enumerate(outer_splits, start=1):\n",
        "            # build pos_train (exposure intervals restricted to train timestamps)\n",
        "            train_ts = pos_all.index[train_pos_outer]\n",
        "            exp_train = compute_exposure_intervals(train_ts, df[tbreak_col].reindex(train_ts), horizon_fallback=None, last_index=idx[-1])\n",
        "            pos_train = exposure_to_pos_intervals(exp_train, idx)\n",
        "            inner_splits = list(purged_cv_splits(pos_train, n_splits=n_inner, embargo=embargo, index=idx, drop_unmapped=True, random_state=random_state))\n",
        "            if len(inner_splits) == 0:\n",
        "                # fallback: single split (train only) -> return neutral score\n",
        "                return 0.5\n",
        "            inner_fold_scores = []\n",
        "            for (tr_pos_inner, val_pos_inner) in inner_splits:\n",
        "                tr_ts = pos_train.index[tr_pos_inner]\n",
        "                val_ts = pos_train.index[val_pos_inner]\n",
        "                if len(val_ts) == 0:\n",
        "                    continue\n",
        "                X_tr = X_all.loc[tr_ts].values\n",
        "                y_tr = y_all.loc[tr_ts].values\n",
        "                X_val = X_all.loc[val_ts].values\n",
        "                y_val = y_all.loc[val_ts].values\n",
        "                # scaler\n",
        "                scaler = StandardScaler()\n",
        "                X_tr_s = scaler.fit_transform(X_tr)\n",
        "                X_val_s = scaler.transform(X_val)\n",
        "                sw = compute_sample_weight(class_weight=\"balanced\", y=y_tr)\n",
        "                model = XGBClassifier(\n",
        "                    use_label_encoder=False,\n",
        "                    eval_metric=\"logloss\",\n",
        "                    random_state=int(random_state),\n",
        "                    **params\n",
        "                )\n",
        "                # fit with early stopping using training as train/val internal; but to keep deterministic we simply fit and predict (inner fold small)\n",
        "                try:\n",
        "                    model.fit(X_tr_s, y_tr, sample_weight=sw, verbose=False)\n",
        "                except TypeError:\n",
        "                    model.fit(X_tr_s, y_tr, verbose=False)\n",
        "                probs = model.predict_proba(X_val_s)[:,1]\n",
        "                if len(np.unique(y_val)) < 2:\n",
        "                    auc = 0.5\n",
        "                else:\n",
        "                    auc = roc_auc_score(y_val, probs)\n",
        "                inner_fold_scores.append(auc)\n",
        "            if inner_fold_scores:\n",
        "                outer_scores.append(np.mean(inner_fold_scores))\n",
        "        # final score\n",
        "        if not outer_scores:\n",
        "            return 0.5\n",
        "        return float(np.mean(outer_scores))\n",
        "    return objective\n",
        "\n",
        "# 6) run Optuna for LONG side (tune for predicting long label==1)\n",
        "print(\"[INFO] starting Optuna tuning (long). trials:\", N_TRIALS)\n",
        "study_long = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
        "obj_long = make_objective(df, idx, pos_all, feature_cols, label_col, N_INNER, EMBARGO, RANDOM_STATE, EARLY_STOP)\n",
        "t0 = time.time()\n",
        "study_long.optimize(obj_long, n_trials=N_TRIALS, n_jobs=1, show_progress_bar=True)\n",
        "print(\"[OK] Optuna long done in %.1fs | best_value=%.4f\" % (time.time()-t0, study_long.best_value))\n",
        "\n",
        "# save study + best params\n",
        "with open(os.path.join(OUT_DIR, \"optuna_study_long.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(study_long, f)\n",
        "with open(os.path.join(OUT_DIR, \"optuna_best_params_long.json\"), \"w\") as f:\n",
        "    json.dump(study_long.best_params, f, indent=2)\n",
        "print(\"[INFO] saved optuna results -> optuna_study_long.pkl, optuna_best_params_long.json\")\n",
        "\n",
        "# 7) train final models per outer fold with best params and produce OOF preds\n",
        "best_params = deepcopy(study_long.best_params)\n",
        "best_params.update({\"verbosity\":0})\n",
        "y_all = (df[label_col] == 1).astype(int)\n",
        "X_all = df[feature_cols].astype(float)\n",
        "oof = pd.Series(index=df.index, dtype=float)\n",
        "oof_fold = pd.Series(index=df.index, dtype=int).fillna(-1)\n",
        "models_meta = []\n",
        "for i, (train_pos_outer, test_pos_outer) in enumerate(outer_splits, start=1):\n",
        "    test_ts = pos_all.index[test_pos_outer]\n",
        "    train_ts = pos_all.index[train_pos_outer]\n",
        "    # compute exposure for train subset to derive inner CV splits for potential early-stopping (optional)\n",
        "    X_tr = X_all.loc[train_ts].values\n",
        "    y_tr = y_all.loc[train_ts].values\n",
        "    X_test = X_all.loc[test_ts].values\n",
        "    # scaler on train\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    # sample_weight\n",
        "    sw = compute_sample_weight(class_weight=\"balanced\", y=y_tr)\n",
        "    model_final = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=RANDOM_STATE, **best_params)\n",
        "    try:\n",
        "        model_final.fit(X_tr_s, y_tr, sample_weight=sw, verbose=False)\n",
        "    except TypeError:\n",
        "        model_final.fit(X_tr_s, y_tr, verbose=False)\n",
        "    # test preds\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "    preds_test = model_final.predict_proba(X_test_s)[:,1]\n",
        "    oof.loc[test_ts] = preds_test\n",
        "    oof_fold.loc[test_ts] = i\n",
        "    models_meta.append({\"fold\": i, \"model\": model_final, \"scaler\": scaler, \"params\": best_params})\n",
        "    # save fold artifact\n",
        "    fold_path = os.path.join(OUT_DIR, f\"optuna_long_cpcv_fold{i}.pkl\")\n",
        "    try:\n",
        "        pd.to_pickle({\"model\": model_final, \"scaler\": scaler, \"params\": best_params}, fold_path)\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(f\"[INFO] saved fold {i} -> {fold_path} | train_len={len(train_ts)} test_len={len(test_ts)}\")\n",
        "\n",
        "# write OOF\n",
        "oof_path = os.path.join(OUT_DIR, \"optuna_long_oof.csv\")\n",
        "oof.fillna(0.5).to_csv(oof_path, index=True)\n",
        "print(\"[INFO] OOF saved ->\", oof_path)\n",
        "\n",
        "# compute OOF AUC for both long and short labels\n",
        "try:\n",
        "    auc_long = roc_auc_score((df[label_col]==1).astype(int), oof.fillna(0.5))\n",
        "    auc_short = roc_auc_score((df[label_col]==-1).astype(int), oof.fillna(0.5))\n",
        "    print(\"[RESULT] OOF AUC (long): %.4f | OOF AUC (short): %.4f\" % (auc_long, auc_short))\n",
        "except Exception as e:\n",
        "    print(\"OOF AUC compute failed:\", e)\n",
        "\n",
        "# 8) save meta\n",
        "with open(os.path.join(OUT_DIR, \"optuna_long_models_meta.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(models_meta, f)\n",
        "\n",
        "print(\"[DONE] Optuna + final models saved. Check OUT_DIR for artifacts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 819,
          "referenced_widgets": [
            "ac73da65eb8a43d08d904c5a6602371f",
            "f8cfbc0aa9044d6795a5244337070f3a",
            "0e224a145c8f465f8de71cd662629375",
            "78e5bc70b98d41b99ed6c1e6206d54bc",
            "59e704ba7d03441eaf065b381fe40377",
            "75feffc232a2445eb31443d3fc0fdce3",
            "1cd74d6c623144fe8d8951bc57a62ff7",
            "886c836b29d14e60b110cd04eebfddc1",
            "1e5c3ed2184c48c9b727075d55bb0063",
            "8346d86816ff4160a18e9db1401fecb3",
            "d0f2432f291e412dac941290aa782263"
          ]
        },
        "id": "rAsyZb-A5ty6",
        "outputId": "9bc1a757-69bd-4826-b9d6-aa6787f9958f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:20:37,295] A new study created in memory with name: no-name-b9b30869-3fd2-4d17-b823-84d6b283a906\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] outer splits: 5\n",
            "[INFO] starting Optuna v3...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac73da65eb8a43d08d904c5a6602371f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:20:43,652] Trial 0 finished with value: 0.4670696301690181 and parameters: {'max_depth': 5, 'learning_rate': 0.17254716573280354, 'subsample': 0.892797576724562, 'colsample_bytree': 0.8394633936788146, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07, 'n_estimators': 82}. Best is trial 0 with value: 0.4670696301690181.\n",
            "[I 2025-12-05 02:21:18,603] Trial 1 finished with value: 0.4753015504184782 and parameters: {'max_depth': 8, 'learning_rate': 0.06054365855469246, 'subsample': 0.8832290311184181, 'colsample_bytree': 0.608233797718321, 'reg_alpha': 5.360294728728285, 'reg_lambda': 0.31044435499483225, 'n_estimators': 166}. Best is trial 1 with value: 0.4753015504184782.\n",
            "[I 2025-12-05 02:21:31,211] Trial 2 finished with value: 0.48490976134568314 and parameters: {'max_depth': 4, 'learning_rate': 0.017322667470546258, 'subsample': 0.7216968971838151, 'colsample_bytree': 0.8099025726528951, 'reg_alpha': 7.71800699380605e-05, 'reg_lambda': 4.17890272377219e-06, 'n_estimators': 387}. Best is trial 2 with value: 0.48490976134568314.\n",
            "[I 2025-12-05 02:21:42,440] Trial 3 finished with value: 0.48609341574092296 and parameters: {'max_depth': 3, 'learning_rate': 0.023993242906812727, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'reg_alpha': 0.1165691561324743, 'reg_lambda': 6.267062696005991e-07, 'n_estimators': 333}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:22:48,891] Trial 4 finished with value: 0.4824235040727209 and parameters: {'max_depth': 6, 'learning_rate': 0.011492999300221412, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.6682096494749166, 'reg_alpha': 3.850031979199519e-08, 'reg_lambda': 3.4671276804481113, 'n_estimators': 582}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:23:33,864] Trial 5 finished with value: 0.4750782723382861 and parameters: {'max_depth': 7, 'learning_rate': 0.0249064396938244, 'subsample': 0.6390688456025535, 'colsample_bytree': 0.8736932106048627, 'reg_alpha': 9.148975058772307e-05, 'reg_lambda': 1.254134495897175e-07, 'n_estimators': 322}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:23:40,740] Trial 6 finished with value: 0.4554977833309465 and parameters: {'max_depth': 3, 'learning_rate': 0.15242391728466367, 'subsample': 0.7035119926400067, 'colsample_bytree': 0.8650089137415928, 'reg_alpha': 6.388511557344611e-06, 'reg_lambda': 0.0004793052550782129, 'n_estimators': 351}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:24:01,587] Trial 7 finished with value: 0.4312462705439079 and parameters: {'max_depth': 4, 'learning_rate': 0.18258230439200238, 'subsample': 0.9100531293444458, 'colsample_bytree': 0.9757995766256756, 'reg_alpha': 1.1309571585271483, 'reg_lambda': 0.002404915432737351, 'n_estimators': 557}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:24:11,887] Trial 8 finished with value: 0.4853261282561364 and parameters: {'max_depth': 3, 'learning_rate': 0.017987863473362915, 'subsample': 0.6180909155642152, 'colsample_bytree': 0.7301321323053057, 'reg_alpha': 3.148441347423712e-05, 'reg_lambda': 2.7678419414850017e-06, 'n_estimators': 506}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:24:38,983] Trial 9 finished with value: 0.47534648637796145 and parameters: {'max_depth': 5, 'learning_rate': 0.023200867504756827, 'subsample': 0.8170784332632994, 'colsample_bytree': 0.6563696899899051, 'reg_alpha': 0.16587190283399655, 'reg_lambda': 4.6876566400928895e-08, 'n_estimators': 593}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:24:56,905] Trial 10 finished with value: 0.47067419399304283 and parameters: {'max_depth': 6, 'learning_rate': 0.05334353369227191, 'subsample': 0.9878148443151463, 'colsample_bytree': 0.7387403565626488, 'reg_alpha': 0.01163452910324984, 'reg_lambda': 2.6462851656372712e-05, 'n_estimators': 239}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:25:06,919] Trial 11 finished with value: 0.4785452743116058 and parameters: {'max_depth': 3, 'learning_rate': 0.03727123279423317, 'subsample': 0.6020451860888983, 'colsample_bytree': 0.7347390152853027, 'reg_alpha': 0.005772232054436559, 'reg_lambda': 7.038500281877632e-06, 'n_estimators': 465}. Best is trial 3 with value: 0.48609341574092296.\n",
            "[I 2025-12-05 02:25:15,194] Trial 12 finished with value: 0.4898883290788776 and parameters: {'max_depth': 3, 'learning_rate': 0.010669866363810981, 'subsample': 0.7496480568633753, 'colsample_bytree': 0.7473869332949584, 'reg_alpha': 0.0029344887951539677, 'reg_lambda': 1.2414117973212671e-08, 'n_estimators': 471}. Best is trial 12 with value: 0.4898883290788776.\n",
            "[I 2025-12-05 02:25:29,741] Trial 13 finished with value: 0.48795005787560064 and parameters: {'max_depth': 4, 'learning_rate': 0.010645416071371466, 'subsample': 0.7509977141542071, 'colsample_bytree': 0.9267397597931739, 'reg_alpha': 0.010414348488089444, 'reg_lambda': 1.06779209725231e-08, 'n_estimators': 421}. Best is trial 12 with value: 0.4898883290788776.\n",
            "[I 2025-12-05 02:25:45,182] Trial 14 finished with value: 0.48792147797793806 and parameters: {'max_depth': 4, 'learning_rate': 0.010088773377773546, 'subsample': 0.7677390941780597, 'colsample_bytree': 0.9516331244950649, 'reg_alpha': 0.0020762047805725705, 'reg_lambda': 2.1062706881196248e-08, 'n_estimators': 436}. Best is trial 12 with value: 0.4898883290788776.\n",
            "[I 2025-12-05 02:26:00,695] Trial 15 finished with value: 0.457351267898235 and parameters: {'max_depth': 4, 'learning_rate': 0.08903012950316418, 'subsample': 0.6795294324627127, 'colsample_bytree': 0.9181817011636544, 'reg_alpha': 0.001126010738756187, 'reg_lambda': 1.7894846805710147e-08, 'n_estimators': 437}. Best is trial 12 with value: 0.4898883290788776.\n",
            "[I 2025-12-05 02:26:16,164] Trial 16 finished with value: 0.48689719001197634 and parameters: {'max_depth': 5, 'learning_rate': 0.014426340871986416, 'subsample': 0.7843562856440282, 'colsample_bytree': 0.9175003081952436, 'reg_alpha': 0.12012982541970996, 'reg_lambda': 1.0515498492427815e-08, 'n_estimators': 263}. Best is trial 12 with value: 0.4898883290788776.\n",
            "[I 2025-12-05 02:26:31,749] Trial 17 finished with value: 0.4741490513534976 and parameters: {'max_depth': 4, 'learning_rate': 0.034205190779708135, 'subsample': 0.6681218421357417, 'colsample_bytree': 0.7938029628688944, 'reg_alpha': 1.668385765330221e-06, 'reg_lambda': 0.022565721363852428, 'n_estimators': 510}. Best is trial 12 with value: 0.4898883290788776.\n",
            "[I 2025-12-05 02:26:38,778] Trial 18 finished with value: 0.4889302415024659 and parameters: {'max_depth': 3, 'learning_rate': 0.014570028707894223, 'subsample': 0.8250051428101366, 'colsample_bytree': 0.6895948444594414, 'reg_alpha': 0.03005670450351336, 'reg_lambda': 9.317357094353522e-05, 'n_estimators': 391}. Best is trial 12 with value: 0.4898883290788776.\n",
            "[I 2025-12-05 02:26:45,222] Trial 19 finished with value: 0.4913437736340231 and parameters: {'max_depth': 3, 'learning_rate': 0.014264482809619892, 'subsample': 0.8456435394965323, 'colsample_bytree': 0.6829746512384693, 'reg_alpha': 0.0003693396023373641, 'reg_lambda': 8.411985243210266e-05, 'n_estimators': 266}. Best is trial 19 with value: 0.4913437736340231.\n",
            "[I 2025-12-05 02:27:17,621] Trial 20 finished with value: 0.4577024760751983 and parameters: {'max_depth': 8, 'learning_rate': 0.08836694769869678, 'subsample': 0.9518033413294811, 'colsample_bytree': 0.6267865556535103, 'reg_alpha': 0.0003726095136808778, 'reg_lambda': 0.00414546349871364, 'n_estimators': 202}. Best is trial 19 with value: 0.4913437736340231.\n",
            "[I 2025-12-05 02:27:24,419] Trial 21 finished with value: 0.4914119674877413 and parameters: {'max_depth': 3, 'learning_rate': 0.013977364307280818, 'subsample': 0.8386356886881808, 'colsample_bytree': 0.691346484168823, 'reg_alpha': 0.030845783288935287, 'reg_lambda': 7.265259184516205e-05, 'n_estimators': 270}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:27:29,597] Trial 22 finished with value: 0.49116534467387013 and parameters: {'max_depth': 3, 'learning_rate': 0.013951920101899452, 'subsample': 0.8641067782695139, 'colsample_bytree': 0.7003030583213072, 'reg_alpha': 0.0005725367338526647, 'reg_lambda': 0.00021891813244441337, 'n_estimators': 280}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:27:35,403] Trial 23 finished with value: 0.4892799679660173 and parameters: {'max_depth': 3, 'learning_rate': 0.018456080657780347, 'subsample': 0.8596671730461117, 'colsample_bytree': 0.6967914140023773, 'reg_alpha': 0.00038398474947150073, 'reg_lambda': 0.00017300286694061945, 'n_estimators': 284}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:27:39,357] Trial 24 finished with value: 0.49006026581760165 and parameters: {'max_depth': 3, 'learning_rate': 0.031360813482791836, 'subsample': 0.8603131333528209, 'colsample_bytree': 0.6463016104575572, 'reg_alpha': 0.7368257037626127, 'reg_lambda': 0.0009682757798750515, 'n_estimators': 148}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:27:45,402] Trial 25 finished with value: 0.4906638742459274 and parameters: {'max_depth': 4, 'learning_rate': 0.01374441586527553, 'subsample': 0.9287211378997813, 'colsample_bytree': 0.6936449235571357, 'reg_alpha': 1.0631065442428805e-05, 'reg_lambda': 3.4761658643264665e-05, 'n_estimators': 213}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:27:59,257] Trial 26 finished with value: 0.48342234104466836 and parameters: {'max_depth': 5, 'learning_rate': 0.02111158639905032, 'subsample': 0.8093720517144154, 'colsample_bytree': 0.6082533211420738, 'reg_alpha': 0.0002922746563549686, 'reg_lambda': 0.0354517748950916, 'n_estimators': 300}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:28:16,361] Trial 27 finished with value: 0.4896838426175127 and parameters: {'max_depth': 7, 'learning_rate': 0.01350027638402298, 'subsample': 0.9520104938890754, 'colsample_bytree': 0.7713510356917049, 'reg_alpha': 0.03360887282170293, 'reg_lambda': 6.718268220654253e-05, 'n_estimators': 111}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:28:21,903] Trial 28 finished with value: 0.4866488815695016 and parameters: {'max_depth': 3, 'learning_rate': 0.030009145823805757, 'subsample': 0.8766405480958176, 'colsample_bytree': 0.7138152517119886, 'reg_alpha': 1.2189043307756991e-06, 'reg_lambda': 0.009583366011089839, 'n_estimators': 243}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:28:27,768] Trial 29 finished with value: 0.4811555459007555 and parameters: {'max_depth': 4, 'learning_rate': 0.046592614316865955, 'subsample': 0.9051450631701504, 'colsample_bytree': 0.832152709236109, 'reg_alpha': 0.0008477776011381448, 'reg_lambda': 8.874360390546402e-07, 'n_estimators': 195}. Best is trial 21 with value: 0.4914119674877413.\n",
            "[I 2025-12-05 02:28:32,883] Trial 30 finished with value: 0.49441297112316196 and parameters: {'max_depth': 5, 'learning_rate': 0.01618156369104911, 'subsample': 0.7924962927726676, 'colsample_bytree': 0.7667296996147117, 'reg_alpha': 1.130722486335355e-07, 'reg_lambda': 1.2496206530417255e-05, 'n_estimators': 62}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:28:39,586] Trial 31 finished with value: 0.49281809100710283 and parameters: {'max_depth': 6, 'learning_rate': 0.01578668762387263, 'subsample': 0.7929035893930687, 'colsample_bytree': 0.7642792676953689, 'reg_alpha': 1.6902749588425065e-08, 'reg_lambda': 1.658990591801067e-05, 'n_estimators': 73}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:28:45,732] Trial 32 finished with value: 0.49397819398703763 and parameters: {'max_depth': 6, 'learning_rate': 0.016811064888360767, 'subsample': 0.8000759953997474, 'colsample_bytree': 0.7705012198355898, 'reg_alpha': 2.3875464922156196e-08, 'reg_lambda': 1.460536938207313e-05, 'n_estimators': 53}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:28:51,181] Trial 33 finished with value: 0.49321992768305717 and parameters: {'max_depth': 6, 'learning_rate': 0.018293529191233098, 'subsample': 0.7918568699288999, 'colsample_bytree': 0.7657724769815869, 'reg_alpha': 1.1793767815133043e-08, 'reg_lambda': 1.1417589710739192e-05, 'n_estimators': 58}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:28:57,279] Trial 34 finished with value: 0.4925354937760669 and parameters: {'max_depth': 6, 'learning_rate': 0.019269436140673918, 'subsample': 0.7963444838382501, 'colsample_bytree': 0.8199550672567522, 'reg_alpha': 1.1630275259679719e-08, 'reg_lambda': 1.1126281067312258e-05, 'n_estimators': 61}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:29:13,804] Trial 35 finished with value: 0.4854121012328769 and parameters: {'max_depth': 7, 'learning_rate': 0.027540557979041463, 'subsample': 0.7257192789517735, 'colsample_bytree': 0.7611688922658226, 'reg_alpha': 1.4869669563707122e-07, 'reg_lambda': 1.6736589658189403e-06, 'n_estimators': 106}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:29:18,843] Trial 36 finished with value: 0.4924666315245653 and parameters: {'max_depth': 6, 'learning_rate': 0.021919167885299948, 'subsample': 0.77922426973064, 'colsample_bytree': 0.7934321918890359, 'reg_alpha': 9.563204108357843e-08, 'reg_lambda': 2.770223553374734e-07, 'n_estimators': 53}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:29:28,897] Trial 37 finished with value: 0.4907567011644499 and parameters: {'max_depth': 6, 'learning_rate': 0.01662222522904513, 'subsample': 0.7632448284143993, 'colsample_bytree': 0.8514107577606862, 'reg_alpha': 1.615873712286949e-08, 'reg_lambda': 9.75779362738227e-06, 'n_estimators': 97}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:29:49,334] Trial 38 finished with value: 0.4829481652697935 and parameters: {'max_depth': 7, 'learning_rate': 0.02591560926565662, 'subsample': 0.7097510940567586, 'colsample_bytree': 0.8095530139643375, 'reg_alpha': 5.445794640647687e-07, 'reg_lambda': 1.9451705192418173e-05, 'n_estimators': 146}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[I 2025-12-05 02:29:57,770] Trial 39 finished with value: 0.49387541046843314 and parameters: {'max_depth': 6, 'learning_rate': 0.011852802112632446, 'subsample': 0.7957900640680878, 'colsample_bytree': 0.7725332565792551, 'reg_alpha': 5.12168967225075e-08, 'reg_lambda': 4.6114591529469253e-07, 'n_estimators': 79}. Best is trial 30 with value: 0.49441297112316196.\n",
            "[OK] done in 560.5s\n",
            "[BEST] score: 0.49441297112316196\n",
            "[BEST PARAMS]: {'max_depth': 5, 'learning_rate': 0.01618156369104911, 'subsample': 0.7924962927726676, 'colsample_bytree': 0.7667296996147117, 'reg_alpha': 1.130722486335355e-07, 'reg_lambda': 1.2496206530417255e-05, 'n_estimators': 62}\n",
            "[DONE] Optuna v3 artifacts saved.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================\n",
        "# OPTUNA v3 — LEAK-PROOF + DUAL OBJECTIVE + SHAP VARIANCE PENALTY (A1)\n",
        "# ============================================\n",
        "\n",
        "import os, time, json, pickle, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from copy import deepcopy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "N_TRIALS = 40\n",
        "N_OUTER = 5\n",
        "N_INNER = 3\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "RSTATE = 42\n",
        "LABEL_H = \"h8\"\n",
        "SHAP_SAMPLES = 300   # small for speed + variance reduction\n",
        "\n",
        "# --------------------------------------------\n",
        "# LOAD SHORTLISTED FEATURES\n",
        "# --------------------------------------------\n",
        "pkl_candidates = sorted([p for p in Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")],\n",
        "                        key=os.path.getmtime)\n",
        "meta_path = str(pkl_candidates[-1])\n",
        "print(\"[INFO] loading df_meta_shortlist:\", meta_path)\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "\n",
        "label_col = f\"tb_label_{LABEL_H}\"\n",
        "tbreak_col = f\"tb_t_break_{LABEL_H}\"\n",
        "feature_cols = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "\n",
        "y_long = (df[label_col] == 1).astype(int)\n",
        "y_short = (df[label_col] == -1).astype(int)\n",
        "\n",
        "# --------------------------------------------\n",
        "# PURGE UTILS\n",
        "# --------------------------------------------\n",
        "from b0_07_purge_utils import (\n",
        "    compute_exposure_intervals,\n",
        "    exposure_to_pos_intervals,\n",
        "    purged_cv_splits\n",
        ")\n",
        "\n",
        "idx = df.index\n",
        "exp_all = compute_exposure_intervals(idx, df[tbreak_col], None, last_index=idx[-1])\n",
        "pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "outer_splits = list(purged_cv_splits(pos_all, N_OUTER, EMBARGO, idx,\n",
        "                                     drop_unmapped=True, random_state=RSTATE))\n",
        "print(\"[INFO] outer splits:\", len(outer_splits))\n",
        "\n",
        "X_all = df[feature_cols].astype(float)\n",
        "\n",
        "# --------------------------------------------\n",
        "# A1 — SHAP-variance penalty helper\n",
        "# --------------------------------------------\n",
        "def compute_shap_variance(model, X_inner, n_samples=300):\n",
        "    \"\"\"Return mean std(|shap|) across features.\"\"\"\n",
        "    if len(X_inner) > n_samples:\n",
        "        X_sub = X_inner.sample(n_samples, random_state=RSTATE)\n",
        "    else:\n",
        "        X_sub = X_inner\n",
        "\n",
        "    expl = shap.TreeExplainer(model)\n",
        "    vals = expl.shap_values(X_sub)\n",
        "    if isinstance(vals, list):  # xgboost sometimes returns list for multiclass\n",
        "        vals = vals[0]\n",
        "\n",
        "    stds = np.std(np.abs(vals), axis=0)\n",
        "    return float(np.mean(stds))\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# OBJECTIVE (dual-AUC * stability penalty)\n",
        "# --------------------------------------------\n",
        "def make_objective():\n",
        "    def obj(trial):\n",
        "        params = {\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
        "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
        "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.6, 1.0),\n",
        "            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
        "            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 600),\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"use_label_encoder\": False\n",
        "        }\n",
        "\n",
        "        outer_scores = []\n",
        "        shap_stds = []\n",
        "\n",
        "        for (tr_pos, val_pos) in outer_splits:\n",
        "            tr_ts = pos_all.index[tr_pos]\n",
        "            val_ts = pos_all.index[val_pos]\n",
        "\n",
        "            X_tr = X_all.loc[tr_ts]\n",
        "            yL_tr = y_long.loc[tr_ts]\n",
        "            yS_tr = y_short.loc[tr_ts]\n",
        "\n",
        "            X_val = X_all.loc[val_ts]\n",
        "            yL_val = y_long.loc[val_ts]\n",
        "            yS_val = y_short.loc[val_ts]\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_s = scaler.fit_transform(X_tr)\n",
        "            X_val_s = scaler.transform(X_val)\n",
        "\n",
        "            sw = compute_sample_weight(\"balanced\", y=yL_tr)\n",
        "\n",
        "            model = XGBClassifier(random_state=RSTATE, **params)\n",
        "            model.fit(X_tr_s, yL_tr, sample_weight=sw, verbose=False)\n",
        "\n",
        "            p = model.predict_proba(X_val_s)[:, 1]\n",
        "\n",
        "            aucL = roc_auc_score(yL_val, p) if len(np.unique(yL_val)) > 1 else 0.5\n",
        "            aucS = roc_auc_score(yS_val, p) if len(np.unique(yS_val)) > 1 else 0.5\n",
        "\n",
        "            outer_scores.append(0.5 * (aucL + aucS))\n",
        "\n",
        "            # ---- SHAP stability ----\n",
        "            shap_std = compute_shap_variance(model, pd.DataFrame(X_tr_s, columns=feature_cols),\n",
        "                                             n_samples=SHAP_SAMPLES)\n",
        "            shap_stds.append(shap_std)\n",
        "\n",
        "        base_auc = float(np.mean(outer_scores))\n",
        "        mean_shap_std = float(np.mean(shap_stds))\n",
        "\n",
        "        penalty = 1.0 / (1.0 + mean_shap_std)\n",
        "        final = base_auc * penalty\n",
        "\n",
        "        trial.set_user_attr(\"base_auc\", base_auc)\n",
        "        trial.set_user_attr(\"shap_std\", mean_shap_std)\n",
        "        trial.set_user_attr(\"penalty\", penalty)\n",
        "\n",
        "        return final\n",
        "\n",
        "    return obj\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# RUN OPTUNA\n",
        "# --------------------------------------------\n",
        "print(\"[INFO] starting Optuna v3...\")\n",
        "study = optuna.create_study(direction=\"maximize\",\n",
        "                            sampler=optuna.samplers.TPESampler(seed=RSTATE))\n",
        "t0 = time.time()\n",
        "study.optimize(make_objective(), n_trials=N_TRIALS, n_jobs=1, show_progress_bar=True)\n",
        "print(\"[OK] done in %.1fs\" % (time.time() - t0))\n",
        "print(\"[BEST] score:\", study.best_value)\n",
        "print(\"[BEST PARAMS]:\", study.best_params)\n",
        "\n",
        "# SAVE\n",
        "pickle.dump(study, open(os.path.join(OUT_DIR, \"optuna_v3_study.pkl\"), \"wb\"))\n",
        "json.dump(study.best_params,\n",
        "          open(os.path.join(OUT_DIR, \"optuna_v3_best_params.json\"), \"w\"),\n",
        "          indent=2)\n",
        "\n",
        "print(\"[DONE] Optuna v3 artifacts saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r0DqgRrE5MpF",
        "outputId": "dcddc4c5-5fb8-4f4f-9361-dee94553bd3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n",
            "[INFO] best params: {'max_depth': 5, 'learning_rate': 0.01618156369104911, 'subsample': 0.7924962927726676, 'colsample_bytree': 0.7667296996147117, 'reg_alpha': 1.130722486335355e-07, 'reg_lambda': 1.2496206530417255e-05, 'n_estimators': 62, 'eval_metric': 'logloss', 'use_label_encoder': False, 'random_state': 42}\n",
            "[INFO] outer splits: 5\n",
            "[INFO] saved fold 1 -> /content/drive/MyDrive/quant_pipeline/mtb_out/a1_final_fold1.pkl | train=14014 test=3505\n",
            "[INFO] saved fold 2 -> /content/drive/MyDrive/quant_pipeline/mtb_out/a1_final_fold2.pkl | train=14010 test=3504\n",
            "[INFO] saved fold 3 -> /content/drive/MyDrive/quant_pipeline/mtb_out/a1_final_fold3.pkl | train=14007 test=3504\n",
            "[INFO] saved fold 4 -> /content/drive/MyDrive/quant_pipeline/mtb_out/a1_final_fold4.pkl | train=13999 test=3504\n",
            "[INFO] saved fold 5 -> /content/drive/MyDrive/quant_pipeline/mtb_out/a1_final_fold5.pkl | train=14009 test=3504\n",
            "[INFO] OOF saved: /content/drive/MyDrive/quant_pipeline/mtb_out/a1_oof.csv\n",
            "\n",
            "[RESULT] FINAL A1 TRAIN:\n",
            "  AUC long : 0.5148\n",
            "  AUC short: 0.5146\n",
            "  AUC mean : 0.5147\n",
            "[DONE] A1 complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==============================================================\n",
        "# FINAL TRAIN — A1 VERSION (dual AUC, leak-proof CPCV)\n",
        "# ==============================================================\n",
        "\n",
        "import os, json, pickle, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# purge utils\n",
        "from b0_07_purge_utils import (\n",
        "    compute_exposure_intervals,\n",
        "    exposure_to_pos_intervals,\n",
        "    purged_cv_splits\n",
        ")\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "LABEL_H = \"h8\"\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "RSTATE = 42\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Load df_meta_shortlist\n",
        "# -----------------------------------------------------\n",
        "pkl_candidates = sorted([p for p in Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")],\n",
        "                        key=os.path.getmtime)\n",
        "if not pkl_candidates:\n",
        "    raise FileNotFoundError(\"df_meta_shortlist missing.\")\n",
        "meta_path = str(pkl_candidates[-1])\n",
        "print(\"[INFO] loading:\", meta_path)\n",
        "\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "\n",
        "label_col = f\"tb_label_{LABEL_H}\"\n",
        "tbreak_col = f\"tb_t_break_{LABEL_H}\"\n",
        "\n",
        "feature_cols = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "\n",
        "y_long  = (df[label_col] == 1).astype(int)\n",
        "y_short = (df[label_col] == -1).astype(int)\n",
        "X_all   = df[feature_cols].astype(float)\n",
        "idx     = df.index\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Load best params from Optuna v3\n",
        "# -----------------------------------------------------\n",
        "best_path = os.path.join(OUT_DIR, \"optuna_v3_best_params.json\")\n",
        "if not os.path.exists(best_path):\n",
        "    raise FileNotFoundError(\"optuna_v3_best_params.json missing. Run Optuna v3 first.\")\n",
        "\n",
        "best_params = json.load(open(best_path))\n",
        "best_params.update({\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"use_label_encoder\": False,\n",
        "    \"random_state\": RSTATE\n",
        "})\n",
        "\n",
        "print(\"[INFO] best params:\", best_params)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Build purge exposure → CPCV outer splits\n",
        "# -----------------------------------------------------\n",
        "exp_all = compute_exposure_intervals(idx, df[tbreak_col], None, last_index=idx[-1])\n",
        "pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "outer_splits = list(purged_cv_splits(pos_all, 5, EMBARGO, idx,\n",
        "                                     drop_unmapped=True, random_state=RSTATE))\n",
        "\n",
        "print(\"[INFO] outer splits:\", len(outer_splits))\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Final CPCV Training per fold\n",
        "# -----------------------------------------------------\n",
        "oof_long  = pd.Series(index=idx, dtype=float)\n",
        "oof_short = pd.Series(index=idx, dtype=float)\n",
        "fold_id   = pd.Series(index=idx, dtype=int)\n",
        "\n",
        "models_meta = []\n",
        "\n",
        "for i, (train_pos, test_pos) in enumerate(outer_splits, start=1):\n",
        "    train_ts = pos_all.index[train_pos]\n",
        "    test_ts  = pos_all.index[test_pos]\n",
        "\n",
        "    X_tr = X_all.loc[train_ts]\n",
        "    X_te = X_all.loc[test_ts]\n",
        "\n",
        "    yL_tr = y_long.loc[train_ts]\n",
        "    yL_te = y_long.loc[test_ts]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_te_s = scaler.transform(X_te)\n",
        "\n",
        "    sw = compute_sample_weight(class_weight=\"balanced\", y=yL_tr)\n",
        "\n",
        "    model = XGBClassifier(**best_params)\n",
        "\n",
        "    try:\n",
        "        model.fit(X_tr_s, yL_tr, sample_weight=sw, verbose=False)\n",
        "    except TypeError:\n",
        "        model.fit(X_tr_s, yL_tr)\n",
        "\n",
        "    preds = model.predict_proba(X_te_s)[:,1]\n",
        "\n",
        "    oof_long.loc[test_ts]  = preds\n",
        "    oof_short.loc[test_ts] = 1 - preds\n",
        "    fold_id.loc[test_ts]   = i\n",
        "\n",
        "    out_path = os.path.join(OUT_DIR, f\"a1_final_fold{i}.pkl\")\n",
        "    pickle.dump({\"model\": model, \"scaler\": scaler, \"params\": best_params}, open(out_path, \"wb\"))\n",
        "    print(f\"[INFO] saved fold {i} -> {out_path} | train={len(train_ts)} test={len(test_ts)}\")\n",
        "\n",
        "    models_meta.append({\n",
        "        \"fold\": i,\n",
        "        \"model\": model,\n",
        "        \"scaler\": scaler,\n",
        "        \"params\": best_params,\n",
        "        \"train_len\": len(train_ts),\n",
        "        \"test_len\": len(test_ts)\n",
        "    })\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Write OOF\n",
        "# -----------------------------------------------------\n",
        "df_oof = pd.DataFrame({\n",
        "    \"oof_long\": oof_long.fillna(0.5),\n",
        "    \"oof_short\": oof_short.fillna(0.5),\n",
        "    \"fold\": fold_id\n",
        "})\n",
        "oof_path = os.path.join(OUT_DIR, \"a1_oof.csv\")\n",
        "df_oof.to_csv(oof_path)\n",
        "print(\"[INFO] OOF saved:\", oof_path)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# AUC results\n",
        "# -----------------------------------------------------\n",
        "auc_long  = roc_auc_score((df[label_col]==1).astype(int), df_oof[\"oof_long\"])\n",
        "auc_short = roc_auc_score((df[label_col]==-1).astype(int), df_oof[\"oof_short\"])\n",
        "auc_comb  = 0.5*(auc_long + auc_short)\n",
        "\n",
        "print(\"\\n[RESULT] FINAL A1 TRAIN:\")\n",
        "print(\"  AUC long :\", round(auc_long, 4))\n",
        "print(\"  AUC short:\", round(auc_short, 4))\n",
        "print(\"  AUC mean :\", round(auc_comb, 4))\n",
        "\n",
        "pickle.dump(models_meta, open(os.path.join(OUT_DIR, \"a1_models_meta.pkl\"), \"wb\"))\n",
        "print(\"[DONE] A1 complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e4Qp_QiY979_",
        "outputId": "b94ed43c-007a-4f30-ab4a-2e7980d9b144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] locating df_meta_shortlist...\n",
            "[INFO] loaded df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl  shape=(17521, 41)\n",
            "[INFO] locating latest OOF file...\n",
            "[INFO] using OOF file: /content/drive/MyDrive/quant_pipeline/mtb_out/a1_oof.csv\n",
            "[INFO] loaded OOF preds: /content/drive/MyDrive/quant_pipeline/mtb_out/a1_oof.csv  n=17521  na_count=0\n",
            "[INFO] aligned OOF -> df_meta index: aligned_non_na=17521 / 17521\n",
            "\n",
            "[OOF DIAGNOSTICS]\n",
            "  mean: 2.9998858512641973\n",
            "  std : 1.4142942754151204\n",
            "  min : 1.0\n",
            "  max : 5.0\n",
            "  NaN : 0\n",
            "\n",
            "[INFO] attempting OOF AUC with label: tb_label_h8\n",
            "  OOF AUC (long):  0.504382\n",
            "  OOF AUC (short): 0.494988\n",
            "\n",
            "[INFO] saved aligned/cleaned OOF -> /content/drive/MyDrive/quant_pipeline/mtb_out/oof_aligned_cleaned_1764901803.csv\n",
            "\n",
            "[UNIT TEST SUGGESTION]\n",
            "  assert aligned.shape[0] == df_meta.shape[0], 'OOF length mismatch with df_meta index'\n",
            "\n",
            "[DONE] OOF loader + diagnostics complete.\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: robust OOF loader + diagnostics (drop-in for A1 diagnostics)\n",
        "import os, glob, json, pickle, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def find_latest_df_meta_shortlist(out_dir=OUT_DIR):\n",
        "    pkl_candidates = sorted(list(Path(out_dir).glob(\"df_meta_shortlist.v*.pkl\")), key=os.path.getmtime)\n",
        "    if not pkl_candidates:\n",
        "        raise FileNotFoundError(\"No df_meta_shortlist.v*.pkl found in OUT_DIR. Create shortlist first.\")\n",
        "    return str(pkl_candidates[-1])\n",
        "\n",
        "def find_latest_oof(out_dir=OUT_DIR, patterns=None):\n",
        "    # common names used in pipeline\n",
        "    if patterns is None:\n",
        "        patterns = [\"*oof*.csv\", \"*oof*.pkl\", \"*_oof.csv\", \"optuna_*_oof.csv\", \"a1_oof.csv\", \"optuna_long_oof.csv\", \"optuna_combined_oof.csv\"]\n",
        "    files = []\n",
        "    for p in patterns:\n",
        "        files.extend(glob.glob(os.path.join(out_dir, p)))\n",
        "    files = sorted(files, key=os.path.getmtime)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No OOF file found in OUT_DIR matching patterns: {patterns}\")\n",
        "    return files[-1]\n",
        "\n",
        "def _csv_to_series(df_csv):\n",
        "    # Accept DataFrame with one column or multiple but pick sensible column\n",
        "    if isinstance(df_csv, pd.Series):\n",
        "        s = df_csv\n",
        "    elif isinstance(df_csv, pd.DataFrame):\n",
        "        if df_csv.shape[1] == 1:\n",
        "            s = df_csv.iloc[:, 0]\n",
        "        else:\n",
        "            # heuristics: try columns named like 'oof','pred','preds','score'\n",
        "            for candidate in (\"oof\",\"pred\",\"preds\",\"score\",\"prob\",\"p\"):\n",
        "                if candidate in df_csv.columns:\n",
        "                    s = df_csv[candidate]\n",
        "                    break\n",
        "            else:\n",
        "                # fallback: use last column (often probability)\n",
        "                s = df_csv.iloc[:, -1]\n",
        "    else:\n",
        "        raise RuntimeError(\"Unsupported CSV object type.\")\n",
        "    return s\n",
        "\n",
        "def load_oof(oof_path):\n",
        "    \"\"\"\n",
        "    Robust loader for OOF predictions. Supports:\n",
        "      - CSV (with/without header)\n",
        "      - PKL (Series or DataFrame saved with pd.to_pickle)\n",
        "    Returns: pandas.Series (index = datetime if parseable)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(oof_path):\n",
        "        raise FileNotFoundError(f\"OOF file not found: {oof_path}\")\n",
        "\n",
        "    ext = os.path.splitext(oof_path)[1].lower()\n",
        "    if ext in (\".pkl\", \".pickle\"):\n",
        "        obj = pd.read_pickle(oof_path)\n",
        "        if isinstance(obj, dict) and \"oof\" in obj:\n",
        "            obj = obj[\"oof\"]\n",
        "        if isinstance(obj, pd.Series):\n",
        "            s = obj\n",
        "        elif isinstance(obj, pd.DataFrame):\n",
        "            s = _csv_to_series(obj)\n",
        "        else:\n",
        "            # try to coerce\n",
        "            s = pd.Series(obj)\n",
        "    elif ext in (\".csv\", \".txt\"):\n",
        "        # try multiple read patterns defensively\n",
        "        # 1) try header=None (index present)\n",
        "        try:\n",
        "            df = pd.read_csv(oof_path, index_col=0)\n",
        "            s = _csv_to_series(df)\n",
        "        except Exception:\n",
        "            # 2) try without index_col\n",
        "            df = pd.read_csv(oof_path)\n",
        "            s = _csv_to_series(df)\n",
        "    else:\n",
        "        raise RuntimeError(\"Unsupported OOF extension: \" + ext)\n",
        "\n",
        "    # try to convert index to datetime (many saved OOFs have timestamp index)\n",
        "    try:\n",
        "        if not isinstance(s.index, pd.DatetimeIndex):\n",
        "            idx_parsed = pd.to_datetime(s.index, utc=True, errors=\"coerce\")\n",
        "            if idx_parsed.notna().any():\n",
        "                # if some parsed -> set where parsed not na, else leave as-is\n",
        "                if idx_parsed.notna().all():\n",
        "                    s.index = idx_parsed\n",
        "                else:\n",
        "                    # partial parse -> preserve original index but try set where possible\n",
        "                    s.index = s.index  # keep original; don't create mixed index\n",
        "        # else already datetime\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # coerce float, fill invalids with NaN\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    return s\n",
        "\n",
        "# ---------- Main: load meta + OOF + diagnostics ----------\n",
        "print(\"[INFO] locating df_meta_shortlist...\")\n",
        "meta_path = find_latest_df_meta_shortlist()\n",
        "df_meta = pd.read_pickle(meta_path)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "print(f\"[INFO] loaded df_meta_shortlist: {meta_path}  shape={df_meta.shape}\")\n",
        "\n",
        "print(\"[INFO] locating latest OOF file...\")\n",
        "oof_path = find_latest_oof()\n",
        "print(f\"[INFO] using OOF file: {oof_path}\")\n",
        "\n",
        "oof = load_oof(oof_path)\n",
        "print(f\"[INFO] loaded OOF preds: {oof_path}  n={len(oof)}  na_count={int(oof.isna().sum())}\")\n",
        "\n",
        "# align OOF to df_meta index where possible\n",
        "aligned = oof.reindex(df_meta.index)\n",
        "n_aligned = int(aligned.notna().sum())\n",
        "print(f\"[INFO] aligned OOF -> df_meta index: aligned_non_na={n_aligned} / {len(df_meta)}\")\n",
        "\n",
        "# basic diagnostics\n",
        "print(\"\\n[OOF DIAGNOSTICS]\")\n",
        "print(\"  mean:\", float(aligned.mean(skipna=True)))\n",
        "print(\"  std :\", float(aligned.std(skipna=True)))\n",
        "print(\"  min :\", float(aligned.min(skipna=True)))\n",
        "print(\"  max :\", float(aligned.max(skipna=True)))\n",
        "print(\"  NaN :\", int(aligned.isna().sum()))\n",
        "\n",
        "# compute OOF AUCs if tb labels present\n",
        "tb_label_cols = [c for c in df_meta.columns if c.startswith(\"tb_label_\")]\n",
        "if tb_label_cols:\n",
        "    # pick the label used in shortlist if available else largest horizon\n",
        "    label_col = next((c for c in tb_label_cols if c.endswith(\"_h8\")), tb_label_cols[-1])\n",
        "    print(f\"\\n[INFO] attempting OOF AUC with label: {label_col}\")\n",
        "    y_long = (df_meta[label_col] == 1).astype(int)\n",
        "    y_short = (df_meta[label_col] == -1).astype(int)\n",
        "    try:\n",
        "        auc_long = roc_auc_score(y_long, aligned.fillna(0.5))\n",
        "        auc_short = roc_auc_score(y_short, aligned.fillna(0.5))\n",
        "        print(f\"  OOF AUC (long):  {auc_long:.6f}\")\n",
        "        print(f\"  OOF AUC (short): {auc_short:.6f}\")\n",
        "    except Exception as e:\n",
        "        print(\"  OOF AUC compute failed:\", e)\n",
        "else:\n",
        "    print(\"\\n[WARN] No tb_label_* columns found in df_meta to compute AUCs.\")\n",
        "\n",
        "# save cleaned aligned OOF as standard CSV for downstream tools\n",
        "clean_oof_path = os.path.join(OUT_DIR, f\"oof_aligned_cleaned_{int(time.time())}.csv\")\n",
        "aligned.fillna(0.5).to_csv(clean_oof_path, index=True, header=True)\n",
        "print(f\"\\n[INFO] saved aligned/cleaned OOF -> {clean_oof_path}\")\n",
        "\n",
        "# quick assertion suggestion (single-line) printed for convenience\n",
        "print(\"\\n[UNIT TEST SUGGESTION]\")\n",
        "print(\"  assert aligned.shape[0] == df_meta.shape[0], 'OOF length mismatch with df_meta index'\")\n",
        "\n",
        "# return objects for interactive workflows (useful in notebooks)\n",
        "# (they will be available in the colab namespace after cell run)\n",
        "OOF_SERIES = aligned\n",
        "DF_META = df_meta\n",
        "OOF_RAW_PATH = oof_path\n",
        "CLEAN_OOF_PATH = clean_oof_path\n",
        "\n",
        "print(\"\\n[DONE] OOF loader + diagnostics complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xNG-TnNh-RO5",
        "outputId": "bee44098-995f-4d58-ea42-b81a3faa0395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl shape= (17521, 41)\n",
            "[INFO] using fold artifacts pattern sample: a1_final_fold1.pkl\n",
            "[INFO] loaded 5 fold artifacts (sample): ['a1_final_fold1.pkl', 'a1_final_fold2.pkl', 'a1_final_fold3.pkl', 'a1_final_fold4.pkl', 'a1_final_fold5.pkl']\n",
            "[INFO] label: tb_label_h8, tbreak: tb_t_break_h8, n_features: 32\n",
            "[INFO] reconstructed outer_splits: 5 folds to use.\n",
            "[INFO] fold 1 -> a1_final_fold1.pkl | test_len=3505\n",
            "[INFO] fold 2 -> a1_final_fold2.pkl | test_len=3504\n",
            "[INFO] fold 3 -> a1_final_fold3.pkl | test_len=3504\n",
            "[INFO] fold 4 -> a1_final_fold4.pkl | test_len=3504\n",
            "[INFO] fold 5 -> a1_final_fold5.pkl | test_len=3504\n",
            "[INFO] rebuilt OOF saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_oof_probs_1764901805.csv\n",
            "[RESULT] Rebuilt OOF AUC (long): 0.514763 | (short): 0.485433\n",
            "[INFO] OOF covered 17521 / 17521 rows (NaNs -> filled with 0.5)\n",
            "\n",
            "[RECOMMENDATION]\n",
            "- If rebuilt OOF probabilities look reasonable (values in [0,1]), use this file for AUC/backtest/stacking.\n",
            "- If values were derived from label-scaling fallback, re-run folding/training code to persist model.predict_proba correctly.\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: rebuild OOF probabilities from saved fold artifacts -> align + diagnostics\n",
        "import os, glob, pickle, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- locate meta shortlist ----------\n",
        "meta_candidates = sorted(list(Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")), key=os.path.getmtime)\n",
        "if not meta_candidates:\n",
        "    raise FileNotFoundError(\"No df_meta_shortlist.v*.pkl found. Create shortlist first.\")\n",
        "meta_path = str(meta_candidates[-1])\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "print(\"[INFO] loaded df_meta_shortlist:\", meta_path, \"shape=\", df.shape)\n",
        "\n",
        "# ---------- find fold artifacts ----------\n",
        "# prefer A1 final folds, fallback to optuna folds or cpcv folds\n",
        "patterns = [\"a1_final_fold*.pkl\", \"optuna_*_cpcv_fold*.pkl\", \"cpcv_*_fold*.pkl\", \"*final_fold*.pkl\"]\n",
        "fold_files = []\n",
        "for p in patterns:\n",
        "    fold_files = sorted(glob.glob(os.path.join(OUT_DIR, p)))\n",
        "    if fold_files:\n",
        "        break\n",
        "if not fold_files:\n",
        "    raise FileNotFoundError(f\"No fold artifacts found in {OUT_DIR} with patterns {patterns}\")\n",
        "print(\"[INFO] using fold artifacts pattern sample:\", os.path.basename(fold_files[0]))\n",
        "\n",
        "# ---------- load artifacts list (ordered by fold id if possible) ----------\n",
        "# artifacts expected: dict with keys like {\"model\":..., \"scaler\":..., \"params\":...}\n",
        "folds = []\n",
        "for path in sorted(fold_files):\n",
        "    try:\n",
        "        obj = pd.read_pickle(path)\n",
        "    except Exception:\n",
        "        try:\n",
        "            with open(path, \"rb\") as f:\n",
        "                obj = pickle.load(f)\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] cannot load\", path, e); continue\n",
        "    # allow either direct model object or dict wrapper\n",
        "    if isinstance(obj, dict) and (\"model\" in obj or \"scaler\" in obj):\n",
        "        folds.append({\"path\": path, \"model\": obj.get(\"model\"), \"scaler\": obj.get(\"scaler\"), \"meta\": {k:v for k,v in obj.items() if k not in (\"model\",\"scaler\")}})\n",
        "    elif hasattr(obj, \"predict_proba\") or hasattr(obj, \"predict\"):\n",
        "        folds.append({\"path\": path, \"model\": obj, \"scaler\": None, \"meta\": {}})\n",
        "    else:\n",
        "        # try common nested layouts\n",
        "        if isinstance(obj, (list, tuple)) and len(obj)>0:\n",
        "            candidate = obj[0]\n",
        "            if isinstance(candidate, dict) and (\"model\" in candidate):\n",
        "                folds.append({\"path\": path, \"model\": candidate.get(\"model\"), \"scaler\": candidate.get(\"scaler\"), \"meta\": {}})\n",
        "            else:\n",
        "                print(\"[WARN] unknown structure in\", path)\n",
        "        else:\n",
        "            print(\"[WARN] unknown artifact structure:\", path)\n",
        "print(f\"[INFO] loaded {len(folds)} fold artifacts (sample):\", [os.path.basename(f['path']) for f in folds[:6]])\n",
        "\n",
        "# ---------- need purge utils to reconstruct outer test indices ----------\n",
        "try:\n",
        "    from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Purge utils missing. Paste patched purge utils (b0_07_purge_utils) into notebook before running this cell.\") from e\n",
        "\n",
        "# identify label/tbreak and feature_cols\n",
        "# label prefer h8, else choose first tb_label_*\n",
        "label_cols = [c for c in df.columns if c.startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"No tb_label_* in df_meta_shortlist.\")\n",
        "label_col = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "tbreak_col = label_col.replace(\"tb_label\", \"tb_t_break\")\n",
        "feature_cols = [c for c in df.columns if not c.startswith(\"tb_\")]\n",
        "print(f\"[INFO] label: {label_col}, tbreak: {tbreak_col}, n_features: {len(feature_cols)}\")\n",
        "\n",
        "# make exposure mapping and outer splits (N = number of folds we have)\n",
        "idx = df.index\n",
        "exp_all = compute_exposure_intervals(idx, df[tbreak_col], horizon_fallback=None, last_index=idx[-1])\n",
        "pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "\n",
        "# infer N_OUTER from number of fold artifacts if possible\n",
        "N_OUTER = len(folds)\n",
        "outer_splits = list(purged_cv_splits(pos_all, n_splits=N_OUTER, embargo=pd.Timedelta(\"1H\"), index=idx, drop_unmapped=True, random_state=42))\n",
        "if len(outer_splits) != N_OUTER:\n",
        "    print(f\"[WARN] artifact count {N_OUTER} != purged_cv_splits produced {len(outer_splits)}. Using min length.\")\n",
        "    N_use = min(N_OUTER, len(outer_splits))\n",
        "    outer_splits = outer_splits[:N_use]\n",
        "    folds = folds[:N_use]\n",
        "\n",
        "print(f\"[INFO] reconstructed outer_splits: {len(outer_splits)} folds to use.\")\n",
        "\n",
        "# ---------- rebuild OOF ----------\n",
        "oof = pd.Series(index=df.index, dtype=float).fillna(np.nan)\n",
        "oof_fold = pd.Series(index=df.index, dtype=int).fillna(-1)\n",
        "\n",
        "for i, (fold_art, (train_pos, test_pos)) in enumerate(zip(folds, outer_splits), start=1):\n",
        "    model = fold_art.get(\"model\")\n",
        "    scaler = fold_art.get(\"scaler\")\n",
        "    path = fold_art.get(\"path\")\n",
        "    if model is None:\n",
        "        print(f\"[WARN] fold {i} no model in artifact {path}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    test_ts = pos_all.index[test_pos]\n",
        "    if len(test_ts) == 0:\n",
        "        print(f\"[WARN] fold {i} has empty test set, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # prepare X_test\n",
        "    X_test = df.loc[test_ts, feature_cols].astype(float).values\n",
        "\n",
        "    # apply scaler if present (support common types)\n",
        "    if scaler is not None:\n",
        "        try:\n",
        "            X_test_s = scaler.transform(X_test)\n",
        "        except Exception:\n",
        "            # scaler may be pickled class, try to call fit_transform fallback? no — prefer transform\n",
        "            X_test_s = X_test\n",
        "    else:\n",
        "        X_test_s = X_test\n",
        "\n",
        "    # obtain probability-ish scores\n",
        "    probs = None\n",
        "    try:\n",
        "        probs_arr = model.predict_proba(X_test_s)\n",
        "        # choose positive class column:\n",
        "        if probs_arr.ndim == 2:\n",
        "            # common: (n,2) -> positive class column 1\n",
        "            if probs_arr.shape[1] == 2:\n",
        "                probs = probs_arr[:, 1]\n",
        "            else:\n",
        "                # multiclass: try to aggregate into a single score: weighted by class index -> normalized\n",
        "                class_idxs = np.arange(probs_arr.shape[1])\n",
        "                probs = (probs_arr * class_idxs).sum(axis=1) / (probs_arr.shape[1] - 1 + 1e-12)\n",
        "        else:\n",
        "            # unexpected shape\n",
        "            probs = pd.to_numeric(probs_arr, errors=\"coerce\")\n",
        "    except Exception as e:\n",
        "        # fallback: try predict -> then scale labels to [0,1] if labels are integer ordinal\n",
        "        try:\n",
        "            preds = model.predict(X_test_s)\n",
        "            preds = np.asarray(preds)\n",
        "            # if preds are {0,1} use directly\n",
        "            unique = np.unique(preds)\n",
        "            if set(unique).issubset({0,1}):\n",
        "                probs = preds.astype(float)\n",
        "            else:\n",
        "                # map min->0, max->1 linearly\n",
        "                mn, mx = float(preds.min()), float(preds.max())\n",
        "                if mx - mn == 0:\n",
        "                    probs = np.full_like(preds, 0.5, dtype=float)\n",
        "                else:\n",
        "                    probs = (preds - mn) / (mx - mn)\n",
        "        except Exception as e2:\n",
        "            print(f\"[ERROR] fold {i} model predict failed: {e}; predict fallback failed: {e2}\")\n",
        "            continue\n",
        "\n",
        "    # write into OOF\n",
        "    oof.loc[test_ts] = probs\n",
        "    oof_fold.loc[test_ts] = i\n",
        "    print(f\"[INFO] fold {i} -> {os.path.basename(path)} | test_len={len(test_ts)}\")\n",
        "\n",
        "# ---------- postprocess & diagnostics ----------\n",
        "# fill any remaining NaNs with 0.5 (neutral)\n",
        "oof_filled = oof.fillna(0.5)\n",
        "\n",
        "# align length\n",
        "aligned = oof_filled.reindex(df.index)\n",
        "\n",
        "# save\n",
        "ts = int(time.time())\n",
        "out_oof_path = os.path.join(OUT_DIR, f\"rebuild_oof_probs_{ts}.csv\")\n",
        "aligned.to_csv(out_oof_path, index=True, header=True)\n",
        "print(\"[INFO] rebuilt OOF saved ->\", out_oof_path)\n",
        "\n",
        "# compute AUCs if labels present\n",
        "y_long = (df[label_col] == 1).astype(int)\n",
        "y_short = (df[label_col] == -1).astype(int)\n",
        "try:\n",
        "    auc_long = roc_auc_score(y_long, aligned)\n",
        "    auc_short = roc_auc_score(y_short, aligned)\n",
        "    print(f\"[RESULT] Rebuilt OOF AUC (long): {auc_long:.6f} | (short): {auc_short:.6f}\")\n",
        "except Exception as e:\n",
        "    print(\"AUC compute failed:\", e)\n",
        "\n",
        "# quick sanity: how many folds covered all indices?\n",
        "covered = (~oof.isna()).sum()\n",
        "print(f\"[INFO] OOF covered {int(covered)} / {len(df)} rows (NaNs -> filled with 0.5)\")\n",
        "\n",
        "# final suggestion printed\n",
        "print(\"\\n[RECOMMENDATION]\")\n",
        "print(\"- If rebuilt OOF probabilities look reasonable (values in [0,1]), use this file for AUC/backtest/stacking.\")\n",
        "print(\"- If values were derived from label-scaling fallback, re-run folding/training code to persist model.predict_proba correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4762657hALrE",
        "outputId": "2fc8d42d-40e9-4c64-e423-587b30255a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n",
            "[INFO] df shape: (17521, 41)\n",
            "[INFO] features: 32 | label: tb_label_h8\n",
            "[INFO] using best params file: optuna_v3_best_params.json\n",
            "[INFO] outer_splits: 5\n",
            "[INFO] Fold 1: train=14014 test=3505\n",
            "[INFO] saved fold 1 -> /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_fold1.pkl\n",
            "[INFO] Fold 2: train=14010 test=3504\n",
            "[INFO] saved fold 2 -> /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_fold2.pkl\n",
            "[INFO] Fold 3: train=14007 test=3504\n",
            "[INFO] saved fold 3 -> /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_fold3.pkl\n",
            "[INFO] Fold 4: train=13999 test=3504\n",
            "[INFO] saved fold 4 -> /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_fold4.pkl\n",
            "[INFO] Fold 5: train=14009 test=3504\n",
            "[INFO] saved fold 5 -> /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_fold5.pkl\n",
            "[INFO] OOF saved -> /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_oof_long.csv and /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_oof_short.csv\n",
            "[RESULT] Rebuilt OOF AUC (long): 0.514763 | (short): 0.506045\n",
            "fold1: n=3505, meanL=0.4933, auc_long=0.5467, meanS=0.5047, auc_short=0.5294\n",
            "fold2: n=3504, meanL=0.4958, auc_long=0.5089, meanS=0.5028, auc_short=0.5035\n",
            "fold3: n=3504, meanL=0.4930, auc_long=0.5252, meanS=0.5075, auc_short=0.5197\n",
            "fold4: n=3504, meanL=0.5055, auc_long=0.5088, meanS=0.4954, auc_short=0.5080\n",
            "fold5: n=3504, meanL=0.4661, auc_long=0.5076, meanS=0.5354, auc_short=0.4818\n",
            "[DONE] rebuild_dual complete. Artifacts written to OUT_DIR.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ONE-CELL: Rebuild CPCV folds — DUAL BINARY (long vs rest) + (short vs rest)\n",
        "import os, glob, json, time, pickle, warnings\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CONFIG\n",
        "N_OUTER = 5\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "RSTATE = 42\n",
        "LABEL_H = \"h8\"\n",
        "FOLD_PREFIX = \"rebuild_dual\"   # outputs: rebuild_dual_fold{i}.pkl, rebuild_dual_oof_long.csv, rebuild_dual_oof_short.csv\n",
        "\n",
        "# 1) load df_meta_shortlist\n",
        "pkl_candidates = sorted([p for p in Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")], key=os.path.getmtime)\n",
        "if not pkl_candidates:\n",
        "    raise FileNotFoundError(\"No df_meta_shortlist found. Run shortlist step first.\")\n",
        "meta_path = str(pkl_candidates[-1])\n",
        "print(\"[INFO] loading df_meta_shortlist:\", meta_path)\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "print(f\"[INFO] df shape: {df.shape}\")\n",
        "\n",
        "# 2) label/features\n",
        "label_col = f\"tb_label_{LABEL_H}\"\n",
        "tbreak_col = f\"tb_t_break_{LABEL_H}\"\n",
        "if label_col not in df.columns or tbreak_col not in df.columns:\n",
        "    raise RuntimeError(f\"{label_col} or {tbreak_col} missing in df_meta_shortlist.\")\n",
        "feature_cols = [c for c in df.columns if (not c.startswith(\"tb_\"))]\n",
        "print(f\"[INFO] features: {len(feature_cols)} | label: {label_col}\")\n",
        "\n",
        "# 3) best params (try pick latest; fallback conservative)\n",
        "param_files = sorted(\n",
        "    glob.glob(os.path.join(OUT_DIR, \"*best_params*.json\")) +\n",
        "    glob.glob(os.path.join(OUT_DIR, \"optuna_v3_best_params.json\")) +\n",
        "    glob.glob(os.path.join(OUT_DIR, \"optuna_best_params_*.json\")),\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "if param_files:\n",
        "    best_path = param_files[-1]\n",
        "    best_params = json.load(open(best_path))\n",
        "    print(\"[INFO] using best params file:\", os.path.basename(best_path))\n",
        "else:\n",
        "    best_path = None\n",
        "    best_params = {\"max_depth\":4, \"learning_rate\":0.02, \"subsample\":0.8, \"colsample_bytree\":0.8, \"reg_alpha\":1e-3, \"reg_lambda\":1.0, \"n_estimators\":200}\n",
        "best_params = {k:v for k,v in best_params.items() if k not in (\"use_label_encoder\",\"eval_metric\")}\n",
        "best_params.update({\"verbosity\":0})\n",
        "\n",
        "# 4) purge utils (patched)\n",
        "try:\n",
        "    from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Paste patched b0_07_purge_utils into notebook before running rebuild.\") from e\n",
        "\n",
        "# 5) compute outer splits once\n",
        "idx = df.index\n",
        "exp_all = compute_exposure_intervals(idx, df[tbreak_col], horizon_fallback=None, last_index=idx[-1])\n",
        "pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "outer_splits = list(purged_cv_splits(pos_all, n_splits=N_OUTER, embargo=EMBARGO, index=idx, drop_unmapped=True, random_state=RSTATE))\n",
        "print(\"[INFO] outer_splits:\", len(outer_splits))\n",
        "if len(outer_splits) < N_OUTER:\n",
        "    print(\"[WARN] fewer outer splits than requested:\", len(outer_splits))\n",
        "\n",
        "# 6) prepare containers for OOF (two sides)\n",
        "oof_long = pd.Series(index=df.index, dtype=float)\n",
        "oof_short = pd.Series(index=df.index, dtype=float)\n",
        "oof_long[:] = np.nan\n",
        "oof_short[:] = np.nan\n",
        "models_meta = []\n",
        "\n",
        "X_all = df[feature_cols].astype(float)\n",
        "y_long_all = (df[label_col] == 1).astype(int)\n",
        "y_short_all = (df[label_col] == -1).astype(int)\n",
        "\n",
        "# 7) train per outer fold for both sides with guards\n",
        "for i, (train_pos_outer, test_pos_outer) in enumerate(outer_splits, start=1):\n",
        "    train_ts = pos_all.index[train_pos_outer]\n",
        "    test_ts = pos_all.index[test_pos_outer]\n",
        "    print(f\"[INFO] Fold {i}: train={len(train_ts)} test={len(test_ts)}\")\n",
        "\n",
        "    X_tr = X_all.loc[train_ts].values\n",
        "    X_test = X_all.loc[test_ts].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    # --- LONG model (long vs rest)\n",
        "    y_tr_long = y_long_all.loc[train_ts].values\n",
        "    clf_long = None\n",
        "    probs_long = None\n",
        "    if len(np.unique(y_tr_long)) < 2 or y_tr_long.sum() == 0:\n",
        "        # guard: no positive class in training\n",
        "        probs_long = np.full(len(test_ts), 0.5)\n",
        "        print(f\"  [GUARD] fold{i} LONG train has single-class (assign 0.5)\")\n",
        "    else:\n",
        "        sw_long = compute_sample_weight(class_weight=\"balanced\", y=y_tr_long)\n",
        "        clf_long = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=RSTATE, **deepcopy(best_params))\n",
        "        try:\n",
        "            clf_long.fit(X_tr_s, y_tr_long, sample_weight=sw_long, verbose=False)\n",
        "        except TypeError:\n",
        "            clf_long.fit(X_tr_s, y_tr_long, verbose=False)\n",
        "        probs_long = clf_long.predict_proba(X_test_s)[:,1]\n",
        "\n",
        "    oof_long.loc[test_ts] = probs_long\n",
        "\n",
        "    # --- SHORT model (short vs rest)\n",
        "    y_tr_short = y_short_all.loc[train_ts].values\n",
        "    clf_short = None\n",
        "    probs_short = None\n",
        "    if len(np.unique(y_tr_short)) < 2 or y_tr_short.sum() == 0:\n",
        "        probs_short = np.full(len(test_ts), 0.5)\n",
        "        print(f\"  [GUARD] fold{i} SHORT train has single-class (assign 0.5)\")\n",
        "    else:\n",
        "        sw_short = compute_sample_weight(class_weight=\"balanced\", y=y_tr_short)\n",
        "        clf_short = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=RSTATE, **deepcopy(best_params))\n",
        "        try:\n",
        "            clf_short.fit(X_tr_s, y_tr_short, sample_weight=sw_short, verbose=False)\n",
        "        except TypeError:\n",
        "            clf_short.fit(X_tr_s, y_tr_short, verbose=False)\n",
        "        probs_short = clf_short.predict_proba(X_test_s)[:,1]\n",
        "\n",
        "    oof_short.loc[test_ts] = probs_short\n",
        "\n",
        "    # persist fold artifact (both models + scaler)\n",
        "    fold_obj = {\n",
        "        \"fold\": i,\n",
        "        \"model_long\": clf_long, \"model_short\": clf_short,\n",
        "        \"scaler\": scaler,\n",
        "        \"params\": deepcopy(best_params),\n",
        "        \"train_index\": train_ts, \"test_index\": test_ts\n",
        "    }\n",
        "    fold_path = os.path.join(OUT_DIR, f\"{FOLD_PREFIX}_fold{i}.pkl\")\n",
        "    pd.to_pickle(fold_obj, fold_path)\n",
        "    models_meta.append({\"fold\": i, \"path\": os.path.basename(fold_path), \"train_len\": len(train_ts), \"test_len\": len(test_ts)})\n",
        "    print(f\"[INFO] saved fold {i} -> {fold_path}\")\n",
        "\n",
        "# 8) save OOFs (explicit header)\n",
        "oof_long_path = os.path.join(OUT_DIR, f\"{FOLD_PREFIX}_oof_long.csv\")\n",
        "oof_short_path = os.path.join(OUT_DIR, f\"{FOLD_PREFIX}_oof_short.csv\")\n",
        "oof_long.fillna(0.5).to_csv(oof_long_path, index=True, header=[\"oof_long_prob\"])\n",
        "oof_short.fillna(0.5).to_csv(oof_short_path, index=True, header=[\"oof_short_prob\"])\n",
        "print(\"[INFO] OOF saved ->\", oof_long_path, \"and\", oof_short_path)\n",
        "\n",
        "# 9) diagnostics: global AUCs & per-fold\n",
        "try:\n",
        "    auc_long = roc_auc_score(y_long_all, oof_long.fillna(0.5))\n",
        "    auc_short = roc_auc_score(y_short_all, oof_short.fillna(0.5))\n",
        "except Exception:\n",
        "    auc_long = auc_short = np.nan\n",
        "print(f\"[RESULT] Rebuilt OOF AUC (long): {auc_long:.6f} | (short): {auc_short:.6f}\")\n",
        "\n",
        "for m in models_meta:\n",
        "    obj = pd.read_pickle(os.path.join(OUT_DIR, m[\"path\"]))\n",
        "    te_idx = obj[\"test_index\"]\n",
        "    probsL = oof_long.reindex(te_idx).values\n",
        "    probsS = oof_short.reindex(te_idx).values\n",
        "    yl = (df.loc[te_idx, label_col] == 1).astype(int).values\n",
        "    ys = (df.loc[te_idx, label_col] == -1).astype(int).values\n",
        "    aucL = roc_auc_score(yl, probsL) if len(np.unique(yl))>1 else np.nan\n",
        "    aucS = roc_auc_score(ys, probsS) if len(np.unique(ys))>1 else np.nan\n",
        "    print(f\"fold{m['fold']}: n={len(te_idx)}, meanL={np.nanmean(probsL):.4f}, auc_long={aucL:.4f}, meanS={np.nanmean(probsS):.4f}, auc_short={aucS:.4f}\")\n",
        "\n",
        "# 10) sanity asserts\n",
        "assert oof_long.notna().all() and oof_short.notna().all(), \"OOF contains NaN -> some test rows uncovered\"\n",
        "assert (oof_long.min() >= 0.0 and oof_long.max() <= 1.0), \"OOF long outside [0,1]\"\n",
        "assert (oof_short.min() >= 0.0 and oof_short.max() <= 1.0), \"OOF short outside [0,1]\"\n",
        "\n",
        "# 11) save meta\n",
        "meta_out = {\n",
        "    \"generated_at\": int(time.time()),\n",
        "    \"n_folds\": len(models_meta),\n",
        "    \"models\": models_meta,\n",
        "    \"oof_long\": os.path.basename(oof_long_path),\n",
        "    \"oof_short\": os.path.basename(oof_short_path),\n",
        "    \"best_params_source\": os.path.basename(best_path) if best_path else \"fallback\"\n",
        "}\n",
        "json.dump(meta_out, open(os.path.join(OUT_DIR, f\"{FOLD_PREFIX}_meta.json\"), \"w\"), indent=2)\n",
        "pd.to_pickle(models_meta, os.path.join(OUT_DIR, f\"{FOLD_PREFIX}_models_meta.pkl\"))\n",
        "print(\"[DONE] rebuild_dual complete. Artifacts written to OUT_DIR.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KuFzZRZkBLss",
        "outputId": "659246b4-a03b-48a9-d616-d605c4e92883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] using OOF long: rebuild_dual_oof_long.csv\n",
            "[INFO] using OOF short: rebuild_dual_oof_short.csv\n",
            "[INFO] found fold artifacts: ['rebuild_dual_fold1.pkl', 'rebuild_dual_fold2.pkl', 'rebuild_dual_fold3.pkl', 'rebuild_dual_fold4.pkl', 'rebuild_dual_fold5.pkl']\n",
            "[INFO] loaded df_meta_shortlist: df_meta_shortlist.v1764900731.pkl shape= (17521, 41)\n",
            "[INFO] reconstructed 5 folds\n",
            "[INFO] per-fold calibration complete\n",
            "[DIAGNOSTICS] Before/After calibration:\n",
            "  auc_long_raw        : 0.514763\n",
            "  auc_short_raw       : 0.506045\n",
            "  brier_long_raw      : 0.248302\n",
            "  brier_short_raw     : 0.248892\n",
            "  auc_long_cal        : 0.510223\n",
            "  auc_short_cal       : 0.497292\n",
            "  brier_long_cal      : 0.233843\n",
            "  brier_short_cal     : 0.234207\n",
            "[RESULT] saved signals: /content/drive/MyDrive/quant_pipeline/mtb_out/ensemble_calib_signals_1764901815.csv\n",
            "[DONE] ensemble calibration + signal build complete.\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL (fixed): Leak-proof ensemble + Platt calibration (per-fold) -> calibrated OOF + signal\n",
        "import os, glob, json, time, pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CONFIG\n",
        "OOF_PATTERN_LONG = \"*_oof_long.csv\"\n",
        "OOF_PATTERN_SHORT = \"*_oof_short.csv\"\n",
        "FOLD_ARTIFACT_GLOB = os.path.join(OUT_DIR, \"rebuild_dual_fold*.pkl\")\n",
        "CALIB_METHOD = \"platt\"   # \"platt\" or \"isotonic\"\n",
        "SAVE_PREFIX = \"ensemble_calib\"\n",
        "\n",
        "# helpers\n",
        "def load_latest(pattern):\n",
        "    files = sorted(glob.glob(os.path.join(OUT_DIR, pattern)), key=os.path.getmtime)\n",
        "    return files[-1] if files else None\n",
        "\n",
        "def to_dt_index(x):\n",
        "    \"\"\"Convert list/ndarray/Index/Series to pd.DatetimeIndex (UTC).\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    if isinstance(x, pd.DatetimeIndex):\n",
        "        return x.tz_convert(\"UTC\") if getattr(x, \"tz\", None) is not None else x.tz_localize(\"UTC\")\n",
        "    # accept numpy array / list / Series / Index\n",
        "    seq = None\n",
        "    if isinstance(x, (list, tuple, np.ndarray, pd.Series, pd.Index)):\n",
        "        seq = list(x)\n",
        "    elif hasattr(x, \"__iter__\"):\n",
        "        try:\n",
        "            seq = list(x)\n",
        "        except Exception:\n",
        "            seq = None\n",
        "    if seq is None:\n",
        "        raise RuntimeError(\"Unable to coerce fold index object to sequence.\")\n",
        "    # convert to DatetimeIndex\n",
        "    idx = pd.to_datetime(pd.Index(seq), utc=True)\n",
        "    return idx\n",
        "\n",
        "def extract_train_test_from_obj(obj):\n",
        "    \"\"\"Robustly extract train_index and test_index from a fold artifact dict.\"\"\"\n",
        "    if not isinstance(obj, dict):\n",
        "        raise RuntimeError(\"Fold artifact is not a dict-like object.\")\n",
        "    keys = list(obj.keys())\n",
        "    # search keys containing 'train' / 'test'\n",
        "    train_key = None\n",
        "    test_key = None\n",
        "    for k in keys:\n",
        "        kl = k.lower()\n",
        "        if (\"train\" in kl) and (train_key is None):\n",
        "            train_key = k\n",
        "        if (\"test\" in kl) and (test_key is None):\n",
        "            test_key = k\n",
        "    # fallback: common names\n",
        "    fallbacks = [\"train_index\", \"train_idx\", \"train_ts\", \"train_index_ts\", \"train_time_index\"]\n",
        "    for fk in fallbacks:\n",
        "        if train_key is None and fk in obj:\n",
        "            train_key = fk\n",
        "    fallbacks2 = [\"test_index\", \"test_idx\", \"test_ts\", \"test_index_ts\", \"test_time_index\"]\n",
        "    for fk in fallbacks2:\n",
        "        if test_key is None and fk in obj:\n",
        "            test_key = fk\n",
        "    # final sanity: must exist\n",
        "    if train_key is None or test_key is None:\n",
        "        # try infer from stored index slices inside 'train_index' like names with nested dict\n",
        "        raise RuntimeError(f\"Could not find train/test keys in fold artifact. Available keys: {keys}\")\n",
        "    train_idx_raw = obj.get(train_key)\n",
        "    test_idx_raw = obj.get(test_key)\n",
        "    train_idx = to_dt_index(train_idx_raw)\n",
        "    test_idx = to_dt_index(test_idx_raw)\n",
        "    return train_idx, test_idx\n",
        "\n",
        "# 1) locate inputs\n",
        "oof_long_path = load_latest(OOF_PATTERN_LONG)\n",
        "oof_short_path = load_latest(OOF_PATTERN_SHORT)\n",
        "fold_files = sorted(glob.glob(FOLD_ARTIFACT_GLOB), key=os.path.getmtime)\n",
        "\n",
        "if oof_long_path is None or oof_short_path is None:\n",
        "    raise FileNotFoundError(\"OOF files not found. Run rebuild_dual first.\")\n",
        "if not fold_files:\n",
        "    raise FileNotFoundError(\"Fold artifacts not found. Run rebuild_dual first.\")\n",
        "\n",
        "print(\"[INFO] using OOF long:\", os.path.basename(oof_long_path))\n",
        "print(\"[INFO] using OOF short:\", os.path.basename(oof_short_path))\n",
        "print(\"[INFO] found fold artifacts:\", [os.path.basename(p) for p in fold_files])\n",
        "\n",
        "# 2) load df_meta_shortlist\n",
        "meta_candidates = sorted([p for p in Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")], key=os.path.getmtime)\n",
        "if not meta_candidates:\n",
        "    raise FileNotFoundError(\"df_meta_shortlist not found in OUT_DIR.\")\n",
        "df_meta = pd.read_pickle(str(meta_candidates[-1]))\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "print(\"[INFO] loaded df_meta_shortlist:\", os.path.basename(str(meta_candidates[-1])), \"shape=\", df_meta.shape)\n",
        "\n",
        "# 3) load OOF series (align to df_meta index)\n",
        "def read_oof(path):\n",
        "    df = pd.read_csv(path, index_col=0)\n",
        "    s = df.iloc[:,0]\n",
        "    s.index = pd.to_datetime(s.index, utc=True)\n",
        "    s = s.reindex(df_meta.index)  # align\n",
        "    return s.astype(float)\n",
        "\n",
        "oof_long = read_oof(oof_long_path)\n",
        "oof_short = read_oof(oof_short_path)\n",
        "assert len(oof_long) == len(df_meta) and len(oof_short) == len(df_meta), \"OOF length mismatch with df_meta\"\n",
        "\n",
        "# 4) parse folds robustly\n",
        "folds = []\n",
        "for p in fold_files:\n",
        "    obj = pd.read_pickle(p)\n",
        "    try:\n",
        "        tr_idx, te_idx = extract_train_test_from_obj(obj)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to extract train/test from fold artifact {p}: {e}\") from e\n",
        "    folds.append({\"fold\": int(obj.get(\"fold\", len(folds)+1)), \"train_index\": tr_idx, \"test_index\": te_idx})\n",
        "print(f\"[INFO] reconstructed {len(folds)} folds\")\n",
        "\n",
        "# 5) per-fold calibrators and apply\n",
        "calibrated_long = pd.Series(index=df_meta.index, dtype=float)\n",
        "calibrated_short = pd.Series(index=df_meta.index, dtype=float)\n",
        "calibrators_meta = []\n",
        "\n",
        "label_cols = [c for c in df_meta.columns if c.startswith(\"tb_label_\")]\n",
        "if not label_cols:\n",
        "    raise RuntimeError(\"No tb_label_* columns in df_meta_shortlist. Cannot calibrate.\")\n",
        "labcol = next((c for c in label_cols if c.endswith(\"_h8\")), label_cols[0])\n",
        "y_long = (df_meta[labcol] == 1).astype(int)\n",
        "y_short = (df_meta[labcol] == -1).astype(int)\n",
        "\n",
        "for f in folds:\n",
        "    tr_idx = f[\"train_index\"]\n",
        "    te_idx = f[\"test_index\"]\n",
        "    pL_tr = oof_long.reindex(tr_idx).dropna()\n",
        "    pS_tr = oof_short.reindex(tr_idx).dropna()\n",
        "    pL_te = oof_long.reindex(te_idx)\n",
        "    pS_te = oof_short.reindex(te_idx)\n",
        "\n",
        "    yL_tr = y_long.reindex(tr_idx).fillna(0).astype(int)\n",
        "    yS_tr = y_short.reindex(tr_idx).fillna(0).astype(int)\n",
        "\n",
        "    # LONG calibrator\n",
        "    if CALIB_METHOD == \"platt\":\n",
        "        if len(np.unique(yL_tr)) < 2 or len(pL_tr)==0:\n",
        "            prior = float(yL_tr.mean()) if len(yL_tr)>0 else 0.0\n",
        "            def mapL(x, prior=prior): return np.clip(np.full_like(np.asarray(x,dtype=float), prior), 0.0, 1.0)\n",
        "            clfL_meta = {\"type\":\"const\", \"prior\": prior}\n",
        "        else:\n",
        "            clf = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
        "            clf.fit(pL_tr.values.reshape(-1,1), yL_tr.values)\n",
        "            def mapL(x, clf=clf): return np.clip(clf.predict_proba(np.asarray(x).reshape(-1,1))[:,1], 0.0, 1.0)\n",
        "            clfL_meta = {\"type\":\"platt\", \"coef\": clf.coef_.tolist(), \"intercept\": clf.intercept_.tolist()}\n",
        "    else:  # isotonic\n",
        "        if len(np.unique(yL_tr)) < 2 or len(pL_tr)==0:\n",
        "            prior = float(yL_tr.mean()) if len(yL_tr)>0 else 0.0\n",
        "            def mapL(x, prior=prior): return np.clip(np.full_like(np.asarray(x,dtype=float), prior), 0.0, 1.0)\n",
        "            clfL_meta = {\"type\":\"const\", \"prior\": prior}\n",
        "        else:\n",
        "            iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "            iso.fit(pL_tr.values, yL_tr.values)\n",
        "            def mapL(x, iso=iso): return np.clip(iso.predict(np.asarray(x)), 0.0, 1.0)\n",
        "            clfL_meta = {\"type\":\"iso\"}\n",
        "\n",
        "    # SHORT calibrator\n",
        "    if CALIB_METHOD == \"platt\":\n",
        "        if len(np.unique(yS_tr)) < 2 or len(pS_tr)==0:\n",
        "            priorS = float(yS_tr.mean()) if len(yS_tr)>0 else 0.0\n",
        "            def mapS(x, prior=priorS): return np.clip(np.full_like(np.asarray(x,dtype=float), prior), 0.0, 1.0)\n",
        "            clfS_meta = {\"type\":\"const\", \"prior\": priorS}\n",
        "        else:\n",
        "            clf = LogisticRegression(solver=\"lbfgs\", max_iter=200)\n",
        "            clf.fit(pS_tr.values.reshape(-1,1), yS_tr.values)\n",
        "            def mapS(x, clf=clf): return np.clip(clf.predict_proba(np.asarray(x).reshape(-1,1))[:,1], 0.0, 1.0)\n",
        "            clfS_meta = {\"type\":\"platt\", \"coef\": clf.coef_.tolist(), \"intercept\": clf.intercept_.tolist()}\n",
        "    else:\n",
        "        if len(np.unique(yS_tr)) < 2 or len(pS_tr)==0:\n",
        "            priorS = float(yS_tr.mean()) if len(yS_tr)>0 else 0.0\n",
        "            def mapS(x, prior=priorS): return np.clip(np.full_like(np.asarray(x,dtype=float), prior), 0.0, 1.0)\n",
        "            clfS_meta = {\"type\":\"const\", \"prior\": priorS}\n",
        "        else:\n",
        "            iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "            iso.fit(pS_tr.values, yS_tr.values)\n",
        "            def mapS(x, iso=iso): return np.clip(iso.predict(np.asarray(x)), 0.0, 1.0)\n",
        "            clfS_meta = {\"type\":\"iso\"}\n",
        "\n",
        "    # apply to test\n",
        "    pL_te_raw = pL_te.values\n",
        "    pS_te_raw = pS_te.values\n",
        "    pL_te_cal = mapL(pL_te_raw)\n",
        "    pS_te_cal = mapS(pS_te_raw)\n",
        "\n",
        "    calibrated_long.loc[te_idx] = pL_te_cal\n",
        "    calibrated_short.loc[te_idx] = pS_te_cal\n",
        "\n",
        "    calibrators_meta.append({\"fold\": f[\"fold\"], \"clfL\": clfL_meta, \"clfS\": clfS_meta,\n",
        "                             \"n_train\": len(pL_tr), \"n_test\": len(pL_te)})\n",
        "\n",
        "print(\"[INFO] per-fold calibration complete\")\n",
        "\n",
        "# 6) diagnostics\n",
        "pL_raw = oof_long.fillna(0.5)\n",
        "pS_raw = oof_short.fillna(0.5)\n",
        "pL_cal = calibrated_long.fillna(0.5)\n",
        "pS_cal = calibrated_short.fillna(0.5)\n",
        "\n",
        "yL = y_long.fillna(0).astype(int)\n",
        "yS = y_short.fillna(0).astype(int)\n",
        "\n",
        "metrics = {\n",
        "    \"auc_long_raw\": roc_auc_score(yL, pL_raw),\n",
        "    \"auc_short_raw\": roc_auc_score(yS, pS_raw),\n",
        "    \"brier_long_raw\": brier_score_loss(yL, pL_raw),\n",
        "    \"brier_short_raw\": brier_score_loss(yS, pS_raw),\n",
        "    \"auc_long_cal\": roc_auc_score(yL, pL_cal),\n",
        "    \"auc_short_cal\": roc_auc_score(yS, pS_cal),\n",
        "    \"brier_long_cal\": brier_score_loss(yL, pL_cal),\n",
        "    \"brier_short_cal\": brier_score_loss(yS, pS_cal),\n",
        "}\n",
        "print(\"[DIAGNOSTICS] Before/After calibration:\")\n",
        "for k,v in metrics.items():\n",
        "    print(f\"  {k:20s}: {v:.6f}\")\n",
        "\n",
        "# 7) build signal and save\n",
        "signal = (pL_cal - pS_cal).fillna(0.0)\n",
        "df_out = df_meta.copy()\n",
        "df_out[\"p_long_cal\"] = pL_cal\n",
        "df_out[\"p_short_cal\"] = pS_cal\n",
        "df_out[\"signal\"] = signal\n",
        "df_out[\"signal_strength\"] = signal.abs()\n",
        "df_out[\"signal_side\"] = signal.apply(np.sign).astype(int)\n",
        "\n",
        "ts = int(time.time())\n",
        "out_signals = os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_signals_{ts}.csv\")\n",
        "pd.DataFrame({\n",
        "    \"p_long_cal\": pL_cal,\n",
        "    \"p_short_cal\": pS_cal,\n",
        "    \"signal\": signal,\n",
        "    \"signal_strength\": signal.abs(),\n",
        "    \"signal_side\": signal.apply(np.sign).astype(int)\n",
        "}).to_csv(out_signals, index=True)\n",
        "pickle.dump(calibrators_meta, open(os.path.join(OUT_DIR, f\"{SAVE_PREFIX}_calibrators_meta_{ts}.pkl\"), \"wb\"))\n",
        "\n",
        "print(\"[RESULT] saved signals:\", out_signals)\n",
        "print(\"[DONE] ensemble calibration + signal build complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YV1C0G97B5zx",
        "outputId": "0a7cedd8-6742-480c-a2b8-83ede530bd00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl\n",
            "[INFO] using signals file: /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_oof_short.csv\n",
            "[INFO] loaded oof long: /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_oof_long.csv short: /content/drive/MyDrive/quant_pipeline/mtb_out/rebuild_dual_oof_short.csv\n",
            "[OK] aligned counts: (17521, 41)\n",
            "[STATS] {'n_rows': 17521, 'mean_p_long': 0.4907433017003986, 'mean_p_short': 0.5091350216393629, 'mean_spread': -0.018391719938964237, 'std_spread': 0.07308855213240993, 'skew_spread': -0.27868095484899963, 'kurt_spread': 0.39341208999438226, 'top_1pct_long': 0.010045088750642087, 'top_1pct_short': 0.010045088750642087, 'frac_extreme_spread_>0.2': 0.014325666343245249}\n",
            "[PLOTS] saved to /content/drive/MyDrive/quant_pipeline/mtb_out/signal_diag_plots\n",
            "[INFO] diag saved: /content/drive/MyDrive/quant_pipeline/mtb_out/signal_diag_summary_1764901818.json\n",
            "[INFO] enriched signals saved: /content/drive/MyDrive/quant_pipeline/mtb_out/signals_enriched_1764901818.csv\n",
            "[P&L PROXY] {'pnl_mean': -4.6483457904972443e-07, 'pnl_sharpe': -0.00020629971168941694}\n",
            "\n",
            "[NOTE] leakage check:\n",
            " - Ensure calibration step used only out-of-fold predictions and holdout. If calibration used test labels or future data, AUC/Brier/diagnostics are invalid.\n",
            " - If you used full-data fit to calibrate and then evaluated on same data, result is optimistic.\n",
            "\n",
            "[DONE] Signal diagnostics complete. Inspect CSV + PNGs in OUT_DIR.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 768x576 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 768x576 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ONE-CELL: SIGNAL DIAGNOSTICS SUITE v1 (FULL)\n",
        "# Paste & run in Colab. Expects artifacts in OUT_DIR:\n",
        "#  - df_meta_shortlist.v*.pkl\n",
        "#  - rebuild_dual_oof_long.csv / rebuild_dual_oof_short.csv  (or ensemble_calib_signals_*.csv)\n",
        "#  - optional: fold artifacts if needed for per-fold diagnostics\n",
        "import os, glob, json, time, warnings\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams[\"figure.dpi\"] = 120\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------- 1) locate inputs -------\n",
        "pkl_candidates = sorted(Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\"), key=os.path.getmtime)\n",
        "if not pkl_candidates:\n",
        "    raise FileNotFoundError(\"df_meta_shortlist not found in OUT_DIR.\")\n",
        "df_path = str(pkl_candidates[-1])\n",
        "print(\"[INFO] loading df_meta_shortlist:\", df_path)\n",
        "df_meta = pd.read_pickle(df_path)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "\n",
        "# find best signals file (prefer calibrated ensemble), fallback to dual rebuild OOFs\n",
        "ensemble_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"ensemble_calib_signals_*.csv\")) + glob.glob(os.path.join(OUT_DIR, \"rebuild_dual_oof_*.csv\")) + glob.glob(os.path.join(OUT_DIR, \"*oof_*long*.csv\")) + glob.glob(os.path.join(OUT_DIR, \"*oof_long*.csv\")))\n",
        "if ensemble_candidates:\n",
        "    # if a single combined CSV (ensemble) exists, prefer it\n",
        "    chosen = ensemble_candidates[-1]\n",
        "    print(\"[INFO] using signals file:\", chosen)\n",
        "    df_sign = pd.read_csv(chosen, index_col=0, parse_dates=True)\n",
        "    # attempt to find p_long / p_short columns\n",
        "    possible_long = [c for c in df_sign.columns if \"long\" in c.lower() or \"p_long\" in c.lower() or \"oof_long\" in c.lower()]\n",
        "    possible_short = [c for c in df_sign.columns if \"short\" in c.lower() or \"p_short\" in c.lower() or \"oof_short\" in c.lower()]\n",
        "    if possible_long and possible_short:\n",
        "        p_long = df_sign[possible_long[0]].astype(float)\n",
        "        p_short = df_sign[possible_short[0]].astype(float)\n",
        "    else:\n",
        "        # if combined not present, try pair files later\n",
        "        p_long = p_short = None\n",
        "else:\n",
        "    p_long = p_short = None\n",
        "\n",
        "# fallback: separate OOF files\n",
        "if p_long is None or p_short is None:\n",
        "    long_files = sorted(glob.glob(os.path.join(OUT_DIR, \"*oof*long*.csv\")) + glob.glob(os.path.join(OUT_DIR, \"rebuild_dual_oof_long.csv\")) + glob.glob(os.path.join(OUT_DIR, \"*_oof_long.csv\")))\n",
        "    short_files = sorted(glob.glob(os.path.join(OUT_DIR, \"*oof*short*.csv\")) + glob.glob(os.path.join(OUT_DIR, \"rebuild_dual_oof_short.csv\")) + glob.glob(os.path.join(OUT_DIR, \"*_oof_short.csv\")))\n",
        "    if not long_files or not short_files:\n",
        "        raise FileNotFoundError(\"Could not find OOF long/short files in OUT_DIR. Expected e.g. rebuild_dual_oof_long.csv and rebuild_dual_oof_short.csv\")\n",
        "    p_long = pd.read_csv(long_files[-1], index_col=0, parse_dates=True).iloc[:,0].astype(float)\n",
        "    p_short = pd.read_csv(short_files[-1], index_col=0, parse_dates=True).iloc[:,0].astype(float)\n",
        "    print(\"[INFO] loaded oof long:\", long_files[-1], \"short:\", short_files[-1])\n",
        "\n",
        "# align indexes to df_meta\n",
        "p_long = p_long.reindex(df_meta.index)\n",
        "p_short = p_short.reindex(df_meta.index)\n",
        "\n",
        "# ------- 2) sanity asserts -------\n",
        "assert df_meta.shape[0] == p_long.shape[0] == p_short.shape[0], \"Length mismatch: df_meta vs p_long/p_short\"\n",
        "print(\"[OK] aligned counts:\", df_meta.shape)\n",
        "\n",
        "# ------- 3) build signals -------\n",
        "signals = pd.DataFrame(index=df_meta.index)\n",
        "signals[\"p_long\"] = p_long.fillna(0.5)\n",
        "signals[\"p_short\"] = p_short.fillna(0.5)\n",
        "signals[\"p_sum\"] = signals[\"p_long\"] + signals[\"p_short\"]\n",
        "signals[\"spread\"] = signals[\"p_long\"] - signals[\"p_short\"]          # main signal\n",
        "signals[\"spread_z\"] = (signals[\"spread\"] - signals[\"spread\"].mean()) / (signals[\"spread\"].std() + 1e-12)\n",
        "signals[\"p_long_z\"] = (signals[\"p_long\"] - signals[\"p_long\"].mean()) / (signals[\"p_long\"].std() + 1e-12)\n",
        "signals[\"p_short_z\"] = (signals[\"p_short\"] - signals[\"p_short\"].mean()) / (signals[\"p_short\"].std() + 1e-12)\n",
        "signals[\"confidence\"] = signals[[\"p_long\",\"p_short\"]].max(axis=1)\n",
        "\n",
        "# add base labels & returns if available (for diagnostics)\n",
        "label_cols = [c for c in df_meta.columns if c.startswith(\"tb_label_\")]\n",
        "if label_cols:\n",
        "    signals[\"label\"] = df_meta[label_cols[0]].reindex(signals.index)\n",
        "if \"close\" in df_meta.columns:\n",
        "    signals[\"close\"] = df_meta[\"close\"].reindex(signals.index)\n",
        "    signals[\"ret_1\"] = signals[\"close\"].pct_change().fillna(0)\n",
        "\n",
        "# ------- 4) basic stats & tail concentration -------\n",
        "stats_summary = {\n",
        "    \"n_rows\": len(signals),\n",
        "    \"mean_p_long\": float(signals[\"p_long\"].mean()),\n",
        "    \"mean_p_short\": float(signals[\"p_short\"].mean()),\n",
        "    \"mean_spread\": float(signals[\"spread\"].mean()),\n",
        "    \"std_spread\": float(signals[\"spread\"].std()),\n",
        "    \"skew_spread\": float(signals[\"spread\"].skew()),\n",
        "    \"kurt_spread\": float(signals[\"spread\"].kurtosis()),\n",
        "    \"top_1pct_long\": float((signals[\"p_long\"] >= signals[\"p_long\"].quantile(0.99)).mean()),\n",
        "    \"top_1pct_short\": float((signals[\"p_short\"] >= signals[\"p_short\"].quantile(0.99)).mean()),\n",
        "    \"frac_extreme_spread_>0.2\": float((signals[\"spread\"].abs() > 0.2).mean())\n",
        "}\n",
        "print(\"[STATS]\", stats_summary)\n",
        "\n",
        "# tail concentration (how many events make 50% of positive spread mass)\n",
        "pos = signals.loc[signals[\"spread\"]>0, \"spread\"]\n",
        "if len(pos):\n",
        "    pos_sorted = pos.sort_values(ascending=False)\n",
        "    cum = pos_sorted.cumsum()\n",
        "    cut = cum / cum.iloc[-1]\n",
        "    n50 = (cut <= 0.5).sum() + 1\n",
        "    stats_summary[\"pos_50pct_count\"] = int(n50)\n",
        "else:\n",
        "    stats_summary[\"pos_50pct_count\"] = 0\n",
        "\n",
        "neg = signals.loc[signals[\"spread\"]<0, \"spread\"].abs()\n",
        "if len(neg):\n",
        "    neg_sorted = neg.sort_values(ascending=False)\n",
        "    cumn = neg_sorted.cumsum()\n",
        "    n50n = (cumn <= 0.5).sum() + 1\n",
        "    stats_summary[\"neg_50pct_count\"] = int(n50n)\n",
        "else:\n",
        "    stats_summary[\"neg_50pct_count\"] = 0\n",
        "\n",
        "# ------- 5) regime diagnostics (volatility & trend) -------\n",
        "# rolling vol on returns (proxy regime)\n",
        "if \"ret_1\" in signals.columns:\n",
        "    signals[\"vol_rolling_24\"] = signals[\"ret_1\"].rolling(24, min_periods=1).std()\n",
        "    # high vol vs low vol masks (top 25% vs bottom 25%)\n",
        "    vq = signals[\"vol_rolling_24\"].quantile([0.25, 0.75]).to_dict()\n",
        "    signals[\"regime_vol\"] = \"mid\"\n",
        "    signals.loc[signals[\"vol_rolling_24\"] <= vq[0.25], \"regime_vol\"] = \"low\"\n",
        "    signals.loc[signals[\"vol_rolling_24\"] >= vq[0.75], \"regime_vol\"] = \"high\"\n",
        "    # trend (simple 24-hr return)\n",
        "    signals[\"trend_24\"] = signals[\"close\"].pct_change(24)\n",
        "    tq = signals[\"trend_24\"].quantile([0.33, 0.66]).to_dict()\n",
        "    signals[\"regime_trend\"] = \"flat\"\n",
        "    signals.loc[signals[\"trend_24\"] >= tq[0.66], \"regime_trend\"] = \"up\"\n",
        "    signals.loc[signals[\"trend_24\"] <= tq[0.33], \"regime_trend\"] = \"down\"\n",
        "else:\n",
        "    signals[\"regime_vol\"] = \"unk\"\n",
        "    signals[\"regime_trend\"] = \"unk\"\n",
        "\n",
        "# regime performance proxies\n",
        "regime_perf = signals.groupby(\"regime_vol\")[\"spread\"].agg([\"count\",\"mean\",\"std\"]).to_dict()\n",
        "\n",
        "# ------- 6) temporal drift (monthly) -------\n",
        "signals[\"month\"] = signals.index.to_period(\"M\")\n",
        "monthly = signals.groupby(\"month\")[\"spread\"].agg([\"mean\",\"std\",\"count\"])\n",
        "monthly = monthly.reset_index()\n",
        "monthly[\"month\"] = monthly[\"month\"].dt.to_timestamp()\n",
        "\n",
        "# ------- 7) autocorrelation (few lags) -------\n",
        "def acf_lags(s, lags=24):\n",
        "    out = {}\n",
        "    s = s.dropna()\n",
        "    for lag in [1,2,6,12,24]:\n",
        "        out[f\"acf_{lag}\"] = float(s.autocorr(lag=lag)) if len(s)>lag else np.nan\n",
        "    return out\n",
        "acf_stats = acf_lags(signals[\"spread\"])\n",
        "\n",
        "# ------- 8) QQ & normality test -------\n",
        "qq = stats.probplot(signals[\"spread\"].dropna(), dist=\"norm\")\n",
        "ks_p = stats.kstest((signals[\"spread\"].dropna()-signals[\"spread\"].mean())/signals[\"spread\"].std(), \"norm\").pvalue\n",
        "\n",
        "# ------- 9) plotting (save PNGs) -------\n",
        "PLOT_DIR = os.path.join(OUT_DIR, \"signal_diag_plots\")\n",
        "Path(PLOT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 9a histogram p_long / p_short / spread\n",
        "plt.figure()\n",
        "plt.hist(signals[\"p_long\"].dropna(), bins=50)\n",
        "plt.title(\"p_long histogram\")\n",
        "plt.xlabel(\"p_long\"); plt.ylabel(\"count\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"hist_p_long.png\")); plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(signals[\"p_short\"].dropna(), bins=50)\n",
        "plt.title(\"p_short histogram\")\n",
        "plt.xlabel(\"p_short\"); plt.ylabel(\"count\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"hist_p_short.png\")); plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(signals[\"spread\"].dropna(), bins=80)\n",
        "plt.title(\"spread (p_long - p_short) histogram\")\n",
        "plt.xlabel(\"spread\"); plt.ylabel(\"count\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"hist_spread.png\")); plt.close()\n",
        "\n",
        "# 9b qq-plot\n",
        "plt.figure()\n",
        "_ = stats.probplot(signals[\"spread\"].dropna(), dist=\"norm\", plot=plt)\n",
        "plt.title(\"QQ plot spread\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"qq_spread.png\")); plt.close()\n",
        "\n",
        "# 9c spread over time (monthly mean series)\n",
        "plt.figure()\n",
        "plt.plot(monthly[\"month\"], monthly[\"mean\"], marker=\"o\")\n",
        "plt.title(\"Monthly mean spread\")\n",
        "plt.xlabel(\"month\"); plt.ylabel(\"mean_spread\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"monthly_mean_spread.png\")); plt.close()\n",
        "\n",
        "# 9d scatter p_long vs p_short\n",
        "plt.figure()\n",
        "plt.scatter(signals[\"p_long\"].values, signals[\"p_short\"].values, s=6, alpha=0.4)\n",
        "plt.title(\"p_long vs p_short scatter\")\n",
        "plt.xlabel(\"p_long\"); plt.ylabel(\"p_short\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"scatter_long_short.png\")); plt.close()\n",
        "\n",
        "# 9e autocorr bar\n",
        "plt.figure()\n",
        "lags = [1,2,6,12,24]\n",
        "vals = [acf_stats[f\"acf_{l}\"] for l in lags]\n",
        "plt.bar([str(x) for x in lags], vals)\n",
        "plt.title(\"Autocorrelation spread (select lags)\")\n",
        "plt.xlabel(\"lag\"); plt.ylabel(\"acf\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"acf_spread.png\")); plt.close()\n",
        "\n",
        "# 9f regime violin-like summaries (boxplots)\n",
        "plt.figure()\n",
        "signals.boxplot(column=\"spread\", by=\"regime_vol\", grid=False)\n",
        "plt.title(\"Spread by vol regime\"); plt.suptitle(\"\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"box_spread_by_vol.png\")); plt.close()\n",
        "\n",
        "plt.figure()\n",
        "signals.boxplot(column=\"spread\", by=\"regime_trend\", grid=False)\n",
        "plt.title(\"Spread by trend regime\"); plt.suptitle(\"\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(PLOT_DIR, \"box_spread_by_trend.png\")); plt.close()\n",
        "\n",
        "print(f\"[PLOTS] saved to {PLOT_DIR}\")\n",
        "\n",
        "# ------- 10) summary diagnostics JSON + CSV output -------\n",
        "diag = {\n",
        "    \"generated_at\": int(time.time()),\n",
        "    \"n\": int(len(signals)),\n",
        "    \"stats_summary\": stats_summary,\n",
        "    \"acf\": acf_stats,\n",
        "    \"ks_p_normality\": float(ks_p),\n",
        "    \"regime_perf_by_vol\": regime_perf,\n",
        "}\n",
        "diag_path = os.path.join(OUT_DIR, f\"signal_diag_summary_{int(time.time())}.json\")\n",
        "with open(diag_path, \"w\") as f:\n",
        "    json.dump(diag, f, indent=2)\n",
        "print(\"[INFO] diag saved:\", diag_path)\n",
        "\n",
        "# save enriched signals CSV\n",
        "out_csv = os.path.join(OUT_DIR, f\"signals_enriched_{int(time.time())}.csv\")\n",
        "signals.to_csv(out_csv, index=True)\n",
        "print(\"[INFO] enriched signals saved:\", out_csv)\n",
        "\n",
        "# ------- 11) quick backtest-ready check (sanity) -------\n",
        "# simple threshold P/L proxy: long when spread_z > z_thr, short when spread_z < -z_thr\n",
        "z_thr = 1.0\n",
        "signals[\"signal_dir\"] = 0\n",
        "signals.loc[signals[\"spread_z\"] > z_thr, \"signal_dir\"] = 1\n",
        "signals.loc[signals[\"spread_z\"] < -z_thr, \"signal_dir\"] = -1\n",
        "if \"ret_1\" in signals.columns:\n",
        "    signals[\"pnl_proxy\"] = signals[\"signal_dir\"].shift(1) * signals[\"ret_1\"]  # naive next-step pnl\n",
        "    pnl_stats = {\"pnl_mean\": float(signals[\"pnl_proxy\"].mean()), \"pnl_sharpe\": float(signals[\"pnl_proxy\"].mean() / (signals[\"pnl_proxy\"].std() + 1e-12))}\n",
        "    print(\"[P&L PROXY]\", pnl_stats)\n",
        "else:\n",
        "    print(\"[P&L PROXY] returns not available in df_meta_shortlist; skip pnl proxy.\")\n",
        "\n",
        "# ------- 12) unit-test suggestion (one-liner assert) -------\n",
        "# ensures size parity and prob range\n",
        "assert signals.shape[0] == df_meta.shape[0], \"UNIT-TEST FAIL: signals length != df_meta length\"\n",
        "assert (signals[\"p_long\"].between(0,1).all() and signals[\"p_short\"].between(0,1).all()), \"UNIT-TEST FAIL: probabilities outside [0,1]\"\n",
        "\n",
        "# ------- 13) potential leakage note (short) -------\n",
        "print(\"\\n[NOTE] leakage check:\")\n",
        "print(\" - Ensure calibration step used only out-of-fold predictions and holdout. If calibration used test labels or future data, AUC/Brier/diagnostics are invalid.\")\n",
        "print(\" - If you used full-data fit to calibrate and then evaluated on same data, result is optimistic.\")\n",
        "\n",
        "print(\"\\n[DONE] Signal diagnostics complete. Inspect CSV + PNGs in OUT_DIR.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zc7p9t7hFEos",
        "outputId": "d0a01989-ad47-486a-c604-fc20a1efc091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded df_meta_shortlist: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.pkl shape=(17521, 41); OOF n=17521\n",
            "[INFO] holdout mode: calibrator rows=3504 | eval rows=14017\n",
            "[INFO] Global calib summary saved: /content/drive/MyDrive/quant_pipeline/mtb_out/calib_summary_1764901819.json\n",
            "  Brier raw/platt/iso: 0.24830186151591715 0.23381397371246126 0.23409595651592635\n",
            "  ECE raw/platt/iso  : 0.11912257055374635 0.0031321831697618263 0.018990487655999033\n",
            "[DONE] Monthly & rolling ECE saved. files -> /content/drive/MyDrive/quant_pipeline/mtb_out\n",
            "\n",
            "NOTES:\n",
            " - Calibrators trained only on OOF-derived data (holdout or cv_on_oof). Do NOT train on full in-sample fitted probs.\n",
            " - If you want to *persist a single calibrator* to apply in forward production, prefer 'holdout' trained calibrator (train on last holdout window).\n",
            " - For small holdout sizes prefer 'cv_on_oof' ensemble approach; but ensure folds are time-blocked.\n",
            " - Use saved calibrator pickles for production transform (apply to new model-produced probs).\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: CPCV-safe calibrators (Platt + Isotonic) + per-month rolling ECE (A + B)\n",
        "# Run after you produced aligned OOF predictions and df_meta_shortlist\n",
        "import os, glob, time, json, pickle, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- USER CONFIG ----------\n",
        "OOF_LONG = os.path.join(OUT_DIR, \"rebuild_dual_oof_long.csv\")\n",
        "OOF_SHORT = os.path.join(OUT_DIR, \"rebuild_dual_oof_short.csv\")  # optional\n",
        "META_PKL = sorted(Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\"), key=os.path.getmtime)[-1]\n",
        "\n",
        "CAL_MODE = \"holdout\"       # \"holdout\" or \"cv_on_oof\"\n",
        "HOLDOUT_PCT = 0.20         # if holdout mode -> last 20% time used for calibrator training\n",
        "CV_FOLDS = 5               # if cv_on_oof -> number of temporal folds on OOF for calibration CV\n",
        "N_BINS = 10\n",
        "ROLL_MONTHS = 3            # rolling ECE window (months)\n",
        "MIN_SAMPLES_PER_MONTH = 30\n",
        "SAVE_TAG = int(time.time())\n",
        "CLIP_EPS = 1e-6            # clip probs before logit\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def load_oof(path):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    df = pd.read_csv(path, index_col=0)\n",
        "    # If single-col csv -> take first column\n",
        "    if df.shape[1] == 1:\n",
        "        ser = df.iloc[:,0].astype(float)\n",
        "    else:\n",
        "        # if header exists \"oof_prob\" or similar\n",
        "        ser = df.iloc[:,0].astype(float)\n",
        "    ser.index = pd.to_datetime(ser.index, utc=True)\n",
        "    return ser\n",
        "\n",
        "def calibration_table(y_true, p, n_bins=10):\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    inds = np.digitize(p, bins, right=True)\n",
        "    rows = []\n",
        "    for b in range(1, n_bins+1):\n",
        "        mask = inds == b\n",
        "        cnt = int(mask.sum())\n",
        "        if cnt == 0:\n",
        "            rows.append({\"bin\": b, \"bin_lo\": bins[b-1], \"bin_hi\": bins[b], \"count\": 0,\n",
        "                         \"mean_pred\": np.nan, \"mean_true\": np.nan, \"bias\": np.nan})\n",
        "            continue\n",
        "        mean_pred = float(p[mask].mean())\n",
        "        mean_true = float(y_true[mask].mean())\n",
        "        rows.append({\"bin\": b, \"bin_lo\": float(bins[b-1]), \"bin_hi\": float(bins[b]),\n",
        "                     \"count\": cnt, \"mean_pred\": mean_pred, \"mean_true\": mean_true,\n",
        "                     \"bias\": mean_pred - mean_true})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def expected_calibration_error(tab):\n",
        "    total = tab[\"count\"].sum()\n",
        "    if total == 0:\n",
        "        return np.nan\n",
        "    return float((tab[\"count\"] * tab[\"bias\"].abs()).sum() / total)\n",
        "\n",
        "def fit_platt(p_train, y_train):\n",
        "    # Fit logistic regression with p as single feature (Platt). Use logit(p) as feature is common,\n",
        "    # but we can also fit directly on p (works). We'll use logit transform for stability.\n",
        "    p = np.clip(p_train, CLIP_EPS, 1-CLIP_EPS)\n",
        "    logit = np.log(p / (1 - p)).reshape(-1,1)\n",
        "    clf = LogisticRegression(solver=\"lbfgs\")\n",
        "    clf.fit(logit, y_train)\n",
        "    def calib(p_in):\n",
        "        p2 = np.clip(p_in, CLIP_EPS, 1-CLIP_EPS)\n",
        "        log = np.log(p2 / (1 - p2)).reshape(-1,1)\n",
        "        return clf.predict_proba(log)[:,1]\n",
        "    return clf, calib\n",
        "\n",
        "def fit_isotonic(p_train, y_train):\n",
        "    ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "    ir.fit(p_train, y_train)\n",
        "    def calib(p_in):\n",
        "        return ir.transform(np.clip(p_in, 0.0, 1.0))\n",
        "    return ir, calib\n",
        "\n",
        "# ---------- load data ----------\n",
        "df_meta = pd.read_pickle(META_PKL)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "oof_long = load_oof(OOF_LONG)\n",
        "oof_short = load_oof(OOF_SHORT) if os.path.exists(OOF_SHORT) else None\n",
        "if oof_long is None:\n",
        "    raise FileNotFoundError(\"OOF long not found. Provide rebuild_dual_oof_long.csv (OOF).\")\n",
        "# align\n",
        "oof_long = oof_long.reindex(df_meta.index)\n",
        "if oof_short is not None:\n",
        "    oof_short = oof_short.reindex(df_meta.index)\n",
        "\n",
        "LABEL_H = \"h8\"\n",
        "label_col = f\"tb_label_{LABEL_H}\"\n",
        "if label_col not in df_meta.columns:\n",
        "    raise RuntimeError(f\"{label_col} missing in df_meta_shortlist\")\n",
        "y_long = (df_meta[label_col] == 1).astype(int)\n",
        "\n",
        "print(f\"[INFO] loaded df_meta_shortlist: {META_PKL} shape={df_meta.shape}; OOF n={len(oof_long)}\")\n",
        "\n",
        "# ---------- calibration dataset selection (safe) ----------\n",
        "if CAL_MODE == \"holdout\":\n",
        "    # use last HOLDOUT_PCT of timeline as calibrator training set\n",
        "    n = len(df_meta)\n",
        "    n_hold = max(int(n * HOLDOUT_PCT), 1)\n",
        "    calibrator_idx = df_meta.index[-n_hold:]\n",
        "    train_idx_for_eval = df_meta.index[:-n_hold]\n",
        "    print(f\"[INFO] holdout mode: calibrator rows={len(calibrator_idx)} | eval rows={len(train_idx_for_eval)}\")\n",
        "    p_cal = oof_long.loc[calibrator_idx].values\n",
        "    y_cal = y_long.loc[calibrator_idx].values\n",
        "    # train both calibrators on this set\n",
        "    platt_model, platt_fn = fit_platt(p_cal, y_cal)\n",
        "    iso_model, iso_fn = fit_isotonic(p_cal, y_cal)\n",
        "    # save calibrators\n",
        "    pickle.dump({\"model\":platt_model, \"type\":\"platt\", \"trained_on\":\"holdout\", \"rows\":len(calibrator_idx)},\n",
        "                open(os.path.join(OUT_DIR, f\"calibrator_platt_holdout_{SAVE_TAG}.pkl\"), \"wb\"))\n",
        "    pickle.dump({\"model\":iso_model, \"type\":\"isotonic\", \"trained_on\":\"holdout\", \"rows\":len(calibrator_idx)},\n",
        "                open(os.path.join(OUT_DIR, f\"calibrator_iso_holdout_{SAVE_TAG}.pkl\"), \"wb\"))\n",
        "    # apply calibrators to full OOF (for diagnostics & downstream usage)\n",
        "    p_platt = platt_fn(oof_long.values)\n",
        "    p_iso = iso_fn(oof_long.values)\n",
        "\n",
        "elif CAL_MODE == \"cv_on_oof\":\n",
        "    # K-fold temporal splits on OOF (use contiguous time blocks)\n",
        "    k = int(CV_FOLDS)\n",
        "    idx = df_meta.index\n",
        "    splits = []\n",
        "    fold_sizes = np.full(k, len(idx) // k, dtype=int)\n",
        "    fold_sizes[:len(idx) % k] += 1\n",
        "    starts = np.cumsum(np.concatenate([[0], fold_sizes[:-1]]))\n",
        "    for s,fs in zip(starts, fold_sizes):\n",
        "        splits.append(idx[s:s+fs])\n",
        "    # For each fold: train calibrator on that fold, evaluate on remaining folds -> get averaged calibrator via stacking\n",
        "    # We'll produce an ensemble calibrator by averaging calibrated probabilities from fold-trained calibrators.\n",
        "    p_platt_all = np.zeros(len(idx))\n",
        "    p_iso_all = np.zeros(len(idx))\n",
        "    counts = np.zeros(len(idx))\n",
        "    calibrator_meta = {\"platt\":[], \"isotonic\":[]}\n",
        "    for i, val_idx in enumerate(splits):\n",
        "        # train on this val_idx (treat as calibrator), apply to all data (safe: trained only on holdout fold)\n",
        "        p_train = oof_long.loc[val_idx].values\n",
        "        y_train = y_long.loc[val_idx].values\n",
        "        if len(p_train) < 10:\n",
        "            # skip too-small fold\n",
        "            continue\n",
        "        platt_m, platt_fn = fit_platt(p_train, y_train)\n",
        "        iso_m, iso_fn = fit_isotonic(p_train, y_train)\n",
        "        calibrator_meta[\"platt\"].append({\"fold\": i, \"rows\": len(val_idx)})\n",
        "        calibrator_meta[\"isotonic\"].append({\"fold\": i, \"rows\": len(val_idx)})\n",
        "        # apply to all indices (we'll average later)\n",
        "        p_platt_all += platt_fn(oof_long.values)\n",
        "        p_iso_all += iso_fn(oof_long.values)\n",
        "        counts += 1\n",
        "    # average ensembles\n",
        "    counts = np.where(counts==0, 1, counts)  # guard\n",
        "    p_platt = p_platt_all / counts\n",
        "    p_iso = p_iso_all / counts\n",
        "    # save meta\n",
        "    pickle.dump({\"meta\":calibrator_meta, \"type\":\"platt_cv_on_oof\", \"folds\":len(splits)}, open(os.path.join(OUT_DIR, f\"calibrator_platt_cv_{SAVE_TAG}.pkl\"), \"wb\"))\n",
        "    pickle.dump({\"meta\":calibrator_meta, \"type\":\"iso_cv_on_oof\", \"folds\":len(splits)}, open(os.path.join(OUT_DIR, f\"calibrator_iso_cv_{SAVE_TAG}.pkl\"), \"wb\"))\n",
        "\n",
        "else:\n",
        "    raise RuntimeError(\"CAL_MODE must be 'holdout' or 'cv_on_oof'\")\n",
        "\n",
        "# ---------- diagnostics: global tables ----------\n",
        "cal_tab_raw = calibration_table(y_long.values, oof_long.values, n_bins=N_BINS)\n",
        "cal_tab_platt = calibration_table(y_long.values, p_platt, n_bins=N_BINS)\n",
        "cal_tab_iso = calibration_table(y_long.values, p_iso, n_bins=N_BINS)\n",
        "\n",
        "brier_raw = brier_score_loss(y_long.values, oof_long.values)\n",
        "brier_platt = brier_score_loss(y_long.values, p_platt)\n",
        "brier_iso = brier_score_loss(y_long.values, p_iso)\n",
        "\n",
        "ece_raw = expected_calibration_error(cal_tab_raw)\n",
        "ece_platt = expected_calibration_error(cal_tab_platt)\n",
        "ece_iso = expected_calibration_error(cal_tab_iso)\n",
        "\n",
        "meta = {\n",
        "    \"brier_raw\": brier_raw, \"brier_platt\": brier_platt, \"brier_iso\": brier_iso,\n",
        "    \"ece_raw\": ece_raw, \"ece_platt\": ece_platt, \"ece_iso\": ece_iso,\n",
        "    \"n_rows\": int(len(oof_long)), \"n_bins\": N_BINS, \"cal_mode\": CAL_MODE\n",
        "}\n",
        "json.dump(meta, open(os.path.join(OUT_DIR, f\"calib_summary_{SAVE_TAG}.json\"), \"w\"), indent=2)\n",
        "\n",
        "# save tables\n",
        "cal_tab_raw.to_csv(os.path.join(OUT_DIR, f\"cal_tab_raw_{SAVE_TAG}.csv\"), index=False)\n",
        "cal_tab_platt.to_csv(os.path.join(OUT_DIR, f\"cal_tab_platt_{SAVE_TAG}.csv\"), index=False)\n",
        "cal_tab_iso.to_csv(os.path.join(OUT_DIR, f\"cal_tab_iso_{SAVE_TAG}.csv\"), index=False)\n",
        "\n",
        "print(\"[INFO] Global calib summary saved:\", os.path.join(OUT_DIR, f\"calib_summary_{SAVE_TAG}.json\"))\n",
        "print(\"  Brier raw/platt/iso:\", brier_raw, brier_platt, brier_iso)\n",
        "print(\"  ECE raw/platt/iso  :\", ece_raw, ece_platt, ece_iso)\n",
        "\n",
        "# ---------- reliability diagrams ----------\n",
        "def save_reliability(y_true, p, fname, bins=N_BINS, title=\"Reliability\"):\n",
        "    prob_true, prob_pred = calibration_curve(y_true, p, n_bins=bins, strategy='uniform')\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(prob_pred, prob_true, 's-', label='reliability')\n",
        "    plt.plot([0,1],[0,1], 'k--', label='perfect')\n",
        "    plt.xlabel(\"Mean predicted probability\")\n",
        "    plt.ylabel(\"Fraction of positives\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(fname, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "save_reliability(y_long.values, oof_long.values, os.path.join(OUT_DIR, f\"rel_raw_{SAVE_TAG}.png\"), title=\"Reliability (raw OOF)\")\n",
        "save_reliability(y_long.values, p_platt, os.path.join(OUT_DIR, f\"rel_platt_{SAVE_TAG}.png\"), title=\"Reliability (Platt)\")\n",
        "save_reliability(y_long.values, p_iso, os.path.join(OUT_DIR, f\"rel_iso_{SAVE_TAG}.png\"), title=\"Reliability (Isotonic)\")\n",
        "\n",
        "# ---------- B: per-month rolling ECE / drift ----------\n",
        "df_diag = pd.DataFrame(index=df_meta.index)\n",
        "df_diag[\"p_raw\"] = oof_long.values\n",
        "df_diag[\"p_platt\"] = p_platt\n",
        "df_diag[\"p_iso\"] = p_iso\n",
        "df_diag[\"y\"] = y_long.values\n",
        "df_diag[\"month\"] = df_diag.index.to_period(\"M\").to_timestamp()\n",
        "\n",
        "# per-month ECE function\n",
        "def month_ece(group):\n",
        "    if len(group) < MIN_SAMPLES_PER_MONTH:\n",
        "        return {\"count\": len(group), \"ece_raw\": np.nan, \"ece_platt\": np.nan, \"ece_iso\": np.nan}\n",
        "    t = calibration_table(group[\"y\"].values, group[\"p_raw\"].values, n_bins=N_BINS)\n",
        "    t2 = calibration_table(group[\"y\"].values, group[\"p_platt\"].values, n_bins=N_BINS)\n",
        "    t3 = calibration_table(group[\"y\"].values, group[\"p_iso\"].values, n_bins=N_BINS)\n",
        "    return {\"count\": len(group), \"ece_raw\": expected_calibration_error(t), \"ece_platt\": expected_calibration_error(t2), \"ece_iso\": expected_calibration_error(t3)}\n",
        "\n",
        "month_stats = df_diag.groupby(\"month\").apply(lambda g: pd.Series(month_ece(g))).reset_index()\n",
        "month_stats.to_csv(os.path.join(OUT_DIR, f\"calib_monthly_{SAVE_TAG}.csv\"), index=False)\n",
        "\n",
        "# rolling months ECE (window of ROLL_MONTHS months)\n",
        "month_stats_sorted = month_stats.sort_values(\"month\")\n",
        "month_stats_sorted[\"ece_raw_roll\"] = month_stats_sorted[\"ece_raw\"].rolling(ROLL_MONTHS, min_periods=1).mean()\n",
        "month_stats_sorted[\"ece_platt_roll\"] = month_stats_sorted[\"ece_platt\"].rolling(ROLL_MONTHS, min_periods=1).mean()\n",
        "month_stats_sorted[\"ece_iso_roll\"] = month_stats_sorted[\"ece_iso\"].rolling(ROLL_MONTHS, min_periods=1).mean()\n",
        "month_stats_sorted.to_csv(os.path.join(OUT_DIR, f\"calib_monthly_rolling_{SAVE_TAG}.csv\"), index=False)\n",
        "\n",
        "# plot rolling ECE\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(month_stats_sorted[\"month\"], month_stats_sorted[\"ece_raw_roll\"], label=\"raw\")\n",
        "plt.plot(month_stats_sorted[\"month\"], month_stats_sorted[\"ece_platt_roll\"], label=\"platt\")\n",
        "plt.plot(month_stats_sorted[\"month\"], month_stats_sorted[\"ece_iso_roll\"], label=\"isotonic\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"month\")\n",
        "plt.ylabel(\"rolling ECE\")\n",
        "plt.title(f\"Rolling ECE (window={ROLL_MONTHS} months)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, f\"calib_ece_rolling_{SAVE_TAG}.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"[DONE] Monthly & rolling ECE saved. files ->\", OUT_DIR)\n",
        "\n",
        "# ---------- final notes ----------\n",
        "print(\"\\nNOTES:\")\n",
        "print(\" - Calibrators trained only on OOF-derived data (holdout or cv_on_oof). Do NOT train on full in-sample fitted probs.\")\n",
        "print(\" - If you want to *persist a single calibrator* to apply in forward production, prefer 'holdout' trained calibrator (train on last holdout window).\")\n",
        "print(\" - For small holdout sizes prefer 'cv_on_oof' ensemble approach; but ensure folds are time-blocked.\")\n",
        "print(\" - Use saved calibrator pickles for production transform (apply to new model-produced probs).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKSwTiBURDg2"
      },
      "source": [
        "## uubu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZF0shUneHbqA",
        "outputId": "bced37b9-017b-4dd8-f733-93c6b415ca32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DONE] A+ created (robust loader).\n",
            " - out: /content/drive/MyDrive/quant_pipeline/mtb_out/Aplus_signals_aligned_1764901821.csv\n",
            " - meta: /content/drive/MyDrive/quant_pipeline/mtb_out/Aplus_meta_aligned.json\n",
            " - best_threshold: 0.10000000000000017\n",
            " - bin_table rows: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ONE-CELL A+ (robust OOF loader + alignment fix + bin-table + threshold + regime scaling)\n",
        "import os, time, json, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# helper: robust OOF loader\n",
        "# ---------------------------\n",
        "def load_oof_csv(path, expected_col_names=None):\n",
        "    # read, accept single-column or multi-column CSVs\n",
        "    df = pd.read_csv(path, index_col=0)\n",
        "    # if single column DataFrame -> series\n",
        "    if isinstance(df, pd.DataFrame) and df.shape[1] == 1:\n",
        "        s = df.iloc[:,0].copy()\n",
        "    else:\n",
        "        # try to detect common names\n",
        "        col = None\n",
        "        if expected_col_names:\n",
        "            for c in expected_col_names:\n",
        "                if c in df.columns:\n",
        "                    col = c; break\n",
        "        if col is None:\n",
        "            # fallback to first numeric column\n",
        "            nums = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]\n",
        "            col = nums[0] if nums else df.columns[0]\n",
        "        s = df[col].copy()\n",
        "    # try parse index as datetime\n",
        "    try:\n",
        "        idx = pd.to_datetime(s.index, utc=True)\n",
        "        s.index = idx\n",
        "    except Exception:\n",
        "        # if index already parsed but tz-naive, try coercing\n",
        "        try:\n",
        "            s.index = pd.to_datetime(s.index)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # ensure float\n",
        "    s = s.astype(float)\n",
        "    s.name = s.name or Path(path).stem\n",
        "    return s\n",
        "\n",
        "# ---------------------------\n",
        "# load df_meta_shortlist\n",
        "# ---------------------------\n",
        "pkl_candidates = sorted(Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\"), key=os.path.getmtime)\n",
        "if not pkl_candidates:\n",
        "    raise FileNotFoundError(\"df_meta_shortlist not found in OUT_DIR.\")\n",
        "df = pd.read_pickle(str(pkl_candidates[-1]))\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "label = df[\"tb_label_h8\"]\n",
        "\n",
        "# ---------------------------\n",
        "# load OOF files robustly\n",
        "# ---------------------------\n",
        "oof_long_path = os.path.join(OUT_DIR, \"rebuild_dual_oof_long.csv\")\n",
        "oof_short_path = os.path.join(OUT_DIR, \"rebuild_dual_oof_short.csv\")\n",
        "if not os.path.exists(oof_long_path) or not os.path.exists(oof_short_path):\n",
        "    raise FileNotFoundError(\"Expected OOF files not found: rebuild_dual_oof_long/short.csv\")\n",
        "\n",
        "s_long = load_oof_csv(oof_long_path, expected_col_names=[\"oof_long_prob\",\"oof_prob\",\"oof\"])\n",
        "s_short = load_oof_csv(oof_short_path, expected_col_names=[\"oof_short_prob\",\"oof_short_prob\",\"oof\"])\n",
        "\n",
        "# ---------------------------\n",
        "# align by reindex (safe) with diagnostics\n",
        "# ---------------------------\n",
        "# coerce both series to UTC datetimes (if not)\n",
        "s_long.index = pd.to_datetime(s_long.index, utc=True)\n",
        "s_short.index = pd.to_datetime(s_short.index, utc=True)\n",
        "\n",
        "# if frequency info differs, just reindex to df.index (order/timezone safe)\n",
        "p_long = s_long.reindex(df.index)\n",
        "p_short = s_short.reindex(df.index)\n",
        "\n",
        "n_miss_long = p_long.isna().sum()\n",
        "n_miss_short = p_short.isna().sum()\n",
        "\n",
        "if n_miss_long or n_miss_short:\n",
        "    print(f\"[WARN] OOF alignment: missing long={n_miss_long}, short={n_miss_short} rows (will fill 0.5).\")\n",
        "p_long = p_long.fillna(0.5)\n",
        "p_short = p_short.fillna(0.5)\n",
        "\n",
        "# ---------------------------\n",
        "# now proceed with A+ pipeline (calib, bin-table, threshold, regime adj)\n",
        "# ---------------------------\n",
        "spread = p_long - p_short\n",
        "N = len(df)\n",
        "h_frac = 0.20\n",
        "cut = int(N * (1 - h_frac))\n",
        "idx_cal = df.index[cut:]\n",
        "idx_fit = df.index[:cut]\n",
        "y_cal = (label.loc[idx_cal] == 1).astype(int)\n",
        "\n",
        "# Platt (Logistic) & Isotonic on holdout only\n",
        "platt = LogisticRegression(max_iter=1000)\n",
        "platt.fit(spread.loc[idx_cal].values.reshape(-1,1), y_cal)\n",
        "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "iso.fit(spread.loc[idx_cal].values, y_cal)\n",
        "\n",
        "p_platt = platt.predict_proba(spread.values.reshape(-1,1))[:,1]\n",
        "p_iso = iso.predict(spread.values)\n",
        "p_cal = 0.5 * p_platt + 0.5 * p_iso\n",
        "\n",
        "# bin-table\n",
        "bins = np.linspace(0,1,11)\n",
        "bin_ids = np.digitize(p_cal, bins) - 1\n",
        "bt = []\n",
        "for b in range(10):\n",
        "    msk = bin_ids == b\n",
        "    if msk.sum() < 20:\n",
        "        continue\n",
        "    bt.append({\n",
        "        \"bin\": int(b),\n",
        "        \"p_min\": float(bins[b]),\n",
        "        \"p_max\": float(bins[b+1]),\n",
        "        \"p_mean\": float(p_cal[msk].mean()),\n",
        "        \"freq_pos\": float((label==1).loc[msk].mean()),\n",
        "        \"count\": int(msk.sum())\n",
        "    })\n",
        "bin_table = pd.DataFrame(bt)\n",
        "\n",
        "# threshold optimizer (OOF-only) - maximize simple match-rate (can be replaced by PnL metric later)\n",
        "ths = np.arange(-0.10, 0.105, 0.005)\n",
        "best_t, best_score = 0.0, -1.0\n",
        "for t in ths:\n",
        "    pred = (spread > t).astype(int)\n",
        "    score = (pred == (label==1)).mean()\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_t = float(t)\n",
        "\n",
        "# regime adjustments (if your df has vol/trend columns, else fallback simple scaling)\n",
        "vol_col = None\n",
        "trend_col = None\n",
        "for c in [\"volatility_50_z\",\"vol_50_z\",\"vol_z\",\"return_50_z\",\"ret_50_z\"]:\n",
        "    if c in df.columns:\n",
        "        if \"vol\" in c: vol_col = c\n",
        "        else: trend_col = c\n",
        "\n",
        "if vol_col is None:\n",
        "    # create fallback vol proxy (rolling std of ret_1 if available)\n",
        "    if \"ret_1\" in df.columns:\n",
        "        vol_series = df[\"ret_1\"].rolling(50, min_periods=1).std().fillna(method=\"bfill\")\n",
        "        vol_series = (vol_series - vol_series.mean()) / (vol_series.std() + 1e-9)\n",
        "    else:\n",
        "        vol_series = pd.Series(0.0, index=df.index)\n",
        "else:\n",
        "    vol_series = df[vol_col].fillna(0.0)\n",
        "\n",
        "if trend_col is None:\n",
        "    if \"ret_50\" in df.columns or \"ret_24\" in df.columns:\n",
        "        tcol = \"ret_50\" if \"ret_50\" in df.columns else \"ret_24\"\n",
        "        trend_series = (df[tcol].rolling(5, min_periods=1).mean()).fillna(0.0)\n",
        "        trend_series = (trend_series - trend_series.mean()) / (trend_series.std() + 1e-9)\n",
        "    else:\n",
        "        trend_series = pd.Series(0.0, index=df.index)\n",
        "else:\n",
        "    trend_series = df[trend_col].fillna(0.0)\n",
        "\n",
        "vol_scale = 1/(1+np.exp(-0.5*vol_series.clip(-4,4)))\n",
        "trend_scale = 1/(1+np.exp(-0.5*trend_series.clip(-4,4)))\n",
        "p_adj = np.clip(0.6*p_cal + 0.2*vol_scale + 0.2*trend_scale, 0, 1)\n",
        "\n",
        "# final signals\n",
        "signal = np.where(spread > best_t, 1, np.where(spread < -best_t, -1, 0))\n",
        "signal_strength = (p_adj - 0.5) * 2\n",
        "\n",
        "out = df.copy()\n",
        "out[\"p_long\"] = p_long\n",
        "out[\"p_short\"] = p_short\n",
        "out[\"spread\"] = spread\n",
        "out[\"p_cal\"] = p_cal\n",
        "out[\"p_adj\"] = p_adj\n",
        "out[\"signal\"] = signal\n",
        "out[\"signal_strength\"] = signal_strength\n",
        "\n",
        "out_path = os.path.join(OUT_DIR, f\"Aplus_signals_aligned_{int(time.time())}.csv\")\n",
        "out.to_csv(out_path)\n",
        "\n",
        "meta = {\n",
        "    \"generated_at\": int(time.time()),\n",
        "    \"best_threshold\": best_t,\n",
        "    \"n_rows\": len(out),\n",
        "    \"missing_long_replaced\": int(n_miss_long),\n",
        "    \"missing_short_replaced\": int(n_miss_short),\n",
        "    \"brier_raw\": float(brier_score_loss((label==1).astype(int), np.clip(spread,0,1))),\n",
        "    \"brier_cal\": float(brier_score_loss((label==1).astype(int), p_cal))\n",
        "}\n",
        "json.dump(meta, open(os.path.join(OUT_DIR, \"Aplus_meta_aligned.json\"), \"w\"), indent=2)\n",
        "\n",
        "print(\"[DONE] A+ created (robust loader).\")\n",
        "print(\" - out:\", out_path)\n",
        "print(\" - meta:\", os.path.join(OUT_DIR, \"Aplus_meta_aligned.json\"))\n",
        "print(\" - best_threshold:\", best_t)\n",
        "print(\" - bin_table rows:\", len(bin_table))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aMk32_RYJZgf",
        "outputId": "d659c8f2-9269-4046-bdee-6c61c85a0e53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded df_meta_shortlist: df_meta_shortlist.v1764900731.pkl shape=(17521, 41)\n",
            "[INFO] loaded OOF long: rebuild_dual_oof_long.csv short: rebuild_dual_oof_short.csv\n",
            "[INFO] outer_splits: 5\n",
            "[INFO] meta features used: ['p_long', 'p_short', 'spread', 'abs_spread', 'spread_ma_24', 'spread_std_24', 'p_long_ma_24', 'p_short_ma_24', 'vol_high', 'hour', 'weekday']\n",
            "\n",
            "[INFO] META outer fold 1/5\n",
            " best_cfg_long: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5157\n",
            " best_cfg_short: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5163\n",
            "[INFO] saved meta fold 1 -> /content/drive/MyDrive/quant_pipeline/mtb_out/meta_stack_v2_1764901823_fold1.pkl\n",
            "\n",
            "[INFO] META outer fold 2/5\n",
            " best_cfg_long: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5157\n",
            " best_cfg_short: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5163\n",
            "[INFO] saved meta fold 2 -> /content/drive/MyDrive/quant_pipeline/mtb_out/meta_stack_v2_1764901823_fold2.pkl\n",
            "\n",
            "[INFO] META outer fold 3/5\n",
            " best_cfg_long: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5157\n",
            " best_cfg_short: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5163\n",
            "[INFO] saved meta fold 3 -> /content/drive/MyDrive/quant_pipeline/mtb_out/meta_stack_v2_1764901823_fold3.pkl\n",
            "\n",
            "[INFO] META outer fold 4/5\n",
            " best_cfg_long: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5157\n",
            " best_cfg_short: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5163\n",
            "[INFO] saved meta fold 4 -> /content/drive/MyDrive/quant_pipeline/mtb_out/meta_stack_v2_1764901823_fold4.pkl\n",
            "\n",
            "[INFO] META outer fold 5/5\n",
            " best_cfg_long: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5157\n",
            " best_cfg_short: {'max_depth': 3, 'learning_rate': 0.02, 'n_estimators': 150} inner_score=0.5163\n",
            "[INFO] saved meta fold 5 -> /content/drive/MyDrive/quant_pipeline/mtb_out/meta_stack_v2_1764901823_fold5.pkl\n",
            "\n",
            "[INFO] saved meta OOF long:meta_stack_v2_1764901823_oof_meta_long.csv short:meta_stack_v2_1764901823_oof_meta_short.csv\n",
            "\n",
            "[RESULT] Meta-stacker finished.\n",
            " OOF meta long AUC: 0.5202  short AUC: 0.5197\n",
            " Brier raw long: 0.2493  calibrated long: 0.2395\n",
            " Saved artifacts -> prefix: meta_stack_v2_1764901823 in /content/drive/MyDrive/quant_pipeline/mtb_out\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ONE-CELL: B2 Meta-Stacker — CPCV-safe, dual meta (long & short), variance-reduced tuning + calibrate\n",
        "# Paste & run in Colab. Expects df_meta_shortlist + rebuild_dual_oof_long/short + patched purge utils present.\n",
        "import os, glob, json, time, pickle, warnings\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LABEL_H = \"h8\"                 # horizon label suffix used in tb columns\n",
        "N_OUTER = 5\n",
        "N_INNER = 3\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "RSTATE = 42\n",
        "VERBOSE = True\n",
        "\n",
        "META_MODEL = \"xgb\"  # \"xgb\" or \"logistic\"\n",
        "XGB_DEFAULT = {\"max_depth\":4, \"learning_rate\":0.02, \"subsample\":0.8, \"colsample_bytree\":0.8, \"reg_alpha\":1e-3, \"reg_lambda\":1.0, \"n_estimators\":200, \"verbosity\":0}\n",
        "\n",
        "# small tuning grid (fast) — you can expand\n",
        "XGB_GRID = [\n",
        "    {\"max_depth\":3, \"learning_rate\":0.02, \"n_estimators\":150},\n",
        "    {\"max_depth\":4, \"learning_rate\":0.02, \"n_estimators\":200},\n",
        "    {\"max_depth\":5, \"learning_rate\":0.01, \"n_estimators\":250},\n",
        "]\n",
        "\n",
        "LOG_REG_GRID = [\n",
        "    {\"C\":0.1, \"penalty\":\"l2\"},\n",
        "    {\"C\":1.0, \"penalty\":\"l2\"}\n",
        "]\n",
        "\n",
        "# output file stems\n",
        "TS = int(time.time())\n",
        "OUT_PREFIX = f\"meta_stack_v2_{TS}\"\n",
        "\n",
        "# ---------- load inputs ----------\n",
        "# df_meta_shortlist\n",
        "pkl_candidates = sorted([p for p in Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")], key=os.path.getmtime)\n",
        "if not pkl_candidates:\n",
        "    raise FileNotFoundError(\"No df_meta_shortlist.v*.pkl found in OUT_DIR.\")\n",
        "meta_path = str(pkl_candidates[-1])\n",
        "df = pd.read_pickle(meta_path)\n",
        "df.index = pd.to_datetime(df.index, utc=True)\n",
        "print(f\"[INFO] loaded df_meta_shortlist: {os.path.basename(meta_path)} shape={df.shape}\")\n",
        "\n",
        "# OOF probs (dual) — robust reading & alignment\n",
        "oof_long_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"*oof_long*.csv\")))\n",
        "oof_short_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"*oof_short*.csv\")))\n",
        "if not oof_long_candidates or not oof_short_candidates:\n",
        "    raise FileNotFoundError(\"OOF long/short csv not found. Run rebuild_dual first.\")\n",
        "\n",
        "def read_oof(path):\n",
        "    df_o = pd.read_csv(path, index_col=0)\n",
        "    # pick first numeric column if multiple, else fail\n",
        "    if df_o.shape[1] > 1:\n",
        "        col = df_o.select_dtypes(include=[float,int]).columns[0]\n",
        "    else:\n",
        "        col = df_o.columns[0]\n",
        "    s = df_o[col].astype(float)\n",
        "    # ensure timezone-aware timestamps when possible\n",
        "    try:\n",
        "        s.index = pd.to_datetime(s.index, utc=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return s\n",
        "\n",
        "oof_long = read_oof(oof_long_candidates[-1])\n",
        "oof_short = read_oof(oof_short_candidates[-1])\n",
        "print(f\"[INFO] loaded OOF long: {os.path.basename(oof_long_candidates[-1])} short: {os.path.basename(oof_short_candidates[-1])}\")\n",
        "\n",
        "# align: take intersection and warn if mismatch; fill missing with 0.5 (neutral)\n",
        "common_idx = df.index.intersection(oof_long.index).intersection(oof_short.index)\n",
        "if len(common_idx) < len(df):\n",
        "    print(f\"[WARN] OOF coverage {len(common_idx)}/{len(df)} rows match df index. Will reindex and fill missing with 0.5.\")\n",
        "oof_long = oof_long.reindex(df.index).fillna(0.5)\n",
        "oof_short = oof_short.reindex(df.index).fillna(0.5)\n",
        "\n",
        "# ---------- build meta features (OOF-only) ----------\n",
        "meta = pd.DataFrame(index=df.index)\n",
        "meta[\"p_long\"] = oof_long\n",
        "meta[\"p_short\"] = oof_short\n",
        "meta[\"spread\"] = meta[\"p_long\"] - meta[\"p_short\"]\n",
        "meta[\"abs_spread\"] = meta[\"spread\"].abs()\n",
        "# rolling features (past-only windows on OOF series)\n",
        "meta[\"spread_ma_24\"] = meta[\"spread\"].rolling(24, min_periods=1).mean()\n",
        "meta[\"spread_std_24\"] = meta[\"spread\"].rolling(24, min_periods=1).std().fillna(0)\n",
        "meta[\"p_long_ma_24\"] = meta[\"p_long\"].rolling(24, min_periods=1).mean()\n",
        "meta[\"p_short_ma_24\"] = meta[\"p_short\"].rolling(24, min_periods=1).mean()\n",
        "\n",
        "# regimes & time\n",
        "if \"regime_flag\" in df.columns:\n",
        "    meta[\"regime_flag\"] = df[\"regime_flag\"].astype(int)\n",
        "else:\n",
        "    if \"ret_std_24\" in df.columns:\n",
        "        v = df[\"ret_std_24\"].fillna(df[\"ret_std_24\"].median())\n",
        "        meta[\"vol_high\"] = (v > v.quantile(0.75)).astype(int)\n",
        "    else:\n",
        "        meta[\"vol_high\"] = 0\n",
        "meta[\"hour\"] = df.index.hour\n",
        "meta[\"weekday\"] = df.index.weekday\n",
        "\n",
        "meta = meta.replace([np.inf, -np.inf], np.nan).fillna(method=\"ffill\").fillna(0)\n",
        "\n",
        "# ---------- target columns ----------\n",
        "label_col = f\"tb_label_{LABEL_H}\"\n",
        "tbreak_col = f\"tb_t_break_{LABEL_H}\"\n",
        "if label_col not in df.columns or tbreak_col not in df.columns:\n",
        "    raise RuntimeError(f\"{label_col} or {tbreak_col} missing in df_meta_shortlist.\")\n",
        "\n",
        "y_long = (df[label_col] == 1).astype(int)\n",
        "y_short = (df[label_col] == -1).astype(int)\n",
        "\n",
        "# ---------- purge utils + CPCV splits ----------\n",
        "try:\n",
        "    from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits, sanity_check_purged_split\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Paste patched b0_07_purge_utils into notebook (compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits).\") from e\n",
        "\n",
        "idx = df.index\n",
        "exp_all = compute_exposure_intervals(idx, df[tbreak_col], horizon_fallback=None, last_index=idx[-1])\n",
        "pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "outer_splits = list(purged_cv_splits(pos_all, n_splits=N_OUTER, embargo=EMBARGO, index=idx, drop_unmapped=True, random_state=RSTATE))\n",
        "if len(outer_splits) < N_OUTER:\n",
        "    print(f\"[WARN] produced fewer outer splits ({len(outer_splits)}) than requested N_OUTER={N_OUTER}\")\n",
        "print(f\"[INFO] outer_splits: {len(outer_splits)}\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def create_model_by_cfg(cfg):\n",
        "    if META_MODEL == \"xgb\":\n",
        "        params = deepcopy(cfg)\n",
        "        params.update({\"use_label_encoder\": False, \"eval_metric\": \"logloss\", \"verbosity\":0})\n",
        "        return XGBClassifier(random_state=RSTATE, **params)\n",
        "    else:\n",
        "        return LogisticRegression(C=cfg[\"C\"], penalty=cfg.get(\"penalty\",\"l2\"), solver=\"lbfgs\", max_iter=200, random_state=RSTATE)\n",
        "\n",
        "def evaluate_cfg_on_splits(feature_cols, cfg, y_series):\n",
        "    \"\"\"Average inner AUC across multiple outer-train subsets to reduce variance.\"\"\"\n",
        "    aucs = []\n",
        "    # loop a few outer folds (or all) and average inner results\n",
        "    outer_to_use = outer_splits if len(outer_splits) <= 4 else outer_splits[:4]\n",
        "    for (train_pos_outer, _) in outer_to_use:\n",
        "        train_ts_outer = pos_all.index[train_pos_outer]\n",
        "        exp_train = compute_exposure_intervals(train_ts_outer, df[tbreak_col].reindex(train_ts_outer), horizon_fallback=None, last_index=idx[-1])\n",
        "        pos_train = exposure_to_pos_intervals(exp_train, idx)\n",
        "        inner_splits = list(purged_cv_splits(pos_train, n_splits=N_INNER, embargo=EMBARGO, index=idx, drop_unmapped=True, random_state=RSTATE))\n",
        "        if not inner_splits:\n",
        "            continue\n",
        "        for tr_pos_inner, val_pos_inner in inner_splits:\n",
        "            tr_ts = pos_train.index[tr_pos_inner]\n",
        "            val_ts = pos_train.index[val_pos_inner]\n",
        "            if len(val_ts)==0:\n",
        "                continue\n",
        "            X_tr = meta.loc[tr_ts, feature_cols].values\n",
        "            X_val = meta.loc[val_ts, feature_cols].values\n",
        "            y_tr = y_series.loc[tr_ts].values\n",
        "            y_val = y_series.loc[val_ts].values\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_s = scaler.fit_transform(X_tr)\n",
        "            X_val_s = scaler.transform(X_val)\n",
        "            sw = compute_sample_weight(class_weight=\"balanced\", y=y_tr)\n",
        "            model = create_model_by_cfg(cfg)\n",
        "            try:\n",
        "                model.fit(X_tr_s, y_tr, sample_weight=sw, verbose=False)\n",
        "            except TypeError:\n",
        "                model.fit(X_tr_s, y_tr, verbose=False)\n",
        "            probs = model.predict_proba(X_val_s)[:,1]\n",
        "            auc = roc_auc_score(y_val, probs) if len(np.unique(y_val))>1 else 0.5\n",
        "            aucs.append(auc)\n",
        "    return float(np.mean(aucs)) if aucs else 0.5\n",
        "\n",
        "# ---------- feature set ----------\n",
        "FEAT_COLS = [\"p_long\",\"p_short\",\"spread\",\"abs_spread\",\"spread_ma_24\",\"spread_std_24\",\"p_long_ma_24\",\"p_short_ma_24\",\"vol_high\",\"hour\",\"weekday\"]\n",
        "FEAT_COLS = [c for c in FEAT_COLS if c in meta.columns]\n",
        "print(f\"[INFO] meta features used: {FEAT_COLS}\")\n",
        "\n",
        "# ---------- CPCV outer loop ----------\n",
        "oof_meta_long = pd.Series(index=meta.index, dtype=float)\n",
        "oof_meta_short = pd.Series(index=meta.index, dtype=float)\n",
        "models_meta = []\n",
        "\n",
        "for i, (train_pos_outer, test_pos_outer) in enumerate(outer_splits, start=1):\n",
        "    print(f\"\\n[INFO] META outer fold {i}/{len(outer_splits)}\")\n",
        "    train_ts = pos_all.index[train_pos_outer]\n",
        "    test_ts = pos_all.index[test_pos_outer]\n",
        "    X_tr = meta.loc[train_ts, FEAT_COLS]\n",
        "    X_test = meta.loc[test_ts, FEAT_COLS]\n",
        "\n",
        "    # --------- tune for long ----------\n",
        "    grid_long = XGB_GRID if META_MODEL==\"xgb\" else LOG_REG_GRID\n",
        "    best_cfg_long, best_score_long = None, -np.inf\n",
        "    for cfg in grid_long:\n",
        "        score = evaluate_cfg_on_splits(FEAT_COLS, cfg, y_long)\n",
        "        if score > best_score_long:\n",
        "            best_score_long, best_cfg_long = score, deepcopy(cfg)\n",
        "    if VERBOSE:\n",
        "        print(f\" best_cfg_long: {best_cfg_long} inner_score={best_score_long:.4f}\")\n",
        "\n",
        "    scaler_long = StandardScaler()\n",
        "    X_tr_s = scaler_long.fit_transform(X_tr.values)\n",
        "    X_test_s = scaler_long.transform(X_test.values)\n",
        "    sw_long = compute_sample_weight(class_weight=\"balanced\", y=y_long.loc[train_ts].values)\n",
        "    model_long = create_model_by_cfg(best_cfg_long if best_cfg_long is not None else XGB_DEFAULT)\n",
        "    try:\n",
        "        model_long.fit(X_tr_s, y_long.loc[train_ts].values, sample_weight=sw_long, verbose=False)\n",
        "    except TypeError:\n",
        "        model_long.fit(X_tr_s, y_long.loc[train_ts].values, verbose=False)\n",
        "    probs_test_long = model_long.predict_proba(X_test_s)[:,1]\n",
        "    oof_meta_long.loc[test_ts] = probs_test_long\n",
        "\n",
        "    # --------- tune for short ----------\n",
        "    grid_short = XGB_GRID if META_MODEL==\"xgb\" else LOG_REG_GRID\n",
        "    best_cfg_short, best_score_short = None, -np.inf\n",
        "    for cfg in grid_short:\n",
        "        score = evaluate_cfg_on_splits(FEAT_COLS, cfg, y_short)\n",
        "        if score > best_score_short:\n",
        "            best_score_short, best_cfg_short = score, deepcopy(cfg)\n",
        "    if VERBOSE:\n",
        "        print(f\" best_cfg_short: {best_cfg_short} inner_score={best_score_short:.4f}\")\n",
        "\n",
        "    scaler_short = StandardScaler()\n",
        "    X_tr_s2 = scaler_short.fit_transform(X_tr.values)\n",
        "    X_test_s2 = scaler_short.transform(X_test.values)\n",
        "    sw_short = compute_sample_weight(class_weight=\"balanced\", y=y_short.loc[train_ts].values)\n",
        "    model_short = create_model_by_cfg(best_cfg_short if best_cfg_short is not None else XGB_DEFAULT)\n",
        "    try:\n",
        "        model_short.fit(X_tr_s2, y_short.loc[train_ts].values, sample_weight=sw_short, verbose=False)\n",
        "    except TypeError:\n",
        "        model_short.fit(X_tr_s2, y_short.loc[train_ts].values, verbose=False)\n",
        "    probs_test_short = model_short.predict_proba(X_test_s2)[:,1]\n",
        "    oof_meta_short.loc[test_ts] = probs_test_short\n",
        "\n",
        "    # persist fold meta\n",
        "    fold_meta = {\n",
        "        \"fold\": i,\n",
        "        \"train_index\": train_ts,\n",
        "        \"test_index\": test_ts,\n",
        "        \"scaler_long\": scaler_long,\n",
        "        \"scaler_short\": scaler_short,\n",
        "        \"model_long\": model_long,\n",
        "        \"model_short\": model_short,\n",
        "        \"cfg_long\": best_cfg_long,\n",
        "        \"cfg_short\": best_cfg_short,\n",
        "        \"inner_score_long\": best_score_long,\n",
        "        \"inner_score_short\": best_score_short\n",
        "    }\n",
        "    fold_path = os.path.join(OUT_DIR, f\"{OUT_PREFIX}_fold{i}.pkl\")\n",
        "    pd.to_pickle(fold_meta, fold_path)\n",
        "    models_meta.append({\"fold\": i, \"path\": os.path.basename(fold_path), \"train_len\": len(train_ts), \"test_len\": len(test_ts)})\n",
        "    print(f\"[INFO] saved meta fold {i} -> {fold_path}\")\n",
        "\n",
        "# ---------- finalize OOFs ----------\n",
        "oof_meta_long = oof_meta_long.fillna(0.5)\n",
        "oof_meta_short = oof_meta_short.fillna(0.5)\n",
        "\n",
        "oof_long_path = os.path.join(OUT_DIR, f\"{OUT_PREFIX}_oof_meta_long.csv\")\n",
        "oof_short_path = os.path.join(OUT_DIR, f\"{OUT_PREFIX}_oof_meta_short.csv\")\n",
        "oof_meta_long.to_frame(\"oof_meta_long_prob\").to_csv(oof_long_path)\n",
        "oof_meta_short.to_frame(\"oof_meta_short_prob\").to_csv(oof_short_path)\n",
        "print(f\"\\n[INFO] saved meta OOF long:{os.path.basename(oof_long_path)} short:{os.path.basename(oof_short_path)}\")\n",
        "\n",
        "# ---------- calibration (holdout on last outer fold) ----------\n",
        "last_fold = models_meta[-1]\n",
        "fold_obj = pd.read_pickle(os.path.join(OUT_DIR, last_fold[\"path\"]))\n",
        "train_idx = fold_obj[\"train_index\"]\n",
        "\n",
        "scaler_l = fold_obj[\"scaler_long\"]\n",
        "scaler_s = fold_obj[\"scaler_short\"]\n",
        "m_l = fold_obj[\"model_long\"]\n",
        "m_s = fold_obj[\"model_short\"]\n",
        "\n",
        "X_train_meta = meta.loc[train_idx, FEAT_COLS].values\n",
        "X_train_l_s = scaler_l.transform(X_train_meta)\n",
        "X_train_s_s = scaler_s.transform(X_train_meta)\n",
        "p_train_l = m_l.predict_proba(X_train_l_s)[:,1]\n",
        "p_train_s = m_s.predict_proba(X_train_s_s)[:,1]\n",
        "y_train_l = y_long.loc[train_idx].values\n",
        "y_train_s = y_short.loc[train_idx].values\n",
        "\n",
        "cal_platt_l = LogisticRegression(C=1.0, solver=\"lbfgs\", max_iter=200).fit(p_train_l.reshape(-1,1), y_train_l)\n",
        "cal_platt_s = LogisticRegression(C=1.0, solver=\"lbfgs\", max_iter=200).fit(p_train_s.reshape(-1,1), y_train_s)\n",
        "\n",
        "p_meta_long_cal = pd.Series(cal_platt_l.predict_proba(oof_meta_long.values.reshape(-1,1))[:,1], index=oof_meta_long.index)\n",
        "p_meta_short_cal = pd.Series(cal_platt_s.predict_proba(oof_meta_short.values.reshape(-1,1))[:,1], index=oof_meta_short.index)\n",
        "\n",
        "pickle.dump(cal_platt_l, open(os.path.join(OUT_DIR, f\"{OUT_PREFIX}_cal_platt_long.pkl\"), \"wb\"))\n",
        "pickle.dump(cal_platt_s, open(os.path.join(OUT_DIR, f\"{OUT_PREFIX}_cal_platt_short.pkl\"), \"wb\"))\n",
        "\n",
        "# ---------- diagnostics ----------\n",
        "diag = {}\n",
        "try:\n",
        "    diag[\"oof_meta_long_auc\"] = float(roc_auc_score(y_long, oof_meta_long.values))\n",
        "    diag[\"oof_meta_short_auc\"] = float(roc_auc_score(y_short, oof_meta_short.values))\n",
        "    diag[\"oof_meta_long_brier\"] = float(brier_score_loss(y_long, oof_meta_long.values))\n",
        "    diag[\"oof_meta_short_brier\"] = float(brier_score_loss(y_short, oof_meta_short.values))\n",
        "    diag[\"oof_meta_long_calib_brier\"] = float(brier_score_loss(y_long, p_meta_long_cal.values))\n",
        "    diag[\"oof_meta_short_calib_brier\"] = float(brier_score_loss(y_short, p_meta_short_cal.values))\n",
        "except Exception as e:\n",
        "    print(\"Diag compute failed:\", e)\n",
        "\n",
        "diag_path = os.path.join(OUT_DIR, f\"{OUT_PREFIX}_diag.json\")\n",
        "with open(diag_path, \"w\") as f:\n",
        "    json.dump({\"meta\":diag, \"models_meta\":models_meta}, f, indent=2)\n",
        "\n",
        "pd.to_pickle(models_meta, os.path.join(OUT_DIR, f\"{OUT_PREFIX}_models_meta.pkl\"))\n",
        "\n",
        "# save enriched signals (aligned)\n",
        "signals = pd.DataFrame(index=meta.index)\n",
        "signals[\"p_long_base\"] = meta[\"p_long\"]\n",
        "signals[\"p_short_base\"] = meta[\"p_short\"]\n",
        "signals[\"p_meta_long\"] = oof_meta_long\n",
        "signals[\"p_meta_short\"] = oof_meta_short\n",
        "signals[\"p_meta_long_cal\"] = p_meta_long_cal\n",
        "signals[\"p_meta_short_cal\"] = p_meta_short_cal\n",
        "signals[\"spread_base\"] = meta[\"spread\"]\n",
        "signals[\"spread_meta\"] = signals[\"p_meta_long\"] - signals[\"p_meta_short\"]\n",
        "signals.to_csv(os.path.join(OUT_DIR, f\"{OUT_PREFIX}_signals_aligned.csv\"))\n",
        "\n",
        "# final prints & asserts\n",
        "print(\"\\n[RESULT] Meta-stacker finished.\")\n",
        "print(f\" OOF meta long AUC: {diag.get('oof_meta_long_auc'):.4f}  short AUC: {diag.get('oof_meta_short_auc'):.4f}\")\n",
        "print(f\" Brier raw long: {diag.get('oof_meta_long_brier'):.4f}  calibrated long: {diag.get('oof_meta_long_calib_brier'):.4f}\")\n",
        "print(f\" Saved artifacts -> prefix: {OUT_PREFIX} in {OUT_DIR}\")\n",
        "assert signals.shape[0] == df.shape[0], \"Signal length mismatch vs df_meta\"\n",
        "assert (signals[[\"p_meta_long\",\"p_meta_short\"]].min().min() >= 0.0) and (signals[[\"p_meta_long\",\"p_meta_short\"]].max().max() <= 1.0), \"Meta OOF outside [0,1]\"\n",
        "\n",
        "json.dump({\"generated_at\":int(time.time()), \"n_rows\":int(signals.shape[0]), \"diag\":diag, \"prefix\":OUT_PREFIX}, open(os.path.join(OUT_DIR, f\"{OUT_PREFIX}_meta.json\"), \"w\"), indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yEizlamQ3Eg",
        "outputId": "5774449b-a591-46f1-b9ce-274c3d552b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded df_meta_shortlist: df_meta_shortlist.v1764900731.pkl rows=17521\n",
            "[INFO] using STEP03 file: df_step03_tb_multi_20251205T015857Z.csv\n",
            "[INFO] STEP03 columns: 18\n",
            "[INFO] STEP03 chosen columns -> tbreak: tb_t_break_h8  ret: tb_ret_at_break_h8\n",
            "[INFO] STEP03 tbreak index not unique (7993 duplicates) -> aggregating by mean.\n",
            "[DONE] merged tb_ret_8 into df_meta -> /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\n",
            "count    17521.000000\n",
            "mean         0.000014\n",
            "std          0.008973\n",
            "min         -0.050126\n",
            "25%         -0.004976\n",
            "50%         -0.003173\n",
            "75%          0.005108\n",
            "max          0.050957\n",
            "Name: tb_ret_8, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# FIXED CELL: robust mapping STEP03 -> df_meta_shortlist (prefer matching H, aggregate duplicates)\n",
        "import os, glob, warnings\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "H = 8\n",
        "meta_pkl = sorted(list(Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\")), key=os.path.getmtime)\n",
        "if not meta_pkl:\n",
        "    raise FileNotFoundError(\"df_meta_shortlist not found.\")\n",
        "meta_path = str(meta_pkl[-1])\n",
        "df_meta = pd.read_pickle(meta_path)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "print(f\"[INFO] loaded df_meta_shortlist: {os.path.basename(meta_path)} rows={len(df_meta)}\")\n",
        "\n",
        "# find STEP03 raw file (prefer *_tb_multi*)\n",
        "step3_candidates = sorted(\n",
        "    list(Path(OUT_DIR).glob(\"df_step03_tb_multi*.pkl\")) +\n",
        "    list(Path(OUT_DIR).glob(\"df_step03_tb_multi*.csv\")) +\n",
        "    list(Path(OUT_DIR).glob(\"*step03*tb_multi*.pkl\")) +\n",
        "    list(Path(OUT_DIR).glob(\"*step03*tb_multi*.csv\")),\n",
        "    key=os.path.getmtime\n",
        ")\n",
        "if not step3_candidates:\n",
        "    raise FileNotFoundError(\"No df_step03_tb_multi* file found in OUT_DIR.\")\n",
        "step3_path = str(step3_candidates[-1])\n",
        "print(f\"[INFO] using STEP03 file: {os.path.basename(step3_path)}\")\n",
        "\n",
        "# load step3\n",
        "if step3_path.lower().endswith(\".pkl\"):\n",
        "    df_step = pd.read_pickle(step3_path)\n",
        "else:\n",
        "    df_step = pd.read_csv(step3_path, low_memory=False)\n",
        "print(f\"[INFO] STEP03 columns: {len(df_step.columns)}\")\n",
        "\n",
        "# prefer exact matching H columns in STEP03\n",
        "tb_meta_col = f\"tb_t_break_h{H}\"\n",
        "ret_meta_col_pref = f\"tb_ret_at_break_h{H}\"\n",
        "ret_meta_col_alt = f\"tb_ret_{H}\"\n",
        "\n",
        "found_tb = tb_meta_col if tb_meta_col in df_step.columns else None\n",
        "found_ret = None\n",
        "if ret_meta_col_pref in df_step.columns:\n",
        "    found_ret = ret_meta_col_pref\n",
        "elif ret_meta_col_alt in df_step.columns:\n",
        "    found_ret = ret_meta_col_alt\n",
        "else:\n",
        "    # fallback: find any tb_ret* and tb_t_break* (but warn)\n",
        "    tb_candidates = [c for c in df_step.columns if \"tb_t_break\" in c.lower() or \"t_break\" in c.lower() or \"tbreak\" in c.lower()]\n",
        "    ret_candidates = [c for c in df_step.columns if c.lower().startswith(\"tb_ret\") or \"ret_at_break\" in c.lower() or \"ret_at\" in c.lower()]\n",
        "    if tb_candidates:\n",
        "        found_tb = tb_candidates[0]\n",
        "    if ret_candidates:\n",
        "        # prefer same-hint if available\n",
        "        found_ret = ret_candidates[0]\n",
        "\n",
        "print(f\"[INFO] STEP03 chosen columns -> tbreak: {found_tb}  ret: {found_ret}\")\n",
        "if not found_ret:\n",
        "    raise RuntimeError(f\"No return column found in STEP03 for H={H}. Searched {ret_meta_col_pref}, {ret_meta_col_alt}, tb_ret*.\")\n",
        "\n",
        "# coerce types\n",
        "if found_tb is not None:\n",
        "    df_step[found_tb] = pd.to_datetime(df_step[found_tb], utc=True, errors=\"coerce\")\n",
        "df_step[found_ret] = pd.to_numeric(df_step[found_ret], errors=\"coerce\")\n",
        "\n",
        "# If chosen tbreak doesn't match H (e.g. we picked tb_t_break_h4 while ret is tb_ret_at_break_h8) warn user\n",
        "if found_tb and (f\"h{H}\" not in found_tb and f\"h{H}\" in found_ret):\n",
        "    print(f\"[WARN] STEP03 tbreak ({found_tb}) does not include h{H} while ret ({found_ret}) is for h{H} — double-check STEP03 file.\")\n",
        "\n",
        "# Build map_ser: aggregate duplicates by mean on the tbreak index\n",
        "if found_tb:\n",
        "    tmp = df_step[[found_tb, found_ret]].dropna(subset=[found_tb])\n",
        "    # set datetime index and groupby index to aggregate duplicates\n",
        "    tmp_idxed = tmp.set_index(found_tb)[found_ret]\n",
        "    # if duplicate index -> aggregate by mean (conservative)\n",
        "    if not tmp_idxed.index.is_unique:\n",
        "        print(f\"[INFO] STEP03 tbreak index not unique ({tmp_idxed.index.duplicated().sum()} duplicates) -> aggregating by mean.\")\n",
        "        map_ser = tmp_idxed.groupby(level=0).mean()\n",
        "    else:\n",
        "        map_ser = tmp_idxed\n",
        "else:\n",
        "    # fallback: try using step3 index\n",
        "    tmp_idxed = df_step[found_ret].copy()\n",
        "    try:\n",
        "        tmp_idxed.index = pd.to_datetime(tmp_idxed.index, utc=True)\n",
        "        if not tmp_idxed.index.is_unique:\n",
        "            map_ser = tmp_idxed.groupby(level=0).mean()\n",
        "        else:\n",
        "            map_ser = tmp_idxed\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"Unable to build mapping series from STEP03 (no tbreak and step3 index not datetime).\")\n",
        "\n",
        "# Map into df_meta using df_meta[tb_meta_col] (preferred) — handle NaT safely\n",
        "if tb_meta_col in df_meta.columns:\n",
        "    tb_idx = pd.to_datetime(df_meta[tb_meta_col], utc=True, errors=\"coerce\")\n",
        "    # map: use pandas.Series.map with map_ser (map_ser has unique DatetimeIndex now)\n",
        "    mapped = tb_idx.map(map_ser)  # NaN where no match\n",
        "    mapped = mapped.fillna(0.0).astype(float)\n",
        "    mapped.index = df_meta.index\n",
        "else:\n",
        "    # fallback: align by intersection of indexes\n",
        "    common = df_meta.index.intersection(map_ser.index)\n",
        "    if len(common) == 0:\n",
        "        raise RuntimeError(f\"{tb_meta_col} missing in df_meta AND no timestamp overlap with STEP03 tbreaks.\")\n",
        "    mapped = pd.Series(index=df_meta.index, dtype=float)\n",
        "    mapped.loc[common] = map_ser.reindex(common).values\n",
        "    mapped = mapped.fillna(0.0)\n",
        "\n",
        "# attach and persist (overwrite tb_ret_{H} intentionally)\n",
        "out_pkl = os.path.join(OUT_DIR, os.path.basename(meta_path).replace(\".pkl\", f\".with_tb_ret_h{H}.pkl\"))\n",
        "df_meta_out = df_meta.copy()\n",
        "df_meta_out[f\"tb_ret_{H}\"] = mapped.values\n",
        "pd.to_pickle(df_meta_out, out_pkl)\n",
        "print(f\"[DONE] merged tb_ret_{H} into df_meta -> {out_pkl}\")\n",
        "print(df_meta_out[f\"tb_ret_{H}\"].describe().round(6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZKo09YuQKKz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "df095eca0d934f4f9bf4391ae83e5925",
            "f099c1ed249a497096143143dbb83e6a",
            "6e3b63b074d14781b80e7d6b9da624c7",
            "f5965d0f88054e3fa47acfb76df6679b",
            "eaecf58e0c2f4b458da0dec793f36b65",
            "006e782c366841b68dbc7f03ec455491",
            "e3943d268ee54f358e3beb10aaa8d052",
            "011f2d1167fa4128bb358bd52c084c86",
            "1e35dff10ca3458794f081e199a92adc",
            "c872171fe6114780b56d1d8ac3c25d7f",
            "f19a8504a19b4c8ba7ed5a76f9fd0a19"
          ]
        },
        "id": "y-A-jI_IR3aD",
        "outputId": "9d968b29-164d-45d5-9f14-9b0458b63793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded df_meta_shortlist: df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl rows=17521\n",
            "[INFO] using signals: meta_stack_v2_1764901823_signals_aligned.csv\n",
            "[INFO] using existing returns column: tb_ret_8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:51:45,691] A new study created in memory with name: no-name-32e31167-40ed-46e1-ba7a-85abb9546734\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] outer_splits: 5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df095eca0d934f4f9bf4391ae83e5925",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:51:45,782] Trial 0 finished with value: -0.3263177342431317 and parameters: {'k': 0.8741722139252526, 'tau': 0.47634286707675977, 'L': 2.3299848545285125, 'WINDOW': 110, 'threshold': 0.07800932022121826, 'tc': 0.0003119890406724053}. Best is trial 0 with value: -0.3263177342431317.\n",
            "[I 2025-12-05 02:51:45,835] Trial 1 finished with value: -1.4140388246052893 and parameters: {'k': 0.30455050190275906, 'tau': 0.4357645499719689, 'L': 2.002787529358022, 'WINDOW': 126, 'threshold': 0.010292247147901223, 'tc': 0.0019398197043239886}. Best is trial 0 with value: -0.3263177342431317.\n",
            "[I 2025-12-05 02:51:45,909] Trial 2 finished with value: -0.6091873802575835 and parameters: {'k': 1.6983967534407591, 'tau': 0.12192277312557255, 'L': 0.9545624180177515, 'WINDOW': 50, 'threshold': 0.15212112147976886, 'tc': 0.0010495128632644758}. Best is trial 0 with value: -0.3263177342431317.\n",
            "[I 2025-12-05 02:51:45,964] Trial 3 finished with value: -0.27607724268480616 and parameters: {'k': 0.9775010335558083, 'tau': 0.1597899872950601, 'L': 2.0296322368059485, 'WINDOW': 44, 'threshold': 0.14607232426760908, 'tc': 0.0007327236865873834}. Best is trial 3 with value: -0.27607724268480616.\n",
            "[I 2025-12-05 02:51:46,024] Trial 4 finished with value: 1.1343873756237393 and parameters: {'k': 1.0209259715906647, 'tau': 0.39688446146864653, 'L': 0.9991844553958993, 'WINDOW': 98, 'threshold': 0.29620728443102123, 'tc': 9.290082543999546e-05}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:51:46,076] Trial 5 finished with value: -2.4454831283873895 and parameters: {'k': 1.293580733422589, 'tau': 0.10185157936989994, 'L': 0.6626289824631988, 'WINDOW': 161, 'threshold': 0.4828160165372797, 'tc': 0.0016167946962329224}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:51:46,135] Trial 6 finished with value: -0.6092080487875695 and parameters: {'k': 0.7483047845120672, 'tau': 0.06688261472306425, 'L': 2.210582566280392, 'WINDOW': 87, 'threshold': 0.06101911742238941, 'tc': 0.0009903538202225403}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:51:46,183] Trial 7 finished with value: -0.40271969428747306 and parameters: {'k': 0.2618993380073931, 'tau': 0.4564737929978154, 'L': 1.1469499540000423, 'WINDOW': 120, 'threshold': 0.15585553804470548, 'tc': 0.0010401360423556217}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:51:46,244] Trial 8 finished with value: -3.0226618828880256 and parameters: {'k': 1.1840785028179035, 'tau': 0.10873013865225298, 'L': 2.9239615694113965, 'WINDOW': 136, 'threshold': 0.46974947078209456, 'tc': 0.0017896547008552977}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:51:46,297] Trial 9 finished with value: -0.45504656217756967 and parameters: {'k': 1.2762199618599532, 'tau': 0.4624996328110961, 'L': 0.7212312551297988, 'WINDOW': 52, 'threshold': 0.022613644455269033, 'tc': 0.0006506606615265287}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:51:46,371] Trial 10 finished with value: 1.1774363150608143 and parameters: {'k': 1.901245779614696, 'tau': 0.34390259258325345, 'L': 1.4218219318027434, 'WINDOW': 85, 'threshold': 0.33453574655642954, 'tc': 7.74503999223658e-05}. Best is trial 10 with value: 1.1774363150608143.\n",
            "[I 2025-12-05 02:51:46,434] Trial 11 finished with value: 1.030206024670993 and parameters: {'k': 1.9598798720277153, 'tau': 0.3262264425013092, 'L': 1.3975730781512494, 'WINDOW': 79, 'threshold': 0.34724953068532616, 'tc': 1.7372071993338438e-05}. Best is trial 10 with value: 1.1774363150608143.\n",
            "[I 2025-12-05 02:51:46,508] Trial 12 finished with value: 1.482252407743955 and parameters: {'k': 1.6368890708713582, 'tau': 0.3294309720160686, 'L': 1.5919036026647067, 'WINDOW': 70, 'threshold': 0.3035921370166128, 'tc': 9.313183902897966e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:46,572] Trial 13 finished with value: -0.11094373391740298 and parameters: {'k': 1.6405069766677305, 'tau': 0.2629969083196696, 'L': 1.5334685759722928, 'WINDOW': 70, 'threshold': 0.3858320716284691, 'tc': 0.00041802185757173734}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:46,647] Trial 14 finished with value: 0.3965438506711272 and parameters: {'k': 1.9789993630567189, 'tau': 0.3012901546699472, 'L': 1.6481232382583206, 'WINDOW': 29, 'threshold': 0.2531342655929461, 'tc': 0.0003115675863863711}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:46,707] Trial 15 finished with value: -1.6442178890039978 and parameters: {'k': 1.5725201410365868, 'tau': 0.3700098003501991, 'L': 1.2654258831327447, 'WINDOW': 70, 'threshold': 0.39899743453950864, 'tc': 0.001404088017068523}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:46,776] Trial 16 finished with value: 0.6375681112461037 and parameters: {'k': 1.4980866655933505, 'tau': 0.19843149667947424, 'L': 1.825831619451232, 'WINDOW': 101, 'threshold': 0.3015226075092704, 'tc': 0.0005787707646077991}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:46,861] Trial 17 finished with value: 0.7770855240541551 and parameters: {'k': 1.8051114800823969, 'tau': 0.2242149617622211, 'L': 2.5511806329667297, 'WINDOW': 67, 'threshold': 0.23805993459819655, 'tc': 4.874243763015304e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:46,934] Trial 18 finished with value: -0.9852104035039617 and parameters: {'k': 1.4372284461432803, 'tau': 0.3464558294720329, 'L': 1.7691442629855598, 'WINDOW': 149, 'threshold': 0.4191904809865727, 'tc': 0.0002096846046498497}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,000] Trial 19 finished with value: -0.07285768094543554 and parameters: {'k': 1.802416044027466, 'tau': 0.28071448946081984, 'L': 1.4343007229300513, 'WINDOW': 25, 'threshold': 0.207591879756188, 'tc': 0.0008104398450412561}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,076] Trial 20 finished with value: 0.6797701540361386 and parameters: {'k': 1.825582673193457, 'tau': 0.4016148799817967, 'L': 0.5329480359523087, 'WINDOW': 85, 'threshold': 0.33636453353847756, 'tc': 0.0004878409229028946}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,138] Trial 21 finished with value: 0.8902902294544323 and parameters: {'k': 0.5838748048492728, 'tau': 0.38657422268576835, 'L': 1.0613038770156324, 'WINDOW': 104, 'threshold': 0.286844733752796, 'tc': 0.0001972196201531145}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,217] Trial 22 finished with value: 0.7937849756845697 and parameters: {'k': 1.002028197968428, 'tau': 0.4139015385801591, 'L': 0.912969328517719, 'WINDOW': 92, 'threshold': 0.3451135316227414, 'tc': 0.00010791900718058479}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,277] Trial 23 finished with value: 1.0371645372129565 and parameters: {'k': 1.375751995096857, 'tau': 0.31970128727448915, 'L': 1.2795490081386227, 'WINDOW': 60, 'threshold': 0.3060317037502506, 'tc': 0.00031125069044541325}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,349] Trial 24 finished with value: 0.6660755249211325 and parameters: {'k': 1.1174160440152239, 'tau': 0.36749799033144875, 'L': 0.8214540250609346, 'WINDOW': 115, 'threshold': 0.2037009019567382, 'tc': 0.00017087775698255566}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,409] Trial 25 finished with value: -1.134001144605702 and parameters: {'k': 0.7094223776597297, 'tau': 0.49621963979087147, 'L': 1.5375699917690149, 'WINDOW': 97, 'threshold': 0.4369029877692981, 'tc': 0.00048015698574747434}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,485] Trial 26 finished with value: 1.0400116964852464 and parameters: {'k': 0.44978589097507793, 'tau': 0.2364466706980447, 'L': 1.0963927471676735, 'WINDOW': 79, 'threshold': 0.2631473067887003, 'tc': 1.488602017088717e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,545] Trial 27 finished with value: -0.8849707160304726 and parameters: {'k': 1.713753701010623, 'tau': 0.3482759291914151, 'L': 1.8856523668268046, 'WINDOW': 38, 'threshold': 0.36128384746839237, 'tc': 0.0011880671718757238}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,615] Trial 28 finished with value: 0.4458940234698761 and parameters: {'k': 1.5549790909828087, 'tau': 0.4193444785835971, 'L': 1.313421916896638, 'WINDOW': 134, 'threshold': 0.20823221761234806, 'tc': 0.0003316598179413068}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,676] Trial 29 finished with value: 1.106750223471588 and parameters: {'k': 0.7703033802637216, 'tau': 0.2835818269073769, 'L': 1.636575342861639, 'WINDOW': 107, 'threshold': 0.3164454635468107, 'tc': 0.00016212498953915123}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,750] Trial 30 finished with value: 0.035419610977004225 and parameters: {'k': 1.9987282587507904, 'tau': 0.3220142472534145, 'L': 2.27433855464104, 'WINDOW': 78, 'threshold': 0.37865738307168045, 'tc': 0.0003576335042880798}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,812] Trial 31 finished with value: 1.0123571655316825 and parameters: {'k': 0.887599301223168, 'tau': 0.2765634446607416, 'L': 1.6221059349940972, 'WINDOW': 105, 'threshold': 0.3215859062170512, 'tc': 0.00014276743970108763}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,903] Trial 32 finished with value: 0.9562616165572951 and parameters: {'k': 0.7807532799582053, 'tau': 0.30188932068987784, 'L': 2.0275553713733934, 'WINDOW': 95, 'threshold': 0.2787877701692334, 'tc': 0.0002468418502395461}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:47,969] Trial 33 finished with value: 0.702147733730131 and parameters: {'k': 0.5888492912915331, 'tau': 0.2394399358407837, 'L': 1.1974758316404717, 'WINDOW': 58, 'threshold': 0.23332965309572573, 'tc': 0.00010286870727645904}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,036] Trial 34 finished with value: 0.6034668135680907 and parameters: {'k': 0.9146824559719694, 'tau': 0.4367980112982066, 'L': 1.4687929943608493, 'WINDOW': 112, 'threshold': 0.3122163013396442, 'tc': 0.0005938246935184904}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,097] Trial 35 finished with value: -0.41395496010530103 and parameters: {'k': 1.0019706293506434, 'tau': 0.19731526759185025, 'L': 1.6971916607920967, 'WINDOW': 119, 'threshold': 0.3662231731148727, 'tc': 0.0008479987948574906}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,169] Trial 36 finished with value: 0.42762295351940893 and parameters: {'k': 1.1100368247390626, 'tau': 0.024351949900243974, 'L': 0.9660841938285761, 'WINDOW': 88, 'threshold': 0.17446996162966522, 'tc': 0.00010695748853463416}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,233] Trial 37 finished with value: -0.6315606977596774 and parameters: {'k': 1.242412913530264, 'tau': 0.34702450744401836, 'L': 1.9233264389464106, 'WINDOW': 108, 'threshold': 0.11457735668872918, 'tc': 0.00047320513494011863}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,312] Trial 38 finished with value: 0.9672580229089922 and parameters: {'k': 0.40123874961911726, 'tau': 0.3825725438692015, 'L': 1.417545547666537, 'WINDOW': 84, 'threshold': 0.2786038236805072, 'tc': 0.000244636794751555}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,377] Trial 39 finished with value: -2.6869307743453352 and parameters: {'k': 0.6779491029249536, 'tau': 0.1421458908154331, 'L': 2.1200651200470055, 'WINDOW': 130, 'threshold': 0.4269058490618016, 'tc': 0.0012387144832796348}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,447] Trial 40 finished with value: -0.6252375093895518 and parameters: {'k': 0.8391284146920515, 'tau': 0.43211933579877937, 'L': 2.4124672999491574, 'WINDOW': 122, 'threshold': 0.4597977223447892, 'tc': 1.6317499835691743e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,513] Trial 41 finished with value: 1.0168603911183853 and parameters: {'k': 0.3753282346752832, 'tau': 0.2427547750098752, 'L': 1.0748151518419633, 'WINDOW': 77, 'threshold': 0.2626073897456873, 'tc': 2.7968522576685677e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,586] Trial 42 finished with value: 1.216584887350906 and parameters: {'k': 0.5314012377010613, 'tau': 0.19347486532331565, 'L': 1.1321318491953576, 'WINDOW': 64, 'threshold': 0.324465012425552, 'tc': 9.333907321837777e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,647] Trial 43 finished with value: 0.7992015874635797 and parameters: {'k': 0.47543235854176424, 'tau': 0.1761606320412179, 'L': 0.8193019863153221, 'WINDOW': 45, 'threshold': 0.3284195876754105, 'tc': 0.00039322995372353443}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,719] Trial 44 finished with value: 0.28917172660209584 and parameters: {'k': 0.2190099498023847, 'tau': 0.2948608383419732, 'L': 1.5650721816683613, 'WINDOW': 62, 'threshold': 0.3994377113423727, 'tc': 0.00023357299316262178}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,785] Trial 45 finished with value: -0.9093856764925792 and parameters: {'k': 0.582786116163198, 'tau': 0.20945939102948255, 'L': 1.3431880203436073, 'WINDOW': 51, 'threshold': 0.30175736888814203, 'tc': 0.0016265347038170069}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,857] Trial 46 finished with value: -2.268995711201253 and parameters: {'k': 1.8805094269591018, 'tau': 0.2651387627487359, 'L': 1.202137844054329, 'WINDOW': 69, 'threshold': 0.35656364790154993, 'tc': 0.001983912930726497}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:48,941] Trial 47 finished with value: 0.6245157310115861 and parameters: {'k': 0.7985453498189743, 'tau': 0.3346972164264038, 'L': 0.6586761506401888, 'WINDOW': 99, 'threshold': 0.22924630668899293, 'tc': 0.0001405241694236532}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,014] Trial 48 finished with value: 0.8013065533840237 and parameters: {'k': 1.672671187402461, 'tau': 0.0789688604982407, 'L': 0.9852912873911838, 'WINDOW': 88, 'threshold': 0.28626960205831037, 'tc': 0.0002821555895819619}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,083] Trial 49 finished with value: 0.46799795107226955 and parameters: {'k': 1.8808200034495093, 'tau': 0.3672023233346134, 'L': 1.7213954137447947, 'WINDOW': 74, 'threshold': 0.337866247463935, 'tc': 0.0006732217522960324}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,162] Trial 50 finished with value: 0.1678625010647571 and parameters: {'k': 0.9513649661121676, 'tau': 0.15791229155470232, 'L': 2.996602331398215, 'WINDOW': 141, 'threshold': 0.3951348655524913, 'tc': 8.120907675223463e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,224] Trial 51 finished with value: 1.0917296113028723 and parameters: {'k': 0.5195432462259038, 'tau': 0.2394639103879532, 'L': 1.1577272061063586, 'WINDOW': 81, 'threshold': 0.2635181292988073, 'tc': 1.0316218493605113e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,296] Trial 52 finished with value: 0.6245327451401049 and parameters: {'k': 0.6348477960738648, 'tau': 0.3030301892917009, 'L': 1.173460837633793, 'WINDOW': 92, 'threshold': 0.2543453177365034, 'tc': 0.00020116323591729213}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,359] Trial 53 finished with value: 1.0660520807596405 and parameters: {'k': 0.5212390977794016, 'tau': 0.21626819671042763, 'L': 1.3695226781611534, 'WINDOW': 66, 'threshold': 0.3208749005258257, 'tc': 8.736823883196552e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,434] Trial 54 finished with value: 1.0802028553943048 and parameters: {'k': 0.31504679494032484, 'tau': 0.2564133953575073, 'L': 0.8587827733348676, 'WINDOW': 83, 'threshold': 0.2863169673004885, 'tc': 7.900168890494433e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,497] Trial 55 finished with value: 1.2464889620286204 and parameters: {'k': 1.3540975767730528, 'tau': 0.1824494388981628, 'L': 1.5213570172244828, 'WINDOW': 74, 'threshold': 0.30105667120253543, 'tc': 0.0001812166381239778}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,570] Trial 56 finished with value: 0.26995671098661456 and parameters: {'k': 1.3905570389725523, 'tau': 0.18144111222780468, 'L': 1.4752311359714192, 'WINDOW': 73, 'threshold': 0.3697568701638443, 'tc': 0.00042540267594916634}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,632] Trial 57 finished with value: 0.783283606471048 and parameters: {'k': 1.1912173979095295, 'tau': 0.1167509549836562, 'L': 1.6140797591425189, 'WINDOW': 56, 'threshold': 0.34676009698177473, 'tc': 0.00019654560528916185}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,707] Trial 58 finished with value: 1.0264041595064162 and parameters: {'k': 1.6170849487054826, 'tau': 0.13456708347868146, 'L': 1.8202744917923381, 'WINDOW': 39, 'threshold': 0.3081558308746716, 'tc': 0.0002937961457802166}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,776] Trial 59 finished with value: -1.4951087985950127 and parameters: {'k': 1.5003694413679138, 'tau': 0.39499495613553753, 'L': 1.2815467470223403, 'WINDOW': 101, 'threshold': 0.2967053269185596, 'tc': 0.0018623157352146737}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,846] Trial 60 finished with value: 1.0336583640633517 and parameters: {'k': 1.7514184685189513, 'tau': 0.28153850698253347, 'L': 1.5184602047317814, 'WINDOW': 168, 'threshold': 0.32865013893828277, 'tc': 0.00017346060027549664}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:49,915] Trial 61 finished with value: 0.5677594956282014 and parameters: {'k': 1.3371293797527193, 'tau': 0.17700651786791388, 'L': 1.0126119682981682, 'WINDOW': 66, 'threshold': 0.2483077792367295, 'tc': 9.791893183265738e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,009] Trial 62 finished with value: 1.1992294692663956 and parameters: {'k': 1.083745746469846, 'tau': 0.22867171287917637, 'L': 1.1546319415059185, 'WINDOW': 93, 'threshold': 0.26951009368093026, 'tc': 8.927084401342019e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,071] Trial 63 finished with value: 0.43576198060044025 and parameters: {'k': 1.1462759583662787, 'tau': 0.19642296792209277, 'L': 1.2441005725879213, 'WINDOW': 93, 'threshold': 0.22449360489374853, 'tc': 0.0003647318862411178}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,147] Trial 64 finished with value: 1.1571757959689128 and parameters: {'k': 1.274707906052615, 'tau': 0.31123666928635063, 'L': 1.6689453243854107, 'WINDOW': 108, 'threshold': 0.2724416882862577, 'tc': 6.969121665113133e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,213] Trial 65 finished with value: 0.6170050707104915 and parameters: {'k': 1.2358723306609194, 'tau': 0.35779298262122894, 'L': 1.7816070759946119, 'WINDOW': 89, 'threshold': 0.18458134557755287, 'tc': 8.093287742207234e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,292] Trial 66 finished with value: 0.9295883625231717 and parameters: {'k': 1.0589331377493334, 'tau': 0.3135026280692502, 'L': 0.6857077512389733, 'WINDOW': 114, 'threshold': 0.27024305184008834, 'tc': 0.000283126772639742}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,359] Trial 67 finished with value: 0.5977233801173063 and parameters: {'k': 1.297580636874724, 'tau': 0.45990568829021056, 'L': 1.3839699171665445, 'WINDOW': 76, 'threshold': 0.24678139795472928, 'tc': 5.733800209882077e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,434] Trial 68 finished with value: 0.5963089514425798 and parameters: {'k': 1.4306707359188668, 'tau': 0.32509791957543044, 'L': 1.9580528995582354, 'WINDOW': 102, 'threshold': 0.2930829538492336, 'tc': 0.0005386811119556342}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,498] Trial 69 finished with value: 0.7330576914296749 and parameters: {'k': 1.040302729751133, 'tau': 0.16124739706365965, 'L': 2.7402253853221223, 'WINDOW': 62, 'threshold': 0.3505055590704184, 'tc': 0.00014681855690908238}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,574] Trial 70 finished with value: -0.23260233559835444 and parameters: {'k': 1.5873293945726237, 'tau': 0.3326107185777595, 'L': 0.9069253430066311, 'WINDOW': 97, 'threshold': 0.2200334527298163, 'tc': 0.0009385997587922783}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,640] Trial 71 finished with value: 1.0413731418577887 and parameters: {'k': 1.0725251328486745, 'tau': 0.28620375388274355, 'L': 1.66760198898787, 'WINDOW': 117, 'threshold': 0.3178501042600338, 'tc': 0.00015652017096108966}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,712] Trial 72 finished with value: 0.9444157974118239 and parameters: {'k': 1.1753493505734594, 'tau': 0.38126431767652025, 'L': 1.5989522825360931, 'WINDOW': 108, 'threshold': 0.278350715318456, 'tc': 0.00024156003904345194}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,779] Trial 73 finished with value: 1.4523389197413055 and parameters: {'k': 1.473145017373677, 'tau': 0.22452208361582943, 'L': 1.868188509522129, 'WINDOW': 123, 'threshold': 0.3030036457515752, 'tc': 5.693612804039036e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,860] Trial 74 finished with value: 1.42226489341929 and parameters: {'k': 1.5218680130332336, 'tau': 0.2556706023376109, 'L': 2.0962836202708046, 'WINDOW': 129, 'threshold': 0.30245119323610986, 'tc': 8.650648412682439e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:50,929] Trial 75 finished with value: 1.1951188433192508 and parameters: {'k': 1.4748237267851962, 'tau': 0.2548707059608795, 'L': 2.1037769463558145, 'WINDOW': 125, 'threshold': 0.334673093555888, 'tc': 6.355671482622004e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,018] Trial 76 finished with value: 1.1589518676376411 and parameters: {'k': 1.5024449061782084, 'tau': 0.25644029121178913, 'L': 2.140827759375407, 'WINDOW': 126, 'threshold': 0.3350278938569718, 'tc': 0.0001328457386488847}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,094] Trial 77 finished with value: 0.6019426085402122 and parameters: {'k': 1.441922341799776, 'tau': 0.2278353869715114, 'L': 2.092344652962345, 'WINDOW': 156, 'threshold': 0.375014808207919, 'tc': 5.3046879685008675e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,170] Trial 78 finished with value: 1.1591534144149105 and parameters: {'k': 1.5655579898355458, 'tau': 0.20996501916949462, 'L': 2.4411285334053296, 'WINDOW': 147, 'threshold': 0.30736328649707123, 'tc': 0.0002140550997037767}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,236] Trial 79 finished with value: 0.15600455240762623 and parameters: {'k': 1.7363661672774076, 'tau': 0.2701044941495694, 'L': 1.8674351936618057, 'WINDOW': 135, 'threshold': 0.3837552869256847, 'tc': 0.00032000285349931853}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,315] Trial 80 finished with value: 0.8841101228636562 and parameters: {'k': 1.523728266116654, 'tau': 0.24844025083214158, 'L': 2.2033178806530156, 'WINDOW': 123, 'threshold': 0.35618726088842695, 'tc': 5.164230088301298e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,379] Trial 81 finished with value: 1.2297147555551557 and parameters: {'k': 1.6667797823989166, 'tau': 0.20702249344355572, 'L': 2.343043035768461, 'WINDOW': 146, 'threshold': 0.30562994531121573, 'tc': 0.00020519926599201063}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,455] Trial 82 finished with value: 1.1486387544407997 and parameters: {'k': 1.6646891103779642, 'tau': 0.1920444398590426, 'L': 2.265762573533714, 'WINDOW': 141, 'threshold': 0.2939927710025913, 'tc': 0.00011516934434232845}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,522] Trial 83 finished with value: -0.30845382374280167 and parameters: {'k': 1.447535969300831, 'tau': 0.21471339041339732, 'L': 2.4540311902170195, 'WINDOW': 128, 'threshold': 0.006110071898636216, 'tc': 0.00027048539625532396}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,598] Trial 84 finished with value: 1.2033313401736068 and parameters: {'k': 1.5914966577632637, 'tau': 0.22667437382065786, 'L': 2.0285911924342725, 'WINDOW': 153, 'threshold': 0.3309263090099436, 'tc': 4.166614749993326e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,668] Trial 85 finished with value: 1.087600162518358 and parameters: {'k': 1.638783031835075, 'tau': 0.22174290845367695, 'L': 2.0043585977835257, 'WINDOW': 147, 'threshold': 0.3383760150801583, 'tc': 0.0001790264483893351}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,744] Trial 86 finished with value: 1.138595414333697 and parameters: {'k': 1.7720957115989262, 'tau': 0.23127407076036963, 'L': 2.359367851420976, 'WINDOW': 156, 'threshold': 0.32022018413992487, 'tc': 1.5836115796543512e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,814] Trial 87 finished with value: 1.351718564238131 and parameters: {'k': 1.3737978138701084, 'tau': 0.1638505850981843, 'L': 2.0970360181798697, 'WINDOW': 141, 'threshold': 0.3029672515903493, 'tc': 0.00012423337658922802}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,891] Trial 88 finished with value: 1.3316705059809582 and parameters: {'k': 1.6051930470698639, 'tau': 0.18681167561764034, 'L': 2.563997082947804, 'WINDOW': 139, 'threshold': 0.2979758232609344, 'tc': 2.3555586632933373e-07}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:51,963] Trial 89 finished with value: 1.4653912486621643 and parameters: {'k': 1.3519565157726392, 'tau': 0.14207910193123702, 'L': 2.6428796776863654, 'WINDOW': 140, 'threshold': 0.3097228264167014, 'tc': 2.7097284083559616e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,048] Trial 90 finished with value: -0.4279563601104906 and parameters: {'k': 1.3374140096355824, 'tau': 0.15775231114138755, 'L': 2.6063175778933743, 'WINDOW': 139, 'threshold': 0.3089681589266947, 'tc': 0.0013085970843314944}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,120] Trial 91 finished with value: 0.9935904671020529 and parameters: {'k': 1.6059376795733922, 'tau': 0.14387275844431274, 'L': 2.73467826067214, 'WINDOW': 131, 'threshold': 0.2875802360532634, 'tc': 0.0001425675320273842}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,198] Trial 92 finished with value: 1.3626638698617868 and parameters: {'k': 1.3825498454362057, 'tau': 0.18616454484917117, 'L': 2.561974586163012, 'WINDOW': 150, 'threshold': 0.2989122630368221, 'tc': 1.3080298987527645e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,266] Trial 93 finished with value: 0.9727562415790181 and parameters: {'k': 1.541465372889976, 'tau': 0.09778361687971462, 'L': 2.5575544887440103, 'WINDOW': 144, 'threshold': 0.2962606024963611, 'tc': 0.00022065362028872491}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,350] Trial 94 finished with value: 1.3867983128008905 and parameters: {'k': 1.3929298601984428, 'tau': 0.16966746239575722, 'L': 2.71282671612211, 'WINDOW': 152, 'threshold': 0.3053490431450553, 'tc': 4.745564591772248e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,426] Trial 95 finished with value: 0.8369550483783047 and parameters: {'k': 1.3968891967956567, 'tau': 0.16773558151158843, 'L': 2.698136790543156, 'WINDOW': 162, 'threshold': 0.2566511979535991, 'tc': 0.00012160298312276982}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,502] Trial 96 finished with value: 1.3829883808578198 and parameters: {'k': 1.7006692211431926, 'tau': 0.13609283351301305, 'L': 2.862392689411697, 'WINDOW': 150, 'threshold': 0.3068594963125677, 'tc': 4.239610870140074e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,570] Trial 97 finished with value: 1.0672568013891108 and parameters: {'k': 1.351158361385155, 'tau': 0.12925320606227722, 'L': 2.8234932526243703, 'WINDOW': 152, 'threshold': 0.28534833299662304, 'tc': 4.72502246702806e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,647] Trial 98 finished with value: 0.8542299146991695 and parameters: {'k': 1.4126471187010103, 'tau': 0.1424949126739145, 'L': 2.8354317253595585, 'WINDOW': 133, 'threshold': 0.2387335427123213, 'tc': 7.966178178376402e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,713] Trial 99 finished with value: 1.1926258497133393 and parameters: {'k': 1.4626026236888015, 'tau': 0.11003536789141267, 'L': 2.905127384251356, 'WINDOW': 137, 'threshold': 0.2780319105103762, 'tc': 4.7462203825963805e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,788] Trial 100 finished with value: 1.0789179241479896 and parameters: {'k': 1.7011792324909445, 'tau': 0.093138871046445, 'L': 2.6332438578619284, 'WINDOW': 159, 'threshold': 0.3158678101291948, 'tc': 0.0001659930169423463}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,858] Trial 101 finished with value: 1.3438663492154683 and parameters: {'k': 1.6659444059338109, 'tau': 0.184421321663579, 'L': 2.6652833591870024, 'WINDOW': 149, 'threshold': 0.3008213971691862, 'tc': 0.0001079049146945104}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:52,940] Trial 102 finished with value: 1.2989839385076916 and parameters: {'k': 1.8473915821818991, 'tau': 0.18199582283378601, 'L': 2.6728462149552565, 'WINDOW': 142, 'threshold': 0.3001062946608024, 'tc': 0.00012000267902959275}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,009] Trial 103 finished with value: 0.7856805925522016 and parameters: {'k': 1.8375675934285143, 'tau': 0.1507353389952992, 'L': 2.4911805531313043, 'WINDOW': 150, 'threshold': 0.34498931112492237, 'tc': 0.0001177821222057314}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,104] Trial 104 finished with value: -0.08588769311022375 and parameters: {'k': 1.9379227292343937, 'tau': 0.16983737003157234, 'L': 2.6569056398604376, 'WINDOW': 143, 'threshold': 0.31525129561807774, 'tc': 0.001115735608635704}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,171] Trial 105 finished with value: 1.2739143102962214 and parameters: {'k': 1.7998803305637334, 'tau': 0.187958614518768, 'L': 2.52902663026677, 'WINDOW': 139, 'threshold': 0.2932553138004408, 'tc': 3.75412165633017e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,279] Trial 106 finished with value: -0.35112253533067306 and parameters: {'k': 1.7178602806333942, 'tau': 0.12821857824256563, 'L': 2.803122849630793, 'WINDOW': 151, 'threshold': 0.0874214769993425, 'tc': 5.1243809391613456e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,394] Trial 107 finished with value: 0.9625528254677572 and parameters: {'k': 1.5212925301602653, 'tau': 0.20061505004389044, 'L': 2.928338297643324, 'WINDOW': 164, 'threshold': 0.263637986189611, 'tc': 0.0001023628026339019}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,513] Trial 108 finished with value: 1.011552794348253 and parameters: {'k': 1.2970055231110875, 'tau': 0.15244888061767237, 'L': 2.5767769648002377, 'WINDOW': 155, 'threshold': 0.32428681435311824, 'tc': 0.00025348861230001035}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,615] Trial 109 finished with value: 1.0708989979089463 and parameters: {'k': 1.8463117100038666, 'tau': 0.1707540919723989, 'L': 2.7792100494479635, 'WINDOW': 143, 'threshold': 0.2813296790379757, 'tc': 0.00013616364937090217}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,713] Trial 110 finished with value: 1.2969169312900928 and parameters: {'k': 1.560407711708538, 'tau': 0.18746719878474777, 'L': 2.670931643953641, 'WINDOW': 138, 'threshold': 0.2998299099831733, 'tc': 7.899996072803373e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,804] Trial 111 finished with value: 1.4265465130594766 and parameters: {'k': 1.5551195893343337, 'tau': 0.16518456346369745, 'L': 2.6808484714227223, 'WINDOW': 137, 'threshold': 0.30081152756476914, 'tc': 4.2752241721937984e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,901] Trial 112 finished with value: 1.4560495996761809 and parameters: {'k': 1.628380859617793, 'tau': 0.12141198950791891, 'L': 2.8852808985898535, 'WINDOW': 133, 'threshold': 0.3048203089788304, 'tc': 4.5951349264707434e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:53,997] Trial 113 finished with value: -0.733812452996177 and parameters: {'k': 1.6443744613898308, 'tau': 0.12092216299533105, 'L': 2.8717261313165534, 'WINDOW': 132, 'threshold': 0.31088130673697345, 'tc': 0.00151431111370522}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,108] Trial 114 finished with value: -0.33008477662351327 and parameters: {'k': 1.6152574374154955, 'tau': 0.13715359370311678, 'L': 2.7266967287626924, 'WINDOW': 135, 'threshold': 0.03202939805555968, 'tc': 5.502187350727046e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,209] Trial 115 finished with value: 1.2189258829991194 and parameters: {'k': 1.5440300021465507, 'tau': 0.0583336027521523, 'L': 2.4973705393556935, 'WINDOW': 129, 'threshold': 0.27525034004946836, 'tc': 3.517516109499436e-07}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,312] Trial 116 finished with value: 1.2014600615249127 and parameters: {'k': 1.483063692716713, 'tau': 0.14909533837530184, 'L': 2.9796521632679345, 'WINDOW': 148, 'threshold': 0.32705238182127555, 'tc': 4.0851272982431757e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,397] Trial 117 finished with value: 1.1598237184640177 and parameters: {'k': 1.6896645443826723, 'tau': 0.16387077232921898, 'L': 2.603836260199624, 'WINDOW': 158, 'threshold': 0.28977553540968504, 'tc': 8.632381452740694e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,488] Trial 118 finished with value: 0.9448377813890966 and parameters: {'k': 1.394624978104273, 'tau': 0.20103097195319272, 'L': 2.87742736119675, 'WINDOW': 145, 'threshold': 0.34071945379945323, 'tc': 0.00017028136495120957}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,577] Trial 119 finished with value: 0.9757720399207823 and parameters: {'k': 1.7644516672999433, 'tau': 0.1744690556726855, 'L': 2.770934351315556, 'WINDOW': 120, 'threshold': 0.35297498135760785, 'tc': 3.174303799358561e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,687] Trial 120 finished with value: 0.9129541027029187 and parameters: {'k': 1.246581997244993, 'tau': 0.10961580519181455, 'L': 2.384252818840838, 'WINDOW': 140, 'threshold': 0.2670049697398248, 'tc': 0.00018928563999487722}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,782] Trial 121 finished with value: 1.2570923757595913 and parameters: {'k': 1.624533371908404, 'tau': 0.18122236734280844, 'L': 2.671312164665942, 'WINDOW': 142, 'threshold': 0.30663887481591806, 'tc': 0.00013423054152585177}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,888] Trial 122 finished with value: 1.2562503122170827 and parameters: {'k': 1.5126564177664474, 'tau': 0.15666670817871817, 'L': 2.7035417580905805, 'WINDOW': 149, 'threshold': 0.2993907263058108, 'tc': 0.00010050503808105774}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:54,988] Trial 123 finished with value: 1.075403944071561 and parameters: {'k': 1.47918294335814, 'tau': 0.13179015806755848, 'L': 2.7636830170344915, 'WINDOW': 136, 'threshold': 0.3189262034893389, 'tc': 7.335858898427751e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,086] Trial 124 finished with value: 0.1076794890468427 and parameters: {'k': 1.432042569471507, 'tau': 0.18512827433295806, 'L': 2.8459676936035074, 'WINDOW': 153, 'threshold': 0.28738277355240316, 'tc': 0.0007604758134468869}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,173] Trial 125 finished with value: 0.8196043136630783 and parameters: {'k': 1.5772924773376138, 'tau': 0.08438664506044699, 'L': 2.9631170209386677, 'WINDOW': 133, 'threshold': 0.25621342402892355, 'tc': 0.00012743177723669667}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,285] Trial 126 finished with value: 1.3350603643930583 and parameters: {'k': 1.9238226240069565, 'tau': 0.11922368361776753, 'L': 1.939624790713706, 'WINDOW': 145, 'threshold': 0.29951804039231833, 'tc': 3.842685250212265e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,374] Trial 127 finished with value: 0.7545220081802627 and parameters: {'k': 1.3719272640144042, 'tau': 0.12327727708409551, 'L': 1.891223845488678, 'WINDOW': 128, 'threshold': 0.3636845721436517, 'tc': 3.8238460547409625e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,466] Trial 128 finished with value: 1.302051947947145 and parameters: {'k': 1.6700373111706655, 'tau': 0.13867366281524737, 'L': 1.978787923258967, 'WINDOW': 145, 'threshold': 0.328208114014305, 'tc': 3.108738961834118e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,555] Trial 129 finished with value: 0.9713240605271372 and parameters: {'k': 1.5467551722837314, 'tau': 0.10580411078021938, 'L': 1.7434979199169724, 'WINDOW': 149, 'threshold': 0.3128886784539269, 'tc': 0.00022397857904504716}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,645] Trial 130 finished with value: 0.6820291045039363 and parameters: {'k': 1.3242933538496797, 'tau': 0.16341098212632932, 'L': 2.2642715958096655, 'WINDOW': 123, 'threshold': 0.24144487803260561, 'tc': 7.896797592695919e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,731] Trial 131 finished with value: 1.2271890174689095 and parameters: {'k': 1.6839628819355412, 'tau': 0.1381705357340143, 'L': 1.9414545460051178, 'WINDOW': 146, 'threshold': 0.32943224356506373, 'tc': 1.030339834029522e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,833] Trial 132 finished with value: 1.3899201988888379 and parameters: {'k': 1.649917618561804, 'tau': 0.15049675922935699, 'L': 1.8081066543226036, 'WINDOW': 138, 'threshold': 0.30554489394467665, 'tc': 4.9817436259482865e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:55,920] Trial 133 finished with value: 1.4581976804388768 and parameters: {'k': 1.9460385027788631, 'tau': 0.1509231908984018, 'L': 1.8472512271652946, 'WINDOW': 139, 'threshold': 0.30495601309264114, 'tc': 4.6736860661902825e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:56,070] Trial 134 finished with value: 1.0902035536129107 and parameters: {'k': 1.8795669765476553, 'tau': 0.11841691246515863, 'L': 1.7998345443520603, 'WINDOW': 136, 'threshold': 0.27683997927773724, 'tc': 0.00016285084509126045}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:56,172] Trial 135 finished with value: 1.46631723893832 and parameters: {'k': 1.9242252313806443, 'tau': 0.14487406646224604, 'L': 1.8663481994258915, 'WINDOW': 131, 'threshold': 0.3059461356472375, 'tc': 4.8174729374917264e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:56,545] Trial 136 finished with value: 1.2789365651807447 and parameters: {'k': 1.9841803511243508, 'tau': 0.14790130732746573, 'L': 1.6664590884283936, 'WINDOW': 132, 'threshold': 0.31178807545421633, 'tc': 8.286003681512444e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:56,739] Trial 137 finished with value: 1.041812206503706 and parameters: {'k': 1.8042570001185851, 'tau': 0.155746309964175, 'L': 1.715481724941448, 'WINDOW': 128, 'threshold': 0.2848424772119389, 'tc': 9.950413288190596e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:56,898] Trial 138 finished with value: 0.6679575386813271 and parameters: {'k': 1.7172396818615365, 'tau': 0.17020923326097817, 'L': 1.8202754175766773, 'WINDOW': 126, 'threshold': 0.3442535687831417, 'tc': 0.00019482976643188247}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:57,245] Trial 139 finished with value: 1.111874490382293 and parameters: {'k': 1.952820784202139, 'tau': 0.1499648117579401, 'L': 1.8789152162168938, 'WINDOW': 154, 'threshold': 0.3205717330648427, 'tc': 4.8200739679069e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:57,412] Trial 140 finished with value: 1.2375305935385932 and parameters: {'k': 1.9084101721964657, 'tau': 0.17495654267539062, 'L': 2.152402227654779, 'WINDOW': 137, 'threshold': 0.30671246575957006, 'tc': 0.00015111993216255723}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:57,774] Trial 141 finished with value: 1.276965418062219 and parameters: {'k': 1.8904659659249945, 'tau': 0.11862134818687889, 'L': 1.850667109890411, 'WINDOW': 142, 'threshold': 0.295920738214585, 'tc': 5.069707881496824e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:51:58,011] Trial 142 finished with value: 1.4901644834461296 and parameters: {'k': 1.9898570651888277, 'tau': 0.13010340811879054, 'L': 1.9320015121292762, 'WINDOW': 151, 'threshold': 0.30341381521310695, 'tc': 5.027786955579793e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:58,136] Trial 143 finished with value: 1.0292575176857193 and parameters: {'k': 1.9997467325282812, 'tau': 0.1283978647017342, 'L': 2.0777556240187303, 'WINDOW': 158, 'threshold': 0.2883481946307886, 'tc': 0.00011336550704907312}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:58,245] Trial 144 finished with value: 1.1336089859960756 and parameters: {'k': 1.9493104075452017, 'tau': 0.14325082389913849, 'L': 2.0618880093728453, 'WINDOW': 162, 'threshold': 0.27242382479739324, 'tc': 8.832450124244804e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:58,376] Trial 145 finished with value: 1.155351836224506 and parameters: {'k': 1.753420877982254, 'tau': 0.1638966689098134, 'L': 1.7883073365507538, 'WINDOW': 151, 'threshold': 0.3175321416669153, 'tc': 4.825701081754249e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:58,535] Trial 146 finished with value: 1.4506283088334506 and parameters: {'k': 1.475329966573482, 'tau': 0.13706332682332914, 'L': 2.183033205330653, 'WINDOW': 130, 'threshold': 0.3085578344294391, 'tc': 8.227092425616193e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:58,708] Trial 147 finished with value: 1.293239562784298 and parameters: {'k': 1.4500268016308913, 'tau': 0.13449147264717262, 'L': 2.162223731016361, 'WINDOW': 130, 'threshold': 0.3386960911301361, 'tc': 1.9268849965992736e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:58,997] Trial 148 finished with value: 1.3702386632003496 and parameters: {'k': 1.3813409315350693, 'tau': 0.09404313180876869, 'L': 1.8999758366126345, 'WINDOW': 133, 'threshold': 0.3085476714831114, 'tc': 6.428532070996775e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:59,136] Trial 149 finished with value: 1.1401024686893655 and parameters: {'k': 1.4144605751953927, 'tau': 0.2448329661827781, 'L': 1.903236284455736, 'WINDOW': 117, 'threshold': 0.3303789344866446, 'tc': 6.334918844663538e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:59,467] Trial 150 finished with value: 1.3218334258679667 and parameters: {'k': 1.513592509231215, 'tau': 0.09573463365014565, 'L': 2.000332876314522, 'WINDOW': 134, 'threshold': 0.3114464339398448, 'tc': 3.5893702798617476e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:51:59,787] Trial 151 finished with value: 1.3564049980058142 and parameters: {'k': 1.3564449997172878, 'tau': 0.06776361335895509, 'L': 1.7608751730338157, 'WINDOW': 125, 'threshold': 0.30586383378709603, 'tc': 0.00013758372737628446}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,044] Trial 152 finished with value: -0.6319947163862965 and parameters: {'k': 1.3083153758491177, 'tau': 0.087647840749228, 'L': 1.8439853183324573, 'WINDOW': 126, 'threshold': 0.49361836645230495, 'tc': 7.42170585823649e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,162] Trial 153 finished with value: 0.9714739168790073 and parameters: {'k': 1.2709037818846756, 'tau': 0.0470105345651238, 'L': 1.7554438220349224, 'WINDOW': 121, 'threshold': 0.31998653808022864, 'tc': 0.00015472284187723458}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,386] Trial 154 finished with value: 1.238461345729906 and parameters: {'k': 1.4837440862225488, 'tau': 0.07772957400173422, 'L': 1.7015047796621319, 'WINDOW': 130, 'threshold': 0.2903056341395791, 'tc': 1.5065658243954371e-06}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,549] Trial 155 finished with value: 1.1918744928579612 and parameters: {'k': 1.3575738420541694, 'tau': 0.11022946119696325, 'L': 1.590805358693418, 'WINDOW': 133, 'threshold': 0.28084064925373114, 'tc': 3.9134205311168174e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,636] Trial 156 finished with value: 1.285838891684332 and parameters: {'k': 1.4152504634828926, 'tau': 0.12812203559999322, 'L': 1.9190533544066, 'WINDOW': 138, 'threshold': 0.3069057836517315, 'tc': 8.858191893756421e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,724] Trial 157 finished with value: 1.1780062674954608 and parameters: {'k': 1.215396645831497, 'tau': 0.06401247381105273, 'L': 1.7719324669875756, 'WINDOW': 112, 'threshold': 0.32474989176271823, 'tc': 0.0001386453097876504}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,858] Trial 158 finished with value: 1.0731517634141154 and parameters: {'k': 1.6289505755367661, 'tau': 0.07865531706262974, 'L': 2.035144981169807, 'WINDOW': 123, 'threshold': 0.2942986411431207, 'tc': 0.00018547249524579205}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:00,973] Trial 159 finished with value: 0.0062758448779418234 and parameters: {'k': 1.5905698349771815, 'tau': 0.10418299144972773, 'L': 1.8505366495870381, 'WINDOW': 32, 'threshold': 0.26813911895057907, 'tc': 0.000885397530390036}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,128] Trial 160 finished with value: 1.208169644889362 and parameters: {'k': 1.4752106882070712, 'tau': 0.045726670389257096, 'L': 1.9667109039603046, 'WINDOW': 130, 'threshold': 0.31292091534044914, 'tc': 3.662183230169801e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,278] Trial 161 finished with value: 1.3536815814628689 and parameters: {'k': 1.3823143178302968, 'tau': 0.14692613819904166, 'L': 2.1923183652979175, 'WINDOW': 140, 'threshold': 0.305684135241715, 'tc': 0.00010768975401535621}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,417] Trial 162 finished with value: 1.4501282408269809 and parameters: {'k': 1.3698450483695397, 'tau': 0.14397112262119235, 'L': 2.2585566036054985, 'WINDOW': 137, 'threshold': 0.3046603803910222, 'tc': 7.771790194280857e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,495] Trial 163 finished with value: 1.1760078926960278 and parameters: {'k': 1.3405526238117258, 'tau': 0.13801147563531008, 'L': 2.9090823816277744, 'WINDOW': 135, 'threshold': 0.29121131514048715, 'tc': 7.106198325035022e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,561] Trial 164 finished with value: 1.273572541344804 and parameters: {'k': 1.4331126529259544, 'tau': 0.26922164402793564, 'L': 2.2370318747539204, 'WINDOW': 126, 'threshold': 0.3344054366825821, 'tc': 1.9150922864111564e-06}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,663] Trial 165 finished with value: 1.158199556003388 and parameters: {'k': 1.5170204129214244, 'tau': 0.15584751153174503, 'L': 1.8189597315535755, 'WINDOW': 133, 'threshold': 0.2811599693471298, 'tc': 6.632779375885003e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,740] Trial 166 finished with value: -0.9876545596101346 and parameters: {'k': 1.574252825558386, 'tau': 0.1135543615026304, 'L': 2.3111916359833335, 'WINDOW': 129, 'threshold': 0.320604029872532, 'tc': 0.0016907653664098036}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,835] Trial 167 finished with value: 1.3190881090473177 and parameters: {'k': 1.4524210106831819, 'tau': 0.13161821016616976, 'L': 2.8069265406405, 'WINDOW': 137, 'threshold': 0.3007159709269219, 'tc': 0.00012743057729239758}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,917] Trial 168 finished with value: 1.2461067869809146 and parameters: {'k': 1.2751348804309588, 'tau': 0.14518228971911665, 'L': 1.9014551053025806, 'WINDOW': 124, 'threshold': 0.31287601753460426, 'tc': 3.716923194506235e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:01,997] Trial 169 finished with value: 1.2891879971624693 and parameters: {'k': 1.3940051955006278, 'tau': 0.10059213443307745, 'L': 1.7440736232752916, 'WINDOW': 55, 'threshold': 0.2959513841509979, 'tc': 4.657428659167278e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,079] Trial 170 finished with value: 1.179537267531776 and parameters: {'k': 1.3370311279929956, 'tau': 0.48777736781566516, 'L': 1.6453931860323692, 'WINDOW': 117, 'threshold': 0.3280053292132276, 'tc': 9.927500439201659e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,156] Trial 171 finished with value: 1.2774912345386993 and parameters: {'k': 1.3834583773073805, 'tau': 0.15458843013175638, 'L': 2.180486218761936, 'WINDOW': 143, 'threshold': 0.3078034012087273, 'tc': 0.00011100527644062356}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,241] Trial 172 finished with value: 1.2994313064336274 and parameters: {'k': 1.3119039890419084, 'tau': 0.1443337052887756, 'L': 2.2369475564712524, 'WINDOW': 140, 'threshold': 0.3012949702924013, 'tc': 0.0001616692729798516}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,368] Trial 173 finished with value: 1.0653869721693903 and parameters: {'k': 1.3692132698176518, 'tau': 0.12931113727238328, 'L': 2.023104257779704, 'WINDOW': 139, 'threshold': 0.28462207976203785, 'tc': 7.633598747094322e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,491] Trial 174 finished with value: 1.3217834810560485 and parameters: {'k': 1.9644455759396855, 'tau': 0.14501995026005537, 'L': 2.3136666377781876, 'WINDOW': 134, 'threshold': 0.30709938177119067, 'tc': 3.376541267536229e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,610] Trial 175 finished with value: 1.2030848391890483 and parameters: {'k': 1.4137293898338281, 'tau': 0.1621175174451479, 'L': 2.618499593924222, 'WINDOW': 48, 'threshold': 0.3118893467874781, 'tc': 0.0001144318024328591}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,848] Trial 176 finished with value: 1.147441672571129 and parameters: {'k': 1.5485050342414368, 'tau': 0.16976413809661856, 'L': 2.1120832966656065, 'WINDOW': 136, 'threshold': 0.32215535940597106, 'tc': 6.563282910338199e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:02,963] Trial 177 finished with value: 1.1421022226247786 and parameters: {'k': 1.5000157272837713, 'tau': 0.12338998548867784, 'L': 2.192424842886893, 'WINDOW': 146, 'threshold': 0.2918072391277896, 'tc': 0.00015524924131938132}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:03,043] Trial 178 finished with value: 0.6807801489392286 and parameters: {'k': 1.6379823255764605, 'tau': 0.13761812383498856, 'L': 1.953260853355496, 'WINDOW': 132, 'threshold': 0.34654093325626756, 'tc': 0.00022764467013860687}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:03,123] Trial 179 finished with value: 1.2269806546092192 and parameters: {'k': 1.4540590657728971, 'tau': 0.15190227999218278, 'L': 2.7444075420948866, 'WINDOW': 127, 'threshold': 0.27722647681269125, 'tc': 3.855377455717261e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:03,236] Trial 180 finished with value: 1.372405846420465 and parameters: {'k': 1.3923004430681796, 'tau': 0.02453261882901165, 'L': 2.425145891310823, 'WINDOW': 140, 'threshold': 0.2990003307078065, 'tc': 9.315242049933727e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:52:03,352] Trial 181 finished with value: 1.51137702895629 and parameters: {'k': 1.3638238907287141, 'tau': 0.055144792245002926, 'L': 2.5027456918553526, 'WINDOW': 140, 'threshold': 0.3021040051441348, 'tc': 9.520228276014767e-08}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:03,466] Trial 182 finished with value: 1.3571818555752782 and parameters: {'k': 1.3505849301859896, 'tau': 0.0694834581353966, 'L': 2.415840328205049, 'WINDOW': 143, 'threshold': 0.29863422742375056, 'tc': 6.455150512053869e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:03,603] Trial 183 finished with value: 1.2695802500035416 and parameters: {'k': 1.4216263365851036, 'tau': 0.028818473041882398, 'L': 2.4012679902892273, 'WINDOW': 147, 'threshold': 0.2932963212301913, 'tc': 8.053833109420493e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:03,709] Trial 184 finished with value: 1.2509721817270023 and parameters: {'k': 1.9145372921606474, 'tau': 0.029847335993314166, 'L': 2.4494452773940973, 'WINDOW': 143, 'threshold': 0.3146642536613815, 'tc': 3.963990925433815e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:03,830] Trial 185 finished with value: 1.0648592171099487 and parameters: {'k': 1.3104067366390024, 'tau': 0.03362215913826527, 'L': 2.5113778558995663, 'WINDOW': 152, 'threshold': 0.28448579592402307, 'tc': 7.204574156891044e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:03,959] Trial 186 finished with value: 1.3821662983022014 and parameters: {'k': 1.4587824020911595, 'tau': 0.05946761460523326, 'L': 2.4564543386779882, 'WINDOW': 137, 'threshold': 0.2996575646681541, 'tc': 1.3214412370006007e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:04,099] Trial 187 finished with value: 0.25187099068287694 and parameters: {'k': 1.4794190376254146, 'tau': 0.053474442966281364, 'L': 2.557492501258634, 'WINDOW': 137, 'threshold': 0.1397132333925804, 'tc': 4.115750089956027e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:04,240] Trial 188 finished with value: 1.3266960811176152 and parameters: {'k': 1.543600613464928, 'tau': 0.08945364219936322, 'L': 2.467441128083111, 'WINDOW': 131, 'threshold': 0.32266455562245216, 'tc': 4.3938021689872673e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:04,361] Trial 189 finished with value: 0.9938154199074392 and parameters: {'k': 1.999144184152784, 'tau': 0.023071864535798844, 'L': 2.5914829043603382, 'WINDOW': 139, 'threshold': 0.26389101219988453, 'tc': 7.25346912568499e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:04,474] Trial 190 finished with value: 1.2299334957249801 and parameters: {'k': 1.6067803158704224, 'tau': 0.03522346887159242, 'L': 2.5174210702823676, 'WINDOW': 136, 'threshold': 0.3354599987433461, 'tc': 5.13981595499935e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:04,608] Trial 191 finished with value: 1.4829372907339682 and parameters: {'k': 1.45929205179789, 'tau': 0.06670588327932613, 'L': 2.4121359428836184, 'WINDOW': 142, 'threshold': 0.30308563131260235, 'tc': 3.293699152852973e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:04,727] Trial 192 finished with value: 1.430121031983721 and parameters: {'k': 1.4695080091337178, 'tau': 0.049070206219472126, 'L': 2.3544053125905395, 'WINDOW': 149, 'threshold': 0.3026875041320714, 'tc': 8.936613547971412e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:04,940] Trial 193 finished with value: 1.1801383897096842 and parameters: {'k': 1.4561985855286208, 'tau': 0.04455986482179057, 'L': 2.350000480801545, 'WINDOW': 140, 'threshold': 0.31601929663679285, 'tc': 8.711651857958335e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:05,080] Trial 194 finished with value: 1.4721542332121846 and parameters: {'k': 1.5110479858229102, 'tau': 0.05833253149776583, 'L': 2.3732193597564546, 'WINDOW': 145, 'threshold': 0.30309996427540314, 'tc': 4.1394470874465725e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:05,233] Trial 195 finished with value: 1.2461506176429227 and parameters: {'k': 1.5149683704581591, 'tau': 0.055081291631743634, 'L': 2.3054803427084893, 'WINDOW': 148, 'threshold': 0.2928380119107944, 'tc': 3.229141532696501e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:05,362] Trial 196 finished with value: 1.0820766466194234 and parameters: {'k': 1.5674429717122034, 'tau': 0.05977778308279367, 'L': 2.2730865706321524, 'WINDOW': 146, 'threshold': 0.27511123506124535, 'tc': 0.00010849999881196314}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:05,477] Trial 197 finished with value: 1.4875871544035135 and parameters: {'k': 1.4947112354115812, 'tau': 0.06660929906675561, 'L': 2.3858580102264177, 'WINDOW': 155, 'threshold': 0.3032970934045253, 'tc': 4.1231551932885055e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:05,604] Trial 198 finished with value: 1.0506769256017823 and parameters: {'k': 1.5271156754433304, 'tau': 0.07722665948177493, 'L': 2.338431865451694, 'WINDOW': 154, 'threshold': 0.2883245805580585, 'tc': 9.211059769175228e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:05,736] Trial 199 finished with value: 1.2899081006121638 and parameters: {'k': 1.503295307830929, 'tau': 0.04215237941200281, 'L': 2.388667259081051, 'WINDOW': 151, 'threshold': 0.32311787347629595, 'tc': 4.19474778703086e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:05,869] Trial 200 finished with value: 1.3856506132700799 and parameters: {'k': 1.4727253142218903, 'tau': 0.05203331923376758, 'L': 2.361972250757723, 'WINDOW': 144, 'threshold': 0.3046486447892567, 'tc': 0.0001286932642139088}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:06,019] Trial 201 finished with value: 1.4040591972089491 and parameters: {'k': 1.4783523088711155, 'tau': 0.06874563816661676, 'L': 2.3809847388277077, 'WINDOW': 157, 'threshold': 0.30382208040283865, 'tc': 7.035263528078106e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:06,165] Trial 202 finished with value: 1.273971593482066 and parameters: {'k': 1.4953881931299822, 'tau': 0.053997587581124044, 'L': 2.3526089965328674, 'WINDOW': 165, 'threshold': 0.30817517095974895, 'tc': 0.0001278094513427083}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:06,329] Trial 203 finished with value: 1.384499217728384 and parameters: {'k': 1.5852081991230877, 'tau': 0.04905467778697488, 'L': 2.288103334074341, 'WINDOW': 157, 'threshold': 0.3056389326468373, 'tc': 8.345047333737562e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:06,464] Trial 204 finished with value: 0.02521560919108094 and parameters: {'k': 1.5687720436175607, 'tau': 0.07427613791128088, 'L': 2.3060737922707264, 'WINDOW': 158, 'threshold': 0.31506704890028786, 'tc': 0.001034182693061703}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:06,622] Trial 205 finished with value: 1.1363017472518468 and parameters: {'k': 1.5358554339121193, 'tau': 0.04087661889149019, 'L': 2.240083277214435, 'WINDOW': 156, 'threshold': 0.28368221699184887, 'tc': 8.379979777902421e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:06,844] Trial 206 finished with value: 1.0224258492044107 and parameters: {'k': 1.4788637011726482, 'tau': 0.06558163748811725, 'L': 2.383891890003895, 'WINDOW': 157, 'threshold': 0.3302510740407618, 'tc': 0.0001566272336888783}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:07,097] Trial 207 finished with value: 1.3927676909596798 and parameters: {'k': 1.5986356328952807, 'tau': 0.049081757044534144, 'L': 2.3828583093008784, 'WINDOW': 162, 'threshold': 0.30277066777263456, 'tc': 0.00010445256473092512}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:07,295] Trial 208 finished with value: 1.0301271900259281 and parameters: {'k': 1.4371599649799214, 'tau': 0.06833244251288728, 'L': 2.3568341702196505, 'WINDOW': 161, 'threshold': 0.31833952707727875, 'tc': 0.00012420966388866463}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:07,415] Trial 209 finished with value: 1.2269264997305307 and parameters: {'k': 1.5411583806073217, 'tau': 0.057179444637073557, 'L': 2.4181194723443813, 'WINDOW': 160, 'threshold': 0.2925863616052327, 'tc': 5.6361899729981475e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:07,531] Trial 210 finished with value: 1.286335026161991 and parameters: {'k': 1.4774431003312378, 'tau': 0.295706853747787, 'L': 2.4765919728741697, 'WINDOW': 144, 'threshold': 0.3022891383357439, 'tc': 0.0001898015294154303}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:07,645] Trial 211 finished with value: 1.3958697354638263 and parameters: {'k': 1.5826588489126916, 'tau': 0.050277618148066534, 'L': 2.2399914790611724, 'WINDOW': 153, 'threshold': 0.3048178015745274, 'tc': 9.343897399045821e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:07,810] Trial 212 finished with value: 1.1805806919319048 and parameters: {'k': 1.5760734175162532, 'tau': 0.03797634569774425, 'L': 2.230432117332567, 'WINDOW': 166, 'threshold': 0.31477347779319914, 'tc': 0.00010659155227004725}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:07,967] Trial 213 finished with value: 1.414005689774843 and parameters: {'k': 1.6357032965990792, 'tau': 0.048508008046845144, 'L': 2.3782610500974855, 'WINDOW': 154, 'threshold': 0.30054632879870335, 'tc': 4.398480334557236e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:08,347] Trial 214 finished with value: 1.183664603948523 and parameters: {'k': 1.637326894245296, 'tau': 0.06431578768111051, 'L': 2.2696056963652724, 'WINDOW': 153, 'threshold': 0.2905106319999949, 'tc': 4.52408431254273e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:08,594] Trial 215 finished with value: 1.2667230396482074 and parameters: {'k': 1.606799023297932, 'tau': 0.043918638628434946, 'L': 2.4097236146962655, 'WINDOW': 155, 'threshold': 0.2985330364994454, 'tc': 6.70884564505131e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:08,817] Trial 216 finished with value: 1.2770852070185015 and parameters: {'k': 1.6320386223974779, 'tau': 0.07223722728572876, 'L': 2.497190515963975, 'WINDOW': 152, 'threshold': 0.2799202113250209, 'tc': 3.680602486557596e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:09,133] Trial 217 finished with value: 1.236944688464323 and parameters: {'k': 1.6665679784793448, 'tau': 0.08264984552220801, 'L': 2.3106132434464635, 'WINDOW': 160, 'threshold': 0.3248618814873274, 'tc': 8.102293747934116e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:09,407] Trial 218 finished with value: 1.3121440426564466 and parameters: {'k': 1.535460295876564, 'tau': 0.04944410157616861, 'L': 2.13917002041018, 'WINDOW': 149, 'threshold': 0.3138362500049566, 'tc': 3.588137176131866e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:09,642] Trial 219 finished with value: 1.0503953175386915 and parameters: {'k': 1.5066863284733323, 'tau': 0.06106615497755872, 'L': 2.3796149145931196, 'WINDOW': 163, 'threshold': 0.2888444335677764, 'tc': 9.828004817305672e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:09,937] Trial 220 finished with value: 1.490716629193747 and parameters: {'k': 1.9589695165254875, 'tau': 0.05068942249123358, 'L': 2.324301370970474, 'WINDOW': 155, 'threshold': 0.30379189593671746, 'tc': 2.0684112040188956e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:10,132] Trial 221 finished with value: 1.4815128264830009 and parameters: {'k': 1.9527052374876466, 'tau': 0.040293826828204975, 'L': 2.3233535405307033, 'WINDOW': 154, 'threshold': 0.30167591684272216, 'tc': 3.166935085303952e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:10,471] Trial 222 finished with value: 0.5356973174794817 and parameters: {'k': 1.956440181777457, 'tau': 0.03413662194327259, 'L': 2.333102252974533, 'WINDOW': 155, 'threshold': 0.3012547634983982, 'tc': 0.0006624184234690193}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:10,828] Trial 223 finished with value: 1.3054698890369811 and parameters: {'k': 1.9429003597340155, 'tau': 0.0485779624622084, 'L': 2.2045119585858663, 'WINDOW': 160, 'threshold': 0.3133622993505673, 'tc': 3.004027133806757e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:11,061] Trial 224 finished with value: 1.165889460443842 and parameters: {'k': 1.8885672433546112, 'tau': 0.04286504142347036, 'L': 2.44441851219989, 'WINDOW': 155, 'threshold': 0.2961762659855231, 'tc': 6.800448469236171e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:11,433] Trial 225 finished with value: 1.3779583701979774 and parameters: {'k': 1.924324002869641, 'tau': 0.06338163137558628, 'L': 2.2708718347877777, 'WINDOW': 149, 'threshold': 0.30704865303821754, 'tc': 3.7845867934618667e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:11,650] Trial 226 finished with value: 1.0851011378249014 and parameters: {'k': 1.8513430344701656, 'tau': 0.03773198612350562, 'L': 2.3494841667260573, 'WINDOW': 154, 'threshold': 0.28616510017782026, 'tc': 3.0840444187156226e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:11,849] Trial 227 finished with value: 1.158859647769087 and parameters: {'k': 1.9776205049343858, 'tau': 0.05640977410052812, 'L': 2.40621099267969, 'WINDOW': 147, 'threshold': 0.29697504188415774, 'tc': 7.354079686884506e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:12,097] Trial 228 finished with value: 1.2822579611445446 and parameters: {'k': 1.963850924846946, 'tau': 0.07592504994992759, 'L': 2.302906241462767, 'WINDOW': 167, 'threshold': 0.32384462075337966, 'tc': 3.797257282576796e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:12,278] Trial 229 finished with value: 1.0942819739186234 and parameters: {'k': 1.9168273298153702, 'tau': 0.021891977853121806, 'L': 2.2368070321671354, 'WINDOW': 163, 'threshold': 0.27178324530463926, 'tc': 0.00011426197025185539}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:12,437] Trial 230 finished with value: 1.404566398411448 and parameters: {'k': 1.595004811775023, 'tau': 0.05063690652873229, 'L': 2.440951720329028, 'WINDOW': 158, 'threshold': 0.30863350940268897, 'tc': 3.7185810712614605e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:12,614] Trial 231 finished with value: 1.4012167733577001 and parameters: {'k': 1.5953908536091943, 'tau': 0.05554535569368972, 'L': 2.4552126701593173, 'WINDOW': 153, 'threshold': 0.3088129452599135, 'tc': 4.0031691318446455e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:12,797] Trial 232 finished with value: 1.2995122392426828 and parameters: {'k': 1.5966401183921928, 'tau': 0.05988254253176706, 'L': 2.45469295604401, 'WINDOW': 158, 'threshold': 0.3146568929319147, 'tc': 1.617987319256728e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:13,063] Trial 233 finished with value: 1.4531381441266846 and parameters: {'k': 1.557361359766962, 'tau': 0.05254105724936132, 'L': 2.5168153970819813, 'WINDOW': 157, 'threshold': 0.3046493633133537, 'tc': 7.533830658171318e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:13,357] Trial 234 finished with value: 1.1909024618143855 and parameters: {'k': 1.55520276405966, 'tau': 0.06887445487668231, 'L': 2.501009789783432, 'WINDOW': 156, 'threshold': 0.3172326034121829, 'tc': 4.6062795340068396e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:13,472] Trial 235 finished with value: 1.2459922303191233 and parameters: {'k': 1.5592837585793422, 'tau': 0.037401265021868246, 'L': 2.511411559085217, 'WINDOW': 153, 'threshold': 0.2951717392875576, 'tc': 7.102260304716847e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:13,631] Trial 236 finished with value: 1.3663284065266794 and parameters: {'k': 1.5157101091912328, 'tau': 0.05285387367818004, 'L': 2.4487979288753055, 'WINDOW': 150, 'threshold': 0.30995090435946754, 'tc': 3.188975447857151e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:13,747] Trial 237 finished with value: 1.3058420767357906 and parameters: {'k': 1.8630035678976793, 'tau': 0.03232725622808845, 'L': 2.543471644140894, 'WINDOW': 158, 'threshold': 0.32825844865260984, 'tc': 1.4774267368265676e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:13,899] Trial 238 finished with value: -0.7088772279342909 and parameters: {'k': 1.99972362712156, 'tau': 0.4096919152197622, 'L': 2.435081589569225, 'WINDOW': 151, 'threshold': 0.28662060753236746, 'tc': 0.0013788160626192155}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:14,077] Trial 239 finished with value: 1.4083688943597796 and parameters: {'k': 1.6173707820341576, 'tau': 0.05050225657541399, 'L': 2.3341919240375484, 'WINDOW': 153, 'threshold': 0.3047223148862945, 'tc': 8.357596691152476e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:14,198] Trial 240 finished with value: 1.0953454900241153 and parameters: {'k': 1.6987254818089312, 'tau': 0.06918188477998197, 'L': 2.381839043575241, 'WINDOW': 158, 'threshold': 0.31945807803249593, 'tc': 5.932225742047298e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:14,341] Trial 241 finished with value: 1.3782582158645844 and parameters: {'k': 1.6172858257965723, 'tau': 0.048025493320033294, 'L': 2.3151608262967023, 'WINDOW': 154, 'threshold': 0.30511595737431474, 'tc': 8.689279179698694e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:14,460] Trial 242 finished with value: 1.1174735601483714 and parameters: {'k': 1.5827128940755144, 'tau': 0.05773909519678276, 'L': 2.3431081594308507, 'WINDOW': 152, 'threshold': 0.2973162989602609, 'tc': 0.0001393297496868471}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:14,602] Trial 243 finished with value: 1.4054544763424734 and parameters: {'k': 1.5296801756521006, 'tau': 0.040722203518088015, 'L': 2.2699329367375767, 'WINDOW': 156, 'threshold': 0.3083903713708238, 'tc': 3.787446266589251e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:14,762] Trial 244 finished with value: 1.2967017943739527 and parameters: {'k': 1.51316000616901, 'tau': 0.04051626510670524, 'L': 2.426634524832519, 'WINDOW': 157, 'threshold': 0.31204177430205654, 'tc': 3.57594531401067e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:14,908] Trial 245 finished with value: 1.1793495634967657 and parameters: {'k': 1.4482069593202531, 'tau': 0.029785717962572313, 'L': 2.2937609307371107, 'WINDOW': 160, 'threshold': 0.2892036581856602, 'tc': 3.422367556331711e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,036] Trial 246 finished with value: 1.381850046044569 and parameters: {'k': 1.5356818935397925, 'tau': 0.06400426792230385, 'L': 2.479396091452459, 'WINDOW': 156, 'threshold': 0.29942368582255113, 'tc': 1.570556032767735e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,162] Trial 247 finished with value: 1.329129020516605 and parameters: {'k': 1.6638104455678102, 'tau': 0.08327167707522432, 'L': 2.38983362252228, 'WINDOW': 148, 'threshold': 0.3101678228989905, 'tc': 6.093374078531804e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,312] Trial 248 finished with value: 1.1799362846462769 and parameters: {'k': 1.4897066886473649, 'tau': 0.04215974317604008, 'L': 2.584782729592481, 'WINDOW': 141, 'threshold': 0.3347654355249948, 'tc': 7.548608804777297e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,429] Trial 249 finished with value: 1.0354460661884888 and parameters: {'k': 1.9557229426716498, 'tau': 0.05460340957733646, 'L': 2.348593299591475, 'WINDOW': 134, 'threshold': 0.3193367503472473, 'tc': 0.00011224780062954169}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,550] Trial 250 finished with value: 1.1851317738718503 and parameters: {'k': 1.5484162778162325, 'tau': 0.276272257417109, 'L': 2.159319879952864, 'WINDOW': 150, 'threshold': 0.28119628438239963, 'tc': 3.2650820225004176e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,676] Trial 251 finished with value: 1.3130253826736302 and parameters: {'k': 1.6351817967558537, 'tau': 0.06991366169214415, 'L': 2.2752747657322967, 'WINDOW': 154, 'threshold': 0.298788912104241, 'tc': 3.5445867820585735e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,820] Trial 252 finished with value: 1.366232015472819 and parameters: {'k': 1.4385550730302765, 'tau': 0.04347593414554463, 'L': 2.530850676546823, 'WINDOW': 145, 'threshold': 0.30977688313635426, 'tc': 8.028416754404214e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:15,939] Trial 253 finished with value: 1.1001542149153112 and parameters: {'k': 1.4877202882918663, 'tau': 0.05646623961286014, 'L': 0.5325383108697508, 'WINDOW': 130, 'threshold': 0.2938185373517569, 'tc': 0.00015592770373270223}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,060] Trial 254 finished with value: 1.2165202555875896 and parameters: {'k': 1.9260792148951718, 'tau': 0.2536716821946478, 'L': 2.40778121950537, 'WINDOW': 159, 'threshold': 0.32603766761809094, 'tc': 6.700873318848052e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,182] Trial 255 finished with value: 1.4869421830783172 and parameters: {'k': 1.5608183829825617, 'tau': 0.033820372057120345, 'L': 2.629049429572551, 'WINDOW': 147, 'threshold': 0.30594783146613536, 'tc': 3.1932533600377634e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,315] Trial 256 finished with value: 1.2598922074361447 and parameters: {'k': 1.5196932251690232, 'tau': 0.025591691350326472, 'L': 1.8584319531255549, 'WINDOW': 142, 'threshold': 0.2920602759369216, 'tc': 5.36903817664664e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,433] Trial 257 finished with value: 1.0914446462291048 and parameters: {'k': 1.5559977020516673, 'tau': 0.03686473925915884, 'L': 2.20180707656451, 'WINDOW': 147, 'threshold': 0.3180072772764239, 'tc': 0.0001138529764586401}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,554] Trial 258 finished with value: 0.755180215101167 and parameters: {'k': 1.4242917603159089, 'tau': 0.13699231864117198, 'L': 2.618260477966819, 'WINDOW': 144, 'threshold': 0.3031280017575458, 'tc': 0.0005636371491244384}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,700] Trial 259 finished with value: 1.2523805241784507 and parameters: {'k': 1.465584026112104, 'tau': 0.23659697459640897, 'L': 2.636629806913581, 'WINDOW': 137, 'threshold': 0.27777828356175177, 'tc': 1.0378422470787699e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,837] Trial 260 finished with value: 0.9953168834753501 and parameters: {'k': 1.8966774412854805, 'tau': 0.43818107778440046, 'L': 1.4880080733059748, 'WINDOW': 134, 'threshold': 0.28587234684363966, 'tc': 8.299928066471503e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:16,964] Trial 261 finished with value: 1.477537248015997 and parameters: {'k': 1.4949628508066901, 'tau': 0.12548311153470143, 'L': 2.2826596877537284, 'WINDOW': 150, 'threshold': 0.3025855743357363, 'tc': 4.153091600445284e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:17,100] Trial 262 finished with value: 1.167636825938237 and parameters: {'k': 1.5378020347166972, 'tau': 0.12872739314817505, 'L': 2.2522362647247136, 'WINDOW': 147, 'threshold': 0.3206534880800552, 'tc': 9.383655872019064e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:17,242] Trial 263 finished with value: 1.285644322351602 and parameters: {'k': 1.9713168083274073, 'tau': 0.021601461562814187, 'L': 2.312275453619907, 'WINDOW': 150, 'threshold': 0.2951917175408556, 'tc': 3.98358787686453e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:17,364] Trial 264 finished with value: 1.2211779744328577 and parameters: {'k': 1.5054992240978455, 'tau': 0.12160338948874275, 'L': 2.335737151231021, 'WINDOW': 141, 'threshold': 0.310907276449232, 'tc': 0.00013549372481351827}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:17,486] Trial 265 finished with value: 1.4685056356515966 and parameters: {'k': 1.6141220811834083, 'tau': 0.11314695524468542, 'L': 2.198720161768426, 'WINDOW': 145, 'threshold': 0.3016742261343339, 'tc': 4.421550662379936e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:17,608] Trial 266 finished with value: -1.4408432600917085 and parameters: {'k': 1.6306989671847363, 'tau': 0.11868304535132339, 'L': 2.1293595555400784, 'WINDOW': 145, 'threshold': 0.2906289584323927, 'tc': 0.0019005872119897216}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:17,736] Trial 267 finished with value: 1.1341159531687754 and parameters: {'k': 1.132923458458425, 'tau': 0.12980570819003084, 'L': 2.2058222885747645, 'WINDOW': 141, 'threshold': 0.3325481411613791, 'tc': 0.00010272890187783917}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:17,884] Trial 268 finished with value: 1.3733434474205068 and parameters: {'k': 1.5619514533813854, 'tau': 0.1450176790290882, 'L': 2.692001167357316, 'WINDOW': 148, 'threshold': 0.3000818666665798, 'tc': 6.109319341504949e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,001] Trial 269 finished with value: 1.2060164496668424 and parameters: {'k': 1.4511810227543342, 'tau': 0.10772148615501989, 'L': 2.2641777178837326, 'WINDOW': 143, 'threshold': 0.2807264161252239, 'tc': 2.93163466859557e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,114] Trial 270 finished with value: 1.0335407229117843 and parameters: {'k': 1.7394418353854684, 'tau': 0.1381485926738666, 'L': 2.1784318172471786, 'WINDOW': 139, 'threshold': 0.3198868938981101, 'tc': 0.00010610605754535183}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,228] Trial 271 finished with value: 1.3719027834857078 and parameters: {'k': 1.5036922796244536, 'tau': 0.11315803333144678, 'L': 2.0588997252288657, 'WINDOW': 131, 'threshold': 0.30035670917675705, 'tc': 4.522900547397709e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,352] Trial 272 finished with value: -0.19996519690427061 and parameters: {'k': 1.6602750966466948, 'tau': 0.03125869561350383, 'L': 2.2443833072595876, 'WINDOW': 150, 'threshold': 0.29097868187522286, 'tc': 0.0011166805463378481}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,488] Trial 273 finished with value: 1.1354389224130121 and parameters: {'k': 1.8004947551650674, 'tau': 0.09961766408075873, 'L': 2.107880382563269, 'WINDOW': 146, 'threshold': 0.31202171377879095, 'tc': 0.00016288985087028698}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,615] Trial 274 finished with value: 1.2315762291366872 and parameters: {'k': 1.9983814670965694, 'tau': 0.33814579219680924, 'L': 2.3112910157545854, 'WINDOW': 136, 'threshold': 0.27003596235643573, 'tc': 8.210056073555628e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,747] Trial 275 finished with value: -0.714808928806881 and parameters: {'k': 1.531036119279617, 'tau': 0.3588089962012195, 'L': 2.2075248556555715, 'WINDOW': 71, 'threshold': 0.45700648613740763, 'tc': 3.4677805734830635e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:18,899] Trial 276 finished with value: 1.3863830177374628 and parameters: {'k': 1.6114113644904187, 'tau': 0.13789274343649313, 'L': 1.8196659378360953, 'WINDOW': 144, 'threshold': 0.30338451697909646, 'tc': 0.000131844535526159}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:19,037] Trial 277 finished with value: 1.2152132829219267 and parameters: {'k': 1.9466025942523746, 'tau': 0.12024106331923548, 'L': 1.8710048510378126, 'WINDOW': 81, 'threshold': 0.32575795510808514, 'tc': 7.160744172753257e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:19,151] Trial 278 finished with value: 1.088981170163522 and parameters: {'k': 1.7008412079387043, 'tau': 0.12884795882354652, 'L': 2.2802935507456965, 'WINDOW': 104, 'threshold': 0.2850342609616757, 'tc': 4.193095202052028e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:19,299] Trial 279 finished with value: 1.200521089909585 and parameters: {'k': 1.4060444743674734, 'tau': 0.035158168798034586, 'L': 2.348019259264481, 'WINDOW': 38, 'threshold': 0.3150005542153996, 'tc': 0.00010848283729850903}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:19,501] Trial 280 finished with value: 1.2195861742232588 and parameters: {'k': 1.566163922728529, 'tau': 0.31372163736828, 'L': 1.9902410428212387, 'WINDOW': 150, 'threshold': 0.2941456381762742, 'tc': 3.7764200678591345e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:19,630] Trial 281 finished with value: 1.4790289413905906 and parameters: {'k': 1.912466356680766, 'tau': 0.1564721401222705, 'L': 1.9297833204462285, 'WINDOW': 139, 'threshold': 0.30518921750886147, 'tc': 1.579581372638772e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:52:19,794] Trial 282 finished with value: 1.513623412078413 and parameters: {'k': 1.922005246265828, 'tau': 0.14991601313493008, 'L': 1.9097757869986336, 'WINDOW': 138, 'threshold': 0.3017275946538778, 'tc': 6.965846925923312e-07}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:19,964] Trial 283 finished with value: 1.25738377938459 and parameters: {'k': 1.8987799954558566, 'tau': 0.14077275909549966, 'L': 1.871822560849272, 'WINDOW': 138, 'threshold': 0.27759586670495123, 'tc': 1.3072821487754326e-06}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:20,126] Trial 284 finished with value: 1.328651553632188 and parameters: {'k': 1.8719531756679195, 'tau': 0.15856092976152414, 'L': 1.945172001632794, 'WINDOW': 134, 'threshold': 0.2951316781748889, 'tc': 6.010499344854084e-06}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:20,256] Trial 285 finished with value: 1.1669762751452104 and parameters: {'k': 1.9236739159980083, 'tau': 0.15368047305916344, 'L': 1.971974420336945, 'WINDOW': 139, 'threshold': 0.3204854092497442, 'tc': 2.5702770898503025e-07}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:20,426] Trial 286 finished with value: 1.0427146268688394 and parameters: {'k': 1.9342921765434316, 'tau': 0.14927677510690535, 'L': 1.911553555688372, 'WINDOW': 141, 'threshold': 0.2854665620845572, 'tc': 7.145945707318847e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:20,618] Trial 287 finished with value: 1.4230020175901226 and parameters: {'k': 1.9697040158563193, 'tau': 0.1575392465609719, 'L': 2.0169909188054036, 'WINDOW': 128, 'threshold': 0.3008441280727215, 'tc': 4.554533754026985e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:20,740] Trial 288 finished with value: 1.2805711882410777 and parameters: {'k': 1.9693623607725916, 'tau': 0.1622473199270234, 'L': 1.9250692028009249, 'WINDOW': 128, 'threshold': 0.3340047666729248, 'tc': 6.348969753414449e-08}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:20,876] Trial 289 finished with value: 1.2684439865238069 and parameters: {'k': 1.8845209415551438, 'tau': 0.1577973310482051, 'L': 2.078897081578937, 'WINDOW': 131, 'threshold': 0.3138849647475057, 'tc': 6.424076710062394e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:20,994] Trial 290 finished with value: 1.3880424602331576 and parameters: {'k': 1.939684906041144, 'tau': 0.17258186996711644, 'L': 2.0434736568845193, 'WINDOW': 127, 'threshold': 0.3025906311324265, 'tc': 0.00011169815452572436}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:21,130] Trial 291 finished with value: 1.2378478855290298 and parameters: {'k': 1.9826205405632793, 'tau': 0.14718391047581217, 'L': 1.974933677911318, 'WINDOW': 135, 'threshold': 0.293417114780752, 'tc': 3.6595021439424506e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:21,248] Trial 292 finished with value: 1.3112086954231412 and parameters: {'k': 1.9572795186145733, 'tau': 0.13284418739394854, 'L': 2.019699094030156, 'WINDOW': 131, 'threshold': 0.31022420866501477, 'tc': 7.492541902690735e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:21,361] Trial 293 finished with value: 0.8007099620821486 and parameters: {'k': 1.8299081636633705, 'tau': 0.26345227886290146, 'L': 1.9201561175686042, 'WINDOW': 134, 'threshold': 0.3254214111068122, 'tc': 0.0004206212427772494}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:21,486] Trial 294 finished with value: 0.9971156663949851 and parameters: {'k': 1.9068918437428033, 'tau': 0.2163278083115829, 'L': 1.8390304504231638, 'WINDOW': 137, 'threshold': 0.2843628553593231, 'tc': 0.00014489684376093383}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:21,602] Trial 295 finished with value: 1.1613029444506913 and parameters: {'k': 1.9103413653389947, 'tau': 0.16241759758469212, 'L': 2.001710509908904, 'WINDOW': 122, 'threshold': 0.3400322114009389, 'tc': 3.477514718121909e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:21,752] Trial 296 finished with value: 0.45017153520112113 and parameters: {'k': 0.9739413552169944, 'tau': 0.28774508437698776, 'L': 2.556427490443195, 'WINDOW': 142, 'threshold': 0.3014232905074292, 'tc': 0.0007099327846645054}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:21,882] Trial 297 finished with value: 1.1903968670021792 and parameters: {'k': 1.8643715720527154, 'tau': 0.14886894918314067, 'L': 1.7945934406896875, 'WINDOW': 139, 'threshold': 0.273453461722376, 'tc': 5.353142018025094e-07}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:22,047] Trial 298 finished with value: 0.7612235427520256 and parameters: {'k': 1.968839873912878, 'tau': 0.140635384327872, 'L': 1.887256925127521, 'WINDOW': 128, 'threshold': 0.2602884332623745, 'tc': 9.731239234663983e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:52:22,233] Trial 299 finished with value: 1.2513614459409546 and parameters: {'k': 1.9388994269675077, 'tau': 0.12454704953480601, 'L': 2.636805802988311, 'WINDOW': 132, 'threshold': 0.3167192621206619, 'tc': 5.5563625841787895e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[OK] Optuna done. best_value (robust score): 1.513623412078413\n",
            "best params -> {'k': 1.922005246265828, 'tau': 0.14991601313493008, 'L': 1.9097757869986336, 'WINDOW': 138, 'threshold': 0.3017275946538778, 'tc': 6.965846925923312e-07, 'best_value': 1.513623412078413}\n",
            "[SAVED] final vol-scaled positions -> /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_volscaled_robust_1764903142.csv\n",
            "Full-sample ann Sharpe (net tc): 0.6153593038705103\n",
            "[DONE] Volatility-Scaled SRC optimizer (robust sharpe) finished.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ONE-CELL — A+ SRC OPTIMIZER (Robust Sharpe aggregator, CPCV-safe)\n",
        "import os, time, json, pickle, warnings, math\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "RSTATE = 42\n",
        "N_TRIALS = 300\n",
        "N_OUTER = 5\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "LABEL_H = \"h8\"\n",
        "H = 8\n",
        "ANNUALIZE = 252 * 24\n",
        "MIN_POS_OBS = 30\n",
        "DEFAULT_TC = 5e-4\n",
        "TARGET_VOL = 0.20\n",
        "\n",
        "# Robust aggregator params\n",
        "TRIM_FRAC = 0.20          # fraction to trim from both tails when computing trimmed mean\n",
        "SCALE_COVERAGE_PENALTY = True\n",
        "COVERAGE_REF = MIN_POS_OBS * 1.5\n",
        "\n",
        "# Safety clamps\n",
        "SCALE_CLAMP = (0.1, 3.0)  # clamp for volatility scaling factor to avoid extreme leverage\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def load_latest_meta(path=OUT_DIR):\n",
        "    pkl = sorted(Path(path).glob(\"df_meta_shortlist.v*.pkl\"), key=os.path.getmtime)\n",
        "    if not pkl:\n",
        "        raise FileNotFoundError(\"df_meta_shortlist.v*.pkl not found in OUT_DIR\")\n",
        "    df = pd.read_pickle(str(pkl[-1]))\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    return df, str(pkl[-1])\n",
        "\n",
        "def load_latest_signals(path=OUT_DIR):\n",
        "    sig_files = list(Path(path).glob(\"*signals_aligned*.csv\")) + list(Path(path).glob(\"*signals*.csv\"))\n",
        "    if not sig_files:\n",
        "        raise FileNotFoundError(\"signals CSV not found in OUT_DIR (need meta signals)\")\n",
        "    sig_path = sorted(sig_files, key=os.path.getmtime)[-1]\n",
        "    sig = pd.read_csv(sig_path, index_col=0)\n",
        "    try:\n",
        "        sig.index = pd.to_datetime(sig.index, utc=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return sig, str(sig_path)\n",
        "\n",
        "def load_step03_returns(path=OUT_DIR, prefer_h=H):\n",
        "    candidates = sorted(Path(path).glob(\"df_step03_tb_multi*.csv\"), key=os.path.getmtime)\n",
        "    if not candidates:\n",
        "        return None, None\n",
        "    df_step = pd.read_csv(str(candidates[-1]), index_col=None)\n",
        "    tbreak_candidates = [c for c in df_step.columns if c.startswith(\"tb_t_break\")]\n",
        "    ret_candidates = [c for c in df_step.columns if c.startswith(\"tb_ret\")]\n",
        "    if not tbreak_candidates or not ret_candidates:\n",
        "        return None, None\n",
        "    tb_col = f\"tb_t_break_h{prefer_h}\"\n",
        "    ret_col = f\"tb_ret_at_break_h{prefer_h}\"\n",
        "    tb_step_col = tb_col if tb_col in df_step.columns else tbreak_candidates[0]\n",
        "    ret_step_col = ret_col if ret_col in df_step.columns else ret_candidates[0]\n",
        "    return df_step, (tb_step_col, ret_step_col)\n",
        "\n",
        "# ---------- volatility-scaled SRC core ----------\n",
        "def compute_positions_vol_scaled(params, pL, pS, vol_series, rets_series, target_vol=TARGET_VOL):\n",
        "    \"\"\"\n",
        "    Vol-scaled SRC with causal vol estimates and safety clamps.\n",
        "    Inputs are pandas.Series aligned to same index.\n",
        "    \"\"\"\n",
        "    raw = (pL - pS).astype(float)\n",
        "    tau = float(params[\"tau\"])\n",
        "    raw_s = np.tanh(raw / tau)  # S-curve squash\n",
        "    # defensive vol handling (causal)\n",
        "    vol_series = vol_series.replace(0, np.nan).fillna(method=\"ffill\").fillna(vol_series.mean() if vol_series.mean() > 0 else 1e-6)\n",
        "    pos_u = float(params[\"k\"]) * (raw_s / vol_series)  # unit-scaled position (array-like / Series)\n",
        "    w = max(1, int(params.get(\"WINDOW\", 24)))\n",
        "    # realized ex-post vol estimate of unit-strategy; use past-only window and shift to be causal\n",
        "    realized_vol = ((pos_u * rets_series).rolling(window=w, min_periods=1).std().shift(1) * math.sqrt(ANNUALIZE)).fillna(method=\"ffill\")\n",
        "    realized_vol = realized_vol.replace(0, np.nan).fillna(realized_vol.mean() if np.isfinite(realized_vol.mean()) else 1e-6)\n",
        "    # scaling factor (annualized target_vol / realized_vol). Clip to safe range.\n",
        "    scale = (target_vol / realized_vol).clip(SCALE_CLAMP[0], SCALE_CLAMP[1])\n",
        "    pos_scaled = pos_u * scale\n",
        "    pos_final = pos_scaled.clip(-params[\"L\"], params[\"L\"])\n",
        "    return pos_final\n",
        "\n",
        "# ---------- fold evaluation (robust sharpe aggregator) ----------\n",
        "def evaluate_params_on_folds(params, p_long_series, p_short_series, rets_series, outer_splits, pos_all, idx, target_vol=TARGET_VOL):\n",
        "    fold_sharpes = []\n",
        "    fold_winrates = []\n",
        "    fold_counts = []\n",
        "    for train_pos, test_pos in outer_splits:\n",
        "        train_ts = pos_all.index[train_pos]\n",
        "        test_ts = pos_all.index[test_pos]\n",
        "        if len(test_ts) == 0:\n",
        "            continue\n",
        "        # causal vol computed on full rets but shifted (past-only)\n",
        "        vol_series_full = rets_series.rolling(window=params[\"WINDOW\"], min_periods=max(1, params[\"WINDOW\"]//2)).std().shift(1)\n",
        "        vol_series = vol_series_full.reindex(test_ts).fillna(method=\"ffill\")\n",
        "        if vol_series.isna().all():\n",
        "            # fallback to train vol\n",
        "            train_vol = rets_series.loc[train_ts].std()\n",
        "            vol_series = pd.Series(train_vol if np.isfinite(train_vol) else 1e-6, index=test_ts)\n",
        "        # thresholding (causal reindex)\n",
        "        if params[\"threshold\"] > 0:\n",
        "            raw_test = (p_long_series - p_short_series).reindex(test_ts)\n",
        "            mask = raw_test.abs() < params[\"threshold\"]\n",
        "            pL_test = p_long_series.reindex(test_ts).copy()\n",
        "            pS_test = p_short_series.reindex(test_ts).copy()\n",
        "            pL_test[mask] = 0.5\n",
        "            pS_test[mask] = 0.5\n",
        "        else:\n",
        "            pL_test = p_long_series.reindex(test_ts)\n",
        "            pS_test = p_short_series.reindex(test_ts)\n",
        "        # compute positions (vol-scaled)\n",
        "        pos = compute_positions_vol_scaled(params, pL_test, pS_test, vol_series, rets_series.reindex(test_ts), target_vol=target_vol).values\n",
        "        rets = rets_series.reindex(test_ts).fillna(0.0).values\n",
        "        pnl = pos * rets\n",
        "        deltas = np.empty_like(pos); deltas[0] = pos[0]; deltas[1:] = pos[1:] - pos[:-1]\n",
        "        tc_costs = float(params.get(\"tc\", DEFAULT_TC)) * np.abs(deltas)\n",
        "        net = pnl - tc_costs\n",
        "        # fold sharpe\n",
        "        if np.nanstd(net) == 0 or np.allclose(net, 0):\n",
        "            sharpe = 0.0\n",
        "        else:\n",
        "            sharpe = (np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)\n",
        "        non_zero = int((np.abs(pos) > 1e-6).sum())\n",
        "        # penalize tiny-sample folds\n",
        "        if non_zero < MIN_POS_OBS:\n",
        "            sharpe *= 0.5\n",
        "        fold_sharpes.append(float(sharpe))\n",
        "        fold_counts.append(non_zero)\n",
        "        # fold winrate diagnostic\n",
        "        nonzero_mask = np.abs(pos) > 1e-6\n",
        "        n_trades = int(nonzero_mask.sum())\n",
        "        if n_trades > 0:\n",
        "            wins = int(((pnl > 0) & nonzero_mask).sum())\n",
        "            winrate = wins / n_trades\n",
        "        else:\n",
        "            winrate = np.nan\n",
        "        fold_winrates.append(winrate)\n",
        "    if not fold_sharpes:\n",
        "        return 0.0, [], float(\"nan\")\n",
        "    fold_sharpes = np.array(fold_sharpes, dtype=float)\n",
        "    # robust aggregator: trimmed mean with median fallback\n",
        "    if len(fold_sharpes) >= 3:\n",
        "        trim = int(np.floor(TRIM_FRAC * len(fold_sharpes)))\n",
        "        sorted_sharpes = np.sort(fold_sharpes)\n",
        "        if len(sorted_sharpes) - 2*trim > 0:\n",
        "            robust_sharpe = float(np.mean(sorted_sharpes[trim:len(sorted_sharpes)-trim]))\n",
        "        else:\n",
        "            robust_sharpe = float(np.median(sorted_sharpes))\n",
        "    else:\n",
        "        robust_sharpe = float(np.median(fold_sharpes))\n",
        "    # aggregate winrate (median across folds)\n",
        "    valid_winrates = np.array([v for v in fold_winrates if not np.isnan(v)], dtype=float)\n",
        "    agg_winrate = float(np.median(valid_winrates)) if len(valid_winrates) > 0 else float(\"nan\")\n",
        "    return robust_sharpe, fold_counts, agg_winrate\n",
        "\n",
        "# ---------- main ----------\n",
        "if __name__ == \"__main__\":\n",
        "    df, meta_p = load_latest_meta(OUT_DIR)\n",
        "    sig, sig_p = load_latest_signals(OUT_DIR)\n",
        "    print(f\"[INFO] loaded df_meta_shortlist: {Path(meta_p).name} rows={df.shape[0]}\")\n",
        "    print(f\"[INFO] using signals: {Path(sig_p).name}\")\n",
        "\n",
        "    # pick calibrated probs preferred\n",
        "    if \"p_meta_long_cal\" in sig.columns and \"p_meta_short_cal\" in sig.columns:\n",
        "        p_long = sig[\"p_meta_long_cal\"].astype(float)\n",
        "        p_short = sig[\"p_meta_short_cal\"].astype(float)\n",
        "    elif \"p_meta_long\" in sig.columns and \"p_meta_short\" in sig.columns:\n",
        "        p_long = sig[\"p_meta_long\"].astype(float)\n",
        "        p_short = sig[\"p_meta_short\"].astype(float)\n",
        "    elif \"p_long\" in sig.columns and \"p_short\" in sig.columns:\n",
        "        p_long = sig[\"p_long\"].astype(float)\n",
        "        p_short = sig[\"p_short\"].astype(float)\n",
        "    else:\n",
        "        raise RuntimeError(\"No suitable probability columns found (p_meta_long_cal / p_meta_long / p_long)\")\n",
        "\n",
        "    p_long = p_long.reindex(df.index).fillna(0.5)\n",
        "    p_short = p_short.reindex(df.index).fillna(0.5)\n",
        "\n",
        "    # ensure returns exist; map from STEP03 if needed\n",
        "    ret_col = f\"tb_ret_{H}\"\n",
        "    if ret_col not in df.columns:\n",
        "        df_step, cols = load_step03_returns(OUT_DIR, prefer_h=H)\n",
        "        if df_step is None:\n",
        "            raise RuntimeError(f\"{ret_col} missing and no step03 CSV found.\")\n",
        "        tb_step_col, ret_step_col = cols\n",
        "        df_step[tb_step_col] = pd.to_datetime(df_step[tb_step_col], utc=True, errors=\"coerce\")\n",
        "        map_ser = df_step.groupby(tb_step_col)[ret_step_col].mean()\n",
        "        tb_meta_col = f\"tb_t_break_h{H}\" if f\"tb_t_break_h{H}\" in df.columns else None\n",
        "        if tb_meta_col is None:\n",
        "            raise RuntimeError(f\"STEP03 exists but df_meta_shortlist has no tb_t_break_h{H} to map returns.\")\n",
        "        tb_idx = pd.to_datetime(df[tb_meta_col], utc=True, errors=\"coerce\")\n",
        "        mapped_rets = tb_idx.map(map_ser).fillna(0.0).astype(float)\n",
        "        df[ret_col] = mapped_rets.values\n",
        "        out_patched = Path(OUT_DIR) / (Path(meta_p).name + f\".with_tb_ret_{H}.pkl\")\n",
        "        pd.to_pickle(df, out_patched)\n",
        "        print(f\"[DONE] merged {ret_col} into df_meta -> {out_patched}\")\n",
        "    else:\n",
        "        print(f\"[INFO] using existing returns column: {ret_col}\")\n",
        "\n",
        "    rets = df[ret_col].reindex(df.index).astype(float).fillna(0.0)\n",
        "\n",
        "    # purge utils (must be present)\n",
        "    try:\n",
        "        from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"Paste patched b0_07_purge_utils into notebook before running.\")\n",
        "\n",
        "    idx = df.index\n",
        "    exp_all = compute_exposure_intervals(idx, df[f\"tb_t_break_{LABEL_H}\"], horizon_fallback=None, last_index=idx[-1])\n",
        "    pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "    outer_splits = list(purged_cv_splits(pos_all, n_splits=N_OUTER, embargo=EMBARGO, index=idx, drop_unmapped=True, random_state=RSTATE))\n",
        "    if len(outer_splits) < N_OUTER:\n",
        "        print(f\"[WARN] produced fewer outer splits ({len(outer_splits)}) than requested N_OUTER={N_OUTER}\")\n",
        "    print(f\"[INFO] outer_splits: {len(outer_splits)}\")\n",
        "\n",
        "    # Optuna objective (robust sharpe)\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"k\": trial.suggest_float(\"k\", 0.2, 2.0),\n",
        "            \"tau\": trial.suggest_float(\"tau\", 0.02, 0.5),\n",
        "            \"L\": trial.suggest_float(\"L\", 0.5, 3.0),\n",
        "            \"WINDOW\": trial.suggest_int(\"WINDOW\", 24, 168),\n",
        "            \"threshold\": trial.suggest_float(\"threshold\", 0.0, 0.5),\n",
        "            \"tc\": trial.suggest_float(\"tc\", 0.0, 0.002),\n",
        "        }\n",
        "        robust_sharpe, fold_counts, agg_winrate = evaluate_params_on_folds(params, p_long, p_short, rets, outer_splits, pos_all, idx, target_vol=TARGET_VOL)\n",
        "        mean_coverage = np.mean(fold_counts) if fold_counts else 0.0\n",
        "        coverage_reg = min(1.0, mean_coverage / COVERAGE_REF) if SCALE_COVERAGE_PENALTY else 1.0\n",
        "        score = float(robust_sharpe * coverage_reg)\n",
        "        # record attrs for diagnostics\n",
        "        trial.set_user_attr(\"coverage_mean\", float(mean_coverage))\n",
        "        trial.set_user_attr(\"raw_robust_sharpe\", float(robust_sharpe))\n",
        "        trial.set_user_attr(\"agg_winrate\", float(agg_winrate) if not np.isnan(agg_winrate) else None)\n",
        "        return score\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RSTATE))\n",
        "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=1)\n",
        "\n",
        "    ts = int(time.time())\n",
        "    best = deepcopy(study.best_trial.params)\n",
        "    best[\"best_value\"] = float(study.best_value)\n",
        "    best_path = os.path.join(OUT_DIR, f\"A_SRC_optuna_best_params_{ts}.json\")\n",
        "    pickle.dump(study, open(os.path.join(OUT_DIR, f\"A_SRC_optuna_study_{ts}.pkl\"), \"wb\"))\n",
        "    with open(best_path, \"w\") as f:\n",
        "        json.dump(best, f, indent=2)\n",
        "    print(\"[OK] Optuna done. best_value (robust score):\", study.best_value)\n",
        "    print(\"best params ->\", best)\n",
        "\n",
        "    # build full-sample vol-scaled positions with best params (causal vol)\n",
        "    best_params = deepcopy(study.best_trial.params)\n",
        "    if best_params.get(\"threshold\", 0.0) > 0:\n",
        "        raw_full = (p_long - p_short)\n",
        "        mask_full = raw_full.abs() < best_params[\"threshold\"]\n",
        "        pL_full = p_long.copy(); pS_full = p_short.copy()\n",
        "        pL_full[mask_full] = 0.5; pS_full[mask_full] = 0.5\n",
        "    else:\n",
        "        pL_full, pS_full = p_long.copy(), p_short.copy()\n",
        "\n",
        "    vol_full = rets.rolling(window=best_params[\"WINDOW\"], min_periods=max(1, best_params[\"WINDOW\"]//2)).std().shift(1)\n",
        "    vol_full = vol_full.fillna(method=\"ffill\").fillna(rets.std() if np.isfinite(rets.std()) else 1e-6)\n",
        "\n",
        "    positions = compute_positions_vol_scaled(best_params, pL_full, pS_full, vol_full, rets, target_vol=TARGET_VOL)\n",
        "    positions = positions.clip(-best_params[\"L\"], best_params[\"L\"])\n",
        "\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out[\"p_long\"] = p_long\n",
        "    out[\"p_short\"] = p_short\n",
        "    out[\"raw\"] = p_long - p_short\n",
        "    out[\"position\"] = positions\n",
        "    out[\"vol\"] = vol_full\n",
        "    out_path = os.path.join(OUT_DIR, f\"A_SRC_positions_volscaled_robust_{ts}.csv\")\n",
        "    out.to_csv(out_path)\n",
        "\n",
        "    # diagnostics full-sample\n",
        "    pos_vals = out[\"position\"].values\n",
        "    pnl = pos_vals * rets.values\n",
        "    deltas = np.empty_like(pos_vals); deltas[0] = pos_vals[0]; deltas[1:] = pos_vals[1:] - pos_vals[:-1]\n",
        "    tc_costs = best_params.get(\"tc\", DEFAULT_TC) * np.abs(deltas)\n",
        "    net = pnl - tc_costs\n",
        "    ann_sharpe = 0.0\n",
        "    if np.nanstd(net) > 0:\n",
        "        ann_sharpe = (np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)\n",
        "\n",
        "    meta_out = {\n",
        "        \"generated_at\": ts,\n",
        "        \"best\": best,\n",
        "        \"ann_sharpe_full\": float(ann_sharpe),\n",
        "        \"target_vol\": TARGET_VOL,\n",
        "        \"trim_frac\": TRIM_FRAC,\n",
        "        \"scale_clamp\": SCALE_CLAMP\n",
        "    }\n",
        "    with open(os.path.join(OUT_DIR, f\"A_SRC_meta_volscaled_robust_{ts}.json\"), \"w\") as f:\n",
        "        json.dump(meta_out, f, indent=2)\n",
        "\n",
        "    print(\"[SAVED] final vol-scaled positions ->\", out_path)\n",
        "    print(\"Full-sample ann Sharpe (net tc):\", ann_sharpe)\n",
        "    print(\"[DONE] Volatility-Scaled SRC optimizer (robust sharpe) finished.\")\n",
        "\n",
        "    # minimal asserts\n",
        "    assert os.path.exists(out_path), \"output CSV not created\"\n",
        "    assert math.isfinite(ann_sharpe) or ann_sharpe == 0.0, \"ann_sharpe must be finite\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ca96e487bd7a4853ad31401ce735defe",
            "122876bc7ba64a638a2df119aa989add",
            "68863319ec8243e2a51464501e1dbe18",
            "edf5c8297c814604b385de1f1ea2e981",
            "db4724a84e7542b8b712e905324f0e23",
            "484fe1a87bb143eebd4ad529a91361e3",
            "31560a39ad7b4230826d4507e6bc636b",
            "eeac7ea55de64e199c297f1e0f081530",
            "dfb644bb139447dc9f406bedcda13f3e",
            "f1481a7df2aa41c3abfab04cdf3bd565",
            "05df991e74944d98970c1cde95892f7a"
          ]
        },
        "id": "gKXic1UKbtFW",
        "outputId": "4714ef0f-7f27-49b5-eb8e-e8365d2cbd10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded df_meta_shortlist: df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl rows=17521\n",
            "[INFO] using signals: meta_stack_v2_1764901823_signals_aligned.csv\n",
            "[INFO] using existing returns column: tb_ret_8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:54:53,390] A new study created in memory with name: no-name-2e5a0d8d-9df2-4633-9a03-8dafff340e5f\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] outer_splits: 5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca96e487bd7a4853ad31401ce735defe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 02:54:53,506] Trial 0 finished with value: -0.3263177342431317 and parameters: {'k': 0.8741722139252526, 'tau': 0.47634286707675977, 'L': 2.3299848545285125, 'WINDOW': 110, 'threshold': 0.07800932022121826, 'tc': 0.0003119890406724053}. Best is trial 0 with value: -0.3263177342431317.\n",
            "[I 2025-12-05 02:54:53,588] Trial 1 finished with value: -1.4140388246052893 and parameters: {'k': 0.30455050190275906, 'tau': 0.4357645499719689, 'L': 2.002787529358022, 'WINDOW': 126, 'threshold': 0.010292247147901223, 'tc': 0.0019398197043239886}. Best is trial 0 with value: -0.3263177342431317.\n",
            "[I 2025-12-05 02:54:53,696] Trial 2 finished with value: -0.6091873802575835 and parameters: {'k': 1.6983967534407591, 'tau': 0.12192277312557255, 'L': 0.9545624180177515, 'WINDOW': 50, 'threshold': 0.15212112147976886, 'tc': 0.0010495128632644758}. Best is trial 0 with value: -0.3263177342431317.\n",
            "[I 2025-12-05 02:54:53,808] Trial 3 finished with value: -0.27607724268480616 and parameters: {'k': 0.9775010335558083, 'tau': 0.1597899872950601, 'L': 2.0296322368059485, 'WINDOW': 44, 'threshold': 0.14607232426760908, 'tc': 0.0007327236865873834}. Best is trial 3 with value: -0.27607724268480616.\n",
            "[I 2025-12-05 02:54:53,910] Trial 4 finished with value: 1.1343873756237393 and parameters: {'k': 1.0209259715906647, 'tau': 0.39688446146864653, 'L': 0.9991844553958993, 'WINDOW': 98, 'threshold': 0.29620728443102123, 'tc': 9.290082543999546e-05}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:54:54,003] Trial 5 finished with value: -2.4454831283873895 and parameters: {'k': 1.293580733422589, 'tau': 0.10185157936989994, 'L': 0.6626289824631988, 'WINDOW': 161, 'threshold': 0.4828160165372797, 'tc': 0.0016167946962329224}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:54:54,183] Trial 6 finished with value: -0.6092080487875695 and parameters: {'k': 0.7483047845120672, 'tau': 0.06688261472306425, 'L': 2.210582566280392, 'WINDOW': 87, 'threshold': 0.06101911742238941, 'tc': 0.0009903538202225403}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:54:54,267] Trial 7 finished with value: -0.40271969428747306 and parameters: {'k': 0.2618993380073931, 'tau': 0.4564737929978154, 'L': 1.1469499540000423, 'WINDOW': 120, 'threshold': 0.15585553804470548, 'tc': 0.0010401360423556217}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:54:54,408] Trial 8 finished with value: -3.0226618828880256 and parameters: {'k': 1.1840785028179035, 'tau': 0.10873013865225298, 'L': 2.9239615694113965, 'WINDOW': 136, 'threshold': 0.46974947078209456, 'tc': 0.0017896547008552977}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:54:54,515] Trial 9 finished with value: -0.45504656217756967 and parameters: {'k': 1.2762199618599532, 'tau': 0.4624996328110961, 'L': 0.7212312551297988, 'WINDOW': 52, 'threshold': 0.022613644455269033, 'tc': 0.0006506606615265287}. Best is trial 4 with value: 1.1343873756237393.\n",
            "[I 2025-12-05 02:54:54,611] Trial 10 finished with value: 1.1774363150608143 and parameters: {'k': 1.901245779614696, 'tau': 0.34390259258325345, 'L': 1.4218219318027434, 'WINDOW': 85, 'threshold': 0.33453574655642954, 'tc': 7.74503999223658e-05}. Best is trial 10 with value: 1.1774363150608143.\n",
            "[I 2025-12-05 02:54:54,716] Trial 11 finished with value: 1.030206024670993 and parameters: {'k': 1.9598798720277153, 'tau': 0.3262264425013092, 'L': 1.3975730781512494, 'WINDOW': 79, 'threshold': 0.34724953068532616, 'tc': 1.7372071993338438e-05}. Best is trial 10 with value: 1.1774363150608143.\n",
            "[I 2025-12-05 02:54:54,828] Trial 12 finished with value: 1.482252407743955 and parameters: {'k': 1.6368890708713582, 'tau': 0.3294309720160686, 'L': 1.5919036026647067, 'WINDOW': 70, 'threshold': 0.3035921370166128, 'tc': 9.313183902897966e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,038] Trial 13 finished with value: -0.11094373391740298 and parameters: {'k': 1.6405069766677305, 'tau': 0.2629969083196696, 'L': 1.5334685759722928, 'WINDOW': 70, 'threshold': 0.3858320716284691, 'tc': 0.00041802185757173734}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,147] Trial 14 finished with value: 0.3965438506711272 and parameters: {'k': 1.9789993630567189, 'tau': 0.3012901546699472, 'L': 1.6481232382583206, 'WINDOW': 29, 'threshold': 0.2531342655929461, 'tc': 0.0003115675863863711}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,260] Trial 15 finished with value: -1.6442178890039978 and parameters: {'k': 1.5725201410365868, 'tau': 0.3700098003501991, 'L': 1.2654258831327447, 'WINDOW': 70, 'threshold': 0.39899743453950864, 'tc': 0.001404088017068523}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,374] Trial 16 finished with value: 0.6375681112461037 and parameters: {'k': 1.4980866655933505, 'tau': 0.19843149667947424, 'L': 1.825831619451232, 'WINDOW': 101, 'threshold': 0.3015226075092704, 'tc': 0.0005787707646077991}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,489] Trial 17 finished with value: 0.7770855240541551 and parameters: {'k': 1.8051114800823969, 'tau': 0.2242149617622211, 'L': 2.5511806329667297, 'WINDOW': 67, 'threshold': 0.23805993459819655, 'tc': 4.874243763015304e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,620] Trial 18 finished with value: -0.9852104035039617 and parameters: {'k': 1.4372284461432803, 'tau': 0.3464558294720329, 'L': 1.7691442629855598, 'WINDOW': 149, 'threshold': 0.4191904809865727, 'tc': 0.0002096846046498497}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,750] Trial 19 finished with value: -0.07285768094543554 and parameters: {'k': 1.802416044027466, 'tau': 0.28071448946081984, 'L': 1.4343007229300513, 'WINDOW': 25, 'threshold': 0.207591879756188, 'tc': 0.0008104398450412561}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:55,909] Trial 20 finished with value: 0.6797701540361386 and parameters: {'k': 1.825582673193457, 'tau': 0.4016148799817967, 'L': 0.5329480359523087, 'WINDOW': 85, 'threshold': 0.33636453353847756, 'tc': 0.0004878409229028946}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:56,044] Trial 21 finished with value: 0.8902902294544323 and parameters: {'k': 0.5838748048492728, 'tau': 0.38657422268576835, 'L': 1.0613038770156324, 'WINDOW': 104, 'threshold': 0.286844733752796, 'tc': 0.0001972196201531145}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:56,174] Trial 22 finished with value: 0.7937849756845697 and parameters: {'k': 1.002028197968428, 'tau': 0.4139015385801591, 'L': 0.912969328517719, 'WINDOW': 92, 'threshold': 0.3451135316227414, 'tc': 0.00010791900718058479}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:56,436] Trial 23 finished with value: 1.0371645372129565 and parameters: {'k': 1.375751995096857, 'tau': 0.31970128727448915, 'L': 1.2795490081386227, 'WINDOW': 60, 'threshold': 0.3060317037502506, 'tc': 0.00031125069044541325}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:56,634] Trial 24 finished with value: 0.6660755249211325 and parameters: {'k': 1.1174160440152239, 'tau': 0.36749799033144875, 'L': 0.8214540250609346, 'WINDOW': 115, 'threshold': 0.2037009019567382, 'tc': 0.00017087775698255566}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:56,828] Trial 25 finished with value: -1.134001144605702 and parameters: {'k': 0.7094223776597297, 'tau': 0.49621963979087147, 'L': 1.5375699917690149, 'WINDOW': 97, 'threshold': 0.4369029877692981, 'tc': 0.00048015698574747434}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:57,128] Trial 26 finished with value: 1.0400116964852464 and parameters: {'k': 0.44978589097507793, 'tau': 0.2364466706980447, 'L': 1.0963927471676735, 'WINDOW': 79, 'threshold': 0.2631473067887003, 'tc': 1.488602017088717e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:57,285] Trial 27 finished with value: -0.8849707160304726 and parameters: {'k': 1.713753701010623, 'tau': 0.3482759291914151, 'L': 1.8856523668268046, 'WINDOW': 38, 'threshold': 0.36128384746839237, 'tc': 0.0011880671718757238}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:57,450] Trial 28 finished with value: 0.4458940234698761 and parameters: {'k': 1.5549790909828087, 'tau': 0.4193444785835971, 'L': 1.313421916896638, 'WINDOW': 134, 'threshold': 0.20823221761234806, 'tc': 0.0003316598179413068}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:57,582] Trial 29 finished with value: 1.106750223471588 and parameters: {'k': 0.7703033802637216, 'tau': 0.2835818269073769, 'L': 1.636575342861639, 'WINDOW': 107, 'threshold': 0.3164454635468107, 'tc': 0.00016212498953915123}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:57,797] Trial 30 finished with value: 0.035419610977004225 and parameters: {'k': 1.9987282587507904, 'tau': 0.3220142472534145, 'L': 2.27433855464104, 'WINDOW': 78, 'threshold': 0.37865738307168045, 'tc': 0.0003576335042880798}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:58,016] Trial 31 finished with value: 1.0123571655316825 and parameters: {'k': 0.887599301223168, 'tau': 0.2765634446607416, 'L': 1.6221059349940972, 'WINDOW': 105, 'threshold': 0.3215859062170512, 'tc': 0.00014276743970108763}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:58,225] Trial 32 finished with value: 0.9562616165572951 and parameters: {'k': 0.7807532799582053, 'tau': 0.30188932068987784, 'L': 2.0275553713733934, 'WINDOW': 95, 'threshold': 0.2787877701692334, 'tc': 0.0002468418502395461}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:58,612] Trial 33 finished with value: 0.702147733730131 and parameters: {'k': 0.5888492912915331, 'tau': 0.2394399358407837, 'L': 1.1974758316404717, 'WINDOW': 58, 'threshold': 0.23332965309572573, 'tc': 0.00010286870727645904}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:59,218] Trial 34 finished with value: 0.6034668135680907 and parameters: {'k': 0.9146824559719694, 'tau': 0.4367980112982066, 'L': 1.4687929943608493, 'WINDOW': 112, 'threshold': 0.3122163013396442, 'tc': 0.0005938246935184904}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:54:59,755] Trial 35 finished with value: -0.41395496010530103 and parameters: {'k': 1.0019706293506434, 'tau': 0.19731526759185025, 'L': 1.6971916607920967, 'WINDOW': 119, 'threshold': 0.3662231731148727, 'tc': 0.0008479987948574906}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:00,217] Trial 36 finished with value: 0.42762295351940893 and parameters: {'k': 1.1100368247390626, 'tau': 0.024351949900243974, 'L': 0.9660841938285761, 'WINDOW': 88, 'threshold': 0.17446996162966522, 'tc': 0.00010695748853463416}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:00,521] Trial 37 finished with value: -0.6315606977596774 and parameters: {'k': 1.242412913530264, 'tau': 0.34702450744401836, 'L': 1.9233264389464106, 'WINDOW': 108, 'threshold': 0.11457735668872918, 'tc': 0.00047320513494011863}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:00,669] Trial 38 finished with value: 0.9672580229089922 and parameters: {'k': 0.40123874961911726, 'tau': 0.3825725438692015, 'L': 1.417545547666537, 'WINDOW': 84, 'threshold': 0.2786038236805072, 'tc': 0.000244636794751555}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:00,780] Trial 39 finished with value: -2.6869307743453352 and parameters: {'k': 0.6779491029249536, 'tau': 0.1421458908154331, 'L': 2.1200651200470055, 'WINDOW': 130, 'threshold': 0.4269058490618016, 'tc': 0.0012387144832796348}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:00,884] Trial 40 finished with value: -0.6252375093895518 and parameters: {'k': 0.8391284146920515, 'tau': 0.43211933579877937, 'L': 2.4124672999491574, 'WINDOW': 122, 'threshold': 0.4597977223447892, 'tc': 1.6317499835691743e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:00,992] Trial 41 finished with value: 1.0168603911183853 and parameters: {'k': 0.3753282346752832, 'tau': 0.2427547750098752, 'L': 1.0748151518419633, 'WINDOW': 77, 'threshold': 0.2626073897456873, 'tc': 2.7968522576685677e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:01,092] Trial 42 finished with value: 1.216584887350906 and parameters: {'k': 0.5314012377010613, 'tau': 0.19347486532331565, 'L': 1.1321318491953576, 'WINDOW': 64, 'threshold': 0.324465012425552, 'tc': 9.333907321837777e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:01,193] Trial 43 finished with value: 0.7992015874635797 and parameters: {'k': 0.47543235854176424, 'tau': 0.1761606320412179, 'L': 0.8193019863153221, 'WINDOW': 45, 'threshold': 0.3284195876754105, 'tc': 0.00039322995372353443}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:01,306] Trial 44 finished with value: 0.28917172660209584 and parameters: {'k': 0.2190099498023847, 'tau': 0.2948608383419732, 'L': 1.5650721816683613, 'WINDOW': 62, 'threshold': 0.3994377113423727, 'tc': 0.00023357299316262178}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:01,414] Trial 45 finished with value: -0.9093856764925792 and parameters: {'k': 0.582786116163198, 'tau': 0.20945939102948255, 'L': 1.3431880203436073, 'WINDOW': 51, 'threshold': 0.30175736888814203, 'tc': 0.0016265347038170069}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:01,550] Trial 46 finished with value: -2.268995711201253 and parameters: {'k': 1.8805094269591018, 'tau': 0.2651387627487359, 'L': 1.202137844054329, 'WINDOW': 69, 'threshold': 0.35656364790154993, 'tc': 0.001983912930726497}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:01,676] Trial 47 finished with value: 0.6245157310115861 and parameters: {'k': 0.7985453498189743, 'tau': 0.3346972164264038, 'L': 0.6586761506401888, 'WINDOW': 99, 'threshold': 0.22924630668899293, 'tc': 0.0001405241694236532}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:01,837] Trial 48 finished with value: 0.8013065533840237 and parameters: {'k': 1.672671187402461, 'tau': 0.0789688604982407, 'L': 0.9852912873911838, 'WINDOW': 88, 'threshold': 0.28626960205831037, 'tc': 0.0002821555895819619}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,022] Trial 49 finished with value: 0.46799795107226955 and parameters: {'k': 1.8808200034495093, 'tau': 0.3672023233346134, 'L': 1.7213954137447947, 'WINDOW': 74, 'threshold': 0.337866247463935, 'tc': 0.0006732217522960324}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,171] Trial 50 finished with value: 0.1678625010647571 and parameters: {'k': 0.9513649661121676, 'tau': 0.15791229155470232, 'L': 2.996602331398215, 'WINDOW': 141, 'threshold': 0.3951348655524913, 'tc': 8.120907675223463e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,278] Trial 51 finished with value: 1.0917296113028723 and parameters: {'k': 0.5195432462259038, 'tau': 0.2394639103879532, 'L': 1.1577272061063586, 'WINDOW': 81, 'threshold': 0.2635181292988073, 'tc': 1.0316218493605113e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,398] Trial 52 finished with value: 0.6245327451401049 and parameters: {'k': 0.6348477960738648, 'tau': 0.3030301892917009, 'L': 1.173460837633793, 'WINDOW': 92, 'threshold': 0.2543453177365034, 'tc': 0.00020116323591729213}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,497] Trial 53 finished with value: 1.0660520807596405 and parameters: {'k': 0.5212390977794016, 'tau': 0.21626819671042763, 'L': 1.3695226781611534, 'WINDOW': 66, 'threshold': 0.3208749005258257, 'tc': 8.736823883196552e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,639] Trial 54 finished with value: 1.0802028553943048 and parameters: {'k': 0.31504679494032484, 'tau': 0.2564133953575073, 'L': 0.8587827733348676, 'WINDOW': 83, 'threshold': 0.2863169673004885, 'tc': 7.900168890494433e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,740] Trial 55 finished with value: 1.2464889620286204 and parameters: {'k': 1.3540975767730528, 'tau': 0.1824494388981628, 'L': 1.5213570172244828, 'WINDOW': 74, 'threshold': 0.30105667120253543, 'tc': 0.0001812166381239778}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,840] Trial 56 finished with value: 0.26995671098661456 and parameters: {'k': 1.3905570389725523, 'tau': 0.18144111222780468, 'L': 1.4752311359714192, 'WINDOW': 73, 'threshold': 0.3697568701638443, 'tc': 0.00042540267594916634}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:02,942] Trial 57 finished with value: 0.783283606471048 and parameters: {'k': 1.1912173979095295, 'tau': 0.1167509549836562, 'L': 1.6140797591425189, 'WINDOW': 56, 'threshold': 0.34676009698177473, 'tc': 0.00019654560528916185}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,052] Trial 58 finished with value: 1.0264041595064162 and parameters: {'k': 1.6170849487054826, 'tau': 0.13456708347868146, 'L': 1.8202744917923381, 'WINDOW': 39, 'threshold': 0.3081558308746716, 'tc': 0.0002937961457802166}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,160] Trial 59 finished with value: -1.4951087985950127 and parameters: {'k': 1.5003694413679138, 'tau': 0.39499495613553753, 'L': 1.2815467470223403, 'WINDOW': 101, 'threshold': 0.2967053269185596, 'tc': 0.0018623157352146737}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,282] Trial 60 finished with value: 1.0336583640633517 and parameters: {'k': 1.7514184685189513, 'tau': 0.28153850698253347, 'L': 1.5184602047317814, 'WINDOW': 168, 'threshold': 0.32865013893828277, 'tc': 0.00017346060027549664}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,389] Trial 61 finished with value: 0.5677594956282014 and parameters: {'k': 1.3371293797527193, 'tau': 0.17700651786791388, 'L': 1.0126119682981682, 'WINDOW': 66, 'threshold': 0.2483077792367295, 'tc': 9.791893183265738e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,501] Trial 62 finished with value: 1.1992294692663956 and parameters: {'k': 1.083745746469846, 'tau': 0.22867171287917637, 'L': 1.1546319415059185, 'WINDOW': 93, 'threshold': 0.26951009368093026, 'tc': 8.927084401342019e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,606] Trial 63 finished with value: 0.43576198060044025 and parameters: {'k': 1.1462759583662787, 'tau': 0.19642296792209277, 'L': 1.2441005725879213, 'WINDOW': 93, 'threshold': 0.22449360489374853, 'tc': 0.0003647318862411178}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,739] Trial 64 finished with value: 1.1571757959689128 and parameters: {'k': 1.274707906052615, 'tau': 0.31123666928635063, 'L': 1.6689453243854107, 'WINDOW': 108, 'threshold': 0.2724416882862577, 'tc': 6.969121665113133e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,846] Trial 65 finished with value: 0.6170050707104915 and parameters: {'k': 1.2358723306609194, 'tau': 0.35779298262122894, 'L': 1.7816070759946119, 'WINDOW': 89, 'threshold': 0.18458134557755287, 'tc': 8.093287742207234e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:03,951] Trial 66 finished with value: 0.9295883625231717 and parameters: {'k': 1.0589331377493334, 'tau': 0.3135026280692502, 'L': 0.6857077512389733, 'WINDOW': 114, 'threshold': 0.27024305184008834, 'tc': 0.000283126772639742}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,056] Trial 67 finished with value: 0.5977233801173063 and parameters: {'k': 1.297580636874724, 'tau': 0.45990568829021056, 'L': 1.3839699171665445, 'WINDOW': 76, 'threshold': 0.24678139795472928, 'tc': 5.733800209882077e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,167] Trial 68 finished with value: 0.5963089514425798 and parameters: {'k': 1.4306707359188668, 'tau': 0.32509791957543044, 'L': 1.9580528995582354, 'WINDOW': 102, 'threshold': 0.2930829538492336, 'tc': 0.0005386811119556342}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,276] Trial 69 finished with value: 0.7330576914296749 and parameters: {'k': 1.040302729751133, 'tau': 0.16124739706365965, 'L': 2.7402253853221223, 'WINDOW': 62, 'threshold': 0.3505055590704184, 'tc': 0.00014681855690908238}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,381] Trial 70 finished with value: -0.23260233559835444 and parameters: {'k': 1.5873293945726237, 'tau': 0.3326107185777595, 'L': 0.9069253430066311, 'WINDOW': 97, 'threshold': 0.2200334527298163, 'tc': 0.0009385997587922783}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,495] Trial 71 finished with value: 1.0413731418577887 and parameters: {'k': 1.0725251328486745, 'tau': 0.28620375388274355, 'L': 1.66760198898787, 'WINDOW': 117, 'threshold': 0.3178501042600338, 'tc': 0.00015652017096108966}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,603] Trial 72 finished with value: 0.9444157974118239 and parameters: {'k': 1.1753493505734594, 'tau': 0.38126431767652025, 'L': 1.5989522825360931, 'WINDOW': 108, 'threshold': 0.278350715318456, 'tc': 0.00024156003904345194}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,741] Trial 73 finished with value: 1.4523389197413055 and parameters: {'k': 1.473145017373677, 'tau': 0.22452208361582943, 'L': 1.868188509522129, 'WINDOW': 123, 'threshold': 0.3030036457515752, 'tc': 5.693612804039036e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,853] Trial 74 finished with value: 1.42226489341929 and parameters: {'k': 1.5218680130332336, 'tau': 0.2556706023376109, 'L': 2.0962836202708046, 'WINDOW': 129, 'threshold': 0.30245119323610986, 'tc': 8.650648412682439e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:04,965] Trial 75 finished with value: 1.1951188433192508 and parameters: {'k': 1.4748237267851962, 'tau': 0.2548707059608795, 'L': 2.1037769463558145, 'WINDOW': 125, 'threshold': 0.334673093555888, 'tc': 6.355671482622004e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,078] Trial 76 finished with value: 1.1589518676376411 and parameters: {'k': 1.5024449061782084, 'tau': 0.25644029121178913, 'L': 2.140827759375407, 'WINDOW': 126, 'threshold': 0.3350278938569718, 'tc': 0.0001328457386488847}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,191] Trial 77 finished with value: 0.6019426085402122 and parameters: {'k': 1.441922341799776, 'tau': 0.2278353869715114, 'L': 2.092344652962345, 'WINDOW': 156, 'threshold': 0.375014808207919, 'tc': 5.3046879685008675e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,298] Trial 78 finished with value: 1.1591534144149105 and parameters: {'k': 1.5655579898355458, 'tau': 0.20996501916949462, 'L': 2.4411285334053296, 'WINDOW': 147, 'threshold': 0.30736328649707123, 'tc': 0.0002140550997037767}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,428] Trial 79 finished with value: 0.15600455240762623 and parameters: {'k': 1.7363661672774076, 'tau': 0.2701044941495694, 'L': 1.8674351936618057, 'WINDOW': 135, 'threshold': 0.3837552869256847, 'tc': 0.00032000285349931853}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,548] Trial 80 finished with value: 0.8841101228636562 and parameters: {'k': 1.523728266116654, 'tau': 0.24844025083214158, 'L': 2.2033178806530156, 'WINDOW': 123, 'threshold': 0.35618726088842695, 'tc': 5.164230088301298e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,657] Trial 81 finished with value: 1.2297147555551557 and parameters: {'k': 1.6667797823989166, 'tau': 0.20702249344355572, 'L': 2.343043035768461, 'WINDOW': 146, 'threshold': 0.30562994531121573, 'tc': 0.00020519926599201063}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,774] Trial 82 finished with value: 1.1486387544407997 and parameters: {'k': 1.6646891103779642, 'tau': 0.1920444398590426, 'L': 2.265762573533714, 'WINDOW': 141, 'threshold': 0.2939927710025913, 'tc': 0.00011516934434232845}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:05,892] Trial 83 finished with value: -0.30845382374280167 and parameters: {'k': 1.447535969300831, 'tau': 0.21471339041339732, 'L': 2.4540311902170195, 'WINDOW': 128, 'threshold': 0.006110071898636216, 'tc': 0.00027048539625532396}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:06,098] Trial 84 finished with value: 1.2033313401736068 and parameters: {'k': 1.5914966577632637, 'tau': 0.22667437382065786, 'L': 2.0285911924342725, 'WINDOW': 153, 'threshold': 0.3309263090099436, 'tc': 4.166614749993326e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:06,332] Trial 85 finished with value: 1.087600162518358 and parameters: {'k': 1.638783031835075, 'tau': 0.22174290845367695, 'L': 2.0043585977835257, 'WINDOW': 147, 'threshold': 0.3383760150801583, 'tc': 0.0001790264483893351}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:06,444] Trial 86 finished with value: 1.138595414333697 and parameters: {'k': 1.7720957115989262, 'tau': 0.23127407076036963, 'L': 2.359367851420976, 'WINDOW': 156, 'threshold': 0.32022018413992487, 'tc': 1.5836115796543512e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:06,557] Trial 87 finished with value: 1.351718564238131 and parameters: {'k': 1.3737978138701084, 'tau': 0.1638505850981843, 'L': 2.0970360181798697, 'WINDOW': 141, 'threshold': 0.3029672515903493, 'tc': 0.00012423337658922802}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:06,674] Trial 88 finished with value: 1.3316705059809582 and parameters: {'k': 1.6051930470698639, 'tau': 0.18681167561764034, 'L': 2.563997082947804, 'WINDOW': 139, 'threshold': 0.2979758232609344, 'tc': 2.3555586632933373e-07}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:06,789] Trial 89 finished with value: 1.4653912486621643 and parameters: {'k': 1.3519565157726392, 'tau': 0.14207910193123702, 'L': 2.6428796776863654, 'WINDOW': 140, 'threshold': 0.3097228264167014, 'tc': 2.7097284083559616e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:06,921] Trial 90 finished with value: -0.4279563601104906 and parameters: {'k': 1.3374140096355824, 'tau': 0.15775231114138755, 'L': 2.6063175778933743, 'WINDOW': 139, 'threshold': 0.3089681589266947, 'tc': 0.0013085970843314944}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,047] Trial 91 finished with value: 0.9935904671020529 and parameters: {'k': 1.6059376795733922, 'tau': 0.14387275844431274, 'L': 2.73467826067214, 'WINDOW': 131, 'threshold': 0.2875802360532634, 'tc': 0.0001425675320273842}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,159] Trial 92 finished with value: 1.3626638698617868 and parameters: {'k': 1.3825498454362057, 'tau': 0.18616454484917117, 'L': 2.561974586163012, 'WINDOW': 150, 'threshold': 0.2989122630368221, 'tc': 1.3080298987527645e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,296] Trial 93 finished with value: 0.9727562415790181 and parameters: {'k': 1.541465372889976, 'tau': 0.09778361687971462, 'L': 2.5575544887440103, 'WINDOW': 144, 'threshold': 0.2962606024963611, 'tc': 0.00022065362028872491}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,405] Trial 94 finished with value: 1.3867983128008905 and parameters: {'k': 1.3929298601984428, 'tau': 0.16966746239575722, 'L': 2.71282671612211, 'WINDOW': 152, 'threshold': 0.3053490431450553, 'tc': 4.745564591772248e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,523] Trial 95 finished with value: 0.8369550483783047 and parameters: {'k': 1.3968891967956567, 'tau': 0.16773558151158843, 'L': 2.698136790543156, 'WINDOW': 162, 'threshold': 0.2566511979535991, 'tc': 0.00012160298312276982}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,632] Trial 96 finished with value: 1.3829883808578198 and parameters: {'k': 1.7006692211431926, 'tau': 0.13609283351301305, 'L': 2.862392689411697, 'WINDOW': 150, 'threshold': 0.3068594963125677, 'tc': 4.239610870140074e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,747] Trial 97 finished with value: 1.0672568013891108 and parameters: {'k': 1.351158361385155, 'tau': 0.12925320606227722, 'L': 2.8234932526243703, 'WINDOW': 152, 'threshold': 0.28534833299662304, 'tc': 4.72502246702806e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:07,859] Trial 98 finished with value: 0.8542299146991695 and parameters: {'k': 1.4126471187010103, 'tau': 0.1424949126739145, 'L': 2.8354317253595585, 'WINDOW': 133, 'threshold': 0.2387335427123213, 'tc': 7.966178178376402e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,014] Trial 99 finished with value: 1.1926258497133393 and parameters: {'k': 1.4626026236888015, 'tau': 0.11003536789141267, 'L': 2.905127384251356, 'WINDOW': 137, 'threshold': 0.2780319105103762, 'tc': 4.7462203825963805e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,120] Trial 100 finished with value: 1.0789179241479896 and parameters: {'k': 1.7011792324909445, 'tau': 0.093138871046445, 'L': 2.6332438578619284, 'WINDOW': 159, 'threshold': 0.3158678101291948, 'tc': 0.0001659930169423463}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,234] Trial 101 finished with value: 1.3438663492154683 and parameters: {'k': 1.6659444059338109, 'tau': 0.184421321663579, 'L': 2.6652833591870024, 'WINDOW': 149, 'threshold': 0.3008213971691862, 'tc': 0.0001079049146945104}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,338] Trial 102 finished with value: 1.2989839385076916 and parameters: {'k': 1.8473915821818991, 'tau': 0.18199582283378601, 'L': 2.6728462149552565, 'WINDOW': 142, 'threshold': 0.3001062946608024, 'tc': 0.00012000267902959275}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,443] Trial 103 finished with value: 0.7856805925522016 and parameters: {'k': 1.8375675934285143, 'tau': 0.1507353389952992, 'L': 2.4911805531313043, 'WINDOW': 150, 'threshold': 0.34498931112492237, 'tc': 0.0001177821222057314}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,576] Trial 104 finished with value: -0.08588769311022375 and parameters: {'k': 1.9379227292343937, 'tau': 0.16983737003157234, 'L': 2.6569056398604376, 'WINDOW': 143, 'threshold': 0.31525129561807774, 'tc': 0.001115735608635704}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,745] Trial 105 finished with value: 1.2739143102962214 and parameters: {'k': 1.7998803305637334, 'tau': 0.187958614518768, 'L': 2.52902663026677, 'WINDOW': 139, 'threshold': 0.2932553138004408, 'tc': 3.75412165633017e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:08,854] Trial 106 finished with value: -0.35112253533067306 and parameters: {'k': 1.7178602806333942, 'tau': 0.12821857824256563, 'L': 2.803122849630793, 'WINDOW': 151, 'threshold': 0.0874214769993425, 'tc': 5.1243809391613456e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,020] Trial 107 finished with value: 0.9625528254677572 and parameters: {'k': 1.5212925301602653, 'tau': 0.20061505004389044, 'L': 2.928338297643324, 'WINDOW': 164, 'threshold': 0.263637986189611, 'tc': 0.0001023628026339019}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,163] Trial 108 finished with value: 1.011552794348253 and parameters: {'k': 1.2970055231110875, 'tau': 0.15244888061767237, 'L': 2.5767769648002377, 'WINDOW': 155, 'threshold': 0.32428681435311824, 'tc': 0.00025348861230001035}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,288] Trial 109 finished with value: 1.0708989979089463 and parameters: {'k': 1.8463117100038666, 'tau': 0.1707540919723989, 'L': 2.7792100494479635, 'WINDOW': 143, 'threshold': 0.2813296790379757, 'tc': 0.00013616364937090217}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,408] Trial 110 finished with value: 1.2969169312900928 and parameters: {'k': 1.560407711708538, 'tau': 0.18746719878474777, 'L': 2.670931643953641, 'WINDOW': 138, 'threshold': 0.2998299099831733, 'tc': 7.899996072803373e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,530] Trial 111 finished with value: 1.4265465130594766 and parameters: {'k': 1.5551195893343337, 'tau': 0.16518456346369745, 'L': 2.6808484714227223, 'WINDOW': 137, 'threshold': 0.30081152756476914, 'tc': 4.2752241721937984e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,641] Trial 112 finished with value: 1.4560495996761809 and parameters: {'k': 1.628380859617793, 'tau': 0.12141198950791891, 'L': 2.8852808985898535, 'WINDOW': 133, 'threshold': 0.3048203089788304, 'tc': 4.5951349264707434e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,767] Trial 113 finished with value: -0.733812452996177 and parameters: {'k': 1.6443744613898308, 'tau': 0.12092216299533105, 'L': 2.8717261313165534, 'WINDOW': 132, 'threshold': 0.31088130673697345, 'tc': 0.00151431111370522}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:09,881] Trial 114 finished with value: -0.33008477662351327 and parameters: {'k': 1.6152574374154955, 'tau': 0.13715359370311678, 'L': 2.7266967287626924, 'WINDOW': 135, 'threshold': 0.03202939805555968, 'tc': 5.502187350727046e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:10,009] Trial 115 finished with value: 1.2189258829991194 and parameters: {'k': 1.5440300021465507, 'tau': 0.0583336027521523, 'L': 2.4973705393556935, 'WINDOW': 129, 'threshold': 0.27525034004946836, 'tc': 3.517516109499436e-07}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:10,130] Trial 116 finished with value: 1.2014600615249127 and parameters: {'k': 1.483063692716713, 'tau': 0.14909533837530184, 'L': 2.9796521632679345, 'WINDOW': 148, 'threshold': 0.32705238182127555, 'tc': 4.0851272982431757e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:10,237] Trial 117 finished with value: 1.1598237184640177 and parameters: {'k': 1.6896645443826723, 'tau': 0.16387077232921898, 'L': 2.603836260199624, 'WINDOW': 158, 'threshold': 0.28977553540968504, 'tc': 8.632381452740694e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:10,349] Trial 118 finished with value: 0.9448377813890966 and parameters: {'k': 1.394624978104273, 'tau': 0.20103097195319272, 'L': 2.87742736119675, 'WINDOW': 145, 'threshold': 0.34071945379945323, 'tc': 0.00017028136495120957}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:10,520] Trial 119 finished with value: 0.9757720399207823 and parameters: {'k': 1.7644516672999433, 'tau': 0.1744690556726855, 'L': 2.770934351315556, 'WINDOW': 120, 'threshold': 0.35297498135760785, 'tc': 3.174303799358561e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:10,737] Trial 120 finished with value: 0.9129541027029187 and parameters: {'k': 1.246581997244993, 'tau': 0.10961580519181455, 'L': 2.384252818840838, 'WINDOW': 140, 'threshold': 0.2670049697398248, 'tc': 0.00018928563999487722}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:10,877] Trial 121 finished with value: 1.2570923757595913 and parameters: {'k': 1.624533371908404, 'tau': 0.18122236734280844, 'L': 2.671312164665942, 'WINDOW': 142, 'threshold': 0.30663887481591806, 'tc': 0.00013423054152585177}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:11,107] Trial 122 finished with value: 1.2562503122170827 and parameters: {'k': 1.5126564177664474, 'tau': 0.15666670817871817, 'L': 2.7035417580905805, 'WINDOW': 149, 'threshold': 0.2993907263058108, 'tc': 0.00010050503808105774}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:11,322] Trial 123 finished with value: 1.075403944071561 and parameters: {'k': 1.47918294335814, 'tau': 0.13179015806755848, 'L': 2.7636830170344915, 'WINDOW': 136, 'threshold': 0.3189262034893389, 'tc': 7.335858898427751e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:11,614] Trial 124 finished with value: 0.1076794890468427 and parameters: {'k': 1.432042569471507, 'tau': 0.18512827433295806, 'L': 2.8459676936035074, 'WINDOW': 153, 'threshold': 0.28738277355240316, 'tc': 0.0007604758134468869}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:11,893] Trial 125 finished with value: 0.8196043136630783 and parameters: {'k': 1.5772924773376138, 'tau': 0.08438664506044699, 'L': 2.9631170209386677, 'WINDOW': 133, 'threshold': 0.25621342402892355, 'tc': 0.00012743177723669667}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:12,042] Trial 126 finished with value: 1.3350603643930583 and parameters: {'k': 1.9238226240069565, 'tau': 0.11922368361776753, 'L': 1.939624790713706, 'WINDOW': 145, 'threshold': 0.29951804039231833, 'tc': 3.842685250212265e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:12,274] Trial 127 finished with value: 0.7545220081802627 and parameters: {'k': 1.3719272640144042, 'tau': 0.12327727708409551, 'L': 1.891223845488678, 'WINDOW': 128, 'threshold': 0.3636845721436517, 'tc': 3.8238460547409625e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:12,457] Trial 128 finished with value: 1.302051947947145 and parameters: {'k': 1.6700373111706655, 'tau': 0.13867366281524737, 'L': 1.978787923258967, 'WINDOW': 145, 'threshold': 0.328208114014305, 'tc': 3.108738961834118e-06}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:12,659] Trial 129 finished with value: 0.9713240605271372 and parameters: {'k': 1.5467551722837314, 'tau': 0.10580411078021938, 'L': 1.7434979199169724, 'WINDOW': 149, 'threshold': 0.3128886784539269, 'tc': 0.00022397857904504716}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:12,867] Trial 130 finished with value: 0.6820291045039363 and parameters: {'k': 1.3242933538496797, 'tau': 0.16341098212632932, 'L': 2.2642715958096655, 'WINDOW': 123, 'threshold': 0.24144487803260561, 'tc': 7.896797592695919e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:13,114] Trial 131 finished with value: 1.2271890174689095 and parameters: {'k': 1.6839628819355412, 'tau': 0.1381705357340143, 'L': 1.9414545460051178, 'WINDOW': 146, 'threshold': 0.32943224356506373, 'tc': 1.030339834029522e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:13,248] Trial 132 finished with value: 1.3899201988888379 and parameters: {'k': 1.649917618561804, 'tau': 0.15049675922935699, 'L': 1.8081066543226036, 'WINDOW': 138, 'threshold': 0.30554489394467665, 'tc': 4.9817436259482865e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:13,477] Trial 133 finished with value: 1.4581976804388768 and parameters: {'k': 1.9460385027788631, 'tau': 0.1509231908984018, 'L': 1.8472512271652946, 'WINDOW': 139, 'threshold': 0.30495601309264114, 'tc': 4.6736860661902825e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:13,654] Trial 134 finished with value: 1.0902035536129107 and parameters: {'k': 1.8795669765476553, 'tau': 0.11841691246515863, 'L': 1.7998345443520603, 'WINDOW': 136, 'threshold': 0.27683997927773724, 'tc': 0.00016285084509126045}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:13,930] Trial 135 finished with value: 1.46631723893832 and parameters: {'k': 1.9242252313806443, 'tau': 0.14487406646224604, 'L': 1.8663481994258915, 'WINDOW': 131, 'threshold': 0.3059461356472375, 'tc': 4.8174729374917264e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:14,237] Trial 136 finished with value: 1.2789365651807447 and parameters: {'k': 1.9841803511243508, 'tau': 0.14790130732746573, 'L': 1.6664590884283936, 'WINDOW': 132, 'threshold': 0.31178807545421633, 'tc': 8.286003681512444e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:14,433] Trial 137 finished with value: 1.041812206503706 and parameters: {'k': 1.8042570001185851, 'tau': 0.155746309964175, 'L': 1.715481724941448, 'WINDOW': 128, 'threshold': 0.2848424772119389, 'tc': 9.950413288190596e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:14,624] Trial 138 finished with value: 0.6679575386813271 and parameters: {'k': 1.7172396818615365, 'tau': 0.17020923326097817, 'L': 1.8202754175766773, 'WINDOW': 126, 'threshold': 0.3442535687831417, 'tc': 0.00019482976643188247}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:14,787] Trial 139 finished with value: 1.111874490382293 and parameters: {'k': 1.952820784202139, 'tau': 0.1499648117579401, 'L': 1.8789152162168938, 'WINDOW': 154, 'threshold': 0.3205717330648427, 'tc': 4.8200739679069e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:15,004] Trial 140 finished with value: 1.2375305935385932 and parameters: {'k': 1.9084101721964657, 'tau': 0.17495654267539062, 'L': 2.152402227654779, 'WINDOW': 137, 'threshold': 0.30671246575957006, 'tc': 0.00015111993216255723}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:15,247] Trial 141 finished with value: 1.276965418062219 and parameters: {'k': 1.8904659659249945, 'tau': 0.11862134818687889, 'L': 1.850667109890411, 'WINDOW': 142, 'threshold': 0.295920738214585, 'tc': 5.069707881496824e-05}. Best is trial 12 with value: 1.482252407743955.\n",
            "[I 2025-12-05 02:55:15,430] Trial 142 finished with value: 1.4901644834461296 and parameters: {'k': 1.9898570651888277, 'tau': 0.13010340811879054, 'L': 1.9320015121292762, 'WINDOW': 151, 'threshold': 0.30341381521310695, 'tc': 5.027786955579793e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:15,612] Trial 143 finished with value: 1.0292575176857193 and parameters: {'k': 1.9997467325282812, 'tau': 0.1283978647017342, 'L': 2.0777556240187303, 'WINDOW': 158, 'threshold': 0.2883481946307886, 'tc': 0.00011336550704907312}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:15,717] Trial 144 finished with value: 1.1336089859960756 and parameters: {'k': 1.9493104075452017, 'tau': 0.14325082389913849, 'L': 2.0618880093728453, 'WINDOW': 162, 'threshold': 0.27242382479739324, 'tc': 8.832450124244804e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:15,828] Trial 145 finished with value: 1.155351836224506 and parameters: {'k': 1.753420877982254, 'tau': 0.1638966689098134, 'L': 1.7883073365507538, 'WINDOW': 151, 'threshold': 0.3175321416669153, 'tc': 4.825701081754249e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:15,945] Trial 146 finished with value: 1.4506283088334506 and parameters: {'k': 1.475329966573482, 'tau': 0.13706332682332914, 'L': 2.183033205330653, 'WINDOW': 130, 'threshold': 0.3085578344294391, 'tc': 8.227092425616193e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:16,076] Trial 147 finished with value: 1.293239562784298 and parameters: {'k': 1.4500268016308913, 'tau': 0.13449147264717262, 'L': 2.162223731016361, 'WINDOW': 130, 'threshold': 0.3386960911301361, 'tc': 1.9268849965992736e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:16,192] Trial 148 finished with value: 1.3702386632003496 and parameters: {'k': 1.3813409315350693, 'tau': 0.09404313180876869, 'L': 1.8999758366126345, 'WINDOW': 133, 'threshold': 0.3085476714831114, 'tc': 6.428532070996775e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:16,314] Trial 149 finished with value: 1.1401024686893655 and parameters: {'k': 1.4144605751953927, 'tau': 0.2448329661827781, 'L': 1.903236284455736, 'WINDOW': 117, 'threshold': 0.3303789344866446, 'tc': 6.334918844663538e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:16,426] Trial 150 finished with value: 1.3218334258679667 and parameters: {'k': 1.513592509231215, 'tau': 0.09573463365014565, 'L': 2.000332876314522, 'WINDOW': 134, 'threshold': 0.3114464339398448, 'tc': 3.5893702798617476e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:16,557] Trial 151 finished with value: 1.3564049980058142 and parameters: {'k': 1.3564449997172878, 'tau': 0.06776361335895509, 'L': 1.7608751730338157, 'WINDOW': 125, 'threshold': 0.30586383378709603, 'tc': 0.00013758372737628446}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:16,755] Trial 152 finished with value: -0.6319947163862965 and parameters: {'k': 1.3083153758491177, 'tau': 0.087647840749228, 'L': 1.8439853183324573, 'WINDOW': 126, 'threshold': 0.49361836645230495, 'tc': 7.42170585823649e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:16,884] Trial 153 finished with value: 0.9714739168790073 and parameters: {'k': 1.2709037818846756, 'tau': 0.0470105345651238, 'L': 1.7554438220349224, 'WINDOW': 121, 'threshold': 0.31998653808022864, 'tc': 0.00015472284187723458}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,024] Trial 154 finished with value: 1.238461345729906 and parameters: {'k': 1.4837440862225488, 'tau': 0.07772957400173422, 'L': 1.7015047796621319, 'WINDOW': 130, 'threshold': 0.2903056341395791, 'tc': 1.5065658243954371e-06}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,149] Trial 155 finished with value: 1.1918744928579612 and parameters: {'k': 1.3575738420541694, 'tau': 0.11022946119696325, 'L': 1.590805358693418, 'WINDOW': 133, 'threshold': 0.28084064925373114, 'tc': 3.9134205311168174e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,271] Trial 156 finished with value: 1.285838891684332 and parameters: {'k': 1.4152504634828926, 'tau': 0.12812203559999322, 'L': 1.9190533544066, 'WINDOW': 138, 'threshold': 0.3069057836517315, 'tc': 8.858191893756421e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,397] Trial 157 finished with value: 1.1780062674954608 and parameters: {'k': 1.215396645831497, 'tau': 0.06401247381105273, 'L': 1.7719324669875756, 'WINDOW': 112, 'threshold': 0.32474989176271823, 'tc': 0.0001386453097876504}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,524] Trial 158 finished with value: 1.0731517634141154 and parameters: {'k': 1.6289505755367661, 'tau': 0.07865531706262974, 'L': 2.035144981169807, 'WINDOW': 123, 'threshold': 0.2942986411431207, 'tc': 0.00018547249524579205}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,645] Trial 159 finished with value: 0.0062758448779418234 and parameters: {'k': 1.5905698349771815, 'tau': 0.10418299144972773, 'L': 1.8505366495870381, 'WINDOW': 32, 'threshold': 0.26813911895057907, 'tc': 0.000885397530390036}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,780] Trial 160 finished with value: 1.208169644889362 and parameters: {'k': 1.4752106882070712, 'tau': 0.045726670389257096, 'L': 1.9667109039603046, 'WINDOW': 130, 'threshold': 0.31292091534044914, 'tc': 3.662183230169801e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:17,913] Trial 161 finished with value: 1.3536815814628689 and parameters: {'k': 1.3823143178302968, 'tau': 0.14692613819904166, 'L': 2.1923183652979175, 'WINDOW': 140, 'threshold': 0.305684135241715, 'tc': 0.00010768975401535621}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,029] Trial 162 finished with value: 1.4501282408269809 and parameters: {'k': 1.3698450483695397, 'tau': 0.14397112262119235, 'L': 2.2585566036054985, 'WINDOW': 137, 'threshold': 0.3046603803910222, 'tc': 7.771790194280857e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,154] Trial 163 finished with value: 1.1760078926960278 and parameters: {'k': 1.3405526238117258, 'tau': 0.13801147563531008, 'L': 2.9090823816277744, 'WINDOW': 135, 'threshold': 0.29121131514048715, 'tc': 7.106198325035022e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,279] Trial 164 finished with value: 1.273572541344804 and parameters: {'k': 1.4331126529259544, 'tau': 0.26922164402793564, 'L': 2.2370318747539204, 'WINDOW': 126, 'threshold': 0.3344054366825821, 'tc': 1.9150922864111564e-06}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,406] Trial 165 finished with value: 1.158199556003388 and parameters: {'k': 1.5170204129214244, 'tau': 0.15584751153174503, 'L': 1.8189597315535755, 'WINDOW': 133, 'threshold': 0.2811599693471298, 'tc': 6.632779375885003e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,536] Trial 166 finished with value: -0.9876545596101346 and parameters: {'k': 1.574252825558386, 'tau': 0.1135543615026304, 'L': 2.3111916359833335, 'WINDOW': 129, 'threshold': 0.320604029872532, 'tc': 0.0016907653664098036}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,665] Trial 167 finished with value: 1.3190881090473177 and parameters: {'k': 1.4524210106831819, 'tau': 0.13161821016616976, 'L': 2.8069265406405, 'WINDOW': 137, 'threshold': 0.3007159709269219, 'tc': 0.00012743057729239758}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,817] Trial 168 finished with value: 1.2461067869809146 and parameters: {'k': 1.2751348804309588, 'tau': 0.14518228971911665, 'L': 1.9014551053025806, 'WINDOW': 124, 'threshold': 0.31287601753460426, 'tc': 3.716923194506235e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:18,937] Trial 169 finished with value: 1.2891879971624693 and parameters: {'k': 1.3940051955006278, 'tau': 0.10059213443307745, 'L': 1.7440736232752916, 'WINDOW': 55, 'threshold': 0.2959513841509979, 'tc': 4.657428659167278e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,060] Trial 170 finished with value: 1.179537267531776 and parameters: {'k': 1.3370311279929956, 'tau': 0.48777736781566516, 'L': 1.6453931860323692, 'WINDOW': 117, 'threshold': 0.3280053292132276, 'tc': 9.927500439201659e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,185] Trial 171 finished with value: 1.2774912345386993 and parameters: {'k': 1.3834583773073805, 'tau': 0.15458843013175638, 'L': 2.180486218761936, 'WINDOW': 143, 'threshold': 0.3078034012087273, 'tc': 0.00011100527644062356}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,307] Trial 172 finished with value: 1.2994313064336274 and parameters: {'k': 1.3119039890419084, 'tau': 0.1443337052887756, 'L': 2.2369475564712524, 'WINDOW': 140, 'threshold': 0.3012949702924013, 'tc': 0.0001616692729798516}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,429] Trial 173 finished with value: 1.0653869721693903 and parameters: {'k': 1.3692132698176518, 'tau': 0.12931113727238328, 'L': 2.023104257779704, 'WINDOW': 139, 'threshold': 0.28462207976203785, 'tc': 7.633598747094322e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,545] Trial 174 finished with value: 1.3217834810560485 and parameters: {'k': 1.9644455759396855, 'tau': 0.14501995026005537, 'L': 2.3136666377781876, 'WINDOW': 134, 'threshold': 0.30709938177119067, 'tc': 3.376541267536229e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,666] Trial 175 finished with value: 1.2030848391890483 and parameters: {'k': 1.4137293898338281, 'tau': 0.1621175174451479, 'L': 2.618499593924222, 'WINDOW': 48, 'threshold': 0.3118893467874781, 'tc': 0.0001144318024328591}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,800] Trial 176 finished with value: 1.147441672571129 and parameters: {'k': 1.5485050342414368, 'tau': 0.16976413809661856, 'L': 2.1120832966656065, 'WINDOW': 136, 'threshold': 0.32215535940597106, 'tc': 6.563282910338199e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:19,963] Trial 177 finished with value: 1.1421022226247786 and parameters: {'k': 1.5000157272837713, 'tau': 0.12338998548867784, 'L': 2.192424842886893, 'WINDOW': 146, 'threshold': 0.2918072391277896, 'tc': 0.00015524924131938132}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:20,081] Trial 178 finished with value: 0.6807801489392286 and parameters: {'k': 1.6379823255764605, 'tau': 0.13761812383498856, 'L': 1.953260853355496, 'WINDOW': 132, 'threshold': 0.34654093325626756, 'tc': 0.00022764467013860687}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:20,212] Trial 179 finished with value: 1.2269806546092192 and parameters: {'k': 1.4540590657728971, 'tau': 0.15190227999218278, 'L': 2.7444075420948866, 'WINDOW': 127, 'threshold': 0.27722647681269125, 'tc': 3.855377455717261e-05}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:20,325] Trial 180 finished with value: 1.372405846420465 and parameters: {'k': 1.3923004430681796, 'tau': 0.02453261882901165, 'L': 2.425145891310823, 'WINDOW': 140, 'threshold': 0.2990003307078065, 'tc': 9.315242049933727e-07}. Best is trial 142 with value: 1.4901644834461296.\n",
            "[I 2025-12-05 02:55:20,449] Trial 181 finished with value: 1.51137702895629 and parameters: {'k': 1.3638238907287141, 'tau': 0.055144792245002926, 'L': 2.5027456918553526, 'WINDOW': 140, 'threshold': 0.3021040051441348, 'tc': 9.520228276014767e-08}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:20,575] Trial 182 finished with value: 1.3571818555752782 and parameters: {'k': 1.3505849301859896, 'tau': 0.0694834581353966, 'L': 2.415840328205049, 'WINDOW': 143, 'threshold': 0.29863422742375056, 'tc': 6.455150512053869e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:20,692] Trial 183 finished with value: 1.2695802500035416 and parameters: {'k': 1.4216263365851036, 'tau': 0.028818473041882398, 'L': 2.4012679902892273, 'WINDOW': 147, 'threshold': 0.2932963212301913, 'tc': 8.053833109420493e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:20,803] Trial 184 finished with value: 1.2509721817270023 and parameters: {'k': 1.9145372921606474, 'tau': 0.029847335993314166, 'L': 2.4494452773940973, 'WINDOW': 143, 'threshold': 0.3146642536613815, 'tc': 3.963990925433815e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:20,930] Trial 185 finished with value: 1.0648592171099487 and parameters: {'k': 1.3104067366390024, 'tau': 0.03362215913826527, 'L': 2.5113778558995663, 'WINDOW': 152, 'threshold': 0.28448579592402307, 'tc': 7.204574156891044e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,042] Trial 186 finished with value: 1.3821662983022014 and parameters: {'k': 1.4587824020911595, 'tau': 0.05946761460523326, 'L': 2.4564543386779882, 'WINDOW': 137, 'threshold': 0.2996575646681541, 'tc': 1.3214412370006007e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,164] Trial 187 finished with value: 0.25187099068287694 and parameters: {'k': 1.4794190376254146, 'tau': 0.053474442966281364, 'L': 2.557492501258634, 'WINDOW': 137, 'threshold': 0.1397132333925804, 'tc': 4.115750089956027e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,289] Trial 188 finished with value: 1.3266960811176152 and parameters: {'k': 1.543600613464928, 'tau': 0.08945364219936322, 'L': 2.467441128083111, 'WINDOW': 131, 'threshold': 0.32266455562245216, 'tc': 4.3938021689872673e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,429] Trial 189 finished with value: 0.9938154199074392 and parameters: {'k': 1.999144184152784, 'tau': 0.023071864535798844, 'L': 2.5914829043603382, 'WINDOW': 139, 'threshold': 0.26389101219988453, 'tc': 7.25346912568499e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,552] Trial 190 finished with value: 1.2299334957249801 and parameters: {'k': 1.6067803158704224, 'tau': 0.03522346887159242, 'L': 2.5174210702823676, 'WINDOW': 136, 'threshold': 0.3354599987433461, 'tc': 5.13981595499935e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,673] Trial 191 finished with value: 1.4829372907339682 and parameters: {'k': 1.45929205179789, 'tau': 0.06670588327932613, 'L': 2.4121359428836184, 'WINDOW': 142, 'threshold': 0.30308563131260235, 'tc': 3.293699152852973e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,784] Trial 192 finished with value: 1.430121031983721 and parameters: {'k': 1.4695080091337178, 'tau': 0.049070206219472126, 'L': 2.3544053125905395, 'WINDOW': 149, 'threshold': 0.3026875041320714, 'tc': 8.936613547971412e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:21,895] Trial 193 finished with value: 1.1801383897096842 and parameters: {'k': 1.4561985855286208, 'tau': 0.04455986482179057, 'L': 2.350000480801545, 'WINDOW': 140, 'threshold': 0.31601929663679285, 'tc': 8.711651857958335e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,035] Trial 194 finished with value: 1.4721542332121846 and parameters: {'k': 1.5110479858229102, 'tau': 0.05833253149776583, 'L': 2.3732193597564546, 'WINDOW': 145, 'threshold': 0.30309996427540314, 'tc': 4.1394470874465725e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,150] Trial 195 finished with value: 1.2461506176429227 and parameters: {'k': 1.5149683704581591, 'tau': 0.055081291631743634, 'L': 2.3054803427084893, 'WINDOW': 148, 'threshold': 0.2928380119107944, 'tc': 3.229141532696501e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,264] Trial 196 finished with value: 1.0820766466194234 and parameters: {'k': 1.5674429717122034, 'tau': 0.05977778308279367, 'L': 2.2730865706321524, 'WINDOW': 146, 'threshold': 0.27511123506124535, 'tc': 0.00010849999881196314}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,379] Trial 197 finished with value: 1.4875871544035135 and parameters: {'k': 1.4947112354115812, 'tau': 0.06660929906675561, 'L': 2.3858580102264177, 'WINDOW': 155, 'threshold': 0.3032970934045253, 'tc': 4.1231551932885055e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,504] Trial 198 finished with value: 1.0506769256017823 and parameters: {'k': 1.5271156754433304, 'tau': 0.07722665948177493, 'L': 2.338431865451694, 'WINDOW': 154, 'threshold': 0.2883245805580585, 'tc': 9.211059769175228e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,579] Trial 199 finished with value: 1.2899081006121638 and parameters: {'k': 1.503295307830929, 'tau': 0.04215237941200281, 'L': 2.388667259081051, 'WINDOW': 151, 'threshold': 0.32311787347629595, 'tc': 4.19474778703086e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,649] Trial 200 finished with value: 1.3856506132700799 and parameters: {'k': 1.4727253142218903, 'tau': 0.05203331923376758, 'L': 2.361972250757723, 'WINDOW': 144, 'threshold': 0.3046486447892567, 'tc': 0.0001286932642139088}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,728] Trial 201 finished with value: 1.4040591972089491 and parameters: {'k': 1.4783523088711155, 'tau': 0.06874563816661676, 'L': 2.3809847388277077, 'WINDOW': 157, 'threshold': 0.30382208040283865, 'tc': 7.035263528078106e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,793] Trial 202 finished with value: 1.273971593482066 and parameters: {'k': 1.4953881931299822, 'tau': 0.053997587581124044, 'L': 2.3526089965328674, 'WINDOW': 165, 'threshold': 0.30817517095974895, 'tc': 0.0001278094513427083}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,873] Trial 203 finished with value: 1.384499217728384 and parameters: {'k': 1.5852081991230877, 'tau': 0.04905467778697488, 'L': 2.288103334074341, 'WINDOW': 157, 'threshold': 0.3056389326468373, 'tc': 8.345047333737562e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:22,941] Trial 204 finished with value: 0.02521560919108094 and parameters: {'k': 1.5687720436175607, 'tau': 0.07427613791128088, 'L': 2.3060737922707264, 'WINDOW': 158, 'threshold': 0.31506704890028786, 'tc': 0.001034182693061703}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,033] Trial 205 finished with value: 1.1363017472518468 and parameters: {'k': 1.5358554339121193, 'tau': 0.04087661889149019, 'L': 2.240083277214435, 'WINDOW': 156, 'threshold': 0.28368221699184887, 'tc': 8.379979777902421e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,107] Trial 206 finished with value: 1.0224258492044107 and parameters: {'k': 1.4788637011726482, 'tau': 0.06558163748811725, 'L': 2.383891890003895, 'WINDOW': 157, 'threshold': 0.3302510740407618, 'tc': 0.0001566272336888783}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,188] Trial 207 finished with value: 1.3927676909596798 and parameters: {'k': 1.5986356328952807, 'tau': 0.049081757044534144, 'L': 2.3828583093008784, 'WINDOW': 162, 'threshold': 0.30277066777263456, 'tc': 0.00010445256473092512}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,257] Trial 208 finished with value: 1.0301271900259281 and parameters: {'k': 1.4371599649799214, 'tau': 0.06833244251288728, 'L': 2.3568341702196505, 'WINDOW': 161, 'threshold': 0.31833952707727875, 'tc': 0.00012420966388866463}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,345] Trial 209 finished with value: 1.2269264997305307 and parameters: {'k': 1.5411583806073217, 'tau': 0.057179444637073557, 'L': 2.4181194723443813, 'WINDOW': 160, 'threshold': 0.2925863616052327, 'tc': 5.6361899729981475e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,412] Trial 210 finished with value: 1.286335026161991 and parameters: {'k': 1.4774431003312378, 'tau': 0.295706853747787, 'L': 2.4765919728741697, 'WINDOW': 144, 'threshold': 0.3022891383357439, 'tc': 0.0001898015294154303}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,487] Trial 211 finished with value: 1.3958697354638263 and parameters: {'k': 1.5826588489126916, 'tau': 0.050277618148066534, 'L': 2.2399914790611724, 'WINDOW': 153, 'threshold': 0.3048178015745274, 'tc': 9.343897399045821e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,555] Trial 212 finished with value: 1.1805806919319048 and parameters: {'k': 1.5760734175162532, 'tau': 0.03797634569774425, 'L': 2.230432117332567, 'WINDOW': 166, 'threshold': 0.31477347779319914, 'tc': 0.00010659155227004725}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,637] Trial 213 finished with value: 1.414005689774843 and parameters: {'k': 1.6357032965990792, 'tau': 0.048508008046845144, 'L': 2.3782610500974855, 'WINDOW': 154, 'threshold': 0.30054632879870335, 'tc': 4.398480334557236e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,702] Trial 214 finished with value: 1.183664603948523 and parameters: {'k': 1.637326894245296, 'tau': 0.06431578768111051, 'L': 2.2696056963652724, 'WINDOW': 153, 'threshold': 0.2905106319999949, 'tc': 4.52408431254273e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,782] Trial 215 finished with value: 1.2667230396482074 and parameters: {'k': 1.606799023297932, 'tau': 0.043918638628434946, 'L': 2.4097236146962655, 'WINDOW': 155, 'threshold': 0.2985330364994454, 'tc': 6.70884564505131e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,850] Trial 216 finished with value: 1.2770852070185015 and parameters: {'k': 1.6320386223974779, 'tau': 0.07223722728572876, 'L': 2.497190515963975, 'WINDOW': 152, 'threshold': 0.2799202113250209, 'tc': 3.680602486557596e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:23,930] Trial 217 finished with value: 1.236944688464323 and parameters: {'k': 1.6665679784793448, 'tau': 0.08264984552220801, 'L': 2.3106132434464635, 'WINDOW': 160, 'threshold': 0.3248618814873274, 'tc': 8.102293747934116e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,004] Trial 218 finished with value: 1.3121440426564466 and parameters: {'k': 1.535460295876564, 'tau': 0.04944410157616861, 'L': 2.13917002041018, 'WINDOW': 149, 'threshold': 0.3138362500049566, 'tc': 3.588137176131866e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,097] Trial 219 finished with value: 1.0503953175386915 and parameters: {'k': 1.5066863284733323, 'tau': 0.06106615497755872, 'L': 2.3796149145931196, 'WINDOW': 163, 'threshold': 0.2888444335677764, 'tc': 9.828004817305672e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,165] Trial 220 finished with value: 1.490716629193747 and parameters: {'k': 1.9589695165254875, 'tau': 0.05068942249123358, 'L': 2.324301370970474, 'WINDOW': 155, 'threshold': 0.30379189593671746, 'tc': 2.0684112040188956e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,253] Trial 221 finished with value: 1.4815128264830009 and parameters: {'k': 1.9527052374876466, 'tau': 0.040293826828204975, 'L': 2.3233535405307033, 'WINDOW': 154, 'threshold': 0.30167591684272216, 'tc': 3.166935085303952e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,321] Trial 222 finished with value: 0.5356973174794817 and parameters: {'k': 1.956440181777457, 'tau': 0.03413662194327259, 'L': 2.333102252974533, 'WINDOW': 155, 'threshold': 0.3012547634983982, 'tc': 0.0006624184234690193}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,397] Trial 223 finished with value: 1.3054698890369811 and parameters: {'k': 1.9429003597340155, 'tau': 0.0485779624622084, 'L': 2.2045119585858663, 'WINDOW': 160, 'threshold': 0.3133622993505673, 'tc': 3.004027133806757e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,473] Trial 224 finished with value: 1.165889460443842 and parameters: {'k': 1.8885672433546112, 'tau': 0.04286504142347036, 'L': 2.44441851219989, 'WINDOW': 155, 'threshold': 0.2961762659855231, 'tc': 6.800448469236171e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,552] Trial 225 finished with value: 1.3779583701979774 and parameters: {'k': 1.924324002869641, 'tau': 0.06338163137558628, 'L': 2.2708718347877777, 'WINDOW': 149, 'threshold': 0.30704865303821754, 'tc': 3.7845867934618667e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,622] Trial 226 finished with value: 1.0851011378249014 and parameters: {'k': 1.8513430344701656, 'tau': 0.03773198612350562, 'L': 2.3494841667260573, 'WINDOW': 154, 'threshold': 0.28616510017782026, 'tc': 3.0840444187156226e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,705] Trial 227 finished with value: 1.158859647769087 and parameters: {'k': 1.9776205049343858, 'tau': 0.05640977410052812, 'L': 2.40621099267969, 'WINDOW': 147, 'threshold': 0.29697504188415774, 'tc': 7.354079686884506e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,773] Trial 228 finished with value: 1.2822579611445446 and parameters: {'k': 1.963850924846946, 'tau': 0.07592504994992759, 'L': 2.302906241462767, 'WINDOW': 167, 'threshold': 0.32384462075337966, 'tc': 3.797257282576796e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,853] Trial 229 finished with value: 1.0942819739186234 and parameters: {'k': 1.9168273298153702, 'tau': 0.021891977853121806, 'L': 2.2368070321671354, 'WINDOW': 163, 'threshold': 0.27178324530463926, 'tc': 0.00011426197025185539}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:24,923] Trial 230 finished with value: 1.404566398411448 and parameters: {'k': 1.595004811775023, 'tau': 0.05063690652873229, 'L': 2.440951720329028, 'WINDOW': 158, 'threshold': 0.30863350940268897, 'tc': 3.7185810712614605e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,003] Trial 231 finished with value: 1.4012167733577001 and parameters: {'k': 1.5953908536091943, 'tau': 0.05554535569368972, 'L': 2.4552126701593173, 'WINDOW': 153, 'threshold': 0.3088129452599135, 'tc': 4.0031691318446455e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,075] Trial 232 finished with value: 1.2995122392426828 and parameters: {'k': 1.5966401183921928, 'tau': 0.05988254253176706, 'L': 2.45469295604401, 'WINDOW': 158, 'threshold': 0.3146568929319147, 'tc': 1.617987319256728e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,178] Trial 233 finished with value: 1.4531381441266846 and parameters: {'k': 1.557361359766962, 'tau': 0.05254105724936132, 'L': 2.5168153970819813, 'WINDOW': 157, 'threshold': 0.3046493633133537, 'tc': 7.533830658171318e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,247] Trial 234 finished with value: 1.1909024618143855 and parameters: {'k': 1.55520276405966, 'tau': 0.06887445487668231, 'L': 2.501009789783432, 'WINDOW': 156, 'threshold': 0.3172326034121829, 'tc': 4.6062795340068396e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,326] Trial 235 finished with value: 1.2459922303191233 and parameters: {'k': 1.5592837585793422, 'tau': 0.037401265021868246, 'L': 2.511411559085217, 'WINDOW': 153, 'threshold': 0.2951717392875576, 'tc': 7.102260304716847e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,402] Trial 236 finished with value: 1.3663284065266794 and parameters: {'k': 1.5157101091912328, 'tau': 0.05285387367818004, 'L': 2.4487979288753055, 'WINDOW': 150, 'threshold': 0.30995090435946754, 'tc': 3.188975447857151e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,519] Trial 237 finished with value: 1.3058420767357906 and parameters: {'k': 1.8630035678976793, 'tau': 0.03232725622808845, 'L': 2.543471644140894, 'WINDOW': 158, 'threshold': 0.32825844865260984, 'tc': 1.4774267368265676e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,626] Trial 238 finished with value: -0.7088772279342909 and parameters: {'k': 1.99972362712156, 'tau': 0.4096919152197622, 'L': 2.435081589569225, 'WINDOW': 151, 'threshold': 0.28662060753236746, 'tc': 0.0013788160626192155}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,734] Trial 239 finished with value: 1.4083688943597796 and parameters: {'k': 1.6173707820341576, 'tau': 0.05050225657541399, 'L': 2.3341919240375484, 'WINDOW': 153, 'threshold': 0.3047223148862945, 'tc': 8.357596691152476e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,836] Trial 240 finished with value: 1.0953454900241153 and parameters: {'k': 1.6987254818089312, 'tau': 0.06918188477998197, 'L': 2.381839043575241, 'WINDOW': 158, 'threshold': 0.31945807803249593, 'tc': 5.932225742047298e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:25,937] Trial 241 finished with value: 1.3782582158645844 and parameters: {'k': 1.6172858257965723, 'tau': 0.048025493320033294, 'L': 2.3151608262967023, 'WINDOW': 154, 'threshold': 0.30511595737431474, 'tc': 8.689279179698694e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,032] Trial 242 finished with value: 1.1174735601483714 and parameters: {'k': 1.5827128940755144, 'tau': 0.05773909519678276, 'L': 2.3431081594308507, 'WINDOW': 152, 'threshold': 0.2973162989602609, 'tc': 0.0001393297496868471}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,139] Trial 243 finished with value: 1.4054544763424734 and parameters: {'k': 1.5296801756521006, 'tau': 0.040722203518088015, 'L': 2.2699329367375767, 'WINDOW': 156, 'threshold': 0.3083903713708238, 'tc': 3.787446266589251e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,240] Trial 244 finished with value: 1.2967017943739527 and parameters: {'k': 1.51316000616901, 'tau': 0.04051626510670524, 'L': 2.426634524832519, 'WINDOW': 157, 'threshold': 0.31204177430205654, 'tc': 3.57594531401067e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,342] Trial 245 finished with value: 1.1793495634967657 and parameters: {'k': 1.4482069593202531, 'tau': 0.029785717962572313, 'L': 2.2937609307371107, 'WINDOW': 160, 'threshold': 0.2892036581856602, 'tc': 3.422367556331711e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,438] Trial 246 finished with value: 1.381850046044569 and parameters: {'k': 1.5356818935397925, 'tau': 0.06400426792230385, 'L': 2.479396091452459, 'WINDOW': 156, 'threshold': 0.29942368582255113, 'tc': 1.570556032767735e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,535] Trial 247 finished with value: 1.329129020516605 and parameters: {'k': 1.6638104455678102, 'tau': 0.08327167707522432, 'L': 2.38983362252228, 'WINDOW': 148, 'threshold': 0.3101678228989905, 'tc': 6.093374078531804e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,630] Trial 248 finished with value: 1.1799362846462769 and parameters: {'k': 1.4897066886473649, 'tau': 0.04215974317604008, 'L': 2.584782729592481, 'WINDOW': 141, 'threshold': 0.3347654355249948, 'tc': 7.548608804777297e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,730] Trial 249 finished with value: 1.0354460661884888 and parameters: {'k': 1.9557229426716498, 'tau': 0.05460340957733646, 'L': 2.348593299591475, 'WINDOW': 134, 'threshold': 0.3193367503472473, 'tc': 0.00011224780062954169}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,819] Trial 250 finished with value: 1.1851317738718503 and parameters: {'k': 1.5484162778162325, 'tau': 0.276272257417109, 'L': 2.159319879952864, 'WINDOW': 150, 'threshold': 0.28119628438239963, 'tc': 3.2650820225004176e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:26,932] Trial 251 finished with value: 1.3130253826736302 and parameters: {'k': 1.6351817967558537, 'tau': 0.06991366169214415, 'L': 2.2752747657322967, 'WINDOW': 154, 'threshold': 0.298788912104241, 'tc': 3.5445867820585735e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,040] Trial 252 finished with value: 1.366232015472819 and parameters: {'k': 1.4385550730302765, 'tau': 0.04347593414554463, 'L': 2.530850676546823, 'WINDOW': 145, 'threshold': 0.30977688313635426, 'tc': 8.028416754404214e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,168] Trial 253 finished with value: 1.1001542149153112 and parameters: {'k': 1.4877202882918663, 'tau': 0.05646623961286014, 'L': 0.5325383108697508, 'WINDOW': 130, 'threshold': 0.2938185373517569, 'tc': 0.00015592770373270223}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,277] Trial 254 finished with value: 1.2165202555875896 and parameters: {'k': 1.9260792148951718, 'tau': 0.2536716821946478, 'L': 2.40778121950537, 'WINDOW': 159, 'threshold': 0.32603766761809094, 'tc': 6.700873318848052e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,384] Trial 255 finished with value: 1.4869421830783172 and parameters: {'k': 1.5608183829825617, 'tau': 0.033820372057120345, 'L': 2.629049429572551, 'WINDOW': 147, 'threshold': 0.30594783146613536, 'tc': 3.1932533600377634e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,481] Trial 256 finished with value: 1.2598922074361447 and parameters: {'k': 1.5196932251690232, 'tau': 0.025591691350326472, 'L': 1.8584319531255549, 'WINDOW': 142, 'threshold': 0.2920602759369216, 'tc': 5.36903817664664e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,582] Trial 257 finished with value: 1.0914446462291048 and parameters: {'k': 1.5559977020516673, 'tau': 0.03686473925915884, 'L': 2.20180707656451, 'WINDOW': 147, 'threshold': 0.3180072772764239, 'tc': 0.0001138529764586401}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,695] Trial 258 finished with value: 0.755180215101167 and parameters: {'k': 1.4242917603159089, 'tau': 0.13699231864117198, 'L': 2.618260477966819, 'WINDOW': 144, 'threshold': 0.3031280017575458, 'tc': 0.0005636371491244384}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,793] Trial 259 finished with value: 1.2523805241784507 and parameters: {'k': 1.465584026112104, 'tau': 0.23659697459640897, 'L': 2.636629806913581, 'WINDOW': 137, 'threshold': 0.27777828356175177, 'tc': 1.0378422470787699e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,887] Trial 260 finished with value: 0.9953168834753501 and parameters: {'k': 1.8966774412854805, 'tau': 0.43818107778440046, 'L': 1.4880080733059748, 'WINDOW': 134, 'threshold': 0.28587234684363966, 'tc': 8.299928066471503e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:27,994] Trial 261 finished with value: 1.477537248015997 and parameters: {'k': 1.4949628508066901, 'tau': 0.12548311153470143, 'L': 2.2826596877537284, 'WINDOW': 150, 'threshold': 0.3025855743357363, 'tc': 4.153091600445284e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,096] Trial 262 finished with value: 1.167636825938237 and parameters: {'k': 1.5378020347166972, 'tau': 0.12872739314817505, 'L': 2.2522362647247136, 'WINDOW': 147, 'threshold': 0.3206534880800552, 'tc': 9.383655872019064e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,198] Trial 263 finished with value: 1.285644322351602 and parameters: {'k': 1.9713168083274073, 'tau': 0.021601461562814187, 'L': 2.312275453619907, 'WINDOW': 150, 'threshold': 0.2951917175408556, 'tc': 3.98358787686453e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,305] Trial 264 finished with value: 1.2211779744328577 and parameters: {'k': 1.5054992240978455, 'tau': 0.12160338948874275, 'L': 2.335737151231021, 'WINDOW': 141, 'threshold': 0.310907276449232, 'tc': 0.00013549372481351827}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,414] Trial 265 finished with value: 1.4685056356515966 and parameters: {'k': 1.6141220811834083, 'tau': 0.11314695524468542, 'L': 2.198720161768426, 'WINDOW': 145, 'threshold': 0.3016742261343339, 'tc': 4.421550662379936e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,516] Trial 266 finished with value: -1.4408432600917085 and parameters: {'k': 1.6306989671847363, 'tau': 0.11868304535132339, 'L': 2.1293595555400784, 'WINDOW': 145, 'threshold': 0.2906289584323927, 'tc': 0.0019005872119897216}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,614] Trial 267 finished with value: 1.1341159531687754 and parameters: {'k': 1.132923458458425, 'tau': 0.12980570819003084, 'L': 2.2058222885747645, 'WINDOW': 141, 'threshold': 0.3325481411613791, 'tc': 0.00010272890187783917}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,712] Trial 268 finished with value: 1.3733434474205068 and parameters: {'k': 1.5619514533813854, 'tau': 0.1450176790290882, 'L': 2.692001167357316, 'WINDOW': 148, 'threshold': 0.3000818666665798, 'tc': 6.109319341504949e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,834] Trial 269 finished with value: 1.2060164496668424 and parameters: {'k': 1.4511810227543342, 'tau': 0.10772148615501989, 'L': 2.2641777178837326, 'WINDOW': 143, 'threshold': 0.2807264161252239, 'tc': 2.93163466859557e-07}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:28,951] Trial 270 finished with value: 1.0335407229117843 and parameters: {'k': 1.7394418353854684, 'tau': 0.1381485926738666, 'L': 2.1784318172471786, 'WINDOW': 139, 'threshold': 0.3198868938981101, 'tc': 0.00010610605754535183}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,059] Trial 271 finished with value: 1.3719027834857078 and parameters: {'k': 1.5036922796244536, 'tau': 0.11315803333144678, 'L': 2.0588997252288657, 'WINDOW': 131, 'threshold': 0.30035670917675705, 'tc': 4.522900547397709e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,176] Trial 272 finished with value: -0.19996519690427061 and parameters: {'k': 1.6602750966466948, 'tau': 0.03125869561350383, 'L': 2.2443833072595876, 'WINDOW': 150, 'threshold': 0.29097868187522286, 'tc': 0.0011166805463378481}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,294] Trial 273 finished with value: 1.1354389224130121 and parameters: {'k': 1.8004947551650674, 'tau': 0.09961766408075873, 'L': 2.107880382563269, 'WINDOW': 146, 'threshold': 0.31202171377879095, 'tc': 0.00016288985087028698}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,416] Trial 274 finished with value: 1.2315762291366872 and parameters: {'k': 1.9983814670965694, 'tau': 0.33814579219680924, 'L': 2.3112910157545854, 'WINDOW': 136, 'threshold': 0.27003596235643573, 'tc': 8.210056073555628e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,534] Trial 275 finished with value: -0.714808928806881 and parameters: {'k': 1.531036119279617, 'tau': 0.3588089962012195, 'L': 2.2075248556555715, 'WINDOW': 71, 'threshold': 0.45700648613740763, 'tc': 3.4677805734830635e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,659] Trial 276 finished with value: 1.3863830177374628 and parameters: {'k': 1.6114113644904187, 'tau': 0.13789274343649313, 'L': 1.8196659378360953, 'WINDOW': 144, 'threshold': 0.30338451697909646, 'tc': 0.000131844535526159}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,769] Trial 277 finished with value: 1.2152132829219267 and parameters: {'k': 1.9466025942523746, 'tau': 0.12024106331923548, 'L': 1.8710048510378126, 'WINDOW': 81, 'threshold': 0.32575795510808514, 'tc': 7.160744172753257e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,871] Trial 278 finished with value: 1.088981170163522 and parameters: {'k': 1.7008412079387043, 'tau': 0.12884795882354652, 'L': 2.2802935507456965, 'WINDOW': 104, 'threshold': 0.2850342609616757, 'tc': 4.193095202052028e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:29,971] Trial 279 finished with value: 1.200521089909585 and parameters: {'k': 1.4060444743674734, 'tau': 0.035158168798034586, 'L': 2.348019259264481, 'WINDOW': 38, 'threshold': 0.3150005542153996, 'tc': 0.00010848283729850903}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:30,045] Trial 280 finished with value: 1.2195861742232588 and parameters: {'k': 1.566163922728529, 'tau': 0.31372163736828, 'L': 1.9902410428212387, 'WINDOW': 150, 'threshold': 0.2941456381762742, 'tc': 3.7764200678591345e-05}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:30,130] Trial 281 finished with value: 1.4790289413905906 and parameters: {'k': 1.912466356680766, 'tau': 0.1564721401222705, 'L': 1.9297833204462285, 'WINDOW': 139, 'threshold': 0.30518921750886147, 'tc': 1.579581372638772e-06}. Best is trial 181 with value: 1.51137702895629.\n",
            "[I 2025-12-05 02:55:30,200] Trial 282 finished with value: 1.513623412078413 and parameters: {'k': 1.922005246265828, 'tau': 0.14991601313493008, 'L': 1.9097757869986336, 'WINDOW': 138, 'threshold': 0.3017275946538778, 'tc': 6.965846925923312e-07}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,289] Trial 283 finished with value: 1.25738377938459 and parameters: {'k': 1.8987799954558566, 'tau': 0.14077275909549966, 'L': 1.871822560849272, 'WINDOW': 138, 'threshold': 0.27759586670495123, 'tc': 1.3072821487754326e-06}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,362] Trial 284 finished with value: 1.328651553632188 and parameters: {'k': 1.8719531756679195, 'tau': 0.15856092976152414, 'L': 1.945172001632794, 'WINDOW': 134, 'threshold': 0.2951316781748889, 'tc': 6.010499344854084e-06}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,441] Trial 285 finished with value: 1.1669762751452104 and parameters: {'k': 1.9236739159980083, 'tau': 0.15368047305916344, 'L': 1.971974420336945, 'WINDOW': 139, 'threshold': 0.3204854092497442, 'tc': 2.5702770898503025e-07}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,531] Trial 286 finished with value: 1.0427146268688394 and parameters: {'k': 1.9342921765434316, 'tau': 0.14927677510690535, 'L': 1.911553555688372, 'WINDOW': 141, 'threshold': 0.2854665620845572, 'tc': 7.145945707318847e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,615] Trial 287 finished with value: 1.4230020175901226 and parameters: {'k': 1.9697040158563193, 'tau': 0.1575392465609719, 'L': 2.0169909188054036, 'WINDOW': 128, 'threshold': 0.3008441280727215, 'tc': 4.554533754026985e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,686] Trial 288 finished with value: 1.2805711882410777 and parameters: {'k': 1.9693623607725916, 'tau': 0.1622473199270234, 'L': 1.9250692028009249, 'WINDOW': 128, 'threshold': 0.3340047666729248, 'tc': 6.348969753414449e-08}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,764] Trial 289 finished with value: 1.2684439865238069 and parameters: {'k': 1.8845209415551438, 'tau': 0.1577973310482051, 'L': 2.078897081578937, 'WINDOW': 131, 'threshold': 0.3138849647475057, 'tc': 6.424076710062394e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,839] Trial 290 finished with value: 1.3880424602331576 and parameters: {'k': 1.939684906041144, 'tau': 0.17258186996711644, 'L': 2.0434736568845193, 'WINDOW': 127, 'threshold': 0.3025906311324265, 'tc': 0.00011169815452572436}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,921] Trial 291 finished with value: 1.2378478855290298 and parameters: {'k': 1.9826205405632793, 'tau': 0.14718391047581217, 'L': 1.974933677911318, 'WINDOW': 135, 'threshold': 0.293417114780752, 'tc': 3.6595021439424506e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:30,993] Trial 292 finished with value: 1.3112086954231412 and parameters: {'k': 1.9572795186145733, 'tau': 0.13284418739394854, 'L': 2.019699094030156, 'WINDOW': 131, 'threshold': 0.31022420866501477, 'tc': 7.492541902690735e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:31,078] Trial 293 finished with value: 0.8007099620821486 and parameters: {'k': 1.8299081636633705, 'tau': 0.26345227886290146, 'L': 1.9201561175686042, 'WINDOW': 134, 'threshold': 0.3254214111068122, 'tc': 0.0004206212427772494}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:31,149] Trial 294 finished with value: 0.9971156663949851 and parameters: {'k': 1.9068918437428033, 'tau': 0.2163278083115829, 'L': 1.8390304504231638, 'WINDOW': 137, 'threshold': 0.2843628553593231, 'tc': 0.00014489684376093383}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:31,236] Trial 295 finished with value: 1.1613029444506913 and parameters: {'k': 1.9103413653389947, 'tau': 0.16241759758469212, 'L': 2.001710509908904, 'WINDOW': 122, 'threshold': 0.3400322114009389, 'tc': 3.477514718121909e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:31,313] Trial 296 finished with value: 0.45017153520112113 and parameters: {'k': 0.9739413552169944, 'tau': 0.28774508437698776, 'L': 2.556427490443195, 'WINDOW': 142, 'threshold': 0.3014232905074292, 'tc': 0.0007099327846645054}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:31,400] Trial 297 finished with value: 1.1903968670021792 and parameters: {'k': 1.8643715720527154, 'tau': 0.14886894918314067, 'L': 1.7945934406896875, 'WINDOW': 139, 'threshold': 0.273453461722376, 'tc': 5.353142018025094e-07}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:31,474] Trial 298 finished with value: 0.7612235427520256 and parameters: {'k': 1.968839873912878, 'tau': 0.140635384327872, 'L': 1.887256925127521, 'WINDOW': 128, 'threshold': 0.2602884332623745, 'tc': 9.731239234663983e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[I 2025-12-05 02:55:31,577] Trial 299 finished with value: 1.2513614459409546 and parameters: {'k': 1.9388994269675077, 'tau': 0.12454704953480601, 'L': 2.636805802988311, 'WINDOW': 132, 'threshold': 0.3167192621206619, 'tc': 5.5563625841787895e-05}. Best is trial 282 with value: 1.513623412078413.\n",
            "[OK] Optuna done. best_value (robust score): 1.513623412078413\n",
            "best params -> {'k': 1.922005246265828, 'tau': 0.14991601313493008, 'L': 1.9097757869986336, 'WINDOW': 138, 'threshold': 0.3017275946538778, 'tc': 6.965846925923312e-07, 'best_value': 1.513623412078413}\n",
            "[SAVED] final vol-scaled positions -> /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_volscaled_robust_1764903331.csv\n",
            "Full-sample ann Sharpe (net tc): 0.6153593038705103\n",
            "[DONE] Volatility-Scaled SRC optimizer (robust sharpe) finished.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ONE-CELL — A+ SRC OPTIMIZER (Robust Sharpe aggregator, CPCV-safe)\n",
        "import os, time, json, pickle, warnings, math\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "RSTATE = 42\n",
        "N_TRIALS = 300\n",
        "N_OUTER = 5\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "LABEL_H = \"h8\"\n",
        "H = 8\n",
        "ANNUALIZE = 252 * 24\n",
        "MIN_POS_OBS = 30\n",
        "DEFAULT_TC = 5e-4\n",
        "TARGET_VOL = 0.20\n",
        "\n",
        "# Robust aggregator params\n",
        "TRIM_FRAC = 0.20          # fraction to trim from both tails when computing trimmed mean\n",
        "SCALE_COVERAGE_PENALTY = True\n",
        "COVERAGE_REF = MIN_POS_OBS * 1.5\n",
        "\n",
        "# Safety clamps\n",
        "SCALE_CLAMP = (0.1, 3.0)  # clamp for volatility scaling factor to avoid extreme leverage\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def load_latest_meta(path=OUT_DIR):\n",
        "    pkl = sorted(Path(path).glob(\"df_meta_shortlist.v*.pkl\"), key=os.path.getmtime)\n",
        "    if not pkl:\n",
        "        raise FileNotFoundError(\"df_meta_shortlist.v*.pkl not found in OUT_DIR\")\n",
        "    df = pd.read_pickle(str(pkl[-1]))\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    return df, str(pkl[-1])\n",
        "\n",
        "def load_latest_signals(path=OUT_DIR):\n",
        "    sig_files = list(Path(path).glob(\"*signals_aligned*.csv\")) + list(Path(path).glob(\"*signals*.csv\"))\n",
        "    if not sig_files:\n",
        "        raise FileNotFoundError(\"signals CSV not found in OUT_DIR (need meta signals)\")\n",
        "    sig_path = sorted(sig_files, key=os.path.getmtime)[-1]\n",
        "    sig = pd.read_csv(sig_path, index_col=0)\n",
        "    try:\n",
        "        sig.index = pd.to_datetime(sig.index, utc=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return sig, str(sig_path)\n",
        "\n",
        "def load_step03_returns(path=OUT_DIR, prefer_h=H):\n",
        "    candidates = sorted(Path(path).glob(\"df_step03_tb_multi*.csv\"), key=os.path.getmtime)\n",
        "    if not candidates:\n",
        "        return None, None\n",
        "    df_step = pd.read_csv(str(candidates[-1]), index_col=None)\n",
        "    tbreak_candidates = [c for c in df_step.columns if c.startswith(\"tb_t_break\")]\n",
        "    ret_candidates = [c for c in df_step.columns if c.startswith(\"tb_ret\")]\n",
        "    if not tbreak_candidates or not ret_candidates:\n",
        "        return None, None\n",
        "    tb_col = f\"tb_t_break_h{prefer_h}\"\n",
        "    ret_col = f\"tb_ret_at_break_h{prefer_h}\"\n",
        "    tb_step_col = tb_col if tb_col in df_step.columns else tbreak_candidates[0]\n",
        "    ret_step_col = ret_col if ret_col in df_step.columns else ret_candidates[0]\n",
        "    return df_step, (tb_step_col, ret_step_col)\n",
        "\n",
        "# ---------- volatility-scaled SRC core ----------\n",
        "def compute_positions_vol_scaled(params, pL, pS, vol_series, rets_series, target_vol=TARGET_VOL):\n",
        "    \"\"\"\n",
        "    Vol-scaled SRC with causal vol estimates and safety clamps.\n",
        "    Inputs are pandas.Series aligned to same index.\n",
        "    \"\"\"\n",
        "    raw = (pL - pS).astype(float)\n",
        "    tau = float(params[\"tau\"])\n",
        "    raw_s = np.tanh(raw / tau)  # S-curve squash\n",
        "    # defensive vol handling (causal)\n",
        "    vol_series = vol_series.replace(0, np.nan).fillna(method=\"ffill\").fillna(vol_series.mean() if vol_series.mean() > 0 else 1e-6)\n",
        "    pos_u = float(params[\"k\"]) * (raw_s / vol_series)  # unit-scaled position (array-like / Series)\n",
        "    w = max(1, int(params.get(\"WINDOW\", 24)))\n",
        "    # realized ex-post vol estimate of unit-strategy; use past-only window and shift to be causal\n",
        "    realized_vol = ((pos_u * rets_series).rolling(window=w, min_periods=1).std().shift(1) * math.sqrt(ANNUALIZE)).fillna(method=\"ffill\")\n",
        "    realized_vol = realized_vol.replace(0, np.nan).fillna(realized_vol.mean() if np.isfinite(realized_vol.mean()) else 1e-6)\n",
        "    # scaling factor (annualized target_vol / realized_vol). Clip to safe range.\n",
        "    scale = (target_vol / realized_vol).clip(SCALE_CLAMP[0], SCALE_CLAMP[1])\n",
        "    pos_scaled = pos_u * scale\n",
        "    pos_final = pos_scaled.clip(-params[\"L\"], params[\"L\"])\n",
        "    return pos_final\n",
        "\n",
        "# ---------- fold evaluation (robust sharpe aggregator) ----------\n",
        "def evaluate_params_on_folds(params, p_long_series, p_short_series, rets_series, outer_splits, pos_all, idx, target_vol=TARGET_VOL):\n",
        "    fold_sharpes = []\n",
        "    fold_winrates = []\n",
        "    fold_counts = []\n",
        "    for train_pos, test_pos in outer_splits:\n",
        "        train_ts = pos_all.index[train_pos]\n",
        "        test_ts = pos_all.index[test_pos]\n",
        "        if len(test_ts) == 0:\n",
        "            continue\n",
        "        # causal vol computed on full rets but shifted (past-only)\n",
        "        vol_series_full = rets_series.rolling(window=params[\"WINDOW\"], min_periods=max(1, params[\"WINDOW\"]//2)).std().shift(1)\n",
        "        vol_series = vol_series_full.reindex(test_ts).fillna(method=\"ffill\")\n",
        "        if vol_series.isna().all():\n",
        "            # fallback to train vol\n",
        "            train_vol = rets_series.loc[train_ts].std()\n",
        "            vol_series = pd.Series(train_vol if np.isfinite(train_vol) else 1e-6, index=test_ts)\n",
        "        # thresholding (causal reindex)\n",
        "        if params[\"threshold\"] > 0:\n",
        "            raw_test = (p_long_series - p_short_series).reindex(test_ts)\n",
        "            mask = raw_test.abs() < params[\"threshold\"]\n",
        "            pL_test = p_long_series.reindex(test_ts).copy()\n",
        "            pS_test = p_short_series.reindex(test_ts).copy()\n",
        "            pL_test[mask] = 0.5\n",
        "            pS_test[mask] = 0.5\n",
        "        else:\n",
        "            pL_test = p_long_series.reindex(test_ts)\n",
        "            pS_test = p_short_series.reindex(test_ts)\n",
        "        # compute positions (vol-scaled)\n",
        "        pos = compute_positions_vol_scaled(params, pL_test, pS_test, vol_series, rets_series.reindex(test_ts), target_vol=target_vol).values\n",
        "        rets = rets_series.reindex(test_ts).fillna(0.0).values\n",
        "        pnl = pos * rets\n",
        "        deltas = np.empty_like(pos); deltas[0] = pos[0]; deltas[1:] = pos[1:] - pos[:-1]\n",
        "        tc_costs = float(params.get(\"tc\", DEFAULT_TC)) * np.abs(deltas)\n",
        "        net = pnl - tc_costs\n",
        "        # fold sharpe\n",
        "        if np.nanstd(net) == 0 or np.allclose(net, 0):\n",
        "            sharpe = 0.0\n",
        "        else:\n",
        "            sharpe = (np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)\n",
        "        non_zero = int((np.abs(pos) > 1e-6).sum())\n",
        "        # penalize tiny-sample folds\n",
        "        if non_zero < MIN_POS_OBS:\n",
        "            sharpe *= 0.5\n",
        "        fold_sharpes.append(float(sharpe))\n",
        "        fold_counts.append(non_zero)\n",
        "        # fold winrate diagnostic\n",
        "        nonzero_mask = np.abs(pos) > 1e-6\n",
        "        n_trades = int(nonzero_mask.sum())\n",
        "        if n_trades > 0:\n",
        "            wins = int(((pnl > 0) & nonzero_mask).sum())\n",
        "            winrate = wins / n_trades\n",
        "        else:\n",
        "            winrate = np.nan\n",
        "        fold_winrates.append(winrate)\n",
        "    if not fold_sharpes:\n",
        "        return 0.0, [], float(\"nan\")\n",
        "    fold_sharpes = np.array(fold_sharpes, dtype=float)\n",
        "    # robust aggregator: trimmed mean with median fallback\n",
        "    if len(fold_sharpes) >= 3:\n",
        "        trim = int(np.floor(TRIM_FRAC * len(fold_sharpes)))\n",
        "        sorted_sharpes = np.sort(fold_sharpes)\n",
        "        if len(sorted_sharpes) - 2*trim > 0:\n",
        "            robust_sharpe = float(np.mean(sorted_sharpes[trim:len(sorted_sharpes)-trim]))\n",
        "        else:\n",
        "            robust_sharpe = float(np.median(sorted_sharpes))\n",
        "    else:\n",
        "        robust_sharpe = float(np.median(fold_sharpes))\n",
        "    # aggregate winrate (median across folds)\n",
        "    valid_winrates = np.array([v for v in fold_winrates if not np.isnan(v)], dtype=float)\n",
        "    agg_winrate = float(np.median(valid_winrates)) if len(valid_winrates) > 0 else float(\"nan\")\n",
        "    return robust_sharpe, fold_counts, agg_winrate\n",
        "\n",
        "# ---------- main ----------\n",
        "if __name__ == \"__main__\":\n",
        "    df, meta_p = load_latest_meta(OUT_DIR)\n",
        "    sig, sig_p = load_latest_signals(OUT_DIR)\n",
        "    print(f\"[INFO] loaded df_meta_shortlist: {Path(meta_p).name} rows={df.shape[0]}\")\n",
        "    print(f\"[INFO] using signals: {Path(sig_p).name}\")\n",
        "\n",
        "    # pick calibrated probs preferred\n",
        "    if \"p_meta_long_cal\" in sig.columns and \"p_meta_short_cal\" in sig.columns:\n",
        "        p_long = sig[\"p_meta_long_cal\"].astype(float)\n",
        "        p_short = sig[\"p_meta_short_cal\"].astype(float)\n",
        "    elif \"p_meta_long\" in sig.columns and \"p_meta_short\" in sig.columns:\n",
        "        p_long = sig[\"p_meta_long\"].astype(float)\n",
        "        p_short = sig[\"p_meta_short\"].astype(float)\n",
        "    elif \"p_long\" in sig.columns and \"p_short\" in sig.columns:\n",
        "        p_long = sig[\"p_long\"].astype(float)\n",
        "        p_short = sig[\"p_short\"].astype(float)\n",
        "    else:\n",
        "        raise RuntimeError(\"No suitable probability columns found (p_meta_long_cal / p_meta_long / p_long)\")\n",
        "\n",
        "    p_long = p_long.reindex(df.index).fillna(0.5)\n",
        "    p_short = p_short.reindex(df.index).fillna(0.5)\n",
        "\n",
        "    # ensure returns exist; map from STEP03 if needed\n",
        "    ret_col = f\"tb_ret_{H}\"\n",
        "    if ret_col not in df.columns:\n",
        "        df_step, cols = load_step03_returns(OUT_DIR, prefer_h=H)\n",
        "        if df_step is None:\n",
        "            raise RuntimeError(f\"{ret_col} missing and no step03 CSV found.\")\n",
        "        tb_step_col, ret_step_col = cols\n",
        "        df_step[tb_step_col] = pd.to_datetime(df_step[tb_step_col], utc=True, errors=\"coerce\")\n",
        "        map_ser = df_step.groupby(tb_step_col)[ret_step_col].mean()\n",
        "        tb_meta_col = f\"tb_t_break_h{H}\" if f\"tb_t_break_h{H}\" in df.columns else None\n",
        "        if tb_meta_col is None:\n",
        "            raise RuntimeError(f\"STEP03 exists but df_meta_shortlist has no tb_t_break_h{H} to map returns.\")\n",
        "        tb_idx = pd.to_datetime(df[tb_meta_col], utc=True, errors=\"coerce\")\n",
        "        mapped_rets = tb_idx.map(map_ser).fillna(0.0).astype(float)\n",
        "        df[ret_col] = mapped_rets.values\n",
        "        out_patched = Path(OUT_DIR) / (Path(meta_p).name + f\".with_tb_ret_{H}.pkl\")\n",
        "        pd.to_pickle(df, out_patched)\n",
        "        print(f\"[DONE] merged {ret_col} into df_meta -> {out_patched}\")\n",
        "    else:\n",
        "        print(f\"[INFO] using existing returns column: {ret_col}\")\n",
        "\n",
        "    rets = df[ret_col].reindex(df.index).astype(float).fillna(0.0)\n",
        "\n",
        "    # purge utils (must be present)\n",
        "    try:\n",
        "        from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"Paste patched b0_07_purge_utils into notebook before running.\")\n",
        "\n",
        "    idx = df.index\n",
        "    exp_all = compute_exposure_intervals(idx, df[f\"tb_t_break_{LABEL_H}\"], horizon_fallback=None, last_index=idx[-1])\n",
        "    pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "    outer_splits = list(purged_cv_splits(pos_all, n_splits=N_OUTER, embargo=EMBARGO, index=idx, drop_unmapped=True, random_state=RSTATE))\n",
        "    if len(outer_splits) < N_OUTER:\n",
        "        print(f\"[WARN] produced fewer outer splits ({len(outer_splits)}) than requested N_OUTER={N_OUTER}\")\n",
        "    print(f\"[INFO] outer_splits: {len(outer_splits)}\")\n",
        "\n",
        "    # Optuna objective (robust sharpe)\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"k\": trial.suggest_float(\"k\", 0.2, 2.0),\n",
        "            \"tau\": trial.suggest_float(\"tau\", 0.02, 0.5),\n",
        "            \"L\": trial.suggest_float(\"L\", 0.5, 3.0),\n",
        "            \"WINDOW\": trial.suggest_int(\"WINDOW\", 24, 168),\n",
        "            \"threshold\": trial.suggest_float(\"threshold\", 0.0, 0.5),\n",
        "            \"tc\": trial.suggest_float(\"tc\", 0.0, 0.002),\n",
        "        }\n",
        "        robust_sharpe, fold_counts, agg_winrate = evaluate_params_on_folds(params, p_long, p_short, rets, outer_splits, pos_all, idx, target_vol=TARGET_VOL)\n",
        "        mean_coverage = np.mean(fold_counts) if fold_counts else 0.0\n",
        "        coverage_reg = min(1.0, mean_coverage / COVERAGE_REF) if SCALE_COVERAGE_PENALTY else 1.0\n",
        "        score = float(robust_sharpe * coverage_reg)\n",
        "        # record attrs for diagnostics\n",
        "        trial.set_user_attr(\"coverage_mean\", float(mean_coverage))\n",
        "        trial.set_user_attr(\"raw_robust_sharpe\", float(robust_sharpe))\n",
        "        trial.set_user_attr(\"agg_winrate\", float(agg_winrate) if not np.isnan(agg_winrate) else None)\n",
        "        return score\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RSTATE))\n",
        "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=1)\n",
        "\n",
        "    ts = int(time.time())\n",
        "    best = deepcopy(study.best_trial.params)\n",
        "    best[\"best_value\"] = float(study.best_value)\n",
        "    best_path = os.path.join(OUT_DIR, f\"A_SRC_optuna_best_params_{ts}.json\")\n",
        "    pickle.dump(study, open(os.path.join(OUT_DIR, f\"A_SRC_optuna_study_{ts}.pkl\"), \"wb\"))\n",
        "    with open(best_path, \"w\") as f:\n",
        "        json.dump(best, f, indent=2)\n",
        "    print(\"[OK] Optuna done. best_value (robust score):\", study.best_value)\n",
        "    print(\"best params ->\", best)\n",
        "\n",
        "    # build full-sample vol-scaled positions with best params (causal vol)\n",
        "    best_params = deepcopy(study.best_trial.params)\n",
        "    if best_params.get(\"threshold\", 0.0) > 0:\n",
        "        raw_full = (p_long - p_short)\n",
        "        mask_full = raw_full.abs() < best_params[\"threshold\"]\n",
        "        pL_full = p_long.copy(); pS_full = p_short.copy()\n",
        "        pL_full[mask_full] = 0.5; pS_full[mask_full] = 0.5\n",
        "    else:\n",
        "        pL_full, pS_full = p_long.copy(), p_short.copy()\n",
        "\n",
        "    vol_full = rets.rolling(window=best_params[\"WINDOW\"], min_periods=max(1, best_params[\"WINDOW\"]//2)).std().shift(1)\n",
        "    vol_full = vol_full.fillna(method=\"ffill\").fillna(rets.std() if np.isfinite(rets.std()) else 1e-6)\n",
        "\n",
        "    positions = compute_positions_vol_scaled(best_params, pL_full, pS_full, vol_full, rets, target_vol=TARGET_VOL)\n",
        "    positions = positions.clip(-best_params[\"L\"], best_params[\"L\"])\n",
        "\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out[\"p_long\"] = p_long\n",
        "    out[\"p_short\"] = p_short\n",
        "    out[\"raw\"] = p_long - p_short\n",
        "    out[\"position\"] = positions\n",
        "    out[\"vol\"] = vol_full\n",
        "    out_path = os.path.join(OUT_DIR, f\"A_SRC_positions_volscaled_robust_{ts}.csv\")\n",
        "    out.to_csv(out_path)\n",
        "\n",
        "    # diagnostics full-sample\n",
        "    pos_vals = out[\"position\"].values\n",
        "    pnl = pos_vals * rets.values\n",
        "    deltas = np.empty_like(pos_vals); deltas[0] = pos_vals[0]; deltas[1:] = pos_vals[1:] - pos_vals[:-1]\n",
        "    tc_costs = best_params.get(\"tc\", DEFAULT_TC) * np.abs(deltas)\n",
        "    net = pnl - tc_costs\n",
        "    ann_sharpe = 0.0\n",
        "    if np.nanstd(net) > 0:\n",
        "        ann_sharpe = (np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)\n",
        "\n",
        "    meta_out = {\n",
        "        \"generated_at\": ts,\n",
        "        \"best\": best,\n",
        "        \"ann_sharpe_full\": float(ann_sharpe),\n",
        "        \"target_vol\": TARGET_VOL,\n",
        "        \"trim_frac\": TRIM_FRAC,\n",
        "        \"scale_clamp\": SCALE_CLAMP\n",
        "    }\n",
        "    with open(os.path.join(OUT_DIR, f\"A_SRC_meta_volscaled_robust_{ts}.json\"), \"w\") as f:\n",
        "        json.dump(meta_out, f, indent=2)\n",
        "\n",
        "    print(\"[SAVED] final vol-scaled positions ->\", out_path)\n",
        "    print(\"Full-sample ann Sharpe (net tc):\", ann_sharpe)\n",
        "    print(\"[DONE] Volatility-Scaled SRC optimizer (robust sharpe) finished.\")\n",
        "\n",
        "    # minimal asserts\n",
        "    assert os.path.exists(out_path), \"output CSV not created\"\n",
        "    assert math.isfinite(ann_sharpe) or ann_sharpe == 0.0, \"ann_sharpe must be finite\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO14KDc1e-Vs",
        "outputId": "049b2252-014e-463b-e0da-e280dbd0bde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] using A_SRC positions file: A_SRC_positions_volscaled_robust_1764903331.csv\n",
            "[INFO] positions shape: (17521, 6); sample head:\n",
            "                             p_long   p_short       raw  position       vol  \\\n",
            "2023-12-06 00:00:00+00:00  0.735025  0.265431  0.469594  1.909776  0.008973   \n",
            "2023-12-06 01:00:00+00:00  0.735025  0.265431  0.469594  1.909776  0.008973   \n",
            "2023-12-06 02:00:00+00:00  0.728129  0.272196  0.455933  1.909776  0.008973   \n",
            "\n",
            "                           tc  \n",
            "2023-12-06 00:00:00+00:00 NaN  \n",
            "2023-12-06 01:00:00+00:00 NaN  \n",
            "2023-12-06 02:00:00+00:00 NaN  \n",
            "[INFO] meta present — running alignment checks\n",
            "[INFO] index intersection: 17521 / 17521 = 100.00%\n",
            "[DONE] loaded A_SRC positions for backtest.\n"
          ]
        }
      ],
      "source": [
        "# REPLACE this block into your backtest v2 (pos loading + alignment guard)\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math, time\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "path = Path(OUT_DIR)\n",
        "\n",
        "def load_a_src_positions():\n",
        "    files = sorted(path.glob(\"A_SRC_positions_volscaled*.csv\"), key=lambda p: p.stat().st_mtime)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(\"No A_SRC_positions_volscaled*.csv found in OUT_DIR. Did optimizer write final positions?\")\n",
        "    # prefer explicitly latest A_SRC file\n",
        "    pos_path = files[-1]\n",
        "    print(f\"[INFO] using A_SRC positions file: {pos_path.name}\")\n",
        "    pos_df = pd.read_csv(pos_path, index_col=0)\n",
        "    # ensure datetime index\n",
        "    try:\n",
        "        pos_df.index = pd.to_datetime(pos_df.index, utc=True)\n",
        "    except Exception:\n",
        "        # try parse index as numeric then to datetime\n",
        "        try:\n",
        "            pos_df.index = pd.to_datetime(pos_df.index.astype(float), unit='s', utc=True)\n",
        "            print(\"[INFO] parsed pos index from epoch seconds\")\n",
        "        except Exception:\n",
        "            print(\"[WARN] could not convert pos index to datetime automatically; leaving as-is\")\n",
        "    # check required column\n",
        "    if \"position\" not in pos_df.columns:\n",
        "        # common alternative: 'pos' or 'positions'\n",
        "        for alt in (\"pos\",\"positions\",\"position_scaled\"):\n",
        "            if alt in pos_df.columns:\n",
        "                pos_df = pos_df.rename(columns={alt: \"position\"})\n",
        "                print(f\"[WARN] renamed column {alt} -> position\")\n",
        "                break\n",
        "    if \"position\" not in pos_df.columns:\n",
        "        raise RuntimeError(\"positions CSV must include 'position' column (or pos/positions).\")\n",
        "    # ensure tc exists or default\n",
        "    if \"tc\" not in pos_df.columns:\n",
        "        pos_df[\"tc\"] = np.nan\n",
        "    # basic sanity\n",
        "    print(f\"[INFO] positions shape: {pos_df.shape}; sample head:\\n{pos_df.head(3)}\")\n",
        "    return pos_df, str(pos_path)\n",
        "\n",
        "# load meta & returns using your existing loader (call existing helper)\n",
        "# assume df_meta and meta_path are produced by your robust loader cell earlier\n",
        "pos_df, pos_path = load_a_src_positions()\n",
        "\n",
        "# quick alignment diagnostics with df_meta (if present)\n",
        "if 'df_meta' in globals():\n",
        "    print(\"[INFO] meta present — running alignment checks\")\n",
        "    # ensure df_meta index datetime\n",
        "    try:\n",
        "        df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # intersection ratio\n",
        "    intersect = df_meta.index.intersection(pos_df.index)\n",
        "    pct = len(intersect) / max(1, len(pos_df.index))\n",
        "    print(f\"[INFO] index intersection: {len(intersect)} / {len(pos_df.index)} = {pct:.2%}\")\n",
        "    if pct < 0.5:\n",
        "        print(\"[WARN] low intersection (<50%). Trying robust nearest-time reindex (forward/backward within tolerance).\")\n",
        "        # attempt nearest reindex with 30m tolerance (adjust to your frequency)\n",
        "        tol = pd.Timedelta(\"30m\")\n",
        "        # build series from meta returns\n",
        "        if \"tb_ret_8\" in df_meta.columns:\n",
        "            ret_ser = df_meta[\"tb_ret_8\"]\n",
        "        else:\n",
        "            # fallback: any tb_ret*\n",
        "            ret_col = next((c for c in df_meta.columns if c.startswith(\"tb_ret\")), None)\n",
        "            if ret_col is None:\n",
        "                raise RuntimeError(\"df_meta lacks tb_ret* column for reindexing.\")\n",
        "            ret_ser = df_meta[ret_col]\n",
        "        # reindex by nearest\n",
        "        reindexed = ret_ser.reindex(pos_df.index, method=None)  # keep default\n",
        "        # if too many NaNs, attempt nearest\n",
        "        if reindexed.isna().mean() > 0.05:\n",
        "            reindexed = ret_ser.reindex(pos_df.index, method=\"nearest\", tolerance=tol)\n",
        "            nan_pct = reindexed.isna().mean()\n",
        "            print(f\"[INFO] after nearest reindex with tol={tol}: NaN% = {nan_pct:.2%}\")\n",
        "            if nan_pct > 0.2:\n",
        "                print(\"[WARN] still many NaNs after nearest reindex — inspect timestamps (frequency/timezone mismatch).\")\n",
        "        # attach for downstream use\n",
        "        df_meta = df_meta.assign(tb_ret_8 = df_meta[\"tb_ret_8\"]).copy() if \"tb_ret_8\" in df_meta.columns else df_meta.copy()\n",
        "        df_meta = df_meta.reindex(df_meta.index)  # no-op to keep object\n",
        "        # save reindexed return series for backtest to use (user can override)\n",
        "        reindexed.name = \"tb_ret_8_reindexed_for_positions\"\n",
        "        reindexed_path = Path(OUT_DIR)/f\"tb_ret_8_reindexed_for_positions_{int(time.time())}.csv\"\n",
        "        reindexed.to_csv(reindexed_path)\n",
        "        print(f\"[INFO] wrote reindexed returns preview to {reindexed_path.name}\")\n",
        "else:\n",
        "    print(\"[INFO] df_meta not in globals() — ensure you call robust meta loader before this cell.\")\n",
        "\n",
        "# final: expose pos_df, pos_path to rest of notebook (same variable names your backtest expects)\n",
        "print(\"[DONE] loaded A_SRC positions for backtest.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-gslYqEfL_0",
        "outputId": "60c4aa65-1cd3-433e-b8ab-ba9586896a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] using A_SRC positions file: A_SRC_positions_volscaled_robust_1764903331.csv\n",
            "[INFO] loaded df_meta_shortlist: df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl rows=17521\n",
            "[WARN] tc column empty -> using default tc_rate=nan\n",
            "[DIAG] mean_abs_pos=0.880278, mean_pos=-0.858478\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Object of type PosixPath is not JSON serializable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3362351591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0mtrades_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrades_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;31m# plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type PosixPath is not JSON serializable"
          ]
        }
      ],
      "source": [
        "\n",
        "# ONE-CELL PATCH — Backtest v2 (robust, diagnostics, fixed Series usage)\n",
        "# Paste replace the old backtest v2 cell.\n",
        "import os, time, json, math, warnings\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------- CONFIG --------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "MIN_POS_THRESH = 1e-6\n",
        "ANNUALIZE = 252 * 24   # hourly-based\n",
        "BOOTSTRAP_ITERS = 1000\n",
        "BOOTSTRAP_BLOCK = 24   # hours\n",
        "PLOT_DPI = 120\n",
        "\n",
        "# -------- helpers --------\n",
        "def load_a_src_positions(out_dir=OUT_DIR):\n",
        "    p = Path(out_dir)\n",
        "    files = sorted(p.glob(\"A_SRC_positions_volscaled*.csv\"), key=lambda f: f.stat().st_mtime)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(\"No A_SRC_positions_volscaled*.csv found in OUT_DIR.\")\n",
        "    pos_path = files[-1]\n",
        "    print(f\"[INFO] using A_SRC positions file: {pos_path.name}\")\n",
        "    pos_df = pd.read_csv(pos_path, index_col=0)\n",
        "    # try robust datetime parsing\n",
        "    try:\n",
        "        pos_df.index = pd.to_datetime(pos_df.index, utc=True)\n",
        "    except Exception:\n",
        "        try:\n",
        "            pos_df.index = pd.to_datetime(pos_df.index.astype(float), unit='s', utc=True)\n",
        "            print(\"[INFO] parsed pos index from epoch seconds\")\n",
        "        except Exception:\n",
        "            print(\"[WARN] pos index could not be parsed to datetime; leaving as-is\")\n",
        "    # standardize column names\n",
        "    if \"position\" not in pos_df.columns:\n",
        "        for alt in (\"pos\",\"positions\",\"position_scaled\",\"position_scaled_vol\"):\n",
        "            if alt in pos_df.columns:\n",
        "                pos_df = pos_df.rename(columns={alt: \"position\"})\n",
        "                print(f\"[WARN] renamed column {alt} -> position\")\n",
        "                break\n",
        "    if \"position\" not in pos_df.columns:\n",
        "        raise RuntimeError(\"positions CSV must include 'position' column.\")\n",
        "    # fill tc column if missing with small default\n",
        "    if \"tc\" not in pos_df.columns:\n",
        "        pos_df[\"tc\"] = np.nan\n",
        "    return pos_df, str(pos_path)\n",
        "\n",
        "def block_bootstrap_sharpe(net_series: np.ndarray, iters: int = 1000, block: int = 24):\n",
        "    n = len(net_series)\n",
        "    if n <= 1:\n",
        "        return (np.nan, np.nan)\n",
        "    boot_sharpes = []\n",
        "    starts = np.arange(0, n - block + 1)\n",
        "    if len(starts) <= 0:\n",
        "        # iid fallback\n",
        "        for _ in range(iters):\n",
        "            s = np.random.choice(net_series, size=n, replace=True)\n",
        "            boot_sharpes.append((np.nanmean(s) / np.nanstd(s)) * np.sqrt(ANNUALIZE) if np.nanstd(s) != 0 else 0.0)\n",
        "    else:\n",
        "        for _ in range(iters):\n",
        "            res = []\n",
        "            while len(res) < n:\n",
        "                st = np.random.choice(starts)\n",
        "                blk = net_series[st:st+block]\n",
        "                res.append(blk)\n",
        "            res_arr = np.concatenate(res)[:n]\n",
        "            boot_sharpes.append((np.nanmean(res_arr) / np.nanstd(res_arr)) * np.sqrt(ANNUALIZE) if np.nanstd(res_arr) != 0 else 0.0)\n",
        "    lo = np.percentile(boot_sharpes, 2.5)\n",
        "    hi = np.percentile(boot_sharpes, 97.5)\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def trade_reconstruction(positions: np.ndarray, index: pd.Index, rets: np.ndarray, tc_rate: float):\n",
        "    mask = np.abs(positions) > MIN_POS_THRESH\n",
        "    if not mask.any():\n",
        "        return []\n",
        "    trades = []\n",
        "    N = len(positions)\n",
        "    i = 0\n",
        "    while i < N:\n",
        "        if not mask[i]:\n",
        "            i += 1\n",
        "            continue\n",
        "        s = i\n",
        "        sgn = np.sign(positions[s])\n",
        "        j = s + 1\n",
        "        while j < N and mask[j] and np.sign(positions[j]) == sgn:\n",
        "            j += 1\n",
        "        pos_slice = positions[s:j]\n",
        "        rets_slice = rets[s:j]\n",
        "        pnl = float((pos_slice * rets_slice).sum())\n",
        "        # transaction cost: sum abs(dpos) * tc_rate\n",
        "        dpos = np.empty(len(pos_slice))\n",
        "        dpos[0] = pos_slice[0]\n",
        "        if len(pos_slice) > 1:\n",
        "            dpos[1:] = pos_slice[1:] - pos_slice[:-1]\n",
        "        tc = float(tc_rate * np.abs(dpos).sum())\n",
        "        net = pnl - tc\n",
        "        trades.append({\n",
        "            \"entry_time\": str(index[s]),\n",
        "            \"exit_time\": str(index[j-1]),\n",
        "            \"entry_idx\": int(s),\n",
        "            \"exit_idx\": int(j-1),\n",
        "            \"entry_pos\": float(pos_slice[0]),\n",
        "            \"exit_pos\": float(pos_slice[-1]),\n",
        "            \"pnl\": pnl,\n",
        "            \"tc\": tc,\n",
        "            \"net\": net,\n",
        "            \"len\": int(j-1 - s + 1),\n",
        "            \"win\": int(net > 0)\n",
        "        })\n",
        "        i = j\n",
        "    return trades\n",
        "\n",
        "# -------- main --------\n",
        "ts = int(time.time())\n",
        "pos_df, pos_path = load_a_src_positions(OUT_DIR)\n",
        "\n",
        "# load meta & returns (robust loader)\n",
        "pkl = sorted(Path(OUT_DIR).glob(\"df_meta_shortlist.v*.pkl\"), key=lambda p: p.stat().st_mtime)\n",
        "if not pkl:\n",
        "    raise FileNotFoundError(\"df_meta_shortlist.v*.pkl not found in OUT_DIR.\")\n",
        "meta_path = pkl[-1]\n",
        "df_meta = pd.read_pickle(meta_path)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "print(f\"[INFO] loaded df_meta_shortlist: {Path(meta_path).name} rows={len(df_meta)}\")\n",
        "\n",
        "# pick return column (prefer tb_ret_8)\n",
        "ret_col = \"tb_ret_8\" if \"tb_ret_8\" in df_meta.columns else next((c for c in df_meta.columns if c.startswith(\"tb_ret\")), None)\n",
        "if ret_col is None:\n",
        "    raise RuntimeError(\"No tb_ret* column found in df_meta_shortlist.\")\n",
        "rets = df_meta[ret_col].reindex(pos_df.index).fillna(0.0).astype(float)\n",
        "rets_arr = rets.values\n",
        "\n",
        "# ensure pos_df index aligned and monotonic\n",
        "if not pos_df.index.is_monotonic_increasing:\n",
        "    pos_df = pos_df.sort_index()\n",
        "    print(\"[INFO] sorted pos_df by index (monotonic fix)\")\n",
        "\n",
        "# reindex pos_df to meta index if necessary (here intersection was 100% in your run)\n",
        "pos_df = pos_df.reindex(rets.index)\n",
        "\n",
        "# extract positions, p_long/p_short, tc\n",
        "positions = pos_df[\"position\"].astype(float).fillna(0.0).values\n",
        "p_long = pos_df.get(\"p_long\", pd.Series(0.5, index=pos_df.index)).reindex(pos_df.index).fillna(0.5).astype(float)\n",
        "p_short = pos_df.get(\"p_short\", pd.Series(0.5, index=pos_df.index)).reindex(pos_df.index).fillna(0.5).astype(float)\n",
        "# tc: if entirely NaN fallback to default small tc\n",
        "if pos_df[\"tc\"].isna().all():\n",
        "    tc_rate = float(pos_df.get(\"tc\", pd.Series([5e-4])).iloc[0])\n",
        "    print(f\"[WARN] tc column empty -> using default tc_rate={tc_rate}\")\n",
        "else:\n",
        "    # use first non-null tc (optimizer may write same per-row)\n",
        "    tc_rate = float(pos_df[\"tc\"].dropna().iloc[0])\n",
        "    print(f\"[INFO] using tc_rate={tc_rate}\")\n",
        "\n",
        "# quick unit checks & diagnostics\n",
        "mean_abs_pos = float(np.nanmean(np.abs(positions)))\n",
        "mean_pos = float(np.nanmean(positions))\n",
        "print(f\"[DIAG] mean_abs_pos={mean_abs_pos:.6f}, mean_pos={mean_pos:.6f}\")\n",
        "if mean_abs_pos > 1.5:\n",
        "    print(\"[DIAG] mean_abs_pos > 1.5 — positions look volatility-scaled / leveraged. This is OK if tb_ret_* are per-bar returns (not percent).\")\n",
        "if np.all((positions >= 0) & (positions <= 1.0)):\n",
        "    print(\"[WARN] all positions in [0,1] — they may be probability/signals rather than signed exposure.\")\n",
        "\n",
        "# compute pnl, tc, net as pandas Series (avoid ndarray-only ops)\n",
        "dpos = np.empty_like(positions)\n",
        "dpos[0] = positions[0]\n",
        "if len(positions) > 1:\n",
        "    dpos[1:] = positions[1:] - positions[:-1]\n",
        "tc_costs = tc_rate * np.abs(dpos)\n",
        "pnl = positions * rets_arr\n",
        "net = pnl - tc_costs\n",
        "\n",
        "# make net/pnl/eq pandas Series for indexing & iloc safety\n",
        "net_s = pd.Series(net, index=pos_df.index, name=\"net\")\n",
        "pnl_s = pd.Series(pnl, index=pos_df.index, name=\"pnl\")\n",
        "tc_s = pd.Series(tc_costs, index=pos_df.index, name=\"tc\")\n",
        "eq = (1.0 + net_s).cumprod()\n",
        "\n",
        "# metrics\n",
        "n_periods = len(net)\n",
        "cum_return = float(eq.iloc[-1] - 1.0) if n_periods > 0 else 0.0\n",
        "ann_return = float(eq.iloc[-1] ** (ANNUALIZE / max(1, n_periods)) - 1.0) if n_periods > 0 else 0.0\n",
        "ann_vol = float(np.nanstd(net) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "ann_sharpe = float((np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "\n",
        "# drawdown and other ratios\n",
        "cum_max = eq.cummax()\n",
        "drawdown = (eq - cum_max) / cum_max\n",
        "max_dd = float(drawdown.min()) if len(drawdown) > 0 else 0.0\n",
        "calmar = float(ann_return / abs(max_dd)) if max_dd < 0 else None\n",
        "\n",
        "# trade reconstruction & trade stats\n",
        "trades = trade_reconstruction(positions, pos_df.index, rets_arr, tc_rate)\n",
        "trades_df = pd.DataFrame(trades)\n",
        "trade_count = len(trades_df)\n",
        "win_count = int(trades_df[\"win\"].sum()) if trade_count > 0 else 0\n",
        "win_rate = float(win_count / trade_count) if trade_count > 0 else float(\"nan\")\n",
        "avg_holding = float(trades_df[\"len\"].mean()) if trade_count > 0 else float(\"nan\")\n",
        "expectancy = float(trades_df[\"net\"].mean()) if trade_count > 0 else 0.0\n",
        "median_trade_net = float(trades_df[\"net\"].median()) if trade_count > 0 else float(\"nan\")\n",
        "\n",
        "# bootstrap sharpe CI\n",
        "sharpe_lo, sharpe_hi = block_bootstrap_sharpe(net_s.values, iters=BOOTSTRAP_ITERS, block=BOOTSTRAP_BLOCK)\n",
        "\n",
        "# turnover & exposure\n",
        "avg_abs_pos = float(np.nanmean(np.abs(positions))) if np.nanmean(np.abs(positions)) != 0 else 1.0\n",
        "turnover = float(np.nanmean(np.abs(dpos)) / avg_abs_pos)\n",
        "\n",
        "# regime breakdown (vol terciles)\n",
        "vol_col = next((c for c in df_meta.columns if \"vol\" in c), None)\n",
        "if vol_col is None:\n",
        "    vol_proxy = df_meta[ret_col].rolling(50, min_periods=1).std().reindex(pos_df.index).fillna(method=\"ffill\")\n",
        "else:\n",
        "    vol_proxy = df_meta[vol_col].reindex(pos_df.index).fillna(method=\"ffill\")\n",
        "vol_bucket = pd.qcut(vol_proxy.fillna(0.0), q=3, labels=[\"low\",\"mid\",\"high\"], duplicates=\"drop\")\n",
        "regime_stats = {}\n",
        "for b in vol_bucket.unique():\n",
        "    mask = vol_bucket == b\n",
        "    if mask.sum() == 0: continue\n",
        "    pnl_r = (positions[mask.values] * rets_arr[mask.values]) - (tc_rate * np.abs(np.concatenate(([positions[mask.values][0]], np.diff(positions[mask.values])))))\n",
        "    ann_sh = float((np.nanmean(pnl_r) / np.nanstd(pnl_r)) * math.sqrt(ANNUALIZE)) if np.nanstd(pnl_r) > 0 else 0.0\n",
        "    regime_stats[f\"vol_{b}_ann_sharpe\"] = ann_sh\n",
        "    regime_stats[f\"vol_{b}_winrate\"] = float(np.mean(pnl_r > 0)) if len(pnl_r) > 0 else float(\"nan\")\n",
        "    regime_stats[f\"vol_{b}_n\"] = int(mask.sum())\n",
        "\n",
        "# collate metrics\n",
        "metrics = {\n",
        "    \"positions_file\": pos_path,\n",
        "    \"meta_file\": meta_path,\n",
        "    \"n_periods\": int(n_periods),\n",
        "    \"cum_return\": cum_return,\n",
        "    \"ann_return\": ann_return,\n",
        "    \"ann_vol\": ann_vol,\n",
        "    \"ann_sharpe\": ann_sharpe,\n",
        "    \"sharpe_ci_95\": [sharpe_lo, sharpe_hi],\n",
        "    \"max_drawdown\": max_dd,\n",
        "    \"calmar\": calmar,\n",
        "    \"trade_count\": trade_count,\n",
        "    \"win_count\": win_count,\n",
        "    \"win_rate\": win_rate,\n",
        "    \"avg_holding\": avg_holding,\n",
        "    \"expectancy\": expectancy,\n",
        "    \"median_trade_net\": median_trade_net,\n",
        "    \"turnover\": turnover,\n",
        "    \"tc_rate\": tc_rate,\n",
        "    \"mean_abs_pos\": mean_abs_pos\n",
        "}\n",
        "metrics.update(regime_stats)\n",
        "\n",
        "# save outputs\n",
        "base = Path(OUT_DIR)\n",
        "eq_csv = base / f\"backtest_v2_equity_{ts}.csv\"\n",
        "trades_csv = base / f\"backtest_v2_trades_{ts}.csv\"\n",
        "metrics_json = base / f\"backtest_v2_metrics_{ts}.json\"\n",
        "plot_png = base / f\"backtest_v2_plots_{ts}.png\"\n",
        "\n",
        "pd.DataFrame({\"net\": net_s, \"pnl\": pnl_s, \"tc\": tc_s, \"eq\": eq}).to_csv(eq_csv)\n",
        "trades_df.to_csv(trades_csv, index=False)\n",
        "with open(metrics_json, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# plots\n",
        "fig, axs = plt.subplots(3,1, figsize=(10,12), dpi=PLOT_DPI, constrained_layout=True)\n",
        "axs[0].plot(eq.index, eq.values); axs[0].set_title(\"Equity Curve (net)\"); axs[0].set_ylabel(\"Equity (growth)\")\n",
        "axs[1].plot(drawdown.index, drawdown.values); axs[1].set_title(\"Drawdown\"); axs[1].set_ylabel(\"Drawdown\")\n",
        "if trade_count > 0:\n",
        "    axs[2].hist(trades_df[\"net\"].values, bins=50); axs[2].set_title(\"Trade net P&L histogram\")\n",
        "else:\n",
        "    axs[2].text(0.1,0.5,\"No trades found\", transform=axs[2].transAxes); axs[2].set_title(\"Trade net P&L histogram\")\n",
        "plt.savefig(plot_png); plt.close(fig)\n",
        "\n",
        "# summary prints\n",
        "print(\"[DONE] backtest v2 finished.\")\n",
        "print(\" - equity csv:\", str(eq_csv))\n",
        "print(\" - trades csv:\", str(trades_csv))\n",
        "print(\" - metrics json:\", str(metrics_json))\n",
        "print(\" - plots:\", str(plot_png))\n",
        "print(\"Summary metrics:\")\n",
        "for k in [\"n_periods\",\"cum_return\",\"ann_return\",\"ann_vol\",\"ann_sharpe\",\"sharpe_ci_95\",\"max_drawdown\",\"trade_count\",\"win_rate\",\"expectancy\",\"turnover\",\"mean_abs_pos\"]:\n",
        "    print(f\"  {k}: {metrics.get(k)}\")\n",
        "\n",
        "# basic asserts\n",
        "assert eq_csv.exists() and trades_csv.exists() and metrics_json.exists(), \"Backtest outputs missing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "35188bf9b395453c9b182a1e310d591c",
            "432426f1f5094a1a9b920f994cba7cea",
            "6cbf421b299e4717a2492f0156e02648",
            "b1b60a1b78444469bfa68d8299b87a7d",
            "726ce4dae954497eb1fc92de607b3fc1",
            "009636aaf0f4450bb5668568eaa99e34",
            "f9395ed788424525965c68ae89cc04f9",
            "25ac154a23b34ff7b42ae0a401d717d2",
            "81ea57ff6c2e489d9d37e2b59d7098ed",
            "4971843e02b0454cb849825eaed2fcb2",
            "e11e8ad9c8be4797a31ccd882d155120"
          ]
        },
        "id": "gJccvvMlhp5g",
        "outputId": "bb707fa6-a7bf-4297-8013-43fc1e7f054b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loaded df_meta_shortlist: df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl rows=17521\n",
            "[INFO] using signals: meta_stack_v2_1764901823_signals_aligned.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 03:20:52,314] A new study created in memory with name: no-name-e98dd58e-6dd9-4128-b280-7fb8c86677bb\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] outer_splits: 5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35188bf9b395453c9b182a1e310d591c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-05 03:20:52,405] Trial 0 finished with value: 0.5078043373018752 and parameters: {'threshold': 0.18727005942368125, 'smoothing': 23, 'invert': False}. Best is trial 0 with value: 0.5078043373018752.\n",
            "[I 2025-12-05 03:20:52,433] Trial 1 finished with value: -0.1282601983870777 and parameters: {'threshold': 0.07800932022121826, 'smoothing': 4, 'invert': True}. Best is trial 0 with value: 0.5078043373018752.\n",
            "[I 2025-12-05 03:20:52,470] Trial 2 finished with value: -0.8054351349308838 and parameters: {'threshold': 0.3005575058716044, 'smoothing': 17, 'invert': True}. Best is trial 0 with value: 0.5078043373018752.\n",
            "[I 2025-12-05 03:20:52,500] Trial 3 finished with value: -0.3980985282466317 and parameters: {'threshold': 0.41622132040021087, 'smoothing': 6, 'invert': True}. Best is trial 0 with value: 0.5078043373018752.\n",
            "[I 2025-12-05 03:20:52,538] Trial 4 finished with value: 0.19401188667201597 and parameters: {'threshold': 0.15212112147976886, 'smoothing': 13, 'invert': False}. Best is trial 0 with value: 0.5078043373018752.\n",
            "[I 2025-12-05 03:20:52,587] Trial 5 finished with value: -1.8916220177922347 and parameters: {'threshold': 0.30592644736118974, 'smoothing': 4, 'invert': True}. Best is trial 0 with value: 0.5078043373018752.\n",
            "[I 2025-12-05 03:20:52,620] Trial 6 finished with value: -0.509530338907607 and parameters: {'threshold': 0.22803499210851796, 'smoothing': 19, 'invert': True}. Best is trial 0 with value: 0.5078043373018752.\n",
            "[I 2025-12-05 03:20:52,649] Trial 7 finished with value: 1.0567601383914438 and parameters: {'threshold': 0.29620728443102123, 'smoothing': 2, 'invert': False}. Best is trial 7 with value: 1.0567601383914438.\n",
            "[I 2025-12-05 03:20:52,699] Trial 8 finished with value: 0.02385946552190372 and parameters: {'threshold': 0.03252579649263976, 'smoothing': 23, 'invert': False}. Best is trial 7 with value: 1.0567601383914438.\n",
            "[I 2025-12-05 03:20:52,719] Trial 9 finished with value: 0.32615972369072166 and parameters: {'threshold': 0.15230688458668534, 'smoothing': 3, 'invert': False}. Best is trial 7 with value: 1.0567601383914438.\n",
            "[I 2025-12-05 03:20:52,763] Trial 10 finished with value: -1.0910928744790356 and parameters: {'threshold': 0.4762784326823435, 'smoothing': 9, 'invert': False}. Best is trial 7 with value: 1.0567601383914438.\n",
            "[I 2025-12-05 03:20:52,816] Trial 11 finished with value: -0.12185004979280652 and parameters: {'threshold': 0.36635939347785607, 'smoothing': 24, 'invert': False}. Best is trial 7 with value: 1.0567601383914438.\n",
            "[I 2025-12-05 03:20:52,854] Trial 12 finished with value: 1.490503602929824 and parameters: {'threshold': 0.2113756625097372, 'smoothing': 12, 'invert': False}. Best is trial 12 with value: 1.490503602929824.\n",
            "[I 2025-12-05 03:20:52,911] Trial 13 finished with value: 1.2369635252057616 and parameters: {'threshold': 0.2802620585994173, 'smoothing': 10, 'invert': False}. Best is trial 12 with value: 1.490503602929824.\n",
            "[I 2025-12-05 03:20:52,972] Trial 14 finished with value: 1.3662592520951147 and parameters: {'threshold': 0.23117229321082272, 'smoothing': 11, 'invert': False}. Best is trial 12 with value: 1.490503602929824.\n",
            "[I 2025-12-05 03:20:53,003] Trial 15 finished with value: 0.21947227116624074 and parameters: {'threshold': 0.09547312138813659, 'smoothing': 14, 'invert': False}. Best is trial 12 with value: 1.490503602929824.\n",
            "[I 2025-12-05 03:20:53,041] Trial 16 finished with value: 1.1558431390735042 and parameters: {'threshold': 0.2005754115147793, 'smoothing': 9, 'invert': False}. Best is trial 12 with value: 1.490503602929824.\n",
            "[I 2025-12-05 03:20:53,090] Trial 17 finished with value: 0.9457115697526335 and parameters: {'threshold': 0.34476830056032104, 'smoothing': 15, 'invert': False}. Best is trial 12 with value: 1.490503602929824.\n",
            "[I 2025-12-05 03:20:53,121] Trial 18 finished with value: 1.6722009535346634 and parameters: {'threshold': 0.25339753766627193, 'smoothing': 11, 'invert': False}. Best is trial 18 with value: 1.6722009535346634.\n",
            "[I 2025-12-05 03:20:53,161] Trial 19 finished with value: 0.14642441234819667 and parameters: {'threshold': 0.13804112754708076, 'smoothing': 7, 'invert': False}. Best is trial 18 with value: 1.6722009535346634.\n",
            "[I 2025-12-05 03:20:53,220] Trial 20 finished with value: 1.162827544727112 and parameters: {'threshold': 0.3999612751508559, 'smoothing': 19, 'invert': False}. Best is trial 18 with value: 1.6722009535346634.\n",
            "[I 2025-12-05 03:20:53,250] Trial 21 finished with value: 1.199677161091385 and parameters: {'threshold': 0.24093085868285558, 'smoothing': 11, 'invert': False}. Best is trial 18 with value: 1.6722009535346634.\n",
            "[I 2025-12-05 03:20:53,289] Trial 22 finished with value: 1.6051660690393288 and parameters: {'threshold': 0.2083728095282107, 'smoothing': 11, 'invert': False}. Best is trial 18 with value: 1.6722009535346634.\n",
            "[I 2025-12-05 03:20:53,354] Trial 23 finished with value: 0.7034301107930623 and parameters: {'threshold': 0.1889022722619666, 'smoothing': 16, 'invert': False}. Best is trial 18 with value: 1.6722009535346634.\n",
            "[I 2025-12-05 03:20:53,391] Trial 24 finished with value: 1.9508047644409352 and parameters: {'threshold': 0.26404551954452765, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,440] Trial 25 finished with value: 1.5799046012317333 and parameters: {'threshold': 0.2648200642185289, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,490] Trial 26 finished with value: -0.2283440264713914 and parameters: {'threshold': 0.11093641632970466, 'smoothing': 7, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,520] Trial 27 finished with value: 1.4091481426324284 and parameters: {'threshold': 0.3340208380863927, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,550] Trial 28 finished with value: 0.6982513053225274 and parameters: {'threshold': 0.2584179854643912, 'smoothing': 1, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,604] Trial 29 finished with value: 0.7767288477335356 and parameters: {'threshold': 0.18421252745812927, 'smoothing': 13, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,636] Trial 30 finished with value: 0.015558174995150598 and parameters: {'threshold': 0.003072756902037177, 'smoothing': 18, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,678] Trial 31 finished with value: 1.6424401742873052 and parameters: {'threshold': 0.2611155382573375, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,723] Trial 32 finished with value: 1.6094501116618627 and parameters: {'threshold': 0.2645126476223031, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,752] Trial 33 finished with value: 1.7498634822644992 and parameters: {'threshold': 0.31370991330748377, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,791] Trial 34 finished with value: -1.6730715802621163 and parameters: {'threshold': 0.32742748322650045, 'smoothing': 5, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,844] Trial 35 finished with value: 0.44268283679228426 and parameters: {'threshold': 0.3803923589896998, 'smoothing': 4, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,878] Trial 36 finished with value: -0.5320513980703638 and parameters: {'threshold': 0.4323836409180794, 'smoothing': 8, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,915] Trial 37 finished with value: 1.1299706818499147 and parameters: {'threshold': 0.2997681425576762, 'smoothing': 2, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:53,994] Trial 38 finished with value: -1.7529574939163262 and parameters: {'threshold': 0.32800305241908356, 'smoothing': 4, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,031] Trial 39 finished with value: 1.6263658423019418 and parameters: {'threshold': 0.2878682379923017, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,078] Trial 40 finished with value: -1.236379653062501 and parameters: {'threshold': 0.31325874576268276, 'smoothing': 9, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,132] Trial 41 finished with value: 1.6645775594877628 and parameters: {'threshold': 0.2823652012655651, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,164] Trial 42 finished with value: 1.0419632901169171 and parameters: {'threshold': 0.3541168348473665, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,202] Trial 43 finished with value: 1.1021825985351144 and parameters: {'threshold': 0.24620037967616662, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,253] Trial 44 finished with value: 1.278832911400525 and parameters: {'threshold': 0.2764129335899218, 'smoothing': 3, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,290] Trial 45 finished with value: 0.8547880852763236 and parameters: {'threshold': 0.17497741666664274, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,329] Trial 46 finished with value: 1.5767434434126 and parameters: {'threshold': 0.31260518723634445, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,387] Trial 47 finished with value: 1.4561972095634528 and parameters: {'threshold': 0.22106197962217256, 'smoothing': 10, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,415] Trial 48 finished with value: 0.11538102596996944 and parameters: {'threshold': 0.37492480751337937, 'smoothing': 3, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,456] Trial 49 finished with value: 1.3279546224272527 and parameters: {'threshold': 0.23642974202473188, 'smoothing': 10, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,515] Trial 50 finished with value: 0.2607991430037894 and parameters: {'threshold': 0.1619809651250411, 'smoothing': 21, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,544] Trial 51 finished with value: 1.387922232122548 and parameters: {'threshold': 0.29331167940669467, 'smoothing': 4, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,583] Trial 52 finished with value: 1.5259523498357495 and parameters: {'threshold': 0.2821237130382652, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,633] Trial 53 finished with value: 1.5991762866882584 and parameters: {'threshold': 0.28421820772646283, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,655] Trial 54 finished with value: 0.6724542013139173 and parameters: {'threshold': 0.2563767717354974, 'smoothing': 1, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,693] Trial 55 finished with value: 1.561044297879362 and parameters: {'threshold': 0.3099708274215628, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,747] Trial 56 finished with value: 1.1015257848808626 and parameters: {'threshold': 0.21890776699129874, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,779] Trial 57 finished with value: 1.0135792575211202 and parameters: {'threshold': 0.35361334274033784, 'smoothing': 12, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,827] Trial 58 finished with value: 1.3148187039452135 and parameters: {'threshold': 0.26750864201578134, 'smoothing': 2, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,876] Trial 59 finished with value: -1.2258392615421492 and parameters: {'threshold': 0.2424094503442709, 'smoothing': 9, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,920] Trial 60 finished with value: 1.2285163364366425 and parameters: {'threshold': 0.29014082259455665, 'smoothing': 14, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:54,971] Trial 61 finished with value: 1.5985528421328847 and parameters: {'threshold': 0.2647332792185632, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,073] Trial 62 finished with value: 1.0931672940082158 and parameters: {'threshold': 0.2024020301847786, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,112] Trial 63 finished with value: 1.2422257419271148 and parameters: {'threshold': 0.22830957843294897, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,146] Trial 64 finished with value: 1.2469351349541282 and parameters: {'threshold': 0.2516681459415063, 'smoothing': 3, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,193] Trial 65 finished with value: 1.4974050091216913 and parameters: {'threshold': 0.3380086366587917, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,224] Trial 66 finished with value: 1.1452861625073463 and parameters: {'threshold': 0.3198675431452588, 'smoothing': 9, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,263] Trial 67 finished with value: 1.6113321187775427 and parameters: {'threshold': 0.2754725424785231, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,314] Trial 68 finished with value: 1.2303771019492755 and parameters: {'threshold': 0.3012261601956692, 'smoothing': 11, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,350] Trial 69 finished with value: 1.7214126876212141 and parameters: {'threshold': 0.27727099370254593, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,393] Trial 70 finished with value: -0.6576174315031247 and parameters: {'threshold': 0.40340530168867084, 'smoothing': 8, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,447] Trial 71 finished with value: 1.4882385699257645 and parameters: {'threshold': 0.2749322030967136, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,478] Trial 72 finished with value: 1.568718486124705 and parameters: {'threshold': 0.29501204450197416, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,520] Trial 73 finished with value: 1.5228346924046599 and parameters: {'threshold': 0.25034444101849124, 'smoothing': 10, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,582] Trial 74 finished with value: 1.5277515621664102 and parameters: {'threshold': 0.2797508565904706, 'smoothing': 4, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,620] Trial 75 finished with value: 1.107347459139684 and parameters: {'threshold': 0.23279373194649072, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,658] Trial 76 finished with value: 1.494004315231363 and parameters: {'threshold': 0.31474542579018266, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,706] Trial 77 finished with value: 1.4002469692167285 and parameters: {'threshold': 0.3454636463357456, 'smoothing': 9, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,738] Trial 78 finished with value: 1.8348849366596258 and parameters: {'threshold': 0.27053290372499367, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,775] Trial 79 finished with value: 1.4730515746535282 and parameters: {'threshold': 0.32809878296177797, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,824] Trial 80 finished with value: 1.1206455705571599 and parameters: {'threshold': 0.21667087587186548, 'smoothing': 4, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,855] Trial 81 finished with value: 1.573115220105796 and parameters: {'threshold': 0.2695239099446708, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,898] Trial 82 finished with value: 1.5956437711702085 and parameters: {'threshold': 0.3012904099912697, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,949] Trial 83 finished with value: 1.6184021030277584 and parameters: {'threshold': 0.2597961789618107, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:55,980] Trial 84 finished with value: 0.9178340520295866 and parameters: {'threshold': 0.19346983959970954, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,020] Trial 85 finished with value: 1.6025511489737958 and parameters: {'threshold': 0.2561631332525038, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,073] Trial 86 finished with value: -1.189822874400149 and parameters: {'threshold': 0.24078619081703437, 'smoothing': 9, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,147] Trial 87 finished with value: 1.7012893199087233 and parameters: {'threshold': 0.28662203485028337, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,188] Trial 88 finished with value: 1.3006905204177621 and parameters: {'threshold': 0.28852314355051384, 'smoothing': 10, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,251] Trial 89 finished with value: 1.2660139098698875 and parameters: {'threshold': 0.30232512202085665, 'smoothing': 12, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,287] Trial 90 finished with value: 1.3629626640010035 and parameters: {'threshold': 0.322645962548467, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,325] Trial 91 finished with value: 1.579438733477024 and parameters: {'threshold': 0.25737841682892043, 'smoothing': 4, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,377] Trial 92 finished with value: -0.4390230071157287 and parameters: {'threshold': 0.49027970215940997, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,409] Trial 93 finished with value: 0.256129470528151 and parameters: {'threshold': 0.056759868538479724, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,450] Trial 94 finished with value: 1.6348025319303876 and parameters: {'threshold': 0.2884941309450801, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,499] Trial 95 finished with value: 1.4877965185210298 and parameters: {'threshold': 0.2930800586151587, 'smoothing': 5, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,531] Trial 96 finished with value: 1.7104382508634086 and parameters: {'threshold': 0.2835541769591582, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,568] Trial 97 finished with value: 1.6562298323567912 and parameters: {'threshold': 0.30732067724557544, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,617] Trial 98 finished with value: 1.569081856192499 and parameters: {'threshold': 0.30990916389337964, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,650] Trial 99 finished with value: -1.144909788082315 and parameters: {'threshold': 0.35523482751032354, 'smoothing': 8, 'invert': True}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,687] Trial 100 finished with value: 1.2505663979022132 and parameters: {'threshold': 0.3378963537607145, 'smoothing': 9, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,733] Trial 101 finished with value: 1.6651395803533056 and parameters: {'threshold': 0.27247573764325905, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,765] Trial 102 finished with value: 1.6305412996923008 and parameters: {'threshold': 0.2736571427460628, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,803] Trial 103 finished with value: 1.2379547124070018 and parameters: {'threshold': 0.24758806308094594, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,847] Trial 104 finished with value: 1.3029046656969379 and parameters: {'threshold': 0.22456954263935236, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,879] Trial 105 finished with value: 1.6046085580011709 and parameters: {'threshold': 0.2680198660318285, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,916] Trial 106 finished with value: 1.3069218845303001 and parameters: {'threshold': 0.23568077059906217, 'smoothing': 10, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,965] Trial 107 finished with value: 1.0867394600020703 and parameters: {'threshold': 0.30301666939422756, 'smoothing': 11, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:56,998] Trial 108 finished with value: 1.6610648788460114 and parameters: {'threshold': 0.2801442843576039, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,038] Trial 109 finished with value: 0.9446763645781523 and parameters: {'threshold': 0.28304477944443396, 'smoothing': 16, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,102] Trial 110 finished with value: 1.6135937123847264 and parameters: {'threshold': 0.31185105296487725, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,137] Trial 111 finished with value: 1.6610648788460114 and parameters: {'threshold': 0.2801951823718118, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,213] Trial 112 finished with value: 1.3464633018011851 and parameters: {'threshold': 0.28235601248893333, 'smoothing': 9, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,266] Trial 113 finished with value: 1.6674338418935684 and parameters: {'threshold': 0.272318206427999, 'smoothing': 7, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,301] Trial 114 finished with value: 1.8979048963120753 and parameters: {'threshold': 0.26949952922832815, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,349] Trial 115 finished with value: 1.270839715779085 and parameters: {'threshold': 0.24814325598318587, 'smoothing': 8, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,407] Trial 116 finished with value: 1.4414373585673608 and parameters: {'threshold': 0.2705799097234211, 'smoothing': 9, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,445] Trial 117 finished with value: 1.003646083781092 and parameters: {'threshold': 0.20882876053277627, 'smoothing': 6, 'invert': False}. Best is trial 24 with value: 1.9508047644409352.\n",
            "[I 2025-12-05 03:20:57,485] Trial 118 finished with value: 1.953344783313776 and parameters: {'threshold': 0.2639029041481145, 'smoothing': 7, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,545] Trial 119 finished with value: -1.9794771617185514 and parameters: {'threshold': 0.2646052129685898, 'smoothing': 8, 'invert': True}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,583] Trial 120 finished with value: 1.3203200691695378 and parameters: {'threshold': 0.23610150844370453, 'smoothing': 10, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,619] Trial 121 finished with value: 1.5755688822070992 and parameters: {'threshold': 0.2538433398362111, 'smoothing': 7, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,666] Trial 122 finished with value: 1.5490892468669806 and parameters: {'threshold': 0.29328939733517567, 'smoothing': 6, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,698] Trial 123 finished with value: 1.5977704760695075 and parameters: {'threshold': 0.2730044631317469, 'smoothing': 7, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,733] Trial 124 finished with value: 1.6597586675017453 and parameters: {'threshold': 0.2951646845505018, 'smoothing': 8, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,782] Trial 125 finished with value: 1.6864663600497871 and parameters: {'threshold': 0.2632163298745597, 'smoothing': 13, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,820] Trial 126 finished with value: 1.6489161855333077 and parameters: {'threshold': 0.26288213732839033, 'smoothing': 14, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,863] Trial 127 finished with value: 1.1729288075000355 and parameters: {'threshold': 0.2473928665350178, 'smoothing': 14, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,919] Trial 128 finished with value: 1.156560952860819 and parameters: {'threshold': 0.22818796842973899, 'smoothing': 13, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,956] Trial 129 finished with value: 1.8603333229544898 and parameters: {'threshold': 0.2586490960278654, 'smoothing': 12, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:57,996] Trial 130 finished with value: 1.1651290472304159 and parameters: {'threshold': 0.24358172822492608, 'smoothing': 11, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:58,047] Trial 131 finished with value: 1.9115848487058202 and parameters: {'threshold': 0.2560202871503661, 'smoothing': 13, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:58,080] Trial 132 finished with value: 1.846642135799523 and parameters: {'threshold': 0.2590392158089116, 'smoothing': 12, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:58,120] Trial 133 finished with value: 1.8444066540507074 and parameters: {'threshold': 0.2572383615736006, 'smoothing': 12, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:58,186] Trial 134 finished with value: 1.9089181436434561 and parameters: {'threshold': 0.25574021694483906, 'smoothing': 13, 'invert': False}. Best is trial 118 with value: 1.953344783313776.\n",
            "[I 2025-12-05 03:20:58,224] Trial 135 finished with value: 1.9619594946868937 and parameters: {'threshold': 0.2564955239310453, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,276] Trial 136 finished with value: 1.9295630246630993 and parameters: {'threshold': 0.25574620400665893, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,346] Trial 137 finished with value: 1.3664162072172599 and parameters: {'threshold': 0.219742238400285, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,387] Trial 138 finished with value: 1.6947916352136019 and parameters: {'threshold': 0.2554690661417858, 'smoothing': 15, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,436] Trial 139 finished with value: 1.0297401315850778 and parameters: {'threshold': 0.2376484696588152, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,489] Trial 140 finished with value: 1.367783849872089 and parameters: {'threshold': 0.23209093024285, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,526] Trial 141 finished with value: 1.8894010846295821 and parameters: {'threshold': 0.2556233648848408, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,562] Trial 142 finished with value: 1.7751175228126705 and parameters: {'threshold': 0.25441766192453147, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,609] Trial 143 finished with value: 1.5597863683180404 and parameters: {'threshold': 0.25266711237065254, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,644] Trial 144 finished with value: 1.0861910892434516 and parameters: {'threshold': 0.24245266701882565, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,716] Trial 145 finished with value: 1.5588548857212998 and parameters: {'threshold': 0.2148324857598497, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,809] Trial 146 finished with value: 1.6825800785248954 and parameters: {'threshold': 0.26048111494299825, 'smoothing': 15, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,885] Trial 147 finished with value: -1.3061911226935676 and parameters: {'threshold': 0.24637131572900603, 'smoothing': 13, 'invert': True}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:58,982] Trial 148 finished with value: 1.3517048249367793 and parameters: {'threshold': 0.22728477838187447, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,036] Trial 149 finished with value: 1.813303786280813 and parameters: {'threshold': 0.26022368348814495, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,071] Trial 150 finished with value: 1.8297770194036649 and parameters: {'threshold': 0.2577285741497226, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,119] Trial 151 finished with value: 1.8961319102025023 and parameters: {'threshold': 0.2595926276448034, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,177] Trial 152 finished with value: 1.686628222402867 and parameters: {'threshold': 0.2563528395790797, 'smoothing': 15, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,217] Trial 153 finished with value: 1.6516701190439247 and parameters: {'threshold': 0.2628229048171741, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,260] Trial 154 finished with value: 1.1330559600590313 and parameters: {'threshold': 0.23571771000917877, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,350] Trial 155 finished with value: 1.4213691263141073 and parameters: {'threshold': 0.24982751787876356, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,387] Trial 156 finished with value: 1.455562990723442 and parameters: {'threshold': 0.26768958874211135, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,446] Trial 157 finished with value: 1.6713256882990617 and parameters: {'threshold': 0.25585868375503895, 'smoothing': 15, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,535] Trial 158 finished with value: 0.9094999907053749 and parameters: {'threshold': 0.19996906772759165, 'smoothing': 16, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,610] Trial 159 finished with value: 1.2652530020132162 and parameters: {'threshold': 0.24245591681339482, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,711] Trial 160 finished with value: 1.1721311550881643 and parameters: {'threshold': 0.22427798057931797, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,814] Trial 161 finished with value: 1.5548625227013382 and parameters: {'threshold': 0.2708319715373791, 'smoothing': 11, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:20:59,922] Trial 162 finished with value: 1.4334440120729544 and parameters: {'threshold': 0.25057586741978055, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:21:00,023] Trial 163 finished with value: -0.07449050845655754 and parameters: {'threshold': 0.4575962298784968, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:21:00,079] Trial 164 finished with value: 1.6489161855333077 and parameters: {'threshold': 0.2629096828819356, 'smoothing': 14, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:21:00,166] Trial 165 finished with value: 1.0729724601917774 and parameters: {'threshold': 0.2403759569104711, 'smoothing': 13, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:21:00,233] Trial 166 finished with value: 1.5527306258438978 and parameters: {'threshold': 0.27329588473597594, 'smoothing': 12, 'invert': False}. Best is trial 135 with value: 1.9619594946868937.\n",
            "[I 2025-12-05 03:21:00,322] Trial 167 finished with value: 2.0562778644000623 and parameters: {'threshold': 0.2568127288996076, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,432] Trial 168 finished with value: 2.021215757791715 and parameters: {'threshold': 0.2595597624413157, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,510] Trial 169 finished with value: 1.2374403225196475 and parameters: {'threshold': 0.2791474045648305, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,613] Trial 170 finished with value: 1.9219572955966988 and parameters: {'threshold': 0.26328296218851066, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,708] Trial 171 finished with value: 1.8721310934713666 and parameters: {'threshold': 0.26426503577698, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,772] Trial 172 finished with value: 1.6811904499141643 and parameters: {'threshold': 0.26950967427966543, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,843] Trial 173 finished with value: 1.264181189900248 and parameters: {'threshold': 0.2475234812831471, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,912] Trial 174 finished with value: 1.3530163433763263 and parameters: {'threshold': 0.29080806955112803, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:00,988] Trial 175 finished with value: 1.2801449472284776 and parameters: {'threshold': 0.2332169661497998, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,064] Trial 176 finished with value: 1.972916475106293 and parameters: {'threshold': 0.2605296361820123, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,172] Trial 177 finished with value: -1.288159622438587 and parameters: {'threshold': 0.2812985814375148, 'smoothing': 11, 'invert': True}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,244] Trial 178 finished with value: 1.5888052210028334 and parameters: {'threshold': 0.2650860534375493, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,365] Trial 179 finished with value: 1.2807040515041634 and parameters: {'threshold': 0.23956251123366987, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,468] Trial 180 finished with value: -0.03845207999777393 and parameters: {'threshold': 0.1230952429286051, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,576] Trial 181 finished with value: 1.9274863094300947 and parameters: {'threshold': 0.2560207263663012, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,669] Trial 182 finished with value: 1.5374748423137936 and parameters: {'threshold': 0.27394519569155923, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,765] Trial 183 finished with value: 1.2059887251945436 and parameters: {'threshold': 0.24634884724440625, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,832] Trial 184 finished with value: 1.9103518984675791 and parameters: {'threshold': 0.2628881044341404, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:01,938] Trial 185 finished with value: 2.034312804194768 and parameters: {'threshold': 0.25825586714950727, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,008] Trial 186 finished with value: 1.7747607620607753 and parameters: {'threshold': 0.26577700546378125, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,096] Trial 187 finished with value: 1.4184288678096653 and parameters: {'threshold': 0.2453532100578847, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,159] Trial 188 finished with value: 1.360758257565947 and parameters: {'threshold': 0.2854206311140409, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,222] Trial 189 finished with value: 1.7872086987067053 and parameters: {'threshold': 0.25405179110072634, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,257] Trial 190 finished with value: 1.6379168613093877 and parameters: {'threshold': 0.2607541134877592, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,293] Trial 191 finished with value: 1.7420305830690654 and parameters: {'threshold': 0.2540553251946436, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,370] Trial 192 finished with value: 1.5114578601072415 and parameters: {'threshold': 0.2758802641803041, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,467] Trial 193 finished with value: 1.1724774740779942 and parameters: {'threshold': 0.23544379114366004, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,555] Trial 194 finished with value: 1.9158397024521867 and parameters: {'threshold': 0.2628293094145476, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,634] Trial 195 finished with value: 1.808156138193119 and parameters: {'threshold': 0.2653680284757453, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,727] Trial 196 finished with value: 1.3746984189468607 and parameters: {'threshold': 0.24448812946651874, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,776] Trial 197 finished with value: 1.3583131244759534 and parameters: {'threshold': 0.2766041981643857, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:02,850] Trial 198 finished with value: 1.1943450689245176 and parameters: {'threshold': 0.22800386761566527, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,002] Trial 199 finished with value: 1.6717055116403177 and parameters: {'threshold': 0.2648885475004666, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,067] Trial 200 finished with value: 1.6683138634791472 and parameters: {'threshold': 0.2528444870715006, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,114] Trial 201 finished with value: 1.9011171444733215 and parameters: {'threshold': 0.2570085589912851, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,150] Trial 202 finished with value: 1.7323393784619079 and parameters: {'threshold': 0.2608582716792385, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,195] Trial 203 finished with value: 1.423520356956078 and parameters: {'threshold': 0.24937163453274258, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,272] Trial 204 finished with value: 1.4426210879344148 and parameters: {'threshold': 0.2726820608955683, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,351] Trial 205 finished with value: 0.2730995478558672 and parameters: {'threshold': 0.2862197067369307, 'smoothing': 24, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,429] Trial 206 finished with value: 1.753797581466813 and parameters: {'threshold': 0.2601876045438415, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,540] Trial 207 finished with value: 1.2183177671985117 and parameters: {'threshold': 0.24397189681576414, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,643] Trial 208 finished with value: 1.6055652826625681 and parameters: {'threshold': 0.26685354850623677, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,758] Trial 209 finished with value: -1.6780642613578927 and parameters: {'threshold': 0.2537118170033809, 'smoothing': 11, 'invert': True}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,818] Trial 210 finished with value: 1.2406898350068638 and parameters: {'threshold': 0.23516802782001536, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,858] Trial 211 finished with value: 1.7323393784619079 and parameters: {'threshold': 0.260919985958637, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,893] Trial 212 finished with value: 1.2618577813806673 and parameters: {'threshold': 0.2778117105865523, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,938] Trial 213 finished with value: 1.5670062206190258 and parameters: {'threshold': 0.2520231065958166, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:03,997] Trial 214 finished with value: 1.5087107399398647 and parameters: {'threshold': 0.2706581879651873, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,055] Trial 215 finished with value: 1.0337550748815358 and parameters: {'threshold': 0.2563390793374095, 'smoothing': 21, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,127] Trial 216 finished with value: 1.0861910892434516 and parameters: {'threshold': 0.242649952489169, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,208] Trial 217 finished with value: 1.7939385175241633 and parameters: {'threshold': 0.2658222789743181, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,285] Trial 218 finished with value: 1.3494743460773562 and parameters: {'threshold': 0.2782179232777391, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,365] Trial 219 finished with value: 1.7728034721908337 and parameters: {'threshold': 0.2544273468498379, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,439] Trial 220 finished with value: 1.3761997294708523 and parameters: {'threshold': 0.2904682797973265, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,516] Trial 221 finished with value: 1.6968127165555824 and parameters: {'threshold': 0.26878945069058624, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,580] Trial 222 finished with value: 1.7719436277453173 and parameters: {'threshold': 0.25988649469960423, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,616] Trial 223 finished with value: 1.2164207106997185 and parameters: {'threshold': 0.2440731266032245, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,678] Trial 224 finished with value: 1.5078759380759568 and parameters: {'threshold': 0.2720292828609376, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,746] Trial 225 finished with value: 1.9089929007450184 and parameters: {'threshold': 0.25586499506593957, 'smoothing': 9, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,789] Trial 226 finished with value: 1.3827427948436974 and parameters: {'threshold': 0.24859238506073852, 'smoothing': 9, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,869] Trial 227 finished with value: 1.2347365031796407 and parameters: {'threshold': 0.23365138020874948, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,925] Trial 228 finished with value: 1.8246776479929339 and parameters: {'threshold': 0.25951907814121544, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:04,994] Trial 229 finished with value: 1.3119105591368525 and parameters: {'threshold': 0.25154770877665145, 'smoothing': 14, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,071] Trial 230 finished with value: 1.2446526015198351 and parameters: {'threshold': 0.2779320347280824, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,168] Trial 231 finished with value: 1.6138768280048066 and parameters: {'threshold': 0.26626795585970914, 'smoothing': 9, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,241] Trial 232 finished with value: 2.007281843977517 and parameters: {'threshold': 0.25937522411160563, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,328] Trial 233 finished with value: 2.027324361173565 and parameters: {'threshold': 0.25790959227503907, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,418] Trial 234 finished with value: 1.2031184613023513 and parameters: {'threshold': 0.24137678641882995, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,469] Trial 235 finished with value: 2.0398040293946975 and parameters: {'threshold': 0.2583519237523741, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,548] Trial 236 finished with value: 1.4853740094426278 and parameters: {'threshold': 0.25080434517143335, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,605] Trial 237 finished with value: 1.686718164132084 and parameters: {'threshold': 0.26752493484217266, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,649] Trial 238 finished with value: 1.6824384270963364 and parameters: {'threshold': 0.26004274343269923, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,693] Trial 239 finished with value: 1.1922549244774872 and parameters: {'threshold': 0.24045817384818216, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,753] Trial 240 finished with value: 1.2835936496631575 and parameters: {'threshold': 0.28339105410556925, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,788] Trial 241 finished with value: 1.7609859834431325 and parameters: {'threshold': 0.2599713437474869, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,834] Trial 242 finished with value: 1.5680604408195997 and parameters: {'threshold': 0.2521966813127941, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,902] Trial 243 finished with value: 1.531641372859724 and parameters: {'threshold': 0.27143685361296577, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:05,963] Trial 244 finished with value: 1.6652635932125839 and parameters: {'threshold': 0.2619568937334042, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,026] Trial 245 finished with value: 1.3529228341879511 and parameters: {'threshold': 0.24901571369082548, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,062] Trial 246 finished with value: 1.5540613580771017 and parameters: {'threshold': 0.2706223279339004, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,123] Trial 247 finished with value: -1.8254413914483139 and parameters: {'threshold': 0.2591671530066788, 'smoothing': 13, 'invert': True}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,159] Trial 248 finished with value: 1.2085048727742147 and parameters: {'threshold': 0.2462592253811085, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,207] Trial 249 finished with value: 1.2844739291785081 and parameters: {'threshold': 0.27819350414692695, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,260] Trial 250 finished with value: 1.2218581358233922 and parameters: {'threshold': 0.2339597826577892, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,299] Trial 251 finished with value: 1.579089392457934 and parameters: {'threshold': 0.2652245945472285, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,352] Trial 252 finished with value: 2.007439997843555 and parameters: {'threshold': 0.2591284064350258, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,412] Trial 253 finished with value: 1.8171126523775623 and parameters: {'threshold': 0.254100813349501, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,451] Trial 254 finished with value: 1.229165370887599 and parameters: {'threshold': 0.2422109883871479, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,519] Trial 255 finished with value: 1.5079426690555064 and parameters: {'threshold': 0.2743438693067006, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,563] Trial 256 finished with value: 1.6865150622652454 and parameters: {'threshold': 0.2672443203228377, 'smoothing': 14, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,610] Trial 257 finished with value: 1.3232180565129537 and parameters: {'threshold': 0.28551651326600885, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,647] Trial 258 finished with value: 1.7627576991377616 and parameters: {'threshold': 0.2540999164681678, 'smoothing': 9, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,689] Trial 259 finished with value: 1.2268101504723472 and parameters: {'threshold': 0.22361390835766898, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,738] Trial 260 finished with value: 1.381213509584994 and parameters: {'threshold': 0.24770344018640753, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,781] Trial 261 finished with value: 1.732033124937948 and parameters: {'threshold': 0.2620941145992035, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,820] Trial 262 finished with value: 1.254846055883815 and parameters: {'threshold': 0.23648050837526566, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,869] Trial 263 finished with value: 1.4591264655843168 and parameters: {'threshold': 0.2752208854463594, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,904] Trial 264 finished with value: 1.9115848487058202 and parameters: {'threshold': 0.25601489940532796, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:06,950] Trial 265 finished with value: 1.421422647408744 and parameters: {'threshold': 0.2502564473649332, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,012] Trial 266 finished with value: -1.455562990723442 and parameters: {'threshold': 0.2675801374095374, 'smoothing': 13, 'invert': True}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,066] Trial 267 finished with value: 1.3350035760863912 and parameters: {'threshold': 0.28370871229643724, 'smoothing': 14, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,112] Trial 268 finished with value: 0.3943242507425052 and parameters: {'threshold': 0.02493351216384701, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,158] Trial 269 finished with value: 1.199677161091385 and parameters: {'threshold': 0.24085028567977504, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,194] Trial 270 finished with value: 1.8348364919290865 and parameters: {'threshold': 0.25729445393807576, 'smoothing': 14, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,240] Trial 271 finished with value: 1.4419094181233936 and parameters: {'threshold': 0.2677904604671621, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,301] Trial 272 finished with value: 0.353937342670394 and parameters: {'threshold': 0.08519400413429609, 'smoothing': 9, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,337] Trial 273 finished with value: 1.374188649704976 and parameters: {'threshold': 0.2292570786275667, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,382] Trial 274 finished with value: 1.3615035841059229 and parameters: {'threshold': 0.2486359460349824, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,435] Trial 275 finished with value: 1.417331814754498 and parameters: {'threshold': 0.29620444871324747, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,473] Trial 276 finished with value: 2.007439997843555 and parameters: {'threshold': 0.25912289845267805, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,537] Trial 277 finished with value: 1.733021090362803 and parameters: {'threshold': 0.25913574244198184, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,624] Trial 278 finished with value: 1.5336589093501534 and parameters: {'threshold': 0.27429136526880254, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,658] Trial 279 finished with value: 1.0119148897081642 and parameters: {'threshold': 0.24004013038488575, 'smoothing': 15, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,718] Trial 280 finished with value: 1.6063321509595714 and parameters: {'threshold': 0.2528054458173034, 'smoothing': 8, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,785] Trial 281 finished with value: 1.2375725157757957 and parameters: {'threshold': 0.28075281682255526, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,821] Trial 282 finished with value: 1.7429806529421374 and parameters: {'threshold': 0.25872669393395775, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,879] Trial 283 finished with value: 1.286024847572499 and parameters: {'threshold': 0.24890607520149463, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,914] Trial 284 finished with value: 1.6766876561909807 and parameters: {'threshold': 0.26954585320733493, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:07,973] Trial 285 finished with value: 1.079470154761812 and parameters: {'threshold': 0.23360931944175617, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,020] Trial 286 finished with value: -1.9676780836095027 and parameters: {'threshold': 0.26235893441836383, 'smoothing': 8, 'invert': True}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,067] Trial 287 finished with value: 1.203605100277336 and parameters: {'threshold': 0.24157576581971285, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,135] Trial 288 finished with value: 1.4721438127410171 and parameters: {'threshold': 0.2757201638524627, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,176] Trial 289 finished with value: 1.3119105591368525 and parameters: {'threshold': 0.25146903783029356, 'smoothing': 14, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,220] Trial 290 finished with value: 1.2986649095042295 and parameters: {'threshold': 0.28633705034951606, 'smoothing': 13, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,261] Trial 291 finished with value: 1.7800896033373583 and parameters: {'threshold': 0.26551584624733604, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,307] Trial 292 finished with value: 1.7515618034599103 and parameters: {'threshold': 0.25396913028957013, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,376] Trial 293 finished with value: 1.3882511195885328 and parameters: {'threshold': 0.22060804744017187, 'smoothing': 12, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,422] Trial 294 finished with value: 1.3441202202589504 and parameters: {'threshold': 0.2428451513206803, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,473] Trial 295 finished with value: 2.007931821765552 and parameters: {'threshold': 0.2617483829239944, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,521] Trial 296 finished with value: 1.4196239805359545 and parameters: {'threshold': 0.27370798627804294, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,573] Trial 297 finished with value: 1.730688092002745 and parameters: {'threshold': 0.26330341547662767, 'smoothing': 9, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,623] Trial 298 finished with value: 1.6127082666091386 and parameters: {'threshold': 0.2717121408920026, 'smoothing': 11, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[I 2025-12-05 03:21:08,666] Trial 299 finished with value: 1.3380810036461888 and parameters: {'threshold': 0.28497567771672255, 'smoothing': 10, 'invert': False}. Best is trial 167 with value: 2.0562778644000623.\n",
            "[OK] Optuna done. best_value (signal metric): 2.0562778644000623\n",
            "best params -> {'threshold': 0.2568127288996076, 'smoothing': 11, 'invert': False, 'best_value': 2.0562778644000623}\n",
            "[SAVED] signals -> /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_signal_opt_signals_1764904868.csv\n",
            "[SAVED] meta -> /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_signal_opt_meta_1764904868.json\n",
            "[DONE] signal-only optimizer finished.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ONE-CELL — Optuna SIGNAL-ONLY optimizer (CPCV-safe, thresholded-Sharpe default)\n",
        "import os, time, json, warnings\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "import numpy as np, pandas as pd, optuna\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "RSTATE = 42\n",
        "N_TRIALS = 300\n",
        "N_OUTER = 5\n",
        "EMBARGO = pd.Timedelta(\"1H\")\n",
        "LABEL_H = \"h8\"       # same as pipeline\n",
        "H = 8\n",
        "ANNUALIZE = 252 * 24\n",
        "MIN_POS_OBS = 30     # minimal non-zero positions per fold (coverage guard)\n",
        "PENALTY_COVERAGE = 0.6  # multiply score if coverage too low\n",
        "RESULT_PREFIX = \"A_SRC_signal_opt\"\n",
        "# choose metric: \"sharpe\" (default), other options could be \"winrate\"\n",
        "METRIC = \"sharpe\"\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def load_latest_meta(path=OUT_DIR):\n",
        "    pkl = sorted(Path(path).glob(\"df_meta_shortlist.v*.pkl\"), key=os.path.getmtime)\n",
        "    if not pkl:\n",
        "        raise FileNotFoundError(\"df_meta_shortlist.v*.pkl not found in OUT_DIR\")\n",
        "    df = pd.read_pickle(str(pkl[-1]))\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    return df, str(pkl[-1])\n",
        "\n",
        "def load_latest_signals(path=OUT_DIR):\n",
        "    sig_files = list(Path(path).glob(\"*signals_aligned*.csv\")) + list(Path(path).glob(\"*signals*.csv\"))\n",
        "    if not sig_files:\n",
        "        raise FileNotFoundError(\"signals CSV not found in OUT_DIR (need meta signals)\")\n",
        "    sig_path = sorted(sig_files, key=os.path.getmtime)[-1]\n",
        "    sig = pd.read_csv(sig_path, index_col=0)\n",
        "    try:\n",
        "        sig.index = pd.to_datetime(sig.index, utc=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return sig, str(sig_path)\n",
        "\n",
        "# compute per-fold Sharpe of a return series (annualized)\n",
        "def ann_sharpe_from_series(x):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if np.nanstd(x) == 0:\n",
        "        return 0.0\n",
        "    return float((np.nanmean(x) / np.nanstd(x)) * np.sqrt(ANNUALIZE))\n",
        "\n",
        "# evaluate signal on outer folds (folds is list of (train_idx,test_idx) tuples of integer positions)\n",
        "def evaluate_signals_on_folds(signal_ser, rets_ser, outer_splits, pos_all_idx):\n",
        "    fold_scores = []\n",
        "    fold_coverages = []\n",
        "    for train_pos, test_pos in outer_splits:\n",
        "        test_ts = pos_all_idx[test_pos]\n",
        "        if len(test_ts) == 0:\n",
        "            continue\n",
        "        s_test = signal_ser.reindex(test_ts).fillna(0.0)\n",
        "        r_test = rets_ser.reindex(test_ts).fillna(0.0)\n",
        "        pnl = s_test.values * r_test.values\n",
        "        # per-fold metric\n",
        "        if METRIC == \"sharpe\":\n",
        "            score = ann_sharpe_from_series(pnl)\n",
        "        elif METRIC == \"winrate\":\n",
        "            trades_mask = s_test.values != 0\n",
        "            if trades_mask.sum() == 0:\n",
        "                score = 0.0\n",
        "            else:\n",
        "                # build quick run-level winrate\n",
        "                # simple: winrate = mean(pnl[trades_mask] > 0)\n",
        "                score = float(np.mean(pnl[trades_mask] > 0))\n",
        "        else:\n",
        "            score = ann_sharpe_from_series(pnl)\n",
        "        coverage = int((np.abs(s_test.values) > 0).sum())\n",
        "        # small coverage penalty\n",
        "        if coverage < MIN_POS_OBS:\n",
        "            score *= PENALTY_COVERAGE\n",
        "        fold_scores.append(score)\n",
        "        fold_coverages.append(coverage)\n",
        "    if not fold_scores:\n",
        "        return 0.0, []\n",
        "    return float(np.mean(fold_scores)), fold_coverages\n",
        "\n",
        "# ---------- main ----------\n",
        "if __name__ == \"__main__\":\n",
        "    # load meta + signals\n",
        "    df_meta, meta_p = load_latest_meta(OUT_DIR)\n",
        "    sig, sig_p = load_latest_signals(OUT_DIR)\n",
        "    print(f\"[INFO] loaded df_meta_shortlist: {Path(meta_p).name} rows={df_meta.shape[0]}\")\n",
        "    print(f\"[INFO] using signals: {Path(sig_p).name}\")\n",
        "\n",
        "    # choose calibrated probabilities if exist (fallback to raw)\n",
        "    if \"p_meta_long_cal\" in sig.columns and \"p_meta_short_cal\" in sig.columns:\n",
        "        p_long = sig[\"p_meta_long_cal\"].astype(float)\n",
        "        p_short = sig[\"p_meta_short_cal\"].astype(float)\n",
        "    elif \"p_meta_long\" in sig.columns and \"p_meta_short\" in sig.columns:\n",
        "        p_long = sig[\"p_meta_long\"].astype(float)\n",
        "        p_short = sig[\"p_meta_short\"].astype(float)\n",
        "    elif \"p_long\" in sig.columns and \"p_short\" in sig.columns:\n",
        "        p_long = sig[\"p_long\"].astype(float)\n",
        "        p_short = sig[\"p_short\"].astype(float)\n",
        "    else:\n",
        "        raise RuntimeError(\"No suitable probability columns found (p_meta_long_cal / p_meta_long / p_long)\")\n",
        "\n",
        "    # align to meta index (causal alignment responsibility: user must ensure signals are aligned to tb_t_break timestamps)\n",
        "    p_long = p_long.reindex(df_meta.index).fillna(0.5)\n",
        "    p_short = p_short.reindex(df_meta.index).fillna(0.5)\n",
        "    spread = (p_long - p_short).astype(float)\n",
        "\n",
        "    # ensure returns exist (tb_ret_h)\n",
        "    ret_col = f\"tb_ret_{H}\"\n",
        "    if ret_col not in df_meta.columns:\n",
        "        raise RuntimeError(f\"{ret_col} missing in df_meta_shortlist; please merge STEP03 returns before running signal-only optimizer.\")\n",
        "    rets = df_meta[ret_col].astype(float).fillna(0.0)\n",
        "\n",
        "    # load purge utils (user should have pasted)\n",
        "    try:\n",
        "        from b0_07_purge_utils import compute_exposure_intervals, exposure_to_pos_intervals, purged_cv_splits\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"Paste patched b0_07_purge_utils into notebook before running (purged CV helper).\")\n",
        "\n",
        "    # build pos_all for purged_cv_splits\n",
        "    idx = df_meta.index\n",
        "    exp_all = compute_exposure_intervals(idx, df_meta[f\"tb_t_break_{LABEL_H}\"], horizon_fallback=None, last_index=idx[-1])\n",
        "    pos_all = exposure_to_pos_intervals(exp_all, idx)\n",
        "    outer_splits = list(purged_cv_splits(pos_all, n_splits=N_OUTER, embargo=EMBARGO, index=idx, drop_unmapped=True, random_state=RSTATE))\n",
        "    if len(outer_splits) < N_OUTER:\n",
        "        print(f\"[WARN] produced fewer outer splits ({len(outer_splits)}) than requested N_OUTER={N_OUTER}\")\n",
        "    print(f\"[INFO] outer_splits: {len(outer_splits)}\")\n",
        "\n",
        "    # Optuna objective: thresholded sign + optional smoothing (median filter)\n",
        "    def objective(trial):\n",
        "        # hyperparams to tune\n",
        "        threshold = trial.suggest_float(\"threshold\", 0.0, 0.5)            # spread threshold\n",
        "        smoothing = trial.suggest_int(\"smoothing\", 1, 24)                # causal rolling median window (>=1)\n",
        "        invert = trial.suggest_categorical(\"invert\", [False, True])     # sometimes p_short/p_long flipped in pipelines\n",
        "\n",
        "        # build causal spread smoothing: use rolling median shift(1) to keep causal\n",
        "        sp = spread.copy()\n",
        "        if invert:\n",
        "            sp = -sp\n",
        "        if smoothing > 1:\n",
        "            sp = sp.rolling(window=smoothing, min_periods=1).median().shift(1).fillna(0.0)\n",
        "        # threshold -> discrete signals: -1,0,1\n",
        "        s = pd.Series(0, index=sp.index, dtype=int)\n",
        "        s.loc[sp > threshold] = 1\n",
        "        s.loc[sp < -threshold] = -1\n",
        "\n",
        "        mean_score, fold_cov = evaluate_signals_on_folds(s, rets, outer_splits, pos_all.index)\n",
        "        mean_coverage = np.mean(fold_cov) if fold_cov else 0\n",
        "        trial.set_user_attr(\"coverage_mean\", float(mean_coverage))\n",
        "        trial.set_user_attr(\"raw_score\", float(mean_score))\n",
        "        return mean_score\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RSTATE))\n",
        "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=1)\n",
        "\n",
        "    # persist best\n",
        "    ts = int(time.time())\n",
        "    best = deepcopy(study.best_trial.params)\n",
        "    best[\"best_value\"] = float(study.best_value)\n",
        "    best_path = os.path.join(OUT_DIR, f\"{RESULT_PREFIX}_best_params_{ts}.json\")\n",
        "    opt_path = os.path.join(OUT_DIR, f\"{RESULT_PREFIX}_study_{ts}.pkl\")\n",
        "    import pickle\n",
        "    pickle.dump(study, open(opt_path, \"wb\"))\n",
        "    with open(best_path, \"w\") as f:\n",
        "        json.dump(best, f, indent=2)\n",
        "    print(\"[OK] Optuna done. best_value (signal metric):\", study.best_value)\n",
        "    print(\"best params ->\", best)\n",
        "\n",
        "    # build final OOF signal with best params\n",
        "    bp = study.best_trial.params\n",
        "    sp = spread.copy()\n",
        "    if bp.get(\"invert\", False):\n",
        "        sp = -sp\n",
        "    smoothing = int(bp.get(\"smoothing\", 1))\n",
        "    if smoothing > 1:\n",
        "        sp = sp.rolling(window=smoothing, min_periods=1).median().shift(1).fillna(0.0)\n",
        "    thr = float(bp.get(\"threshold\", 0.0))\n",
        "    s_final = pd.Series(0, index=sp.index, dtype=int)\n",
        "    s_final.loc[sp > thr] = 1\n",
        "    s_final.loc[sp < -thr] = -1\n",
        "\n",
        "    # diagnostics: per-fold stats and OOF series\n",
        "    fold_stats = []\n",
        "    oof_pnl = []\n",
        "    for i, (train_pos, test_pos) in enumerate(outer_splits):\n",
        "        test_idx = pos_all.index[test_pos]\n",
        "        s_test = s_final.reindex(test_idx).fillna(0.0)\n",
        "        r_test = rets.reindex(test_idx).fillna(0.0)\n",
        "        pnl = (s_test.values * r_test.values)\n",
        "        fold_sh = ann_sharpe_from_series(pnl)\n",
        "        fold_cov = int((np.abs(s_test.values) > 0).sum())\n",
        "        fold_stats.append({\"fold\": i, \"n\": int(len(test_idx)), \"coverage\": fold_cov, \"ann_sharpe\": fold_sh})\n",
        "        # accumulate OOF\n",
        "        oof_pnl.append(pd.Series(pnl, index=test_idx))\n",
        "    if oof_pnl:\n",
        "        oof_pnl_ser = pd.concat(oof_pnl).sort_index()\n",
        "    else:\n",
        "        oof_pnl_ser = pd.Series([], dtype=float)\n",
        "\n",
        "    # save final signals (discrete) aligned to meta\n",
        "    out_df = pd.DataFrame(index=df_meta.index)\n",
        "    out_df[\"p_long\"] = p_long.reindex(df_meta.index).fillna(0.5)\n",
        "    out_df[\"p_short\"] = p_short.reindex(df_meta.index).fillna(0.5)\n",
        "    out_df[\"spread\"] = (out_df[\"p_long\"] - out_df[\"p_short\"]).astype(float)\n",
        "    out_df[\"signal\"] = s_final.reindex(df_meta.index).fillna(0).astype(int)\n",
        "    sig_out_path = os.path.join(OUT_DIR, f\"{RESULT_PREFIX}_signals_{ts}.csv\")\n",
        "    out_df.to_csv(sig_out_path)\n",
        "\n",
        "    meta = {\n",
        "        \"generated_at\": ts,\n",
        "        \"optuna_best\": best,\n",
        "        \"n_trials\": N_TRIALS,\n",
        "        \"outer_splits\": len(outer_splits),\n",
        "        \"per_fold\": fold_stats,\n",
        "        \"oof_sharpe\": ann_sharpe_from_series(oof_pnl_ser.values) if len(oof_pnl_ser)>0 else None,\n",
        "        \"oof_coverage\": int((np.abs(s_final.values)>0).sum())\n",
        "    }\n",
        "    meta_path = os.path.join(OUT_DIR, f\"{RESULT_PREFIX}_meta_{ts}.json\")\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(f\"[SAVED] signals -> {sig_out_path}\")\n",
        "    print(f\"[SAVED] meta -> {meta_path}\")\n",
        "    print(\"[DONE] signal-only optimizer finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CCAEn5JikAfg",
        "outputId": "49f3f59c-71ab-4215-f3fd-0b38bc0518f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DONE] signal-only backtest\n",
            " signal_file: /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_signal_opt_signals_1764904868.csv\n",
            " meta_file: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\n",
            " n_periods: 17521\n",
            " cum_return: -0.10045581745438814\n",
            " ann_return: -0.03588415445950499\n",
            " ann_vol: 0.2331550791892869\n",
            " ann_sharpe: -0.039963673360506644\n",
            " sharpe_ci_95: [-2.2911469130975615, 2.446282417697516]\n",
            " trade_count: 416\n",
            " win_rate: 0.49278846153846156\n",
            " expectancy: -6.488795063195715e-05\n",
            " outputs: /content/drive/MyDrive/quant_pipeline/mtb_out/signal_only_backtest_equity_1764905821.csv /content/drive/MyDrive/quant_pipeline/mtb_out/signal_only_backtest_trades_1764905821.csv /content/drive/MyDrive/quant_pipeline/mtb_out/signal_only_backtest_metrics_1764905821.json\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# SIGNAL-ONLY Backtest (one cell)\n",
        "import os, time, json, math, warnings\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CONFIG\n",
        "ANNUALIZE = 252 * 24        # hourly base\n",
        "BOOTSTRAP_ITERS = 500\n",
        "BOOTSTRAP_BLOCK = 24\n",
        "PLOT_DPI = 120\n",
        "\n",
        "def load_latest(pattern: str):\n",
        "    files = sorted(OUT_DIR.glob(pattern), key=os.path.getmtime)\n",
        "    return files[-1] if files else None\n",
        "\n",
        "def block_bootstrap_sharpe(arr: np.ndarray, iters=500, block=24, seed=42):\n",
        "    n = len(arr)\n",
        "    if n <= 1:\n",
        "        return (np.nan, np.nan)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    sharpes = []\n",
        "    starts = np.arange(0, max(1, n - block + 1))\n",
        "    if len(starts) == 0:\n",
        "        for _ in range(iters):\n",
        "            s = rng.choice(arr, size=n, replace=True)\n",
        "            sharpes.append(0.0 if np.nanstd(s) == 0 else (np.nanmean(s)/np.nanstd(s)) * math.sqrt(ANNUALIZE))\n",
        "    else:\n",
        "        for _ in range(iters):\n",
        "            res = []\n",
        "            while len(res) < n:\n",
        "                st = int(rng.choice(starts))\n",
        "                blk = arr[st:st+block]\n",
        "                res.append(blk)\n",
        "            a = np.concatenate(res)[:n]\n",
        "            sharpes.append(0.0 if np.nanstd(a)==0 else (np.nanmean(a)/np.nanstd(a)) * math.sqrt(ANNUALIZE))\n",
        "    return float(np.percentile(sharpes, 2.5)), float(np.percentile(sharpes, 97.5))\n",
        "\n",
        "def signal_to_side(sig_df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Map available signal columns to continuous side in [-1,1].\"\"\"\n",
        "    df = sig_df.copy()\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    if {\"p_long\",\"p_short\"}.issubset(df.columns):\n",
        "        side = (df[\"p_long\"].fillna(0.5) - df[\"p_short\"].fillna(0.5)).astype(float)\n",
        "    elif \"prob\" in df.columns:\n",
        "        side = ((df[\"prob\"].fillna(0.5) - 0.5) * 2.0).astype(float)\n",
        "    elif \"signal\" in df.columns:\n",
        "        side = df[\"signal\"].fillna(0.0).astype(float)\n",
        "    else:\n",
        "        # fallback to first numeric column\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        if len(num_cols) == 0:\n",
        "            raise RuntimeError(\"No usable numeric column in signal file (p_long/p_short/prob/signal).\")\n",
        "        side = df[num_cols[0]].fillna(0.0).astype(float)\n",
        "    # clip to [-1,1] (safe)\n",
        "    side = side.clip(-1.0, 1.0)\n",
        "    return side\n",
        "\n",
        "def trades_from_sign_runs(side: pd.Series, rets: pd.Series):\n",
        "    \"\"\"Reconstruct trades as contiguous runs where sign != 0 (and same sign).\"\"\"\n",
        "    s = side.fillna(0.0)\n",
        "    mask = s.abs() > 1e-9\n",
        "    trades = []\n",
        "    idx = s.index\n",
        "    n = len(s)\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        if not mask.iat[i]:\n",
        "            i += 1\n",
        "            continue\n",
        "        j = i + 1\n",
        "        sign_i = np.sign(s.iat[i])\n",
        "        while j < n and mask.iat[j] and np.sign(s.iat[j]) == sign_i:\n",
        "            j += 1\n",
        "        pos_slice = s.iloc[i:j].values\n",
        "        ret_slice = rets.iloc[i:j].values\n",
        "        pnl = float((pos_slice * ret_slice).sum())           # signal * return\n",
        "        entry_time = str(idx[i]); exit_time = str(idx[j-1])\n",
        "        trades.append({\n",
        "            \"entry_idx\": int(i), \"exit_idx\": int(j-1),\n",
        "            \"entry_time\": entry_time, \"exit_time\": exit_time,\n",
        "            \"entry_side\": float(s.iat[i]), \"exit_side\": float(s.iat[j-1]),\n",
        "            \"pnl\": pnl, \"len\": int(j-i), \"win\": int(pnl>0)\n",
        "        })\n",
        "        i = j\n",
        "    return pd.DataFrame(trades)\n",
        "\n",
        "# --- load artifacts\n",
        "sig_file = load_latest(\"A_SRC_signal_opt_signals_*.csv\") or load_latest(\"A_SRC_signals_*.csv\")\n",
        "if sig_file is None:\n",
        "    raise RuntimeError(\"No signal file found (A_SRC_signal_opt_signals_*.csv or A_SRC_signals_*.csv) in OUT_DIR\")\n",
        "\n",
        "meta_file = load_latest(\"df_meta_shortlist.v*.pkl\")\n",
        "if meta_file is None:\n",
        "    raise RuntimeError(\"df_meta_shortlist.v*.pkl not found in OUT_DIR\")\n",
        "\n",
        "# read\n",
        "sig_df = pd.read_csv(sig_file, index_col=0)\n",
        "try: sig_df.index = pd.to_datetime(sig_df.index, utc=True)\n",
        "except: pass\n",
        "\n",
        "df_meta = pd.read_pickle(meta_file)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "\n",
        "# select tb_ret_* if exists\n",
        "ret_cols = [c for c in df_meta.columns if c.startswith(\"tb_ret\")]\n",
        "if len(ret_cols) == 0:\n",
        "    ret_col = next((c for c in df_meta.columns if \"ret\" in c and np.issubdtype(df_meta[c].dtype, np.number)), None)\n",
        "else:\n",
        "    ret_col = sorted(ret_cols)[0]\n",
        "if ret_col is None:\n",
        "    raise RuntimeError(\"No return column found in df_meta_shortlist for alignment\")\n",
        "\n",
        "# map signals -> side in [-1,1] and align returns\n",
        "side = signal_to_side(sig_df)\n",
        "rets = df_meta[ret_col].reindex(side.index).fillna(0.0).astype(float)\n",
        "\n",
        "# compute per-bar pnl (signal * return) -- NO sizing, NO tc\n",
        "pnl = side * rets\n",
        "net = pnl.copy()  # same for signal-only\n",
        "eq = (1.0 + net).cumprod()\n",
        "\n",
        "# stats\n",
        "n_periods = len(net)\n",
        "cum_return = float(eq.iloc[-1] - 1.0) if n_periods>0 else 0.0\n",
        "ann_return = float(eq.iloc[-1]**(ANNUALIZE / max(1,n_periods)) - 1.0) if n_periods>0 else 0.0\n",
        "ann_vol = float(net.std(ddof=0) * math.sqrt(ANNUALIZE)) if net.std(ddof=0)>0 else 0.0\n",
        "ann_sharpe = float((net.mean() / net.std(ddof=0)) * math.sqrt(ANNUALIZE)) if net.std(ddof=0)>0 else 0.0\n",
        "sharpe_lo, sharpe_hi = block_bootstrap_sharpe(net.values, iters=BOOTSTRAP_ITERS, block=BOOTSTRAP_BLOCK)\n",
        "\n",
        "# trade reconstruction\n",
        "trades_df = trades_from_sign_runs(side, rets)\n",
        "trade_count = len(trades_df)\n",
        "win_rate = float(trades_df[\"win\"].sum()/trade_count) if trade_count>0 else float(\"nan\")\n",
        "expectancy = float(trades_df[\"pnl\"].mean()) if trade_count>0 else 0.0\n",
        "median_trade_net = float(trades_df[\"pnl\"].median()) if trade_count>0 else float(\"nan\")\n",
        "\n",
        "# save outputs\n",
        "ts = int(time.time())\n",
        "OUT_EQ = OUT_DIR / f\"signal_only_backtest_equity_{ts}.csv\"\n",
        "OUT_TRADES = OUT_DIR / f\"signal_only_backtest_trades_{ts}.csv\"\n",
        "OUT_METRICS = OUT_DIR / f\"signal_only_backtest_metrics_{ts}.json\"\n",
        "pd.DataFrame({\"net\": net, \"pnl\": pnl, \"eq\": eq}).to_csv(OUT_EQ)\n",
        "trades_df.to_csv(OUT_TRADES, index=False)\n",
        "metrics = {\n",
        "    \"signal_file\": str(sig_file), \"meta_file\": str(meta_file),\n",
        "    \"n_periods\": int(n_periods), \"cum_return\": cum_return, \"ann_return\": ann_return,\n",
        "    \"ann_vol\": ann_vol, \"ann_sharpe\": ann_sharpe, \"sharpe_ci_95\": [sharpe_lo,sharpe_hi],\n",
        "    \"trade_count\": trade_count, \"win_rate\": win_rate, \"expectancy\": expectancy, \"median_trade_net\": median_trade_net\n",
        "}\n",
        "with open(OUT_METRICS, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# quick plots\n",
        "fig, axs = plt.subplots(2,1, figsize=(10,8), dpi=PLOT_DPI, constrained_layout=True)\n",
        "axs[0].plot(eq.index, eq.values); axs[0].set_title(\"Signal-only Equity (no sizing, no tc)\")\n",
        "axs[1].hist(trades_df[\"pnl\"].values if trade_count>0 else [0], bins=40); axs[1].set_title(\"Trade PnL histogram\")\n",
        "plt.savefig(OUT_DIR / f\"signal_only_backtest_plots_{ts}.png\"); plt.close(fig)\n",
        "\n",
        "# print concise summary\n",
        "print(\"[DONE] signal-only backtest\")\n",
        "print(\" signal_file:\", str(sig_file))\n",
        "print(\" meta_file:\", str(meta_file))\n",
        "print(\" n_periods:\", n_periods)\n",
        "print(\" cum_return:\", cum_return)\n",
        "print(\" ann_return:\", ann_return)\n",
        "print(\" ann_vol:\", ann_vol)\n",
        "print(\" ann_sharpe:\", ann_sharpe)\n",
        "print(\" sharpe_ci_95:\", [sharpe_lo, sharpe_hi])\n",
        "print(\" trade_count:\", trade_count)\n",
        "print(\" win_rate:\", win_rate)\n",
        "print(\" expectancy:\", expectancy)\n",
        "print(\" outputs:\", str(OUT_EQ), str(OUT_TRADES), str(OUT_METRICS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjdX0qBYmKHq",
        "outputId": "901e8af4-ced9-4a84-bb07-82af817b69e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "signal cols: ['p_long', 'p_short', 'spread', 'signal']\n",
            "using returns column: tb_ret_at_break_h4\n",
            "sizes -> signal: 17521, meta: 17521, intersection: 17521 (100.0%)\n",
            "candidate signal cols: ['p_long', 'p_short', 'signal']\n",
            "         mean_signal  corr_sig_ret  sharpe_prob2signed  sharpe_signed  \\\n",
            "col                                                                     \n",
            "p_long      0.364761     -0.000100           -0.058172       0.064308   \n",
            "p_short     0.635257      0.001634            0.121743       0.082052   \n",
            "signal     -0.551167      0.035801            1.096549       1.734781   \n",
            "\n",
            "         sharpe_thr  cum_prob2  cum_signed    cum_thr  \n",
            "col                                                    \n",
            "p_long     0.357447  -0.090718   -0.030843   0.091186  \n",
            "p_short   -0.510431   0.011477   -0.128344  -0.768422  \n",
            "signal     2.714025   3.324251    7.305642  72.368771  \n",
            "diag csv -> /content/drive/MyDrive/quant_pipeline/mtb_out/signal_diagnostics_1764906023.csv\n",
            "diag plot -> /content/drive/MyDrive/quant_pipeline/mtb_out/signal_diagnostics_1764906023.png\n"
          ]
        }
      ],
      "source": [
        "# DIAGNOSTIC CELL — paste & run in your Colab / notebook (uses your drive paths)\n",
        "import os, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === EDIT IF NEEDED: paths (use the ones from your logs) ===\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "signal_path = Path(OUT_DIR) / \"A_SRC_signal_opt_signals_1764904868.csv\"\n",
        "meta_path   = Path(OUT_DIR) / \"df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\"\n",
        "\n",
        "# load\n",
        "sig = pd.read_csv(signal_path, index_col=0)\n",
        "sig.index = pd.to_datetime(sig.index, utc=True)\n",
        "meta = pd.read_pickle(meta_path)\n",
        "meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "\n",
        "print(\"signal cols:\", sig.columns.tolist())\n",
        "ret_cols = [c for c in meta.columns if c.startswith(\"tb_ret\")]\n",
        "if not ret_cols:\n",
        "    raise RuntimeError(\"No tb_ret found in meta. Check your df_meta_shortlist.\")\n",
        "ret_col = ret_cols[0]\n",
        "print(\"using returns column:\", ret_col)\n",
        "\n",
        "# align\n",
        "idx = sig.index.intersection(meta.index)\n",
        "print(f\"sizes -> signal: {len(sig)}, meta: {len(meta)}, intersection: {len(idx)} ({len(idx)/len(sig):.1%})\")\n",
        "sig_a = sig.reindex(idx).fillna(method='ffill').fillna(0.0)\n",
        "meta_a = meta.reindex(idx).fillna(0.0)\n",
        "rets = meta_a[ret_col].astype(float).values\n",
        "\n",
        "# detect signal candidate columns\n",
        "cands = [c for c in sig_a.columns if any(x in c.lower() for x in ['p_long','p_short','signal','score','prob','side'])]\n",
        "if not cands:\n",
        "    # fallback: numeric\n",
        "    cands = sig_a.select_dtypes(include=[float,int]).columns.tolist()[:1]\n",
        "print(\"candidate signal cols:\", cands)\n",
        "\n",
        "def metrics_from_pos(position, rets, tc_rate=0.0, annualize=252*24):\n",
        "    pos = np.asarray(position, dtype=float)\n",
        "    dpos = np.empty_like(pos); dpos[0]=pos[0]; dpos[1:]=pos[1:]-pos[:-1]\n",
        "    tc = tc_rate * np.abs(dpos)\n",
        "    pnl = pos * rets\n",
        "    net = pnl - tc\n",
        "    n = len(net)\n",
        "    cum = float(np.prod(1.0+net) - 1.0) if n>0 else 0.0\n",
        "    ann_ret = float((1.0+cum) ** (annualize/max(1,n)) - 1.0) if n>0 else 0.0\n",
        "    ann_vol = float(np.nanstd(net)*np.sqrt(annualize)) if np.nanstd(net)>0 else 0.0\n",
        "    sharpe = float((np.nanmean(net)/np.nanstd(net))*np.sqrt(annualize)) if np.nanstd(net)>0 else float(\"nan\")\n",
        "    return {\"cum\":cum,\"ann_ret\":ann_ret,\"ann_vol\":ann_vol,\"sharpe\":sharpe,\"mean_net\":float(np.nanmean(net))}\n",
        "\n",
        "results = {}\n",
        "for col in cands:\n",
        "    s = sig_a[col].astype(float)\n",
        "    # A: treat as probability p_long -> signed pos in [-1,1]\n",
        "    posA = (2.0*(s-0.5)).values\n",
        "    # B: treat as signed signal as-is\n",
        "    posB = s.values\n",
        "    # C: thresholded signed via median\n",
        "    thr = s.median()\n",
        "    posC = ((s>thr).astype(float)*2.0 - 1.0).values\n",
        "    results[col] = {\n",
        "        \"mean_signal\": float(s.mean()),\n",
        "        \"corr_sig_ret\": float(np.corrcoef(s.values, rets)[0,1]) if len(rets)>1 else float(\"nan\"),\n",
        "        \"A_prob2signed\": metrics_from_pos(posA, rets),\n",
        "        \"B_signed\": metrics_from_pos(posB, rets),\n",
        "        \"C_thr_signed\": metrics_from_pos(posC, rets),\n",
        "        \"median\": float(thr)\n",
        "    }\n",
        "\n",
        "# show table\n",
        "df_out = []\n",
        "for col,v in results.items():\n",
        "    df_out.append([col, v[\"mean_signal\"], v[\"corr_sig_ret\"],\n",
        "                   v[\"A_prob2signed\"][\"sharpe\"], v[\"B_signed\"][\"sharpe\"], v[\"C_thr_signed\"][\"sharpe\"],\n",
        "                   v[\"A_prob2signed\"][\"cum\"], v[\"B_signed\"][\"cum\"], v[\"C_thr_signed\"][\"cum\"]])\n",
        "cols = [\"col\",\"mean_signal\",\"corr_sig_ret\",\"sharpe_prob2signed\",\"sharpe_signed\",\"sharpe_thr\",\"cum_prob2\",\"cum_signed\",\"cum_thr\"]\n",
        "print(pd.DataFrame(df_out, columns=cols).set_index(\"col\"))\n",
        "\n",
        "# save diagnostics CSV & plot for the first candidate\n",
        "ts = int(time.time())\n",
        "diag = pd.DataFrame({\n",
        "    \"ret\": rets,\n",
        "    \"signal\": sig_a[cands[0]].astype(float).values,\n",
        "    \"pos_prob2signed\": (2*(sig_a[cands[0]].astype(float)-0.5)).values,\n",
        "    \"pos_signed\": sig_a[cands[0]].astype(float).values,\n",
        "    \"pnl_prob\": (2*(sig_a[cands[0]].astype(float)-0.5)).values * rets,\n",
        "    \"pnl_signed\": sig_a[cands[0]].astype(float).values * rets\n",
        "}, index=idx)\n",
        "diag_csv = Path(OUT_DIR)/f\"signal_diagnostics_{ts}.csv\"\n",
        "diag.to_csv(diag_csv)\n",
        "print(\"diag csv ->\", diag_csv)\n",
        "\n",
        "# plot cumulative\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(idx, (1+diag[\"pnl_prob\"]).cumprod()-1, label=\"prob->signed\")\n",
        "plt.plot(idx, (1+diag[\"pnl_signed\"]).cumprod()-1, label=\"signed-as-is\")\n",
        "plt.legend(); plt.title(\"cum returns\")\n",
        "png = Path(OUT_DIR)/f\"signal_diagnostics_{ts}.png\"\n",
        "plt.savefig(png, dpi=120, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"diag plot ->\", png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOOk-0KunOq1",
        "outputId": "9697aa2c-e394-47bc-aa01-019008e84a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary (method, cum_return, ann_sharpe, trade_count, win_rate):\n",
            "A pct-scale-clip: 4.955557200252127 1.5006302309338801 333 0.5615615615615616\n",
            "B rank-transform: 6.416601391807253 2.447615978256439 648 0.5509259259259259\n",
            "C thr(median): 37.381532230761415 2.35023958438634 648 0.5509259259259259\n",
            "Diagnostics saved to /content/drive/MyDrive/quant_pipeline/mtb_out\n"
          ]
        }
      ],
      "source": [
        "# RUN THIS CELL in your Colab / notebook (uses your OUT_DIR)\n",
        "import os, time, json, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# files from your run (edit if different)\n",
        "sig_file = Path(OUT_DIR) / \"A_SRC_signal_opt_signals_1764904868.csv\"\n",
        "meta_pkl = Path(OUT_DIR) / \"df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\"  # ensure this has tb_ret_at_break_h4 or tb_ret_* used\n",
        "\n",
        "# params for conversion/backtest\n",
        "SIGNAL_COL = \"signal\"            # column to use\n",
        "PCT_SCALE = 99.0                 # denominator percentile (robust scaling)\n",
        "TARGET_LEVERAGE = 1.0            # max abs position after scaling\n",
        "CLIP = True                      # clip to [-TARGET_LEVERAGE, TARGET_LEVERAGE]\n",
        "TC_RATE = 5e-4                   # fallback tc if missing in pos file\n",
        "ANNUALIZE = 252 * 24             # hourly base used in your pipeline\n",
        "\n",
        "def load_align(sig_file, meta_pkl):\n",
        "    sig = pd.read_csv(sig_file, index_col=0)\n",
        "    sig.index = pd.to_datetime(sig.index, utc=True)\n",
        "    meta = pd.read_pickle(meta_pkl)\n",
        "    meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "    # choose returns column already present (use tb_ret_at_break_h4 in your diag)\n",
        "    ret_cols = [c for c in meta.columns if c.startswith(\"tb_ret\")]\n",
        "    if not ret_cols:\n",
        "        raise RuntimeError(\"No tb_ret_* in meta pickle\")\n",
        "    ret_col = ret_cols[0]\n",
        "    idx = sig.index.intersection(meta.index)\n",
        "    sig = sig.reindex(idx).fillna(method=\"ffill\").fillna(0.0)\n",
        "    meta = meta.reindex(idx).fillna(0.0)\n",
        "    rets = meta[ret_col].astype(float)\n",
        "    return sig, meta, rets\n",
        "\n",
        "def pct_scale_and_clip(s: pd.Series, pct=99.0, target_lev=1.0, clip=True):\n",
        "    abs_vals = np.abs(s.values)\n",
        "    denom = np.percentile(abs_vals, pct) if np.any(abs_vals>0) else 1.0\n",
        "    if denom == 0:\n",
        "        denom = 1.0\n",
        "    scaled = s.values / denom * target_lev\n",
        "    if clip:\n",
        "        scaled = np.clip(scaled, -target_lev, target_lev)\n",
        "    return scaled\n",
        "\n",
        "def backtest_from_positions(positions, rets, tc_rate=TC_RATE, annualize=ANNUALIZE):\n",
        "    pos = np.asarray(positions, dtype=float)\n",
        "    dpos = np.empty_like(pos); dpos[0]=pos[0]; dpos[1:]=pos[1:]-pos[:-1]\n",
        "    tc = tc_rate * np.abs(dpos)\n",
        "    pnl = pos * rets.values\n",
        "    net = pnl - tc\n",
        "    # equity (growth of 1.0 base)\n",
        "    eq = pd.Series((1.0 + net).cumprod(), index=rets.index)\n",
        "    n = len(net)\n",
        "    cum_return = float(eq.iloc[-1] - 1.0) if n>0 else 0.0\n",
        "    ann_return = float(eq.iloc[-1] ** (annualize/max(1,n)) - 1.0) if n>0 else 0.0\n",
        "    ann_vol = float(np.nanstd(net) * math.sqrt(annualize)) if np.nanstd(net)>0 else 0.0\n",
        "    ann_sharpe = float((np.nanmean(net)/np.nanstd(net)) * math.sqrt(annualize)) if np.nanstd(net)>0 else float(\"nan\")\n",
        "    # trades simple reconstruction (runs of same sign)\n",
        "    mask = np.abs(pos) > 1e-6\n",
        "    trades = []\n",
        "    i = 0\n",
        "    N = len(pos)\n",
        "    while i < N:\n",
        "        if not mask[i]:\n",
        "            i += 1; continue\n",
        "        s = i\n",
        "        sgn = np.sign(pos[s])\n",
        "        j = s+1\n",
        "        while j < N and mask[j] and np.sign(pos[j]) == sgn:\n",
        "            j += 1\n",
        "        entry_idx, exit_idx = s, j-1\n",
        "        slice_pos = pos[entry_idx:j]\n",
        "        slice_rets = rets.values[entry_idx:j]\n",
        "        pnl_trade = float((slice_pos * slice_rets).sum())\n",
        "        dpos_slice = np.empty(len(slice_pos))\n",
        "        dpos_slice[0] = slice_pos[0]\n",
        "        if len(slice_pos) > 1:\n",
        "            dpos_slice[1:] = slice_pos[1:] - slice_pos[:-1]\n",
        "        tc_trade = float(tc_rate * np.abs(dpos_slice).sum())\n",
        "        net_trade = pnl_trade - tc_trade\n",
        "        trades.append({\"entry_idx\":int(entry_idx),\"exit_idx\":int(exit_idx),\"pnl\":pnl_trade,\"tc\":tc_trade,\"net\":net_trade,\"len\":int(exit_idx-entry_idx+1),\"win\":int(net_trade>0)})\n",
        "        i = j\n",
        "    trades_df = pd.DataFrame(trades)\n",
        "    trade_count = len(trades_df)\n",
        "    win_rate = float(trades_df[\"win\"].sum()/trade_count) if trade_count>0 else float(\"nan\")\n",
        "    expectancy = float(trades_df[\"net\"].mean()) if trade_count>0 else 0.0\n",
        "    return {\n",
        "        \"eq\": eq, \"net\": net, \"pnl\": pnl, \"tc\": tc,\n",
        "        \"cum_return\": cum_return, \"ann_return\": ann_return,\n",
        "        \"ann_vol\": ann_vol, \"ann_sharpe\": ann_sharpe,\n",
        "        \"trade_count\": trade_count, \"win_rate\": win_rate, \"expectancy\": expectancy,\n",
        "        \"trades_df\": trades_df\n",
        "    }\n",
        "\n",
        "# ---- run\n",
        "sig, meta, rets = load_align(sig_file, meta_pkl)\n",
        "s = sig[SIGNAL_COL].astype(float)\n",
        "\n",
        "# Option 1: percentile scale + clip (recommended default)\n",
        "posA = pct_scale_and_clip(s, pct=PCT_SCALE, target_lev=TARGET_LEVERAGE, clip=CLIP)\n",
        "\n",
        "# Option 2: rank transform to [-1,1]\n",
        "r = s.rank(method=\"average\", pct=True)  # [0,1]\n",
        "posB = (2.0 * (r.values - 0.5)) * TARGET_LEVERAGE\n",
        "\n",
        "# Option 3: threshold (median) -> +/-1\n",
        "thr = s.median()\n",
        "posC = ((s > thr).astype(float)*2.0 - 1.0) * TARGET_LEVERAGE\n",
        "\n",
        "outA = backtest_from_positions(posA, rets)\n",
        "outB = backtest_from_positions(posB, rets)\n",
        "outC = backtest_from_positions(posC, rets)\n",
        "\n",
        "ts = int(time.time())\n",
        "# save equity & trades & metrics for A (you can repeat for B/C)\n",
        "base = Path(OUT_DIR)\n",
        "pd.DataFrame({\"eq\": outA[\"eq\"], \"net\": outA[\"net\"], \"pnl\": outA[\"pnl\"], \"tc\": outA[\"tc\"]}).to_csv(base / f\"signal_only_backtest_equity_scaledA_{ts}.csv\")\n",
        "outA[\"trades_df\"].to_csv(base / f\"signal_only_backtest_trades_scaledA_{ts}.csv\", index=False)\n",
        "metricsA = {\n",
        "    \"method\":\"pct_scale_clip\", \"pct\":PCT_SCALE, \"target_leverage\":TARGET_LEVERAGE,\n",
        "    \"cum_return\": outA[\"cum_return\"], \"ann_return\": outA[\"ann_return\"], \"ann_vol\": outA[\"ann_vol\"],\n",
        "    \"ann_sharpe\": outA[\"ann_sharpe\"], \"trade_count\": outA[\"trade_count\"], \"win_rate\": outA[\"win_rate\"],\n",
        "    \"expectancy\": outA[\"expectancy\"]\n",
        "}\n",
        "with open(base / f\"signal_only_backtest_metrics_scaledA_{ts}.json\",\"w\") as f:\n",
        "    json.dump(metricsA, f, indent=2)\n",
        "\n",
        "# quick print summary for three options\n",
        "print(\"Summary (method, cum_return, ann_sharpe, trade_count, win_rate):\")\n",
        "print(\"A pct-scale-clip:\", outA[\"cum_return\"], outA[\"ann_sharpe\"], outA[\"trade_count\"], outA[\"win_rate\"])\n",
        "print(\"B rank-transform:\", outB[\"cum_return\"], outB[\"ann_sharpe\"], outB[\"trade_count\"], outB[\"win_rate\"])\n",
        "print(\"C thr(median):\", outC[\"cum_return\"], outC[\"ann_sharpe\"], outC[\"trade_count\"], outC[\"win_rate\"])\n",
        "\n",
        "print(\"Diagnostics saved to\", base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70SbhGi0oJAN",
        "outputId": "2cbf635e-aa77-4a50-b532-b10d64ae8a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] using signal file: /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_signal_opt_signals_1764904868.csv\n",
            "[INFO] loaded meta: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl  using returns column: tb_ret_at_break_h4\n",
            "[INFO] signal: 17521, meta: 17521, intersection: 17521 (100.00%)\n",
            "[INFO] using signal col: signal\n",
            "[INFO] tc_rate used: 0.0005\n",
            "[DONE] signal-only backtest (B: rank-transform lev1x) finished.\n",
            " - equity csv: /content/drive/MyDrive/quant_pipeline/mtb_out/signal_only_backtest_equity_1764906555.csv\n",
            " - trades csv: /content/drive/MyDrive/quant_pipeline/mtb_out/signal_only_backtest_trades_1764906555.csv\n",
            " - metrics json: /content/drive/MyDrive/quant_pipeline/mtb_out/signal_only_backtest_metrics_1764906555.json\n",
            " - plots: /content/drive/MyDrive/quant_pipeline/mtb_out/signal_only_backtest_plots_1764906555.png\n",
            "Summary metrics:\n",
            "  n_periods: 17521\n",
            "  cum_return: 1.3780937084571523\n",
            "  ann_return: 0.3485556945727586\n",
            "  ann_vol: 0.2146403586994946\n",
            "  ann_sharpe: 1.5006302309338804\n",
            "  sharpe_ci_95: [-0.4876751757262733, 3.291406535331316]\n",
            "  max_drawdown: -0.39200332352952627\n",
            "  trade_count: 333\n",
            "  win_rate: 0.5615615615615616\n",
            "  expectancy: 0.0030200787020828574\n",
            "  turnover: 0.06743737957610792\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ONE-CELL: Signal-only backtest — Rank-transform -> lev 1x -> vectorized backtest + bootstrap CI\n",
        "# Mode: B (rank-transform). Designed for hourly data. Clean, defensive, reproducible.\n",
        "import os, time, json, math, warnings\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Backtest settings\n",
        "ANNUALIZE = 252 * 24        # hourly base\n",
        "BOOTSTRAP_ITERS = 1000\n",
        "BOOTSTRAP_BLOCK = 24        # block length in hours for block bootstrap\n",
        "PLOT_DPI = 120\n",
        "SEED = 42\n",
        "MIN_POS_THRESH = 1e-8\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Preferred returns columns (priority)\n",
        "RET_PREFS = [\"tb_ret_at_break_h4\", \"tb_ret_8\", \"tb_ret_4\", \"tb_ret\"]\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def load_latest_file(glob_pattern: str) -> str:\n",
        "    files = sorted(Path(OUT_DIR).glob(glob_pattern), key=os.path.getmtime)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No files match {glob_pattern} in {OUT_DIR}\")\n",
        "    return str(files[-1])\n",
        "\n",
        "def load_meta_and_returns(prefer_ret_col: str = None) -> Tuple[pd.DataFrame, str, str]:\n",
        "    \"\"\"Load df_meta_shortlist*.pkl and ensure a returns series exists. Returns (df_meta, meta_path, used_ret_col)\"\"\"\n",
        "    pkl = load_latest_file(\"df_meta_shortlist.v*.pkl\")\n",
        "    df = pd.read_pickle(pkl)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    # find returns column\n",
        "    if prefer_ret_col and prefer_ret_col in df.columns:\n",
        "        return df, pkl, prefer_ret_col\n",
        "    for c in RET_PREFS:\n",
        "        if c in df.columns:\n",
        "            return df, pkl, c\n",
        "    # fallback: any col that startswith tb_ret\n",
        "    tb = [c for c in df.columns if c.startswith(\"tb_ret\")]\n",
        "    if tb:\n",
        "        return df, pkl, tb[0]\n",
        "    # if not present, attempt map from df_step03 tb_ret mapping CSV (best-effort)\n",
        "    step_csvs = sorted(Path(OUT_DIR).glob(\"df_step03_tb_multi*.csv\"), key=os.path.getmtime)\n",
        "    if step_csvs:\n",
        "        df_step = pd.read_csv(step_csvs[-1])\n",
        "        tbreak_cols = [c for c in df_step.columns if c.startswith(\"tb_t_break\")]\n",
        "        ret_cols = [c for c in df_step.columns if c.startswith(\"tb_ret\")]\n",
        "        if tbreak_cols and ret_cols:\n",
        "            tb_step_col = tbreak_cols[0]\n",
        "            ret_step_col = ret_cols[0]\n",
        "            df_step[tb_step_col] = pd.to_datetime(df_step[tb_step_col], utc=True, errors=\"coerce\")\n",
        "            map_ser = df_step.groupby(tb_step_col)[ret_step_col].mean()\n",
        "            tb_meta_col = next((c for c in df.columns if c.startswith(\"tb_t_break\")), None)\n",
        "            if tb_meta_col is not None:\n",
        "                tb_idx = pd.to_datetime(df[tb_meta_col], utc=True, errors=\"coerce\")\n",
        "                df[\"__mapped_tb_ret__\"] = tb_idx.map(map_ser).fillna(0.0).astype(float)\n",
        "                return df, pkl, \"__mapped_tb_ret__\"\n",
        "    raise RuntimeError(\"No returns column found in df_meta_shortlist and no STEP03 mapping available.\")\n",
        "\n",
        "def block_bootstrap_sharpe(net_series: np.ndarray, iters: int = 1000, block: int = 24, annualize: int = ANNUALIZE, seed: int = SEED) -> Tuple[float, float]:\n",
        "    \"\"\"Block bootstrap Sharpe CI (2.5%,97.5%). Returns (lo, hi).\"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(net_series)\n",
        "    if n <= 1:\n",
        "        return float(\"nan\"), float(\"nan\")\n",
        "    boot_sharpes = []\n",
        "    starts = np.arange(0, max(1, n - block + 1))\n",
        "    if len(starts) <= 0:\n",
        "        # iid fallback\n",
        "        for _ in range(iters):\n",
        "            s = rng.choice(net_series, size=n, replace=True)\n",
        "            if np.nanstd(s) == 0:\n",
        "                boot_sharpes.append(0.0)\n",
        "            else:\n",
        "                boot_sharpes.append((np.nanmean(s) / np.nanstd(s)) * np.sqrt(annualize))\n",
        "    else:\n",
        "        for _ in range(iters):\n",
        "            res = []\n",
        "            while len(res) < n:\n",
        "                st = int(rng.choice(starts))\n",
        "                blk = net_series[st:st+block]\n",
        "                res.append(blk)\n",
        "            res_arr = np.concatenate(res)[:n]\n",
        "            if np.nanstd(res_arr) == 0:\n",
        "                boot_sharpes.append(0.0)\n",
        "            else:\n",
        "                boot_sharpes.append((np.nanmean(res_arr) / np.nanstd(res_arr)) * np.sqrt(annualize))\n",
        "    lo = float(np.percentile(boot_sharpes, 2.5))\n",
        "    hi = float(np.percentile(boot_sharpes, 97.5))\n",
        "    return lo, hi\n",
        "\n",
        "def rank_transform_signal(sig: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Rank transform signals while preserving sign. Output scaled to [-1,1].\"\"\"\n",
        "    # safe nan handling\n",
        "    mask = ~np.isnan(sig)\n",
        "    out = np.zeros_like(sig, dtype=float)\n",
        "    if mask.sum() == 0:\n",
        "        return out\n",
        "    s = sig[mask]\n",
        "    signs = np.sign(s)\n",
        "    abs_s = np.abs(s)\n",
        "    # rank within absolute magnitude then apply original sign\n",
        "    ranks = pd.Series(abs_s).rank(method=\"average\").values  # 1..N\n",
        "    ranks = (ranks - 1) / (len(ranks) - 1) if len(ranks) > 1 else ranks * 0.0\n",
        "    # map to (0..1), then to (0..1] and to [-1,1]\n",
        "    scaled = ranks * 2.0 - 1.0\n",
        "    out[mask] = scaled * signs\n",
        "    # ensure within [-1,1]\n",
        "    out = np.clip(out, -1.0, 1.0)\n",
        "    return out\n",
        "\n",
        "def trade_reconstruction(positions: np.ndarray, index: pd.Index, rets: np.ndarray, tc_rate: float) -> List[dict]:\n",
        "    \"\"\"Detect contiguous runs of same-signed non-zero position; return trades list.\"\"\"\n",
        "    # positions: float array\n",
        "    mask = np.abs(positions) > MIN_POS_THRESH\n",
        "    if not mask.any():\n",
        "        return []\n",
        "    N = len(positions)\n",
        "    trades = []\n",
        "    i = 0\n",
        "    while i < N:\n",
        "        if not mask[i]:\n",
        "            i += 1; continue\n",
        "        s = i\n",
        "        sgn = np.sign(positions[s])\n",
        "        j = s + 1\n",
        "        while j < N and mask[j] and np.sign(positions[j]) == sgn:\n",
        "            j += 1\n",
        "        # run [s, j-1]\n",
        "        pos_slice = positions[s:j]\n",
        "        rets_slice = rets[s:j]\n",
        "        pnl = float((pos_slice * rets_slice).sum())\n",
        "        # transaction cost: sum abs(dpos) multiplied by tc_rate\n",
        "        dpos = np.empty(len(pos_slice))\n",
        "        dpos[0] = pos_slice[0]\n",
        "        if len(pos_slice) > 1:\n",
        "            dpos[1:] = pos_slice[1:] - pos_slice[:-1]\n",
        "        tc = float(tc_rate * np.abs(dpos).sum())\n",
        "        net = pnl - tc\n",
        "        trades.append({\n",
        "            \"entry_time\": str(index[s]),\n",
        "            \"exit_time\": str(index[j-1]),\n",
        "            \"entry_idx\": int(s),\n",
        "            \"exit_idx\": int(j-1),\n",
        "            \"entry_pos\": float(pos_slice[0]),\n",
        "            \"exit_pos\": float(pos_slice[-1]),\n",
        "            \"pnl\": pnl,\n",
        "            \"tc\": tc,\n",
        "            \"net\": net,\n",
        "            \"len\": int(j-1 - s + 1),\n",
        "            \"win\": int(net > 0)\n",
        "        })\n",
        "        i = j\n",
        "    return trades\n",
        "\n",
        "# ---------------- main ----------------\n",
        "ts = int(time.time())\n",
        "\n",
        "# load latest signal file (signal-only optimizer outputs)\n",
        "sig_path = load_latest_file(\"A_SRC_signal_opt_signals_*.csv\")\n",
        "print(f\"[INFO] using signal file: {sig_path}\")\n",
        "sig_df = pd.read_csv(sig_path, index_col=0)\n",
        "try:\n",
        "    sig_df.index = pd.to_datetime(sig_df.index, utc=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# load meta & returns\n",
        "df_meta, meta_path, used_ret_col = load_meta_and_returns()\n",
        "print(f\"[INFO] loaded meta: {meta_path}  using returns column: {used_ret_col}\")\n",
        "\n",
        "# align indexes\n",
        "# Reindex signal -> meta intersection (we treat meta as ground truth); we will reindex signals to meta index (causal-safe)\n",
        "common_index = sig_df.index.intersection(df_meta.index)\n",
        "if len(common_index) == 0:\n",
        "    raise RuntimeError(\"No intersection between signal index and meta index.\")\n",
        "print(f\"[INFO] signal: {len(sig_df)}, meta: {len(df_meta)}, intersection: {len(common_index)} ({len(common_index)/len(df_meta):.2%})\")\n",
        "\n",
        "# Reindex both to intersection (preserve ordering)\n",
        "sig_df = sig_df.reindex(common_index)\n",
        "df_meta = df_meta.reindex(common_index)\n",
        "rets = df_meta[used_ret_col].fillna(0.0).astype(float).values\n",
        "\n",
        "# Choose candidate signal column:\n",
        "# preference: 'signal' or p_long-p_short; fallbacks: p_long, p_short, spread\n",
        "sig_col = None\n",
        "for c in [\"signal\", \"p_long\", \"p_long\", \"p_short\", \"spread\"]:\n",
        "    if c in sig_df.columns:\n",
        "        sig_col = c\n",
        "        break\n",
        "if sig_col is None:\n",
        "    # try any numeric column\n",
        "    num_cols = sig_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if not num_cols:\n",
        "        raise RuntimeError(\"No numeric signal columns found in signal CSV.\")\n",
        "    sig_col = num_cols[0]\n",
        "\n",
        "print(f\"[INFO] using signal col: {sig_col}\")\n",
        "\n",
        "raw_signal = sig_df[sig_col].astype(float).values\n",
        "\n",
        "# Rank-transform (Mode B) -> then lev 1x (positions in [-1,1])\n",
        "rank_pos = rank_transform_signal(raw_signal)\n",
        "# For clarity: leverage 1x means positions are already scaled to [-1,1] interpret as fraction of capital\n",
        "positions = rank_pos.copy()\n",
        "\n",
        "# Transaction cost: if 'tc' in signal file, use median/first; else default\n",
        "if \"tc\" in sig_df.columns:\n",
        "    tc_rate = float(sig_df[\"tc\"].dropna().iloc[0]) if sig_df[\"tc\"].dropna().size else float(sig_df[\"tc\"].fillna(0.0).iloc[0])\n",
        "else:\n",
        "    tc_rate = 5e-4  # default\n",
        "print(f\"[INFO] tc_rate used: {tc_rate}\")\n",
        "\n",
        "# Vectorized pnl & net\n",
        "dpos = np.empty_like(positions); dpos[0] = positions[0]; dpos[1:] = positions[1:] - positions[:-1]\n",
        "tc_costs = tc_rate * np.abs(dpos)\n",
        "pnl = positions * rets\n",
        "net = pnl - tc_costs\n",
        "\n",
        "# equity series (handle numpy/pandas)\n",
        "if isinstance(net, np.ndarray):\n",
        "    eq_values = np.cumprod(1.0 + net)\n",
        "    eq_index = pd.DatetimeIndex(common_index)\n",
        "    eq = pd.Series(eq_values, index=eq_index)\n",
        "else:\n",
        "    eq = (1.0 + net).cumprod()\n",
        "\n",
        "n_periods = len(net)\n",
        "cum_return = float(eq.iloc[-1] - 1.0) if n_periods > 0 else 0.0\n",
        "ann_return = float(eq.iloc[-1] ** (ANNUALIZE / max(1, n_periods)) - 1.0) if n_periods > 0 else 0.0\n",
        "ann_vol = float(np.nanstd(net) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "ann_sharpe = float((np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "\n",
        "# downside/sortino\n",
        "neg = net.copy()\n",
        "neg[neg > 0] = 0.0\n",
        "downside = float(np.sqrt(np.nanmean(neg**2)) * math.sqrt(ANNUALIZE)) if np.any(neg != 0) else 0.0\n",
        "sortino = float((np.nanmean(net) * math.sqrt(ANNUALIZE) / downside)) if downside > 0 else float(\"nan\")\n",
        "\n",
        "# drawdown\n",
        "cum_max = eq.cummax()\n",
        "drawdown = (eq - cum_max) / cum_max\n",
        "max_dd = float(drawdown.min()) if len(drawdown) > 0 else 0.0\n",
        "calmar = float(ann_return / abs(max_dd)) if max_dd < 0 else None\n",
        "\n",
        "# turnover & exposure\n",
        "avg_abs_pos = float(np.nanmean(np.abs(positions))) if np.nanmean(np.abs(positions)) > 0 else 1.0\n",
        "turnover = float(np.nanmean(np.abs(dpos)) / avg_abs_pos)\n",
        "\n",
        "# trade reconstruction\n",
        "trades = trade_reconstruction(positions, common_index, rets, tc_rate)\n",
        "trades_df = pd.DataFrame(trades)\n",
        "trade_count = len(trades_df)\n",
        "win_count = int(trades_df[\"win\"].sum()) if trade_count > 0 else 0\n",
        "win_rate = float(win_count / trade_count) if trade_count > 0 else float(\"nan\")\n",
        "avg_holding = float(trades_df[\"len\"].mean()) if trade_count > 0 else float(\"nan\")\n",
        "expectancy = float(trades_df[\"net\"].mean()) if trade_count > 0 else 0.0\n",
        "median_trade_net = float(trades_df[\"net\"].median()) if trade_count > 0 else float(\"nan\")\n",
        "\n",
        "# bootstrap Sharpe CI\n",
        "sharpe_lo, sharpe_hi = block_bootstrap_sharpe(net, iters=BOOTSTRAP_ITERS, block=BOOTSTRAP_BLOCK, seed=SEED)\n",
        "\n",
        "# regime breakdown by vol terciles (best-effort)\n",
        "vol_col = next((c for c in df_meta.columns if \"vol\" in c), None)\n",
        "if vol_col is None:\n",
        "    vol_proxy = df_meta[used_ret_col].rolling(50, min_periods=1).std().reindex(common_index).fillna(method=\"ffill\").fillna(0.0)\n",
        "else:\n",
        "    vol_proxy = df_meta[vol_col].reindex(common_index).fillna(method=\"ffill\").fillna(0.0)\n",
        "vol_bucket = pd.qcut(vol_proxy, q=3, labels=[\"low\",\"mid\",\"high\"], duplicates=\"drop\")\n",
        "regime_stats = {}\n",
        "for b in vol_bucket.unique():\n",
        "    mask = vol_bucket == b\n",
        "    if mask.sum() == 0:\n",
        "        continue\n",
        "    pnl_r = (positions[mask.values] * rets[mask.values]) - (tc_rate * np.abs(np.concatenate(([positions[mask.values][0]], np.diff(positions[mask.values])))))\n",
        "    ann_sh = float((np.nanmean(pnl_r) / np.nanstd(pnl_r)) * math.sqrt(ANNUALIZE)) if np.nanstd(pnl_r) > 0 else 0.0\n",
        "    regime_stats[f\"vol_{b}_ann_sharpe\"] = ann_sh\n",
        "    regime_stats[f\"vol_{b}_winrate\"] = float(np.mean(pnl_r > 0)) if len(pnl_r) > 0 else float(\"nan\")\n",
        "    regime_stats[f\"vol_{b}_n\"] = int(mask.sum())\n",
        "\n",
        "# metrics\n",
        "metrics = {\n",
        "    \"signal_file\": sig_path,\n",
        "    \"meta_file\": meta_path,\n",
        "    \"n_periods\": int(n_periods),\n",
        "    \"cum_return\": cum_return,\n",
        "    \"ann_return\": ann_return,\n",
        "    \"ann_vol\": ann_vol,\n",
        "    \"ann_sharpe\": ann_sharpe,\n",
        "    \"sharpe_ci_95\": [sharpe_lo, sharpe_hi],\n",
        "    \"sortino\": sortino,\n",
        "    \"max_drawdown\": max_dd,\n",
        "    \"calmar\": calmar,\n",
        "    \"trade_count\": trade_count,\n",
        "    \"win_count\": win_count,\n",
        "    \"win_rate\": win_rate,\n",
        "    \"avg_holding\": avg_holding,\n",
        "    \"expectancy\": expectancy,\n",
        "    \"median_trade_net\": median_trade_net,\n",
        "    \"turnover\": turnover,\n",
        "    \"tc_rate\": tc_rate,\n",
        "    \"rank_mode\": \"B_rank_transform_lev1x\",\n",
        "    \"seed\": SEED,\n",
        "}\n",
        "metrics.update(regime_stats)\n",
        "\n",
        "# save outputs\n",
        "base = Path(OUT_DIR)\n",
        "eq_csv = base / f\"signal_only_backtest_equity_{ts}.csv\"\n",
        "trades_csv = base / f\"signal_only_backtest_trades_{ts}.csv\"\n",
        "metrics_json = base / f\"signal_only_backtest_metrics_{ts}.json\"\n",
        "plot_png = base / f\"signal_only_backtest_plots_{ts}.png\"\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"net\": net,\n",
        "    \"pnl\": pnl,\n",
        "    \"tc\": tc_costs,\n",
        "    \"eq\": eq.values,\n",
        "    \"position\": positions\n",
        "}, index=common_index).to_csv(eq_csv)\n",
        "\n",
        "if not trades_df.empty:\n",
        "    trades_df.to_csv(trades_csv, index=False)\n",
        "else:\n",
        "    # create empty\n",
        "    pd.DataFrame(columns=[\"entry_time\",\"exit_time\",\"entry_idx\",\"exit_idx\",\"entry_pos\",\"exit_pos\",\"pnl\",\"tc\",\"net\",\"len\",\"win\"]).to_csv(trades_csv, index=False)\n",
        "\n",
        "with open(metrics_json, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# plots\n",
        "fig, axs = plt.subplots(3,1, figsize=(12, 10), dpi=PLOT_DPI, constrained_layout=True)\n",
        "axs[0].plot(eq.index, eq.values)\n",
        "axs[0].set_title(\"Equity Curve (net) — Rank-transform lev1x\")\n",
        "axs[0].set_ylabel(\"Equity (growth)\")\n",
        "\n",
        "axs[1].plot(drawdown.index, drawdown.values)\n",
        "axs[1].set_title(\"Drawdown\")\n",
        "axs[1].set_ylabel(\"Drawdown\")\n",
        "\n",
        "if trade_count > 0:\n",
        "    axs[2].hist(trades_df[\"net\"].values, bins=50)\n",
        "    axs[2].set_title(\"Trade net P&L histogram\")\n",
        "else:\n",
        "    axs[2].text(0.1, 0.5, \"No trades found\", transform=axs[2].transAxes)\n",
        "    axs[2].set_title(\"Trade net P&L histogram\")\n",
        "\n",
        "plt.savefig(plot_png)\n",
        "plt.close(fig)\n",
        "\n",
        "# summary prints\n",
        "print(\"[DONE] signal-only backtest (B: rank-transform lev1x) finished.\")\n",
        "print(\" - equity csv:\", str(eq_csv))\n",
        "print(\" - trades csv:\", str(trades_csv))\n",
        "print(\" - metrics json:\", str(metrics_json))\n",
        "print(\" - plots:\", str(plot_png))\n",
        "print(\"Summary metrics:\")\n",
        "for k in [\"n_periods\",\"cum_return\",\"ann_return\",\"ann_vol\",\"ann_sharpe\",\"sharpe_ci_95\",\"max_drawdown\",\"trade_count\",\"win_rate\",\"expectancy\",\"turnover\"]:\n",
        "    print(f\"  {k}: {metrics.get(k)}\")\n",
        "\n",
        "# quick sanity checks & warnings\n",
        "if metrics[\"n_periods\"] != len(sig_df):\n",
        "    print(\"[WARN] n_periods != signal length after alignment; check timestamps/index timezone mismatches.\")\n",
        "if abs(metrics[\"ann_sharpe\"]) > 10:\n",
        "    print(\"[WARN] unusually large Sharpe; verify returns scaling and that signal wasn't computed using future info (data leakage).\")\n",
        "if np.isnan(metrics[\"win_rate\"]):\n",
        "    print(\"[WARN] no trades reconstructed - check signal thresholding or MIN_POS_THRESH.\")\n",
        "\n",
        "# assert outputs exist\n",
        "assert eq_csv.exists() and trades_csv.exists() and metrics_json.exists(), \"Backtest outputs missing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpya3vqNqHm8",
        "outputId": "70b4cddb-fa1b-40b3-e853-4eaccbe0a9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] using signal file: A_SRC_signal_opt_signals_1764904868.csv\n",
            "[INFO] using meta pickle: df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\n",
            "[INFO] using signal column sample: signal\n",
            "[INFO] using returns column: tb_ret_at_break_h4\n",
            "[SAVED] positions -> A_SRC_positions_volscaled_1764907071.csv\n",
            "[INFO] positions shape: (17521, 6) sample head:\n",
            "                            p_long  p_short       raw  position       vol  \\\n",
            "2023-12-06 00:00:00+00:00     0.5      0.5  0.000000       0.0  0.000000   \n",
            "2023-12-06 01:00:00+00:00     1.0      0.0  0.718623       1.0  0.140324   \n",
            "2023-12-06 02:00:00+00:00     1.0      0.0  0.718623       1.0  0.106943   \n",
            "\n",
            "                               tc  \n",
            "2023-12-06 00:00:00+00:00  0.0005  \n",
            "2023-12-06 01:00:00+00:00  0.0005  \n",
            "2023-12-06 02:00:00+00:00  0.0005  \n",
            "[DONE] positions->backtest finished.\n",
            " - positions csv: A_SRC_positions_volscaled_1764907071.csv\n",
            " - equity csv: backtest_positions_equity_1764907071.csv\n",
            " - trades csv: backtest_positions_trades_1764907071.csv\n",
            " - metrics json: backtest_positions_metrics_1764907071.json\n",
            " - plots: backtest_positions_plots_1764907071.png\n",
            "Summary metrics:\n",
            "  n_periods: 17521\n",
            "  cum_return: 1.4359300435942872\n",
            "  ann_return: 0.35978796645996103\n",
            "  ann_vol: 0.2052906013812934\n",
            "  ann_sharpe: 1.5998184201032966\n",
            "  sharpe_ci_95: [-0.27070529569652174, 3.4240471911246053]\n",
            "  max_drawdown: -0.39168713196204274\n",
            "  trade_count: 333\n",
            "  win_rate: 0.5615615615615616\n",
            "  expectancy: 0.003082629192844961\n",
            "  turnover: 0.07173034164051563\n",
            "[INFO] signal file appears not to contain tb_ret / tb_t_break columns.\n"
          ]
        }
      ],
      "source": [
        "# ONE-CELL: signal -> A_SRC positions (vol-scaled) + quick vectorized backtest\n",
        "# Requirements: pandas, numpy, matplotlib (for optional plot saving)\n",
        "# Outputs saved under OUT_DIR: A_SRC_positions_volscaled_*.csv, backtest_* files\n",
        "\n",
        "import os, time, json, math, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ------------- CONFIG -------------\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# transform options: \"rank\", \"pct_scale_clip\", \"threshold\"\n",
        "TRANSFORM = \"rank\"          # default chosen from your diagnostics (B: rank-transform)\n",
        "THRESHOLD = None            # used if TRANSFORM == \"threshold\" (e.g. median or 0.0)\n",
        "CLIP_PCT = 0.99             # used by pct_scale_clip\n",
        "TARGET_ANN_VOL = 0.35       # target annual vol for scaled position (e.g. 35%).\n",
        "ANNUALIZE = 252 * 24        # hourly-based (your pipeline uses hourly)\n",
        "TC_RATE = 0.0005            # default tc used in earlier runs (adjust as needed)\n",
        "MIN_POS = 1e-6              # treat below as zero for position\n",
        "BOOTSTRAP_ITERS = 1000\n",
        "BOOTSTRAP_BLOCK = 24        # block length in hours for bootstrap\n",
        "\n",
        "# ------------- helpers -------------\n",
        "def load_latest(path_glob):\n",
        "    files = sorted(OUT_DIR.glob(path_glob), key=os.path.getmtime)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No files match {path_glob} in {OUT_DIR}\")\n",
        "    return files[-1]\n",
        "\n",
        "def block_bootstrap_sharpe(net_series: np.ndarray, iters=1000, block=24):\n",
        "    n = len(net_series)\n",
        "    if n <= 1:\n",
        "        return (np.nan, np.nan)\n",
        "    starts = np.arange(0, max(1, n - block + 1))\n",
        "    boot_sharpes = []\n",
        "    for _ in range(iters):\n",
        "        if len(starts) <= 0:\n",
        "            s = np.random.choice(net_series, size=n, replace=True)\n",
        "            arr = s\n",
        "        else:\n",
        "            res = []\n",
        "            while len(np.concatenate(res) if res else []) < n:\n",
        "                st = np.random.choice(starts)\n",
        "                blk = net_series[st:st+block]\n",
        "                res.append(blk)\n",
        "            arr = np.concatenate(res)[:n]\n",
        "        std = np.nanstd(arr)\n",
        "        if std == 0 or np.isnan(std):\n",
        "            boot_sharpes.append(0.0)\n",
        "        else:\n",
        "            boot_sharpes.append((np.nanmean(arr)/std) * np.sqrt(ANNUALIZE))\n",
        "    lo = float(np.percentile(boot_sharpes, 2.5))\n",
        "    hi = float(np.percentile(boot_sharpes, 97.5))\n",
        "    return lo, hi\n",
        "\n",
        "# ------------- load latest signal & meta -------------\n",
        "sig_path = load_latest(\"A_SRC_signal_opt_signals_*.csv\")\n",
        "meta_pkl = load_latest(\"df_meta_shortlist.v*.pkl\")\n",
        "print(\"[INFO] using signal file:\", sig_path.name)\n",
        "print(\"[INFO] using meta pickle:\", meta_pkl.name)\n",
        "\n",
        "sig = pd.read_csv(sig_path, index_col=0)\n",
        "try:\n",
        "    sig.index = pd.to_datetime(sig.index, utc=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "meta = pd.read_pickle(meta_pkl)\n",
        "try:\n",
        "    meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Align indices (intersection)\n",
        "idx_int = sig.index.intersection(meta.index)\n",
        "if len(idx_int) == 0:\n",
        "    raise RuntimeError(\"No index intersection between signal and meta!\")\n",
        "if len(idx_int) != len(sig):\n",
        "    print(f\"[WARN] intersection reduced: signal {len(sig)} -> {len(idx_int)} (use intersection)\")\n",
        "sig = sig.reindex(idx_int)\n",
        "meta = meta.reindex(idx_int)\n",
        "\n",
        "# choose signal column (prefer 'signal' or fallbacks)\n",
        "sig_cols = [c for c in sig.columns if c in (\"signal\",\"p_long\",\"p_short\")]\n",
        "if \"signal\" in sig.columns:\n",
        "    s = sig[\"signal\"].astype(float).fillna(0.0)\n",
        "elif \"p_long\" in sig.columns and \"p_short\" in sig.columns:\n",
        "    # convert to single signed signal: long - short\n",
        "    s = (sig[\"p_long\"].astype(float).fillna(0.5) - sig[\"p_short\"].astype(float).fillna(0.5))\n",
        "else:\n",
        "    # fallback to first numeric column\n",
        "    s = sig.select_dtypes(include=[np.number]).iloc[:,0].astype(float).fillna(0.0)\n",
        "print(f\"[INFO] using signal column sample: {s.name if hasattr(s,'name') else 'signal'}\")\n",
        "\n",
        "# ------------- transform to raw position [-1,1] -------------\n",
        "if TRANSFORM == \"rank\":\n",
        "    # rank from -1..1 (preserve sign direction of signal)\n",
        "    # we use signed rank: rank on absolute then apply original sign to densify tails\n",
        "    sign = np.sign(s)\n",
        "    abs_rank = s.abs().rank(method=\"average\", pct=True).astype(float)  # 0..1\n",
        "    raw_pos = (abs_rank * sign).fillna(0.0)\n",
        "    # optional small rescale to map median magnitude -> ~0.5; keep as is for lev1\n",
        "elif TRANSFORM == \"pct_scale_clip\":\n",
        "    # scale by percentile of abs and clip to [-1,1]\n",
        "    p = s.rank(pct=True)  # 0..1\n",
        "    raw_pos = (2*p - 1).fillna(0.0)  # map to [-1,1]\n",
        "    raw_pos = raw_pos.clip(-CLIP_PCT, CLIP_PCT)\n",
        "elif TRANSFORM == \"threshold\":\n",
        "    thr = THRESHOLD if THRESHOLD is not None else s.median()\n",
        "    raw_pos = s.copy()\n",
        "    raw_pos[s > thr] = 1.0\n",
        "    raw_pos[s < -thr] = -1.0\n",
        "    raw_pos[(s <= thr) & (s >= -thr)] = 0.0\n",
        "else:\n",
        "    raise ValueError(\"Unknown TRANSFORM\")\n",
        "\n",
        "raw_pos = raw_pos.fillna(0.0).astype(float)\n",
        "\n",
        "# ensure lev1x semantics: cap absolute to 1.0\n",
        "raw_pos = raw_pos.clip(-1.0, 1.0)\n",
        "\n",
        "# ------------- volatility scaling -------------\n",
        "# find returns column in meta (prefer tb_ret_at_break_h4 or tb_ret_8 or tb_ret_*)\n",
        "ret_col_candidates = [c for c in meta.columns if c.startswith(\"tb_ret\")]\n",
        "if not ret_col_candidates:\n",
        "    raise RuntimeError(\"No tb_ret* found in meta pickle.\")\n",
        "ret_col = ret_col_candidates[0]\n",
        "rets = meta[ret_col].astype(float).fillna(0.0).reindex(raw_pos.index)\n",
        "print(f\"[INFO] using returns column: {ret_col}\")\n",
        "\n",
        "# compute realized hourly vol (rolling std)\n",
        "roll_window_hours = int(round(24 * 7))  # 1-week by default (you may adjust)\n",
        "realized_vol = rets.rolling(roll_window_hours, min_periods=1).std().fillna(method=\"ffill\").fillna(0.0)\n",
        "\n",
        "# convert to annualized vol\n",
        "realized_ann_vol = realized_vol * np.sqrt(ANNUALIZE)\n",
        "\n",
        "# scaling factor so position * realized_ann_vol -> TARGET_ANN_VOL (i.e. position = raw_pos * scale)\n",
        "# avoid division by zero\n",
        "scale = (TARGET_ANN_VOL / realized_ann_vol).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "# clip extreme scaling\n",
        "scale = scale.clip(-10.0, 10.0)\n",
        "\n",
        "pos_series = raw_pos * scale\n",
        "# normalize to lev1x: ensure abs(position) <= 1.0\n",
        "pos_series = pos_series.clip(-1.0, 1.0)\n",
        "\n",
        "# optional: zero tiny positions\n",
        "pos_series[np.abs(pos_series) < MIN_POS] = 0.0\n",
        "\n",
        "# add other useful cols\n",
        "positions_df = pd.DataFrame({\n",
        "    \"p_long\": ((pos_series + 1.0)/2.0).clip(0.0,1.0),   # interpret as probability-like for compatibility\n",
        "    \"p_short\": ((1.0 - pos_series)/2.0).clip(0.0,1.0),\n",
        "    \"raw\": raw_pos,\n",
        "    \"position\": pos_series,\n",
        "    \"vol\": realized_ann_vol,\n",
        "    \"tc\": TC_RATE\n",
        "}, index=pos_series.index)\n",
        "\n",
        "# ------------- save positions (A_SRC positions volscaled) -------------\n",
        "ts = int(time.time())\n",
        "out_positions = OUT_DIR / f\"A_SRC_positions_volscaled_{ts}.csv\"\n",
        "positions_df.to_csv(out_positions)\n",
        "print(\"[SAVED] positions ->\", out_positions.name)\n",
        "print(\"[INFO] positions shape:\", positions_df.shape, \"sample head:\\n\", positions_df.head(3))\n",
        "\n",
        "# ------------- quick vectorized backtest using these positions -------------\n",
        "# net = position * ret - tc * abs(dpos)\n",
        "positions = positions_df[\"position\"].astype(float).values\n",
        "rets_arr = rets.values\n",
        "N = len(positions)\n",
        "dpos = np.empty_like(positions)\n",
        "dpos[0] = positions[0]\n",
        "if N > 1:\n",
        "    dpos[1:] = positions[1:] - positions[:-1]\n",
        "tc_costs = TC_RATE * np.abs(dpos)\n",
        "pnl = positions * rets_arr\n",
        "net = pnl - tc_costs\n",
        "\n",
        "# equity / metrics\n",
        "eq = pd.Series((1.0 + net).cumprod(), index=positions_df.index)\n",
        "n_periods = len(net)\n",
        "cum_return = float(eq.iloc[-1] - 1.0) if n_periods > 0 else 0.0\n",
        "ann_return = float(eq.iloc[-1] ** (ANNUALIZE / max(1, n_periods)) - 1.0) if n_periods > 0 else 0.0\n",
        "ann_vol = float(np.nanstd(net) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "ann_sharpe = float((np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "\n",
        "# drawdown\n",
        "cum_max = eq.cummax()\n",
        "drawdown = (eq - cum_max) / cum_max\n",
        "max_dd = float(drawdown.min()) if len(drawdown) > 0 else 0.0\n",
        "\n",
        "# trades reconstruction (simple: contiguous non-zero runs)\n",
        "trades = []\n",
        "i = 0\n",
        "while i < N:\n",
        "    if abs(positions[i]) <= MIN_POS:\n",
        "        i += 1\n",
        "        continue\n",
        "    sidx = i\n",
        "    sgn = np.sign(positions[sidx])\n",
        "    j = sidx + 1\n",
        "    while j < N and (abs(positions[j]) > MIN_POS) and (np.sign(positions[j]) == sgn):\n",
        "        j += 1\n",
        "    entry_idx = sidx\n",
        "    exit_idx = j-1\n",
        "    pos_slice = positions[entry_idx:j]\n",
        "    rets_slice = rets_arr[entry_idx:j]\n",
        "    pnl_t = float((pos_slice * rets_slice).sum())\n",
        "    # tc for that run\n",
        "    dpos_slice = np.empty(len(pos_slice))\n",
        "    dpos_slice[0] = pos_slice[0]\n",
        "    if len(pos_slice) > 1:\n",
        "        dpos_slice[1:] = pos_slice[1:] - pos_slice[:-1]\n",
        "    tc_t = float(TC_RATE * np.abs(dpos_slice).sum())\n",
        "    net_t = pnl_t - tc_t\n",
        "    trades.append({\n",
        "        \"entry_time\": str(positions_df.index[entry_idx]),\n",
        "        \"exit_time\": str(positions_df.index[exit_idx]),\n",
        "        \"entry_idx\": int(entry_idx),\n",
        "        \"exit_idx\": int(exit_idx),\n",
        "        \"entry_pos\": float(pos_slice[0]),\n",
        "        \"exit_pos\": float(pos_slice[-1]),\n",
        "        \"pnl\": pnl_t,\n",
        "        \"tc\": tc_t,\n",
        "        \"net\": net_t,\n",
        "        \"len\": int(exit_idx - entry_idx + 1),\n",
        "        \"win\": int(net_t > 0)\n",
        "    })\n",
        "    i = j\n",
        "\n",
        "trades_df = pd.DataFrame(trades)\n",
        "trade_count = len(trades_df)\n",
        "win_count = int(trades_df[\"win\"].sum()) if trade_count>0 else 0\n",
        "win_rate = float(win_count / trade_count) if trade_count>0 else float(\"nan\")\n",
        "expectancy = float(trades_df[\"net\"].mean()) if trade_count>0 else 0.0\n",
        "median_trade_net = float(trades_df[\"net\"].median()) if trade_count>0 else float(\"nan\")\n",
        "avg_abs_pos = float(np.nanmean(np.abs(positions))) if np.nanmean(np.abs(positions))>0 else 1.0\n",
        "turnover = float(np.nanmean(np.abs(dpos)) / avg_abs_pos)\n",
        "\n",
        "# bootstrap sharpe ci\n",
        "sh_lo, sh_hi = block_bootstrap_sharpe(net, iters=BOOTSTRAP_ITERS, block=BOOTSTRAP_BLOCK)\n",
        "\n",
        "# save outputs\n",
        "eq_csv = OUT_DIR / f\"backtest_positions_equity_{ts}.csv\"\n",
        "trades_csv = OUT_DIR / f\"backtest_positions_trades_{ts}.csv\"\n",
        "metrics_json = OUT_DIR / f\"backtest_positions_metrics_{ts}.json\"\n",
        "pd.DataFrame({\"net\": net, \"pnl\": pnl, \"tc\": tc_costs, \"eq\": eq}).to_csv(eq_csv)\n",
        "trades_df.to_csv(trades_csv, index=False)\n",
        "metrics = {\n",
        "    \"positions_file\": str(out_positions),\n",
        "    \"signal_file\": str(sig_path),\n",
        "    \"meta_file\": str(meta_pkl),\n",
        "    \"n_periods\": int(n_periods),\n",
        "    \"cum_return\": cum_return,\n",
        "    \"ann_return\": ann_return,\n",
        "    \"ann_vol\": ann_vol,\n",
        "    \"ann_sharpe\": ann_sharpe,\n",
        "    \"sharpe_ci_95\": [sh_lo, sh_hi],\n",
        "    \"max_drawdown\": max_dd,\n",
        "    \"trade_count\": trade_count,\n",
        "    \"win_count\": win_count,\n",
        "    \"win_rate\": win_rate,\n",
        "    \"expectancy\": expectancy,\n",
        "    \"median_trade_net\": median_trade_net,\n",
        "    \"turnover\": turnover,\n",
        "    \"tc_rate\": TC_RATE,\n",
        "    \"transform\": TRANSFORM,\n",
        "    \"target_ann_vol\": TARGET_ANN_VOL\n",
        "}\n",
        "with open(metrics_json, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# save plot (equity + drawdown)\n",
        "fig, axs = plt.subplots(2,1, figsize=(10,6), constrained_layout=True)\n",
        "axs[0].plot(eq.index, eq.values)\n",
        "axs[0].set_title(\"Equity (positions vol-scaled)\")\n",
        "axs[0].set_ylabel(\"Equity growth\")\n",
        "axs[1].plot(drawdown.index, drawdown.values)\n",
        "axs[1].set_title(\"Drawdown\")\n",
        "plot_png = OUT_DIR / f\"backtest_positions_plots_{ts}.png\"\n",
        "plt.savefig(plot_png, dpi=120)\n",
        "plt.close(fig)\n",
        "\n",
        "# prints\n",
        "print(\"[DONE] positions->backtest finished.\")\n",
        "print(\" - positions csv:\", out_positions.name)\n",
        "print(\" - equity csv:\", eq_csv.name)\n",
        "print(\" - trades csv:\", trades_csv.name)\n",
        "print(\" - metrics json:\", metrics_json.name)\n",
        "print(\" - plots:\", plot_png.name)\n",
        "print(\"Summary metrics:\")\n",
        "keys = [\"n_periods\",\"cum_return\",\"ann_return\",\"ann_vol\",\"ann_sharpe\",\"sharpe_ci_95\",\"max_drawdown\",\"trade_count\",\"win_rate\",\"expectancy\",\"turnover\"]\n",
        "for k in keys:\n",
        "    print(f\"  {k}: {metrics.get(k)}\")\n",
        "\n",
        "# quick leakage reminder\n",
        "if \"tb_ret\" in sig.columns or \"tb_t_break\" in sig.columns:\n",
        "    print(\"[WARN] signal file contains tb_ret / tb_t_break columns — verify that Optuna/signals were produced without peeking future returns.\")\n",
        "else:\n",
        "    print(\"[INFO] signal file appears not to contain tb_ret / tb_t_break columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-T9ql2rruWr",
        "outputId": "b7ae7457-b412-4195-aef3-346d0cb68a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Running backtest_fast with tc_rate_default = 0.0005\n",
            "[DONE] backtest_fast tc=0.0005 -> eq: backtest_fast_equity_1764907486_tc500.csv, trades: backtest_fast_trades_1764907486_tc500.csv, metrics: backtest_fast_metrics_1764907486_tc500.json\n",
            "  n_periods: 17521\n",
            "  cum_return: 1.4359300435942872\n",
            "  ann_return: 0.35978796645996103\n",
            "  ann_vol: 0.20529060138129337\n",
            "  ann_sharpe: 1.5998184201032972\n",
            "  sharpe_ci_95: [-0.18507861406140486, 3.560973699522607]\n",
            "  max_drawdown: -0.39168713196204274\n",
            "  trade_count: 332\n",
            "  win_rate: 0.5602409638554217\n",
            "  expectancy: 0.0030911427246680563\n",
            "  turnover: 0.07173034164051563\n",
            "[ALL DONE]\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ONE-CELL — Fast backtest (RLE trade reconstruction) + optional TC sweep\n",
        "import os, time, json, math, warnings\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# ---------- CONFIG ----------\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "MIN_POS_THRESH = 1e-6\n",
        "ANNUALIZE = 252 * 24   # hourly data\n",
        "BOOTSTRAP_ITERS = 1000\n",
        "BOOTSTRAP_BLOCK = 24\n",
        "PLOT_DPI = 120\n",
        "# if you want a TC sweep, set list like [0.0, 5e-4, 1e-3]\n",
        "TC_SWEEP: List[float] = []  # keep empty to skip sweep\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def load_latest_file(glob_pattern: str) -> str:\n",
        "    files = sorted(Path(OUT_DIR).glob(glob_pattern), key=os.path.getmtime)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No files match {glob_pattern} in {OUT_DIR}\")\n",
        "    return str(files[-1])\n",
        "\n",
        "def block_bootstrap_sharpe(net_series: np.ndarray, iters:int=1000, block:int=24) -> Tuple[float,float]:\n",
        "    n = len(net_series)\n",
        "    if n <= 1:\n",
        "        return (float(\"nan\"), float(\"nan\"))\n",
        "    boot_sharpes = []\n",
        "    starts = np.arange(0, max(1, n - block + 1))\n",
        "    if len(starts) <= 0:\n",
        "        # iid bootstrap fallback\n",
        "        for _ in range(iters):\n",
        "            s = np.random.choice(net_series, size=n, replace=True)\n",
        "            if np.nanstd(s) == 0:\n",
        "                boot_sharpes.append(0.0)\n",
        "            else:\n",
        "                boot_sharpes.append((np.nanmean(s)/np.nanstd(s)) * np.sqrt(ANNUALIZE))\n",
        "    else:\n",
        "        for _ in range(iters):\n",
        "            res = []\n",
        "            while len(res) < n:\n",
        "                st = np.random.choice(starts)\n",
        "                blk = net_series[st:st+block]\n",
        "                res.append(blk)\n",
        "            res_arr = np.concatenate(res)[:n]\n",
        "            if np.nanstd(res_arr) == 0:\n",
        "                boot_sharpes.append(0.0)\n",
        "            else:\n",
        "                boot_sharpes.append((np.nanmean(res_arr)/np.nanstd(res_arr)) * np.sqrt(ANNUALIZE))\n",
        "    lo = np.percentile(boot_sharpes, 2.5)\n",
        "    hi = np.percentile(boot_sharpes, 97.5)\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def fast_rle_trades(positions: np.ndarray, index: pd.Index, rets: np.ndarray, tc_rate: float):\n",
        "    \"\"\"Return trades list (dicts) using run-length encoding on non-zero-sign positions.\"\"\"\n",
        "    mask = np.abs(positions) > MIN_POS_THRESH\n",
        "    if not mask.any():\n",
        "        return []\n",
        "    # Find boundaries of True runs\n",
        "    m = mask.astype(np.int8)\n",
        "    # pad to detect edges\n",
        "    diff = np.diff(np.concatenate(([0], m, [0])))\n",
        "    starts = np.where(diff == 1)[0]\n",
        "    ends = np.where(diff == -1)[0] - 1\n",
        "    trades = []\n",
        "    for s,e in zip(starts, ends):\n",
        "        slice_pos = positions[s:e+1]\n",
        "        slice_rets = rets[s:e+1]\n",
        "        pnl = float((slice_pos * slice_rets).sum())\n",
        "        # dpos per trade (first change counted as entry)\n",
        "        d = np.empty(len(slice_pos), dtype=float)\n",
        "        d[0] = slice_pos[0]\n",
        "        if len(slice_pos) > 1:\n",
        "            d[1:] = slice_pos[1:] - slice_pos[:-1]\n",
        "        tc = float(tc_rate * np.abs(d).sum())\n",
        "        net = pnl - tc\n",
        "        trades.append({\n",
        "            \"entry_time\": str(index[s]),\n",
        "            \"exit_time\": str(index[e]),\n",
        "            \"entry_idx\": int(s),\n",
        "            \"exit_idx\": int(e),\n",
        "            \"entry_pos\": float(slice_pos[0]),\n",
        "            \"exit_pos\": float(slice_pos[-1]),\n",
        "            \"pnl\": pnl,\n",
        "            \"tc\": tc,\n",
        "            \"net\": net,\n",
        "            \"len\": int(e - s + 1),\n",
        "            \"win\": int(net > 0)\n",
        "        })\n",
        "    return trades\n",
        "\n",
        "def compute_metrics_from_series(net: np.ndarray, pnl: np.ndarray, positions: np.ndarray, trades_df: pd.DataFrame, tc_rate: float):\n",
        "    eq = pd.Series((1.0 + net).cumprod())\n",
        "    n_periods = len(net)\n",
        "    cum_return = float(eq.iloc[-1] - 1.0) if n_periods > 0 else 0.0\n",
        "    ann_return = float(eq.iloc[-1] ** (ANNUALIZE / max(1, n_periods)) - 1.0) if n_periods > 0 else 0.0\n",
        "    ann_vol = float(np.nanstd(net) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "    ann_sharpe = float((np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "    # downside / sortino\n",
        "    neg = net.copy()\n",
        "    neg[neg > 0] = 0.0\n",
        "    downside = float(np.sqrt(np.nanmean(neg**2)) * math.sqrt(ANNUALIZE)) if np.any(neg != 0) else 0.0\n",
        "    sortino = float((np.nanmean(net) * math.sqrt(ANNUALIZE) / downside)) if downside > 0 else float(\"nan\")\n",
        "    # drawdown\n",
        "    cum_max = eq.cummax()\n",
        "    drawdown = (eq - cum_max) / cum_max\n",
        "    max_dd = float(drawdown.min()) if len(drawdown)>0 else 0.0\n",
        "    calmar = float(ann_return / abs(max_dd)) if max_dd < 0 else None\n",
        "    avg_abs_pos = float(np.nanmean(np.abs(positions))) if np.nanmean(np.abs(positions)) > 0 else 1.0\n",
        "    dpos = np.empty_like(positions); dpos[0] = positions[0]; dpos[1:] = positions[1:] - positions[:-1]\n",
        "    turnover = float(np.nanmean(np.abs(dpos)) / avg_abs_pos)\n",
        "    # trades\n",
        "    trade_count = int(len(trades_df)) if trades_df is not None else 0\n",
        "    win_count = int(trades_df[\"win\"].sum()) if trade_count>0 else 0\n",
        "    win_rate = float(win_count / trade_count) if trade_count>0 else float(\"nan\")\n",
        "    expectancy = float(trades_df[\"net\"].mean()) if trade_count>0 else 0.0\n",
        "    median_trade_net = float(trades_df[\"net\"].median()) if trade_count>0 else float(\"nan\")\n",
        "    # bootstrap CI\n",
        "    sh_lo, sh_hi = block_bootstrap_sharpe(net, iters=BOOTSTRAP_ITERS, block=BOOTSTRAP_BLOCK)\n",
        "    metrics = dict(\n",
        "        n_periods=int(n_periods),\n",
        "        cum_return=cum_return,\n",
        "        ann_return=ann_return,\n",
        "        ann_vol=ann_vol,\n",
        "        ann_sharpe=ann_sharpe,\n",
        "        sharpe_ci_95=[sh_lo, sh_hi],\n",
        "        sortino=sortino,\n",
        "        max_drawdown=max_dd,\n",
        "        calmar=calmar,\n",
        "        trade_count=trade_count,\n",
        "        win_count=win_count,\n",
        "        win_rate=win_rate,\n",
        "        avg_holding=float(trades_df[\"len\"].mean()) if trade_count>0 else float(\"nan\"),\n",
        "        expectancy=expectancy,\n",
        "        median_trade_net=median_trade_net,\n",
        "        turnover=turnover,\n",
        "        tc_rate=tc_rate\n",
        "    )\n",
        "    return eq, drawdown, metrics\n",
        "\n",
        "# ---------- main ----------\n",
        "ts = int(time.time())\n",
        "\n",
        "# load latest positions (A_SRC positions expected)\n",
        "pos_path = load_latest_file(\"A_SRC_positions_volscaled*.csv\")\n",
        "pos_df = pd.read_csv(pos_path, index_col=0)\n",
        "try:\n",
        "    pos_df.index = pd.to_datetime(pos_df.index, utc=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if \"position\" not in pos_df.columns:\n",
        "    raise RuntimeError(\"positions CSV must include 'position' column\")\n",
        "\n",
        "# load meta & returns (try preferred col, fallback mapping)\n",
        "meta_pkl = load_latest_file(\"df_meta_shortlist.v*.pkl\")\n",
        "df_meta = pd.read_pickle(meta_pkl)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "\n",
        "# choose return col heuristically (prioritize tb_ret_at_break_h4 then tb_ret_8 then tb_ret)\n",
        "for prefer in [\"tb_ret_at_break_h4\", \"tb_ret_8\", \"tb_ret\"]:\n",
        "    if prefer in df_meta.columns:\n",
        "        ret_col = prefer\n",
        "        break\n",
        "else:\n",
        "    raise RuntimeError(\"No suitable tb_ret_* column found in meta pickle\")\n",
        "\n",
        "# align\n",
        "df_meta = df_meta.reindex(pos_df.index)\n",
        "rets = df_meta[ret_col].fillna(0.0).astype(float).reindex(pos_df.index).fillna(0.0)\n",
        "rets_arr = rets.values\n",
        "positions = pos_df[\"position\"].astype(float).reindex(pos_df.index).fillna(0.0).values\n",
        "\n",
        "# derive tc_rate default (if present in pos_df first value) else 5e-4\n",
        "tc_rate_default = float(pos_df[\"tc\"].iloc[0]) if \"tc\" in pos_df.columns and not pos_df[\"tc\"].isnull().all() else 5e-4\n",
        "\n",
        "# single-run function\n",
        "def run_backtest_for_tc(tc_rate: float):\n",
        "    # vectorized pnl/net\n",
        "    dpos = np.empty_like(positions); dpos[0] = positions[0]; dpos[1:] = positions[1:] - positions[:-1]\n",
        "    tc_costs = tc_rate * np.abs(dpos)\n",
        "    pnl = positions * rets_arr\n",
        "    net = pnl - tc_costs\n",
        "    # fast trades\n",
        "    trades = fast_rle_trades(positions, pos_df.index, rets_arr, tc_rate)\n",
        "    trades_df = pd.DataFrame(trades) if trades else pd.DataFrame(columns=[\"net\",\"pnl\",\"tc\",\"win\",\"len\"])\n",
        "    # compute metrics and save outputs\n",
        "    eq, drawdown, metrics = compute_metrics_from_series(net, pnl, positions, trades_df, tc_rate)\n",
        "    # file names\n",
        "    out_base = Path(OUT_DIR)\n",
        "    eq_csv = out_base / f\"backtest_fast_equity_{ts}_tc{int(tc_rate*1e6)}.csv\"\n",
        "    trades_csv = out_base / f\"backtest_fast_trades_{ts}_tc{int(tc_rate*1e6)}.csv\"\n",
        "    metrics_json = out_base / f\"backtest_fast_metrics_{ts}_tc{int(tc_rate*1e6)}.json\"\n",
        "    plot_png = out_base / f\"backtest_fast_plots_{ts}_tc{int(tc_rate*1e6)}.png\"\n",
        "    pd.DataFrame({\"net\": net, \"pnl\": pnl, \"tc\": tc_costs, \"eq\": eq.values}, index=pos_df.index).to_csv(eq_csv)\n",
        "    trades_df.to_csv(trades_csv, index=False)\n",
        "    with open(metrics_json, \"w\") as f:\n",
        "        json.dump({\"positions_file\": pos_path, \"meta_file\": meta_pkl, **metrics}, f, indent=2)\n",
        "    # plots\n",
        "    fig, axs = plt.subplots(3,1, figsize=(10,12), dpi=PLOT_DPI, constrained_layout=True)\n",
        "    axs[0].plot(pos_df.index, eq.values)\n",
        "    axs[0].set_title(f\"Equity Curve (tc={tc_rate})\")\n",
        "    axs[0].set_ylabel(\"Equity (growth)\")\n",
        "    axs[1].plot(pos_df.index, drawdown.values)\n",
        "    axs[1].set_title(\"Drawdown\")\n",
        "    if len(trades_df)>0:\n",
        "        axs[2].hist(trades_df[\"net\"].values, bins=50)\n",
        "        axs[2].set_title(\"Trade net P&L histogram\")\n",
        "    else:\n",
        "        axs[2].text(0.1,0.5,\"No trades found\", transform=axs[2].transAxes)\n",
        "        axs[2].set_title(\"Trade net P&L histogram\")\n",
        "    plt.savefig(plot_png); plt.close(fig)\n",
        "    # summary print (concise)\n",
        "    print(f\"[DONE] backtest_fast tc={tc_rate} -> eq: {eq_csv.name}, trades: {trades_csv.name}, metrics: {metrics_json.name}\")\n",
        "    for k in [\"n_periods\",\"cum_return\",\"ann_return\",\"ann_vol\",\"ann_sharpe\",\"sharpe_ci_95\",\"max_drawdown\",\"trade_count\",\"win_rate\",\"expectancy\",\"turnover\"]:\n",
        "        print(f\"  {k}: {metrics.get(k)}\")\n",
        "    return metrics, str(eq_csv), str(trades_csv), str(metrics_json), str(plot_png)\n",
        "\n",
        "# run default\n",
        "print(\"[INFO] Running backtest_fast with tc_rate_default =\", tc_rate_default)\n",
        "metrics_default, eq_csv, trades_csv, metrics_json, plot_png = run_backtest_for_tc(tc_rate_default)\n",
        "\n",
        "# optional sweep\n",
        "sweep_results = {}\n",
        "if TC_SWEEP:\n",
        "    print(\"[INFO] Running TC sweep for:\", TC_SWEEP)\n",
        "    for tc in TC_SWEEP:\n",
        "        m, _, _, _, _ = run_backtest_for_tc(tc)\n",
        "        sweep_results[tc] = m\n",
        "    # persist sweep\n",
        "    with open(Path(OUT_DIR)/f\"backtest_fast_tc_sweep_{ts}.json\", \"w\") as f:\n",
        "        json.dump({str(k):v for k,v in sweep_results.items()}, f, indent=2)\n",
        "\n",
        "print(\"[ALL DONE]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbGYaEwj-1pj",
        "outputId": "9601311e-7d83-4882-dd47-a06486b55047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using params: {'k': 1.922005246265828, 'L': 1.9097757869986336, 'WINDOW': 138}\n",
            "len common index: 17521\n",
            "candidate return columns: ['tb_ret_at_break_h8', 'tb_ret_at_break_h4', 'tb_ret_at_break_h12', 'tb_ret_8']\n",
            "built 20 vol candidates\n",
            "Saved vol-candidate summary -> /content/drive/MyDrive/quant_pipeline/mtb_out/vol_match_results_1764912520.csv\n",
            "                             name  max_abs  mean_abs  count_gt_eps\n",
            " tb_ret_at_break_h8.roll138.ddof0 1.700103  0.818387          9861\n",
            " tb_ret_at_break_h8.roll138.ddof1 1.700103  0.818387          9861\n",
            "  tb_ret_at_break_h8.roll69.ddof0 1.700103  0.818387          9861\n",
            "   tb_ret_at_break_h8.ewm_span138 1.700103  0.818387          9861\n",
            " tb_ret_at_break_h4.roll138.ddof0 1.700103  0.818387          9861\n",
            " tb_ret_at_break_h4.roll138.ddof1 1.700103  0.818387          9861\n",
            "  tb_ret_at_break_h4.roll69.ddof0 1.700103  0.818387          9861\n",
            "   tb_ret_at_break_h4.ewm_span138 1.700103  0.818387          9861\n",
            "tb_ret_at_break_h12.roll138.ddof0 1.700103  0.818387          9861\n",
            "tb_ret_at_break_h12.roll138.ddof1 1.700103  0.818387          9861\n",
            "\n",
            "BEST CANDIDATE: tb_ret_at_break_h8.roll138.ddof0 mean_abs: 0.8183870572764657 max_abs: 1.7001031640248188\n",
            "Saved best reconstructed positions -> /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_reconstructed_best_1764912520.csv\n",
            "Saved best diag (top mismatches) -> /content/drive/MyDrive/quant_pipeline/mtb_out/reconstruct_best_diag_1764912520.csv\n",
            "Done. Top candidates saved. Inspect the best diag to understand mismatch patterns.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# One-cell: automatic candidate volatility matcher and reconstructor\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ====== USER PATHS / PARAMS (edit if needed) ======\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "sig_path = \"/content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_signal_opt_signals_1764904868.csv\"\n",
        "pos_path = \"/content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_volscaled_1764907071.csv\"\n",
        "meta_pkl  = \"/content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\"\n",
        "meta_json = \"/content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_meta_volscaled_robust_1764903331.json\"\n",
        "# optimizer params you reported\n",
        "params = dict(k=1.922005246265828, L=1.9097757869986336, WINDOW=138)\n",
        "# safety\n",
        "EPS = 1e-12\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ====== load ======\n",
        "sig = pd.read_csv(sig_path, index_col=0, parse_dates=True)\n",
        "pos = pd.read_csv(pos_path, index_col=0, parse_dates=True)\n",
        "meta = pd.read_pickle(meta_pkl)\n",
        "meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "\n",
        "k = float(params.get(\"k\", 1.0))\n",
        "L = float(params.get(\"L\", 1.0))\n",
        "WINDOW = int(params.get(\"WINDOW\", 138))\n",
        "print(\"Using params:\", dict(k=k, L=L, WINDOW=WINDOW))\n",
        "\n",
        "# align indices\n",
        "common_index = pos.index.intersection(meta.index).intersection(sig.index)\n",
        "print(\"len common index:\", len(common_index))\n",
        "pos = pos.reindex(common_index)\n",
        "meta = meta.reindex(common_index)\n",
        "sig = sig.reindex(common_index)\n",
        "\n",
        "# derive raw signal (priority: pos.raw, sig.raw, fallback signal*spread, else first col)\n",
        "if \"raw\" in pos.columns:\n",
        "    raw = pos[\"raw\"].astype(float).reindex(common_index).fillna(0.0)\n",
        "elif \"raw\" in sig.columns:\n",
        "    raw = sig[\"raw\"].astype(float).reindex(common_index).fillna(0.0)\n",
        "elif (\"signal\" in sig.columns) and (\"spread\" in sig.columns):\n",
        "    raw = (sig[\"signal\"] * sig[\"spread\"]).astype(float).reindex(common_index).fillna(0.0)\n",
        "else:\n",
        "    raw = sig.iloc[:,0].astype(float).reindex(common_index).fillna(0.0)\n",
        "raw = raw.fillna(0.0)\n",
        "\n",
        "# original positions\n",
        "if \"position\" not in pos.columns:\n",
        "    raise KeyError(\"positions CSV must include 'position' column\")\n",
        "pos_orig = pos[\"position\"].astype(float).reindex(common_index).fillna(0.0)\n",
        "\n",
        "# candidate return columns to try (prefer break & fixed horizons)\n",
        "cand_ret_cols = []\n",
        "for c in [\"tb_ret_at_break_h8\", \"tb_ret_at_break_h4\", \"tb_ret_at_break_h12\", \"tb_ret_8\", \"ret_8\", \"ret_1\"]:\n",
        "    if c in meta.columns:\n",
        "        cand_ret_cols.append(c)\n",
        "cand_ret_cols = list(dict.fromkeys(cand_ret_cols))  # unique preserve order\n",
        "if not cand_ret_cols:\n",
        "    raise KeyError(\"No candidate return columns found in meta. Columns: %s\" % meta.columns.tolist()[:30])\n",
        "print(\"candidate return columns:\", cand_ret_cols)\n",
        "\n",
        "# volatility candidate constructors\n",
        "def rolling_std(series, window, ddof=0):\n",
        "    return series.rolling(window, min_periods=1).std(ddof=ddof).fillna(method=\"ffill\").fillna(0.0)\n",
        "\n",
        "def ewm_std(series, span):\n",
        "    # EWM variance estimator: var = E[x^2] - E[x]^2 (population style)\n",
        "    s1 = series.ewm(span=span, adjust=False).mean()\n",
        "    s2 = (series**2).ewm(span=span, adjust=False).mean()\n",
        "    var = (s2 - s1**2).clip(lower=0.0)\n",
        "    return np.sqrt(var).fillna(method=\"ffill\").fillna(0.0)\n",
        "\n",
        "# build candidates: tuple(name, vol_series)\n",
        "candidates = []\n",
        "for ret_col in cand_ret_cols:\n",
        "    s = meta[ret_col].reindex(common_index).astype(float)\n",
        "    # rolling ddof=0 and ddof=1\n",
        "    candidates.append((f\"{ret_col}.roll{WINDOW}.ddof0\", rolling_std(s, WINDOW, ddof=0)))\n",
        "    candidates.append((f\"{ret_col}.roll{WINDOW}.ddof1\", rolling_std(s, WINDOW, ddof=1)))\n",
        "    # smaller window variant (half)\n",
        "    w2 = max(3, WINDOW//2)\n",
        "    candidates.append((f\"{ret_col}.roll{w2}.ddof0\", rolling_std(s, w2, ddof=0)))\n",
        "    # EWM span ~ WINDOW\n",
        "    candidates.append((f\"{ret_col}.ewm_span{WINDOW}\", ewm_std(s, span=WINDOW)))\n",
        "\n",
        "# additionally try absolute-return MA as robust scale (MAD-ish)\n",
        "for ret_col in cand_ret_cols:\n",
        "    s = meta[ret_col].reindex(common_index).astype(float)\n",
        "    mad = s.rolling(WINDOW, min_periods=1).apply(lambda x: np.median(np.abs(x - np.median(x))), raw=True).fillna(method=\"ffill\").fillna(0.0)\n",
        "    candidates.append((f\"{ret_col}.rolling_mad{WINDOW}\", mad.clip(lower=EPS)))\n",
        "\n",
        "# dedupe candidate names\n",
        "seen = set(); final_candidates=[]\n",
        "for name, series in candidates:\n",
        "    if name in seen: continue\n",
        "    seen.add(name); final_candidates.append((name, series.clip(lower=EPS)))\n",
        "candidates = final_candidates\n",
        "\n",
        "print(\"built %d vol candidates\" % len(candidates))\n",
        "\n",
        "# function to reconstruct pos and evaluate\n",
        "def reconstruct_and_score(vol_ser):\n",
        "    vol = vol_ser.values\n",
        "    # avoid zero vol\n",
        "    vol = np.where(vol <= 0, EPS, vol)\n",
        "    pos_u = k * (raw.values / vol)\n",
        "    pos_rec = np.sign(pos_u) * np.minimum(np.abs(pos_u), L)\n",
        "    diff = pos_orig.values - pos_rec\n",
        "    absdiff = np.abs(diff)\n",
        "    # metrics\n",
        "    return {\n",
        "        \"pos_rec\": pos_rec,\n",
        "        \"diff\": diff,\n",
        "        \"absdiff\": absdiff,\n",
        "        \"max_abs\": float(absdiff.max()),\n",
        "        \"mean_abs\": float(absdiff.mean()),\n",
        "        \"count_gt_eps\": int((absdiff > 1e-6).sum())\n",
        "    }\n",
        "\n",
        "# iterate candidates, compute metrics\n",
        "results = []\n",
        "for name, vol_ser in candidates:\n",
        "    try:\n",
        "        out = reconstruct_and_score(vol_ser)\n",
        "    except Exception as e:\n",
        "        print(\"candidate fail:\", name, e)\n",
        "        continue\n",
        "    results.append((name, out))\n",
        "\n",
        "# convert to df for ranking\n",
        "rows = []\n",
        "for name, out in results:\n",
        "    rows.append({\n",
        "        \"name\": name,\n",
        "        \"max_abs\": out[\"max_abs\"],\n",
        "        \"mean_abs\": out[\"mean_abs\"],\n",
        "        \"count_gt_eps\": out[\"count_gt_eps\"]\n",
        "    })\n",
        "res_df = pd.DataFrame(rows).sort_values([\"mean_abs\",\"max_abs\"]).reset_index(drop=True)\n",
        "ts = int(time.time())\n",
        "res_csv = Path(OUT_DIR)/f\"vol_match_results_{ts}.csv\"\n",
        "res_df.to_csv(res_csv, index=False)\n",
        "\n",
        "print(\"Saved vol-candidate summary ->\", str(res_csv))\n",
        "print(res_df.head(10).to_string(index=False))\n",
        "\n",
        "# save best candidate detailed diag and reconstructed pos CSV\n",
        "if len(results) == 0:\n",
        "    raise RuntimeError(\"No successful candidates\")\n",
        "\n",
        "best_name, best_out = sorted(results, key=lambda x: (x[1][\"mean_abs\"], x[1][\"max_abs\"]))[0]\n",
        "print(\"\\nBEST CANDIDATE:\", best_name, \"mean_abs:\", best_out[\"mean_abs\"], \"max_abs:\", best_out[\"max_abs\"])\n",
        "\n",
        "rec_df = pd.DataFrame({\n",
        "    \"pos_orig\": pos_orig,\n",
        "    \"pos_rec\": best_out[\"pos_rec\"],\n",
        "    \"pos_diff\": best_out[\"diff\"],\n",
        "    \"pos_diff_abs\": best_out[\"absdiff\"],\n",
        "    \"raw\": raw\n",
        "}, index=common_index)\n",
        "\n",
        "rec_csv = Path(OUT_DIR)/f\"A_SRC_positions_reconstructed_best_{ts}.csv\"\n",
        "diag_csv = Path(OUT_DIR)/f\"reconstruct_best_diag_{ts}.csv\"\n",
        "rec_df.to_csv(rec_csv)\n",
        "rec_df.sort_values(\"pos_diff_abs\", ascending=False).head(200).to_csv(diag_csv)\n",
        "\n",
        "print(\"Saved best reconstructed positions ->\", rec_csv)\n",
        "print(\"Saved best diag (top mismatches) ->\", diag_csv)\n",
        "\n",
        "# also save top-10 candidates CSV\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "topk = res_df.head(10)\n",
        "(topk.assign(rank=np.arange(1, len(topk)+1))\n",
        " .to_csv(Path(OUT_DIR)/f\"vol_match_top10_{ts}.csv\", index=False))\n",
        "\n",
        "print(\"Done. Top candidates saved. Inspect the best diag to understand mismatch patterns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IxZX7S9AQo6",
        "outputId": "fb529e08-3db4-4138-a7dd-ea7336e89c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid size: 64 combos. Running...\n",
            "Saved recon grid top50 -> /content/drive/MyDrive/quant_pipeline/mtb_out/recon_grid_top50_1764912866.csv\n",
            "BEST COMBO: {'raw_name': 'pos.raw', 'floor': 1e-06, 'scale': 1.0, 'flip': False, 'mean_abs': 0.8183870572764657, 'max_abs': 1.7001031640248188, 'count_gt_1e-6': 9861, 'median_abs': 1.3142936367851767}\n",
            "Saved best rec -> /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_reconstructed_grid_best_1764912866.csv\n",
            "Saved best diag -> /content/drive/MyDrive/quant_pipeline/mtb_out/reconstruct_grid_best_diag_1764912866.csv\n",
            "Saved top200 summary -> /content/drive/MyDrive/quant_pipeline/mtb_out/recon_grid_top200_1764912866.csv\n",
            "\n",
            "Top 5 combos:\n",
            " raw_name    floor     scale  flip  mean_abs  max_abs  count_gt_1e-6  median_abs\n",
            " pos.raw 0.000001  1.000000 False  0.818387 1.700103           9861    1.314294\n",
            " pos.raw 0.000001 68.452125 False  0.818387 1.700103           9861    1.314294\n",
            " pos.raw 0.000100  1.000000 False  0.818387 1.700103           9861    1.314294\n",
            " pos.raw 0.000100 68.452125 False  0.818387 1.700103           9861    1.314294\n",
            " pos.raw 0.001000  1.000000 False  0.818387 1.700103           9861    1.314294\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# One-cell: exhaustive-ish reconstruction grid (floors, raw sources, sign flip, scale heur)\n",
        "import os, time, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "sig_path = \"/content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_signal_opt_signals_1764904868.csv\"\n",
        "pos_path = \"/content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_volscaled_1764907071.csv\"\n",
        "meta_pkl  = \"/content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\"\n",
        "\n",
        "# reported optimizer params\n",
        "k = 1.922005246265828\n",
        "L = 1.9097757869986336\n",
        "WINDOW = 138\n",
        "EPS = 1e-12\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sig = pd.read_csv(sig_path, index_col=0, parse_dates=True)\n",
        "pos = pd.read_csv(pos_path, index_col=0, parse_dates=True)\n",
        "meta = pd.read_pickle(meta_pkl)\n",
        "meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "\n",
        "common_index = pos.index.intersection(meta.index).intersection(sig.index)\n",
        "pos = pos.reindex(common_index)\n",
        "meta = meta.reindex(common_index)\n",
        "sig = sig.reindex(common_index)\n",
        "\n",
        "# candidate vol used previously (best single candidate reported)\n",
        "cand_vol = meta[\"tb_ret_at_break_h8\"].rolling(WINDOW, min_periods=1).std(ddof=0).fillna(method=\"ffill\").fillna(0.0).clip(lower=EPS)\n",
        "\n",
        "# possible raw sources (ordered)\n",
        "raw_sources = {}\n",
        "if \"raw\" in pos.columns:\n",
        "    raw_sources[\"pos.raw\"] = pos[\"raw\"].astype(float).reindex(common_index).fillna(0.0)\n",
        "if \"raw\" in sig.columns:\n",
        "    raw_sources[\"sig.raw\"] = sig[\"raw\"].astype(float).reindex(common_index).fillna(0.0)\n",
        "# signal * spread\n",
        "if (\"signal\" in sig.columns) and (\"spread\" in sig.columns):\n",
        "    raw_sources[\"sig.signal*spread\"] = (sig[\"signal\"].astype(float) * sig[\"spread\"].astype(float)).reindex(common_index).fillna(0.0)\n",
        "# p_long - p_short\n",
        "if (\"p_long\" in sig.columns) and (\"p_short\" in sig.columns):\n",
        "    raw_sources[\"p_long_minus_p_short\"] = (sig[\"p_long\"].astype(float) - sig[\"p_short\"].astype(float)).reindex(common_index).fillna(0.0)\n",
        "# fallback signal or first column\n",
        "raw_sources.setdefault(\"sig.firstcol\", sig.iloc[:,0].astype(float).reindex(common_index).fillna(0.0))\n",
        "raw_sources = {k: v.fillna(0.0) for k, v in raw_sources.items()}\n",
        "\n",
        "# vol floors to test\n",
        "floors = [1e-6, 1e-4, 1e-3, 1e-2]\n",
        "\n",
        "# scale strategies: 1.0 and median-match heuristic (scale = median(pos.vol) / median(candidate_vol) if pos.vol available)\n",
        "scale_options = [1.0]\n",
        "if \"vol\" in pos.columns:\n",
        "    pos_vol_med = float(pos[\"vol\"].abs().median() if pos[\"vol\"].abs().median() > 0 else 1.0)\n",
        "    cand_med = float(np.nanmedian(cand_vol.values) if np.nanmedian(cand_vol.values) > 0 else 1.0)\n",
        "    if cand_med > 0:\n",
        "        scale_options.append(pos_vol_med / cand_med)\n",
        "\n",
        "# boolean sign flip\n",
        "sign_flip_opts = [False, True]\n",
        "\n",
        "pos_orig = pos[\"position\"].astype(float).reindex(common_index).fillna(0.0).values\n",
        "pos_vol = pos[\"vol\"].astype(float).reindex(common_index).fillna(0.0).values if \"vol\" in pos.columns else None\n",
        "cand_vol_arr = cand_vol.reindex(common_index).astype(float).fillna(EPS).values\n",
        "\n",
        "results = []\n",
        "total = len(raw_sources) * len(floors) * len(scale_options) * len(sign_flip_opts)\n",
        "print(f\"Grid size: {total} combos. Running...\")\n",
        "\n",
        "for raw_name, raw_ser in raw_sources.items():\n",
        "    raw_arr = raw_ser.values\n",
        "    for floor in floors:\n",
        "        for scale in scale_options:\n",
        "            for flip in sign_flip_opts:\n",
        "                vol_arr = np.maximum(cand_vol_arr, floor)\n",
        "                # effective scale applied to numerator\n",
        "                effective_scale = float(scale)\n",
        "                # compute unsigned u then sign optionally flipped\n",
        "                with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                    u = (k * effective_scale * raw_arr) / vol_arr\n",
        "                if flip:\n",
        "                    u = -u\n",
        "                pos_rec = np.sign(u) * np.minimum(np.abs(u), L)\n",
        "                diff = pos_orig - pos_rec\n",
        "                absdiff = np.abs(diff)\n",
        "                metrics = {\n",
        "                    \"raw_name\": raw_name,\n",
        "                    \"floor\": float(floor),\n",
        "                    \"scale\": float(effective_scale),\n",
        "                    \"flip\": bool(flip),\n",
        "                    \"mean_abs\": float(np.nanmean(absdiff)),\n",
        "                    \"max_abs\": float(np.nanmax(absdiff)),\n",
        "                    \"count_gt_1e-6\": int((absdiff > 1e-6).sum()),\n",
        "                    \"median_abs\": float(np.nanmedian(absdiff)),\n",
        "                }\n",
        "                results.append((metrics, pos_rec, diff, absdiff))\n",
        "\n",
        "# sort results by mean_abs then max_abs\n",
        "results_sorted = sorted(results, key=lambda x: (x[0][\"mean_abs\"], x[0][\"max_abs\"]))\n",
        "# save top 20 summary\n",
        "ts = int(time.time())\n",
        "rows = [r[0] for r in results_sorted[:50]]\n",
        "pd.DataFrame(rows).to_csv(Path(OUT_DIR)/f\"recon_grid_top50_{ts}.csv\", index=False)\n",
        "print(\"Saved recon grid top50 ->\", str(Path(OUT_DIR)/f\"recon_grid_top50_{ts}.csv\"))\n",
        "\n",
        "best_metrics, best_pos_rec, best_diff, best_absdiff = results_sorted[0]\n",
        "print(\"BEST COMBO:\", best_metrics)\n",
        "\n",
        "# save best reconstructed positions + diag\n",
        "rec_df = pd.DataFrame({\n",
        "    \"pos_orig\": pos_orig,\n",
        "    \"pos_rec\": best_pos_rec,\n",
        "    \"pos_diff\": best_diff,\n",
        "    \"pos_diff_abs\": best_absdiff\n",
        "}, index=common_index)\n",
        "rec_csv = Path(OUT_DIR)/f\"A_SRC_positions_reconstructed_grid_best_{ts}.csv\"\n",
        "diag_csv = Path(OUT_DIR)/f\"reconstruct_grid_best_diag_{ts}.csv\"\n",
        "rec_df.to_csv(rec_csv)\n",
        "rec_df.sort_values(\"pos_diff_abs\", ascending=False).head(300).to_csv(diag_csv)\n",
        "print(\"Saved best rec ->\", rec_csv)\n",
        "print(\"Saved best diag ->\", diag_csv)\n",
        "\n",
        "# optional: save full results summary (top 200)\n",
        "pd.DataFrame([r[0] for r in results_sorted[:200]]).to_csv(Path(OUT_DIR)/f\"recon_grid_top200_{ts}.csv\", index=False)\n",
        "print(\"Saved top200 summary ->\", Path(OUT_DIR)/f\"recon_grid_top200_{ts}.csv\")\n",
        "\n",
        "# quick diagnostics print\n",
        "top5 = pd.DataFrame([r[0] for r in results_sorted[:5]])\n",
        "print(\"\\nTop 5 combos:\\n\", top5.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIjyubVaAg37",
        "outputId": "c5b0d470-9319-4915-8db2-ca5c1ae668e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved recon estimates top20 -> /content/drive/MyDrive/quant_pipeline/mtb_out/recon_estimates_top20_1764912933.csv\n",
            "BEST ESTIMATE: {'vol_name': 'pos.vol', 'raw_name': 'pos.raw', 'k_est': 0.35, 'L_try': 1.0, 'len_common': 17521, 'mean_abs': 1.5303990311867565e-13, 'median_abs': 1.1102230246251565e-16, 'max_abs': 3.4999999999999997e-13, 'count_gt_1e-6': 0, 'corr_pos_rec': 0.9999999999999981}\n",
            "Saved reconstructed (est) -> /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_reconstructed_est_1764912933.csv\n",
            "Saved diag -> /content/drive/MyDrive/quant_pipeline/mtb_out/reconstruct_est_diag_1764912933.csv\n"
          ]
        }
      ],
      "source": [
        "# One-cell: estimate k/L from data & try reconstruction (quick diagnostic)\n",
        "import time, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/quant_pipeline/mtb_out\"\n",
        "sig_path = Path(OUT_DIR)/\"A_SRC_signal_opt_signals_1764904868.csv\"\n",
        "pos_path = Path(OUT_DIR)/\"A_SRC_positions_volscaled_1764907071.csv\"\n",
        "meta_pkl = Path(OUT_DIR)/\"df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\"\n",
        "\n",
        "ts = int(time.time())\n",
        "meta = pd.read_pickle(meta_pkl)\n",
        "meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "pos = pd.read_csv(pos_path, index_col=0, parse_dates=True)\n",
        "sig = pd.read_csv(sig_path, index_col=0, parse_dates=True)\n",
        "\n",
        "common = pos.index.intersection(sig.index).intersection(meta.index)\n",
        "pos = pos.reindex(common)\n",
        "sig = sig.reindex(common)\n",
        "meta = meta.reindex(common)\n",
        "\n",
        "# candidates for vol proxy (include pos.vol if present)\n",
        "WINDOW = 138\n",
        "eps = 1e-12\n",
        "vol_candidates = {}\n",
        "# candidate used earlier: tb_ret_at_break_h8 rolling std (ddof=0)\n",
        "if \"tb_ret_at_break_h8\" in meta.columns:\n",
        "    vol_candidates[\"tb_h8_roll138_ddof0\"] = meta[\"tb_ret_at_break_h8\"].rolling(WINDOW, min_periods=1).std(ddof=0).fillna(method=\"ffill\").fillna(eps)\n",
        "# also try tb_h8 roll std ddof=1 and ewm\n",
        "if \"tb_ret_at_break_h8\" in meta.columns:\n",
        "    vol_candidates[\"tb_h8_roll138_ddof1\"] = meta[\"tb_ret_at_break_h8\"].rolling(WINDOW, min_periods=1).std(ddof=1).fillna(method=\"ffill\").fillna(eps)\n",
        "    vol_candidates[\"tb_h8_ewm_span138\"] = meta[\"tb_ret_at_break_h8\"].ewm(span=WINDOW, adjust=False).std().fillna(eps)\n",
        "# candidate: pos.vol (likely the 'true' vol used for scaling)\n",
        "if \"vol\" in pos.columns:\n",
        "    vol_candidates[\"pos.vol\"] = pos[\"vol\"].astype(float).fillna(eps)\n",
        "# fallback: simple close-return rolling std if present in meta (ret_12h or ret_48h_z_small isn't suitable but include)\n",
        "for c in [\"ret_12h\",\"ret_48h_z_small\",\"ret_std_24\"]:\n",
        "    if c in meta.columns:\n",
        "        vol_candidates.setdefault(c, meta[c].rolling(WINDOW, min_periods=1).std().fillna(eps))\n",
        "\n",
        "raw_choices = []\n",
        "# prefer pos.raw, sig.raw, and synthetic (p_long - p_short), signal*spread\n",
        "if \"raw\" in pos.columns:\n",
        "    raw_choices.append((\"pos.raw\", pos[\"raw\"].astype(float).fillna(0.0)))\n",
        "if \"raw\" in sig.columns:\n",
        "    raw_choices.append((\"sig.raw\", sig[\"raw\"].astype(float).fillna(0.0)))\n",
        "if (\"p_long\" in sig.columns) and (\"p_short\" in sig.columns):\n",
        "    raw_choices.append((\"p_long_minus_p_short\", (sig[\"p_long\"].astype(float) - sig[\"p_short\"].astype(float)).fillna(0.0)))\n",
        "if (\"signal\" in sig.columns) and (\"spread\" in sig.columns):\n",
        "    raw_choices.append((\"signal_times_spread\", (sig[\"signal\"].astype(float) * sig[\"spread\"].astype(float)).fillna(0.0)))\n",
        "if not raw_choices:\n",
        "    # fallback: first sig column\n",
        "    raw_choices.append((\"sig_first\", sig.iloc[:,0].astype(float).fillna(0.0)))\n",
        "\n",
        "pos_arr = pos[\"position\"].astype(float).fillna(0.0).values\n",
        "\n",
        "out_rows = []\n",
        "best = None\n",
        "\n",
        "for vol_name, vol_ser in vol_candidates.items():\n",
        "    vol = vol_ser.astype(float).reindex(common).fillna(eps).values\n",
        "    for raw_name, raw_ser in raw_choices:\n",
        "        raw = raw_ser.reindex(common).astype(float).fillna(0.0).values\n",
        "        u = np.zeros_like(raw) + eps\n",
        "        nz = np.abs(raw) > 0\n",
        "        u[nz] = raw[nz] / np.maximum(vol[nz], eps)   # raw / vol\n",
        "        # only use indices where pos != 0 and u != 0 to estimate k\n",
        "        mask = (np.abs(pos_arr) > 1e-12) & (np.abs(u) > 1e-12)\n",
        "        if mask.sum() < 10:\n",
        "            k_med = np.nan\n",
        "        else:\n",
        "            ratios = pos_arr[mask] / u[mask]\n",
        "            # robust estimate: median of ratios, also compute Huber-like trimmed mean\n",
        "            k_med = float(np.median(ratios))\n",
        "            # fallback to trimmed mean\n",
        "            trim = np.percentile(ratios, [5,95])\n",
        "            k_trim = float(np.mean(ratios[(ratios>=trim[0])&(ratios<=trim[1])])) if mask.sum()>=20 else k_med\n",
        "        # estimate L as 99.9 percentile of abs(pos) (or max)\n",
        "        L_max = float(np.max(np.abs(pos_arr))) if pos_arr.size>0 else 1.0\n",
        "        L_99 = float(np.percentile(np.abs(pos_arr), 99.9)) if pos_arr.size>0 else L_max\n",
        "        # try both L candidates and both k choices\n",
        "        for k_try in [k_med, k_trim]:\n",
        "            for L_try in [L_99, L_max]:\n",
        "                if np.isnan(k_try) or k_try == 0:\n",
        "                    continue\n",
        "                u_eff = u * k_try\n",
        "                pos_rec = np.sign(u_eff) * np.minimum(np.abs(u_eff), L_try)\n",
        "                diff = pos_arr - pos_rec\n",
        "                absdiff = np.abs(diff)\n",
        "                row = {\n",
        "                    \"vol_name\": vol_name,\n",
        "                    \"raw_name\": raw_name,\n",
        "                    \"k_est\": float(k_try),\n",
        "                    \"L_try\": float(L_try),\n",
        "                    \"len_common\": int(len(pos_arr)),\n",
        "                    \"mean_abs\": float(np.nanmean(absdiff)),\n",
        "                    \"median_abs\": float(np.nanmedian(absdiff)),\n",
        "                    \"max_abs\": float(np.nanmax(absdiff)),\n",
        "                    \"count_gt_1e-6\": int((absdiff>1e-6).sum()),\n",
        "                    \"corr_pos_rec\": float(np.corrcoef(pos_arr, pos_rec)[0,1]) if (np.isfinite(pos_arr).all() and np.isfinite(pos_rec).all()) else np.nan\n",
        "                }\n",
        "                out_rows.append((row, pos_rec, diff, absdiff, vol_name, raw_name))\n",
        "# sort\n",
        "out_rows_sorted = sorted(out_rows, key=lambda x: (x[0][\"mean_abs\"], -x[0][\"corr_pos_rec\"] if not np.isnan(x[0][\"corr_pos_rec\"]) else 0.0))\n",
        "# save top 20\n",
        "top_rows = [r[0] for r in out_rows_sorted[:20]]\n",
        "pd.DataFrame(top_rows).to_csv(Path(OUT_DIR)/f\"recon_estimates_top20_{ts}.csv\", index=False)\n",
        "print(\"Saved recon estimates top20 ->\", Path(OUT_DIR)/f\"recon_estimates_top20_{ts}.csv\")\n",
        "\n",
        "if out_rows_sorted:\n",
        "    best_row, best_pos_rec, best_diff, best_absdiff, vol_name_b, raw_name_b = out_rows_sorted[0]\n",
        "    print(\"BEST ESTIMATE:\", best_row)\n",
        "    rec_df = pd.DataFrame({\n",
        "        \"pos_orig\": pos_arr,\n",
        "        \"pos_rec\": best_pos_rec,\n",
        "        \"pos_diff\": best_diff,\n",
        "        \"pos_diff_abs\": best_absdiff,\n",
        "        \"vol_used\": vol_candidates[vol_name_b].reindex(common).astype(float).fillna(eps).values,\n",
        "        \"raw_used\": raw_choices[[i for i,(n,s) in enumerate(raw_choices) if n==raw_name_b][0]][1].reindex(common).astype(float).fillna(0.0).values\n",
        "    }, index=common)\n",
        "    rec_csv = Path(OUT_DIR)/f\"A_SRC_positions_reconstructed_est_{ts}.csv\"\n",
        "    diag_csv = Path(OUT_DIR)/f\"reconstruct_est_diag_{ts}.csv\"\n",
        "    rec_df.to_csv(rec_csv)\n",
        "    rec_df.sort_values(\"pos_diff_abs\", ascending=False).head(500).to_csv(diag_csv)\n",
        "    print(\"Saved reconstructed (est) ->\", rec_csv)\n",
        "    print(\"Saved diag ->\", diag_csv)\n",
        "else:\n",
        "    print(\"No valid estimates produced (insufficient non-zero samples).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xtJqrMcCBBCU",
        "outputId": "b495c1f4-ed50-4c9a-b666-8ed1af9fff9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DONE] backtest_fast (A_SRC positions) finished.\n",
            " - positions file: /content/drive/MyDrive/quant_pipeline/mtb_out/A_SRC_positions_volscaled_1764907071.csv\n",
            " - equity csv: /content/drive/MyDrive/quant_pipeline/mtb_out/backtest_positions_equity_1764913072.csv\n",
            " - trades csv: /content/drive/MyDrive/quant_pipeline/mtb_out/backtest_positions_trades_1764913072.csv\n",
            " - metrics json: /content/drive/MyDrive/quant_pipeline/mtb_out/backtest_positions_metrics_1764913072.json\n",
            " - plots: /content/drive/MyDrive/quant_pipeline/mtb_out/backtest_positions_plots_1764913072.png\n",
            "Summary metrics:\n",
            "  n_periods: 17521\n",
            "  cum_return: 1.4359300435942872\n",
            "  ann_return: 0.35978796645996103\n",
            "  ann_vol: 0.20529060138129337\n",
            "  ann_sharpe: 1.5998184201032972\n",
            "  sharpe_ci_95: [-0.5070895946577084, 3.5611819806419542]\n",
            "  max_drawdown: -0.39168713196204274\n",
            "  trade_count: 332\n",
            "  win_rate: 0.5602409638554217\n",
            "  expectancy: 0.0030911427246680563\n",
            "  turnover: 0.07173034164051563\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# ONE-CELL: Fast backtest using A_SRC_positions_volscaled*.csv (final positions)\n",
        "# Requirements: pandas, numpy, matplotlib\n",
        "import os, time, json, math, warnings\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ANNUALIZE = 252 * 24     # hourly data baseline in your pipeline\n",
        "BOOTSTRAP_ITERS = 1000\n",
        "BOOTSTRAP_BLOCK = 24\n",
        "PLOT_DPI = 120\n",
        "MIN_POS_THRESH = 1e-9\n",
        "DEFAULT_TC = 0.0005\n",
        "TS = int(time.time())\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def load_latest_file(glob_pattern: str) -> Path:\n",
        "    files = sorted(OUT_DIR.glob(glob_pattern), key=os.path.getmtime)\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No files match {glob_pattern} in {OUT_DIR}\")\n",
        "    return files[-1]\n",
        "\n",
        "def load_meta_and_returns(prefer_ret_col: str = None) -> Tuple[pd.DataFrame, str]:\n",
        "    pkl = load_latest_file(\"df_meta_shortlist.v*.pkl\")\n",
        "    df = pd.read_pickle(pkl)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    # auto-choose return column if not provided\n",
        "    if prefer_ret_col is None:\n",
        "        cand = [c for c in df.columns if c.startswith(\"tb_ret\")]\n",
        "        prefer_ret_col = cand[0] if cand else None\n",
        "    if prefer_ret_col is None or prefer_ret_col not in df.columns:\n",
        "        raise RuntimeError(\"No tb_ret column found in df_meta_shortlist; cannot backtest safely.\")\n",
        "    return df, prefer_ret_col, str(pkl)\n",
        "\n",
        "def block_bootstrap_sharpe(net_series: np.ndarray, iters: int = 1000, block: int = 24):\n",
        "    n = len(net_series)\n",
        "    if n <= 1:\n",
        "        return float(\"nan\"), float(\"nan\")\n",
        "    boot = []\n",
        "    starts = np.arange(0, max(1, n - block + 1))\n",
        "    if len(starts) <= 0:\n",
        "        # iid fallback\n",
        "        for _ in range(iters):\n",
        "            s = np.random.choice(net_series, size=n, replace=True)\n",
        "            boot.append((np.nanmean(s) / np.nanstd(s)) * np.sqrt(ANNUALIZE) if np.nanstd(s) > 0 else 0.0)\n",
        "    else:\n",
        "        for _ in range(iters):\n",
        "            res = []\n",
        "            while len(res) < n:\n",
        "                st = np.random.choice(starts)\n",
        "                blk = net_series[st:st+block]\n",
        "                res.append(blk)\n",
        "            arr = np.concatenate(res)[:n]\n",
        "            boot.append((np.nanmean(arr) / np.nanstd(arr)) * np.sqrt(ANNUALIZE) if np.nanstd(arr) > 0 else 0.0)\n",
        "    lo = np.percentile(boot, 2.5)\n",
        "    hi = np.percentile(boot, 97.5)\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def fast_trade_reconstruction(positions: np.ndarray, rets: np.ndarray, tc_rate: float):\n",
        "    \"\"\"\n",
        "    Vectorized reconstruction of trades:\n",
        "    - identify contiguous runs where sign(position) != 0\n",
        "    - compute pnl = sum(position * returns) over run\n",
        "    - compute tc = tc_rate * sum(abs(dpos)) over run (including entry)\n",
        "    Returns DataFrame of trades.\n",
        "    \"\"\"\n",
        "    pos = positions\n",
        "    n = len(pos)\n",
        "    mask_nonzero = np.abs(pos) > MIN_POS_THRESH\n",
        "    if not mask_nonzero.any():\n",
        "        return pd.DataFrame(columns=[\"entry_idx\",\"exit_idx\",\"entry_time\",\"exit_time\",\"entry_pos\",\"exit_pos\",\"pnl\",\"tc\",\"net\",\"len\",\"win\"])\n",
        "    # indices where mask changes (including boundaries)\n",
        "    nz = mask_nonzero.astype(int)\n",
        "    diff = np.diff(np.concatenate(([0], nz, [0])))\n",
        "    starts = np.where(diff == 1)[0]\n",
        "    ends = np.where(diff == -1)[0] - 1\n",
        "    assert len(starts) == len(ends)\n",
        "    trades = []\n",
        "    for s, e in zip(starts, ends):\n",
        "        slice_pos = pos[s:e+1]\n",
        "        slice_ret = rets[s:e+1]\n",
        "        pnl = float((slice_pos * slice_ret).sum())\n",
        "        # transaction cost: abs change in position within slice including entry\n",
        "        dpos = np.empty_like(slice_pos)\n",
        "        dpos[0] = slice_pos[0]\n",
        "        if len(slice_pos) > 1:\n",
        "            dpos[1:] = slice_pos[1:] - slice_pos[:-1]\n",
        "        tc = float(tc_rate * np.abs(dpos).sum())\n",
        "        net = pnl - tc\n",
        "        trades.append((int(s), int(e), float(slice_pos[0]), float(slice_pos[-1]), pnl, tc, net, int(e-s+1)))\n",
        "    trades_df = pd.DataFrame(trades, columns=[\"entry_idx\",\"exit_idx\",\"entry_pos\",\"exit_pos\",\"pnl\",\"tc\",\"net\",\"len\"])\n",
        "    return trades_df\n",
        "\n",
        "# ---------------- main ----------------\n",
        "# 1) load latest A_SRC positions (volscaled final positions)\n",
        "pos_file = load_latest_file(\"A_SRC_positions_volscaled*.csv\")\n",
        "pos_df = pd.read_csv(pos_file, index_col=0, parse_dates=True)\n",
        "pos_df.index = pd.to_datetime(pos_df.index, utc=True)\n",
        "\n",
        "# require 'position' column\n",
        "if \"position\" not in pos_df.columns:\n",
        "    raise RuntimeError(\"positions CSV must include 'position' column (final positions).\")\n",
        "\n",
        "# 2) load meta & returns\n",
        "meta_df, ret_col, meta_pkl_path = load_meta_and_returns()  # will raise if no tb_ret found\n",
        "meta_df = meta_df.reindex(pos_df.index)  # align causally (assumes same freq)\n",
        "rets = meta_df[ret_col].fillna(0.0).astype(float).reindex(pos_df.index).fillna(0.0).values\n",
        "\n",
        "# 3) ensure single tc_rate (use column if present, otherwise default)\n",
        "if \"tc\" in pos_df.columns:\n",
        "    # take first non-null\n",
        "    tc_vals = pos_df[\"tc\"].dropna().unique()\n",
        "    tc_rate = float(tc_vals[0]) if len(tc_vals) > 0 else DEFAULT_TC\n",
        "else:\n",
        "    tc_rate = DEFAULT_TC\n",
        "\n",
        "# 4) extract positions vector\n",
        "positions = pos_df[\"position\"].astype(float).reindex(pos_df.index).fillna(0.0).values\n",
        "\n",
        "# quick alignment checks\n",
        "len_common = len(positions)\n",
        "assert len_common == len(rets), f\"positions and returns length mismatch: {len_common} vs {len(rets)}\"\n",
        "\n",
        "# 5) vectorized pnl / net series\n",
        "dpos = np.empty_like(positions)\n",
        "dpos[0] = positions[0]\n",
        "if len(positions) > 1:\n",
        "    dpos[1:] = positions[1:] - positions[:-1]\n",
        "tc_costs = tc_rate * np.abs(dpos)\n",
        "pnl = positions * rets\n",
        "net = pnl - tc_costs\n",
        "eq = pd.Series((1.0 + net).cumprod(), index=pos_df.index)\n",
        "\n",
        "# 6) aggregate metrics\n",
        "n_periods = len(net)\n",
        "cum_return = float(eq.iloc[-1] - 1.0) if n_periods > 0 else 0.0\n",
        "ann_return = float(eq.iloc[-1] ** (ANNUALIZE / max(1, n_periods)) - 1.0) if n_periods > 0 else 0.0\n",
        "ann_vol = float(np.nanstd(net) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "ann_sharpe = float((np.nanmean(net) / np.nanstd(net)) * math.sqrt(ANNUALIZE)) if np.nanstd(net) > 0 else 0.0\n",
        "\n",
        "# 7) downside/Sortino\n",
        "neg = net.copy()\n",
        "neg[neg > 0] = 0.0\n",
        "downside = float(np.sqrt(np.nanmean(neg**2)) * math.sqrt(ANNUALIZE)) if np.any(neg != 0) else 0.0\n",
        "sortino = float((np.nanmean(net) * math.sqrt(ANNUALIZE) / downside)) if downside > 0 else float(\"nan\")\n",
        "\n",
        "# 8) drawdown / calmar\n",
        "cum_max = eq.cummax()\n",
        "drawdown = (eq - cum_max) / cum_max\n",
        "max_dd = float(drawdown.min()) if len(drawdown) > 0 else 0.0\n",
        "calmar = float(ann_return / abs(max_dd)) if max_dd < 0 else None\n",
        "\n",
        "# 9) turnover / exposure\n",
        "avg_abs_pos = float(np.nanmean(np.abs(positions))) if np.nanmean(np.abs(positions)) > 0 else 1.0\n",
        "turnover = float(np.nanmean(np.abs(dpos)) / avg_abs_pos)\n",
        "\n",
        "# 10) trades reconstruction (fast)\n",
        "trades_df = fast_trade_reconstruction(positions, rets, tc_rate)\n",
        "trade_count = len(trades_df)\n",
        "win_count = int((trades_df[\"net\"] > 0).sum()) if trade_count > 0 else 0\n",
        "win_rate = float(win_count / trade_count) if trade_count > 0 else float(\"nan\")\n",
        "expectancy = float(trades_df[\"net\"].mean()) if trade_count > 0 else 0.0\n",
        "median_trade_net = float(trades_df[\"net\"].median()) if trade_count > 0 else float(\"nan\")\n",
        "avg_holding = float(trades_df[\"len\"].mean()) if trade_count > 0 else float(\"nan\")\n",
        "\n",
        "# 11) bootstrap CI for Sharpe (block)\n",
        "sharpe_lo, sharpe_hi = block_bootstrap_sharpe(net, iters=BOOTSTRAP_ITERS, block=BOOTSTRAP_BLOCK)\n",
        "\n",
        "# 12) save outputs\n",
        "eq_csv = OUT_DIR / f\"backtest_positions_equity_{TS}.csv\"\n",
        "trades_csv = OUT_DIR / f\"backtest_positions_trades_{TS}.csv\"\n",
        "metrics_json = OUT_DIR / f\"backtest_positions_metrics_{TS}.json\"\n",
        "plot_png = OUT_DIR / f\"backtest_positions_plots_{TS}.png\"\n",
        "\n",
        "pd.DataFrame({\"net\": net, \"pnl\": pnl, \"tc\": tc_costs, \"eq\": eq}).to_csv(eq_csv)\n",
        "trades_df.to_csv(trades_csv, index=False)\n",
        "\n",
        "metrics = {\n",
        "    \"positions_file\": str(pos_file),\n",
        "    \"meta_file\": meta_pkl_path,\n",
        "    \"ret_col\": ret_col,\n",
        "    \"n_periods\": int(n_periods),\n",
        "    \"cum_return\": cum_return,\n",
        "    \"ann_return\": ann_return,\n",
        "    \"ann_vol\": ann_vol,\n",
        "    \"ann_sharpe\": ann_sharpe,\n",
        "    \"sharpe_ci_95\": [sharpe_lo, sharpe_hi],\n",
        "    \"sortino\": sortino,\n",
        "    \"max_drawdown\": max_dd,\n",
        "    \"calmar\": calmar,\n",
        "    \"trade_count\": int(trade_count),\n",
        "    \"win_count\": int(win_count),\n",
        "    \"win_rate\": win_rate,\n",
        "    \"avg_holding\": avg_holding,\n",
        "    \"expectancy\": expectancy,\n",
        "    \"median_trade_net\": median_trade_net,\n",
        "    \"turnover\": turnover,\n",
        "    \"tc_rate\": tc_rate,\n",
        "}\n",
        "with open(metrics_json, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# 13) plots: equity + drawdown + trade net hist\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10, 12), dpi=PLOT_DPI, constrained_layout=True)\n",
        "axs[0].plot(eq.index, eq.values)\n",
        "axs[0].set_title(\"Equity Curve (positions file)\")\n",
        "axs[0].set_ylabel(\"Equity (growth)\")\n",
        "axs[1].plot(drawdown.index, drawdown.values)\n",
        "axs[1].set_title(\"Drawdown\")\n",
        "axs[1].set_ylabel(\"Drawdown\")\n",
        "if trade_count > 0:\n",
        "    axs[2].hist(trades_df[\"net\"].values, bins=50)\n",
        "    axs[2].set_title(\"Trade net P&L histogram\")\n",
        "else:\n",
        "    axs[2].text(0.1, 0.5, \"No trades found\", transform=axs[2].transAxes)\n",
        "    axs[2].set_title(\"Trade net P&L histogram\")\n",
        "plt.savefig(plot_png)\n",
        "plt.close(fig)\n",
        "\n",
        "# 14) prints & minimal checks\n",
        "print(\"[DONE] backtest_fast (A_SRC positions) finished.\")\n",
        "print(\" - positions file:\", pos_file)\n",
        "print(\" - equity csv:\", eq_csv)\n",
        "print(\" - trades csv:\", trades_csv)\n",
        "print(\" - metrics json:\", metrics_json)\n",
        "print(\" - plots:\", plot_png)\n",
        "print(\"Summary metrics:\")\n",
        "for k in [\"n_periods\",\"cum_return\",\"ann_return\",\"ann_vol\",\"ann_sharpe\",\"sharpe_ci_95\",\"max_drawdown\",\"trade_count\",\"win_rate\",\"expectancy\",\"turnover\"]:\n",
        "    print(f\"  {k}: {metrics.get(k)}\")\n",
        "\n",
        "# 15) quick data-leakage sanity checks (informative, not blocking)\n",
        "leak_warnings = []\n",
        "if rets is None or np.all(rets == 0):\n",
        "    leak_warnings.append(\"returns series is all zeros — check tb_ret in meta.\")\n",
        "if pos_df.index.equals(meta_df.index) is False:\n",
        "    leak_warnings.append(\"index misalignment between positions and meta after reindex; ensure causal alignment.\")\n",
        "if leak_warnings:\n",
        "    print(\"\\n[WARN] Potential issues detected:\")\n",
        "    for w in leak_warnings:\n",
        "        print(\" -\", w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Minimal live runner (paper/small-live). Synchronous, deterministic.\n",
        "Replace execute_order(...) with your exchange/broker call.\n",
        "\"\"\"\n",
        "import time, json, math, logging\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "LOG = logging.getLogger(\"live_runner\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "OUT = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "# choose file (latest)\n",
        "pos_file = sorted(OUT.glob(\"A_SRC_positions_*.csv\"), key=lambda p: p.stat().st_mtime)[-1]\n",
        "meta_file = sorted(OUT.glob(\"df_meta_shortlist.v*.pkl\"), key=lambda p: p.stat().st_mtime)[-1]\n",
        "\n",
        "# PARAMETERS (tune before live)\n",
        "EXEC_SHIFT = 1             # ensure same as backtest\n",
        "MAX_ABS_POS = 1.0          # maximum allowed absolute position (units)\n",
        "MAX_ORDER_NOTIONAL = 0.02  # fraction of equity per single order (if using sizing)\n",
        "PAPER_MODE = True          # if True, no real orders sent\n",
        "SLEEP_BETWEEN_TICKS = 1.0  # seconds (if streaming), adjust to frequency\n",
        "\n",
        "# placeholder: replace with your order execution API\n",
        "def execute_order(instrument, qty, side, price=None, paper=True):\n",
        "    \"\"\"\n",
        "    Implement actual execution here (ccxt, alpaca, broker SDK).\n",
        "    For paper==True just log/return fake fill.\n",
        "    \"\"\"\n",
        "    if paper:\n",
        "        LOG.info(\"PAPER ORDER: %s %s qty=%.6f price=%s\", side, instrument, qty, price)\n",
        "        return {\"status\": \"filled\", \"qty\": qty, \"fill_price\": price or 0.0}\n",
        "    # real broker call here...\n",
        "    raise NotImplementedError(\"Replace execute_order with broker call\")\n",
        "\n",
        "# --- load signals/positions/returns ---\n",
        "posdf = pd.read_csv(pos_file, index_col=0, parse_dates=True)\n",
        "posdf.index = pd.to_datetime(posdf.index, utc=True)\n",
        "# pick numeric column\n",
        "numcols = [c for c in posdf.columns if np.issubdtype(posdf[c].dtype, np.number)]\n",
        "assert numcols, \"no numeric cols in position file\"\n",
        "pos_series = posdf[numcols[0]].astype(float).sort_index()\n",
        "LOG.info(\"Using pos column: %s from %s\", numcols[0], pos_file.name)\n",
        "\n",
        "meta = pd.read_pickle(meta_file)\n",
        "meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "# ensure alignment - backtest used tb_ret_8\n",
        "if \"tb_ret_8\" not in meta.columns:\n",
        "    LOG.warning(\"meta missing tb_ret_8; live runner will not compute pnl\")\n",
        "rets = meta[\"tb_ret_8\"].reindex(pos_series.index).fillna(0.0)\n",
        "\n",
        "# basic safety asserts\n",
        "assert pos_series.index.is_monotonic_increasing\n",
        "# ensure causal history present: no NaN before first pos\n",
        "first_nonzero = pos_series.ne(0).idxmax() if pos_series.ne(0).any() else None\n",
        "LOG.info(\"first_nonzero_idx: %s\", first_nonzero)\n",
        "\n",
        "# simple sizing function (example)\n",
        "def size_from_pos_value(pos_value, account_equity=1.0, max_abs=MAX_ABS_POS):\n",
        "    \"\"\"Map normalized pos ([-L,L]) -> qty (notional fraction).\"\"\"\n",
        "    pos_clipped = float(np.clip(pos_value, -max_abs, max_abs))\n",
        "    # simple linear mapping: pos=1 => MAX_ORDER_NOTIONAL of equity\n",
        "    qty = pos_clipped * MAX_ORDER_NOTIONAL * account_equity\n",
        "    return qty\n",
        "\n",
        "# Live loop skeleton (iterate over new timestamps)\n",
        "# In production, replace with streaming tick handler.\n",
        "for ts, target_pos in pos_series.iloc[::-1].sort_index().items():  # iterate forward\n",
        "    # NOTE: this example iterates all historical timestamps sequentially.\n",
        "    # For real-live, pick latest timestamp or wait for new file update.\n",
        "    LOG.info(\"TS %s target_pos=%.6f\", ts, target_pos)\n",
        "    # risk checks\n",
        "    if abs(target_pos) > MAX_ABS_POS * 1.001:\n",
        "        LOG.error(\"target_pos exceeds MAX_ABS_POS -> skip: %.6f\", target_pos)\n",
        "        continue\n",
        "    # compute order qty relative to current position (you need current position tracking)\n",
        "    # for demo we assume starting from 0\n",
        "    current_pos = 0.0\n",
        "    delta_pos = float(target_pos - current_pos)\n",
        "    if abs(delta_pos) < 1e-6:\n",
        "        LOG.debug(\"no change\")\n",
        "        continue\n",
        "    qty = size_from_pos_value(delta_pos, account_equity=1.0)\n",
        "    side = \"BUY\" if qty > 0 else \"SELL\"\n",
        "    # execute\n",
        "    try:\n",
        "        res = execute_order(\"INSTRUMENT_PLACEHOLDER\", abs(qty), side, price=None, paper=PAPER_MODE)\n",
        "    except Exception as e:\n",
        "        LOG.exception(\"execution failed: %s\", e)\n",
        "        break\n",
        "    # optional: record fill and update current_pos accordingly\n",
        "    current_pos = target_pos\n",
        "    # simple monitoring output\n",
        "    LOG.info(\"order result: %s\", res)\n",
        "    # throttle loop in live\n",
        "    time.sleep(SLEEP_BETWEEN_TICKS)\n",
        "\n",
        "LOG.info(\"Live runner finished (demo).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "hfzMGG8rGVhw",
        "outputId": "1541d78e-9eea-401c-f3ae-b221155cdda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-790495426.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mLOG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order result: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# throttle loop in live\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSLEEP_BETWEEN_TICKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mLOG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Live runner finished (demo).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual-signal printer (prints, optional ccxt price fetch)\n",
        "# Replace OUT_DIR, FILE patterns, SYMBOL/EXCHANGE as needed.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Optional: ccxt for live price reference (install if needed: pip install ccxt)\n",
        "USE_CCXT = True\n",
        "EXCHANGE_ID = \"bitfinex\"         # change if you use other exchange in ccxt\n",
        "SYMBOL = \"BTC/USDT\"             # pair for price reference (optional)\n",
        "LATEST_N = 50                   # how many recent timestamps to print\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "\n",
        "# file selection (latest)\n",
        "sig_file = sorted(OUT_DIR.glob(\"Aplus_signals_aligned_*.csv\"), key=lambda p: p.stat().st_mtime)[-1]\n",
        "pos_file = sorted(OUT_DIR.glob(\"A_SRC_positions_*.csv\"), key=lambda p: p.stat().st_mtime)[-1]\n",
        "\n",
        "# parameters\n",
        "SIGNAL_COL_PLONG = \"p_long\"\n",
        "SIGNAL_COL_PSHORT = \"p_short\"\n",
        "POS_COL_CANDIDATES = [\"position\", \"pos_pen\", \"pos_final\", \"pos_ensemble_raw\", \"pos\"]\n",
        "THRESHOLD = 0.01   # minimal signed signal magnitude to trigger non-HOLD\n",
        "\n",
        "# helper: safe ccxt\n",
        "def fetch_price_ccxt(exchange_id, symbol):\n",
        "    try:\n",
        "        import ccxt\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"error\": f\"ccxt not available: {e}\"}\n",
        "    try:\n",
        "        ex = getattr(ccxt, exchange_id)()\n",
        "        # use fetch_ticker for mid/last\n",
        "        t = ex.fetch_ticker(symbol)\n",
        "        return {\"ok\": True, \"price\": float(t.get(\"last\") or t.get(\"close\") or t.get(\"last\")), \"raw\": t}\n",
        "    except Exception as e:\n",
        "        return {\"ok\": False, \"error\": str(e)}\n",
        "\n",
        "# load files\n",
        "sig = pd.read_csv(sig_file, index_col=0, parse_dates=True)\n",
        "sig.index = pd.to_datetime(sig.index, utc=True)\n",
        "posdf = pd.read_csv(pos_file, index_col=0, parse_dates=True)\n",
        "posdf.index = pd.to_datetime(posdf.index, utc=True)\n",
        "\n",
        "# pick pos col\n",
        "pos_cols = [c for c in posdf.columns if np.issubdtype(posdf[c].dtype, np.number)]\n",
        "if not pos_cols:\n",
        "    raise RuntimeError(\"No numeric column found in positions CSV.\")\n",
        "pos_col = next((c for c in POS_COL_CANDIDATES if c in posdf.columns), pos_cols[0])\n",
        "\n",
        "# build signal series (signed)\n",
        "if SIGNAL_COL_PLONG in sig.columns and SIGNAL_COL_PSHORT in sig.columns:\n",
        "    signed_signal = (sig[SIGNAL_COL_PLONG].astype(float) - sig[SIGNAL_COL_PSHORT].astype(float)).rename(\"signed_signal\")\n",
        "    p_long = sig[SIGNAL_COL_PLONG].astype(float).rename(\"p_long\")\n",
        "    p_short = sig[SIGNAL_COL_PSHORT].astype(float).rename(\"p_short\")\n",
        "else:\n",
        "    # fallback: try single column named 'p_long' or 'signal'\n",
        "    col_candidate = None\n",
        "    for cand in (\"p_long\",\"signal\",\"kelly_pos\",\"kelly_raw\"):\n",
        "        if cand in sig.columns:\n",
        "            col_candidate = cand; break\n",
        "    if col_candidate is None:\n",
        "        raise RuntimeError(\"Signal columns not found in signal CSV.\")\n",
        "    signed_signal = sig[col_candidate].astype(float).rename(\"signed_signal\")\n",
        "    p_long = sig[col_candidate].astype(float).rename(\"p_long\")\n",
        "    p_short = pd.Series(0.0, index=signed_signal.index, name=\"p_short\")\n",
        "\n",
        "# align to pos index (we print using the intersection)\n",
        "idx = posdf.index.intersection(signed_signal.index)\n",
        "if idx.empty:\n",
        "    # fallback to signal index\n",
        "    idx = signed_signal.index\n",
        "\n",
        "sig_view = pd.DataFrame({\n",
        "    \"p_long\": p_long.reindex(idx).fillna(0.0),\n",
        "    \"p_short\": p_short.reindex(idx).fillna(0.0),\n",
        "    \"signed_signal\": signed_signal.reindex(idx).fillna(0.0),\n",
        "    \"position\": posdf[pos_col].reindex(idx).fillna(0.0)\n",
        "}, index=idx)\n",
        "\n",
        "# compute recommended manual action and suggested relative size (simple linear mapping)\n",
        "def recommend_action(val, threshold):\n",
        "    if abs(val) < threshold:\n",
        "        return \"HOLD\"\n",
        "    return \"BUY\" if val > 0 else \"SELL\"\n",
        "\n",
        "# suggested notional fraction (example): map position value -> fraction (tweak to taste)\n",
        "MAX_NOTIONAL_FRAC = 0.05   # 5% of equity when pos==1\n",
        "def suggested_notional_frac(pos_value):\n",
        "    # normalize by max observed abs position (avoid surprises)\n",
        "    max_abs = max(1.0, np.nanmax(np.abs(sig_view[\"position\"].values)))\n",
        "    return float(np.clip(pos_value / max_abs, -1.0, 1.0)) * MAX_NOTIONAL_FRAC\n",
        "\n",
        "sig_view[\"action\"] = sig_view[\"signed_signal\"].apply(lambda v: recommend_action(v, THRESHOLD))\n",
        "sig_view[\"suggested_frac\"] = sig_view[\"position\"].apply(suggested_notional_frac)\n",
        "\n",
        "# price fetch (optional)\n",
        "price_info = None\n",
        "if USE_CCXT:\n",
        "    price_info = fetch_price_ccxt(EXCHANGE_ID, SYMBOL)\n",
        "    if not price_info[\"ok\"]:\n",
        "        # print informative message but continue\n",
        "        print(\"CCXT price fetch failed:\", price_info.get(\"error\"))\n",
        "    else:\n",
        "        print(f\"Reference price for {SYMBOL}: {price_info['price']} (exchange={EXCHANGE_ID})\")\n",
        "\n",
        "# print header + short diagnostic\n",
        "print(\"Source files:\", sig_file.name, \"|\", pos_file.name)\n",
        "print(\"Using pos column:\", pos_col)\n",
        "# warn if pos is nonzero for entire sample (buy-and-hold style)\n",
        "nonzero_frac = (sig_view[\"position\"].abs() > 1e-9).mean()\n",
        "if nonzero_frac > 0.99:\n",
        "    print(\"WARNING: position non-zero across ~100% of timestamps -> likely ensemble/buy-and-hold scaling (not event-level).\")\n",
        "print(f\"THRESHOLD = {THRESHOLD:.6f} | Latest {LATEST_N} rows (most recent last):\\n\")\n",
        "\n",
        "# prepare printable slice (last N)\n",
        "out_df = sig_view.tail(LATEST_N).copy()\n",
        "# format columns for human readable print\n",
        "out_df_print = out_df.reset_index().rename(columns={\"index\":\"ts\"})\n",
        "out_df_print[\"ts\"] = out_df_print[\"ts\"].dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
        "out_df_print[\"signed_signal\"] = out_df_print[\"signed_signal\"].map(lambda x: f\"{x:.6f}\")\n",
        "out_df_print[\"p_long\"] = out_df_print[\"p_long\"].map(lambda x: f\"{x:.6f}\")\n",
        "out_df_print[\"p_short\"] = out_df_print[\"p_short\"].map(lambda x: f\"{x:.6f}\")\n",
        "out_df_print[\"position\"] = out_df_print[\"position\"].map(lambda x: f\"{x:.6f}\")\n",
        "out_df_print[\"suggested_frac\"] = out_df_print[\"suggested_frac\"].map(lambda x: f\"{x:.6f}\")\n",
        "# print nicely\n",
        "print(out_df_print[[\"ts\",\"p_long\",\"p_short\",\"signed_signal\",\"position\",\"action\",\"suggested_frac\"]].to_string(index=False))\n",
        "\n",
        "# optionally save a simple CSV of latest signals for manual copy/paste\n",
        "export_csv = OUT_DIR / f\"manual_signals_latest_{pd.Timestamp.utcnow().strftime('%Y%m%dT%H%M%SZ')}.csv\"\n",
        "out_df[[\"p_long\",\"p_short\",\"signed_signal\",\"position\",\"action\",\"suggested_frac\"]].to_csv(export_csv)\n",
        "print(\"\\nSaved latest manual signals ->\", export_csv.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFB2c5t8G06j",
        "outputId": "b0c2ee52-904b-4c4e-d8ff-9ce3418b898a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference price for BTC/USDT: 91931.0 (exchange=bitfinex)\n",
            "Source files: Aplus_signals_aligned_1764901821.csv | A_SRC_positions_reconstructed_est_1764912933.csv\n",
            "Using pos column: pos_orig\n",
            "THRESHOLD = 0.010000 | Latest 50 rows (most recent last):\n",
            "\n",
            "                      ts   p_long  p_short signed_signal  position action suggested_frac\n",
            "2025-12-02 23:00:00+0000 0.475414 0.565691     -0.090277 -0.381151   SELL      -0.019058\n",
            "2025-12-03 00:00:00+0000 0.439371 0.604237     -0.164865 -0.379563   SELL      -0.018978\n",
            "2025-12-03 01:00:00+0000 0.472212 0.566785     -0.094573 -0.379177   SELL      -0.018959\n",
            "2025-12-03 02:00:00+0000 0.470145 0.559690     -0.089545 -0.379564   SELL      -0.018978\n",
            "2025-12-03 03:00:00+0000 0.470145 0.559690     -0.089545 -0.380618   SELL      -0.019031\n",
            "2025-12-03 04:00:00+0000 0.470145 0.559376     -0.089231 -0.380341   SELL      -0.019017\n",
            "2025-12-03 05:00:00+0000 0.470145 0.559376     -0.089231 -0.380701   SELL      -0.019035\n",
            "2025-12-03 06:00:00+0000 0.470145 0.559376     -0.089231 -0.380699   SELL      -0.019035\n",
            "2025-12-03 07:00:00+0000 0.472957 0.557105     -0.084148 -0.381359   SELL      -0.019068\n",
            "2025-12-03 08:00:00+0000 0.484924 0.542183     -0.057259 -0.382339   SELL      -0.019117\n",
            "2025-12-03 09:00:00+0000 0.484033 0.543145     -0.059112 -0.382426   SELL      -0.019121\n",
            "2025-12-03 10:00:00+0000 0.482014 0.543145     -0.061131 -0.381933   SELL      -0.019097\n",
            "2025-12-03 11:00:00+0000 0.489190 0.536128     -0.046938 -0.381492   SELL      -0.019075\n",
            "2025-12-03 12:00:00+0000 0.489190 0.536128     -0.046938 -0.383283   SELL      -0.019164\n",
            "2025-12-03 13:00:00+0000 0.489190 0.536128     -0.046938 -0.382272   SELL      -0.019114\n",
            "2025-12-03 14:00:00+0000 0.502957 0.529294     -0.026337  0.000000   SELL       0.000000\n",
            "2025-12-03 15:00:00+0000 0.493698 0.529294     -0.035597  0.000000   SELL       0.000000\n",
            "2025-12-03 16:00:00+0000 0.482387 0.549631     -0.067244  0.000000   SELL       0.000000\n",
            "2025-12-03 17:00:00+0000 0.494807 0.537363     -0.042556  0.000000   SELL       0.000000\n",
            "2025-12-03 18:00:00+0000 0.482387 0.553170     -0.070783  0.000000   SELL       0.000000\n",
            "2025-12-03 19:00:00+0000 0.480861 0.558760     -0.077899  0.000000   SELL       0.000000\n",
            "2025-12-03 20:00:00+0000 0.487242 0.554679     -0.067437  0.000000   SELL       0.000000\n",
            "2025-12-03 21:00:00+0000 0.487242 0.554679     -0.067437  0.000000   SELL       0.000000\n",
            "2025-12-03 22:00:00+0000 0.487242 0.554679     -0.067437  0.000000   SELL       0.000000\n",
            "2025-12-03 23:00:00+0000 0.484948 0.558317     -0.073369 -0.413068   SELL      -0.020653\n",
            "2025-12-04 00:00:00+0000 0.488776 0.554355     -0.065579 -0.414494   SELL      -0.020725\n",
            "2025-12-04 01:00:00+0000 0.488776 0.554355     -0.065579 -0.414490   SELL      -0.020724\n",
            "2025-12-04 02:00:00+0000 0.477112 0.561467     -0.084355 -0.414309   SELL      -0.020715\n",
            "2025-12-04 03:00:00+0000 0.474441 0.567963     -0.093522 -0.413805   SELL      -0.020690\n",
            "2025-12-04 04:00:00+0000 0.490638 0.532778     -0.042140 -0.413785   SELL      -0.020689\n",
            "2025-12-04 05:00:00+0000 0.486215 0.535255     -0.049040 -0.414074   SELL      -0.020704\n",
            "2025-12-04 06:00:00+0000 0.479052 0.540187     -0.061136 -0.414423   SELL      -0.020721\n",
            "2025-12-04 07:00:00+0000 0.477793 0.545627     -0.067834 -0.414001   SELL      -0.020700\n",
            "2025-12-04 08:00:00+0000 0.490680 0.531438     -0.040758 -0.414450   SELL      -0.020723\n",
            "2025-12-04 09:00:00+0000 0.498853 0.507537     -0.008684 -0.414574   HOLD      -0.020729\n",
            "2025-12-04 10:00:00+0000 0.498853 0.507537     -0.008684 -0.414540   HOLD      -0.020727\n",
            "2025-12-04 11:00:00+0000 0.495059 0.511997     -0.016937 -0.414468   SELL      -0.020723\n",
            "2025-12-04 12:00:00+0000 0.495059 0.508582     -0.013522 -0.414458   SELL      -0.020723\n",
            "2025-12-04 13:00:00+0000 0.495059 0.508582     -0.013522 -0.414366   SELL      -0.020718\n",
            "2025-12-04 14:00:00+0000 0.499608 0.506095     -0.006487 -0.413412   HOLD      -0.020671\n",
            "2025-12-04 15:00:00+0000 0.496285 0.521533     -0.025248 -0.413413   SELL      -0.020671\n",
            "2025-12-04 16:00:00+0000 0.501422 0.514645     -0.013223 -0.413164   SELL      -0.020658\n",
            "2025-12-04 17:00:00+0000 0.492068 0.518496     -0.026427 -0.412900   SELL      -0.020645\n",
            "2025-12-04 18:00:00+0000 0.493108 0.524351     -0.031243 -0.412818   SELL      -0.020641\n",
            "2025-12-04 19:00:00+0000 0.498996 0.514182     -0.015186 -0.412876   SELL      -0.020644\n",
            "2025-12-04 20:00:00+0000 0.495561 0.522399     -0.026838 -0.412785   SELL      -0.020639\n",
            "2025-12-04 21:00:00+0000 0.497390 0.520604     -0.023214 -0.413582   SELL      -0.020679\n",
            "2025-12-04 22:00:00+0000 0.503335 0.506653     -0.003318 -0.414059   HOLD      -0.020703\n",
            "2025-12-04 23:00:00+0000 0.502225 0.508250     -0.006025 -0.414533   HOLD      -0.020727\n",
            "2025-12-05 00:00:00+0000 0.507828 0.506741      0.001087 -0.414869   HOLD      -0.020743\n",
            "\n",
            "Saved latest manual signals -> manual_signals_latest_20251205T060325Z.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# live_signal_from_existing_pipeline.py\n",
        "# Run in the same environment where your pipeline modules and model artifacts live.\n",
        "# Usage: edit CONFIG below (EXCHANGE / SYMBOL / PATHS) and run.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# optional deps\n",
        "try:\n",
        "    import ccxt\n",
        "except Exception:\n",
        "    ccxt = None\n",
        "\n",
        "# model loaders\n",
        "try:\n",
        "    import joblib\n",
        "except Exception:\n",
        "    joblib = None\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")  # change if needed\n",
        "MODEL_GLOB = OUT_DIR / \"A_SRC_model_*.joblib\"   # try to find your trained model\n",
        "XGB_GLOB = OUT_DIR / \"A_SRC_model_xgb_*.json\"   # alternate\n",
        "BEST_PARAMS_GLOB = OUT_DIR / \"A_SRC_optuna_best_params_penalized_*.json\"\n",
        "FEATURE_MODULES = [\n",
        "    \"step04_final\",      # guesses from convo\n",
        "    \"mtb_step04\",\n",
        "    \"features\",\n",
        "    \"feature_pipeline\"\n",
        "]\n",
        "FEATURE_FUNC_NAMES = [\"build_features_live\", \"build_features\", \"make_features\"]\n",
        "POS_FUNC_NAMES = [\"compute_positions_from_params\", \"compute_positions\", \"positions_from_signal\"]\n",
        "VOL_SCALE_FUNC_NAMES = [\"vol_scale_positions\", \"apply_vol_scaling\"]\n",
        "\n",
        "# CCXT settings (optional). If ccxt not available or you prefer CSV, set USE_CCXT=False.\n",
        "USE_CCXT = True\n",
        "EXCHANGE_ID = \"bitfinex\"\n",
        "SYMBOL = \"BTC/USDT\"      # change to your asset\n",
        "TIMEFRAME = \"1h\"         # must match training bars\n",
        "SINCE = None             # ms since epoch, or None -> last N bars\n",
        "FETCH_LIMIT = 5000\n",
        "\n",
        "# safety\n",
        "EXEC_SHIFT = 1  # conservative execution shift\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def find_callable(modules, func_names):\n",
        "    \"\"\"Try import candidate modules and return first callable matching func_names.\"\"\"\n",
        "    for mod_name in modules:\n",
        "        try:\n",
        "            mod = __import__(mod_name, fromlist=[\"*\"])\n",
        "        except Exception:\n",
        "            continue\n",
        "        for fn in func_names:\n",
        "            if hasattr(mod, fn):\n",
        "                return getattr(mod, fn), mod_name, fn\n",
        "    return None, None, None\n",
        "\n",
        "def find_latest(glob_pattern):\n",
        "    hits = list(Path().glob(str(glob_pattern)))\n",
        "    if not hits:\n",
        "        return None\n",
        "    hits = sorted(hits, key=lambda p: p.stat().st_mtime)\n",
        "    return hits[-1]\n",
        "\n",
        "def load_json_latest(pattern):\n",
        "    p = find_latest(pattern)\n",
        "    if p is None:\n",
        "        return None, None\n",
        "    return json.load(open(p, \"r\")), p\n",
        "\n",
        "# ---------------- main logic ----------------\n",
        "def fetch_ohlcv_ccxt(exchange_id, symbol, timeframe, since=None, limit=2000):\n",
        "    if ccxt is None:\n",
        "        raise RuntimeError(\"ccxt not installed in environment. Set USE_CCXT = False or install ccxt.\")\n",
        "    ex = getattr(ccxt, exchange_id)()\n",
        "    # user credentials if needed: ex.apiKey = \"...\"; ex.secret = \"...\"\n",
        "    ohlcv = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=limit)\n",
        "    df = pd.DataFrame(ohlcv, columns=[\"ts\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "    df[\"datetime\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True)\n",
        "    df = df.set_index(\"datetime\").drop(columns=[\"ts\"])\n",
        "    return df\n",
        "\n",
        "def load_recent_local_ohlcv(out_dir, pattern=\"ohlcv_*.csv\"):\n",
        "    hits = sorted(out_dir.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
        "    if not hits:\n",
        "        raise FileNotFoundError(\"No local ohlcv CSV found in OUT_DIR with pattern \" + pattern)\n",
        "    df = pd.read_csv(hits[-1], index_col=0)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    return df\n",
        "\n",
        "def load_model_candidate():\n",
        "    m = find_latest(MODEL_GLOB)\n",
        "    if m is not None:\n",
        "        try:\n",
        "            return joblib.load(m), str(m)\n",
        "        except Exception:\n",
        "            pass\n",
        "    x = find_latest(XGB_GLOB)\n",
        "    if x is not None:\n",
        "        try:\n",
        "            # xgboost JSON dump -> load Booster\n",
        "            if xgb is None:\n",
        "                raise RuntimeError(\"xgboost not installed\")\n",
        "            booster = xgb.Booster()\n",
        "            booster.load_model(str(x))\n",
        "            return booster, str(x)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None, None\n",
        "\n",
        "def predict_model(model, X):\n",
        "    \"\"\"Try joblib/sklearn API, then xgboost Booster\"\"\"\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"model is None\")\n",
        "    # sklearn-like predict_proba\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        # assume binary long model, return proba of positive class\n",
        "        proba = model.predict_proba(X)\n",
        "        # if proba shape Nx2 -> take second column\n",
        "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
        "            return proba[:, 1]\n",
        "        return proba.ravel()\n",
        "    if hasattr(model, \"predict\"):\n",
        "        return model.predict(X)\n",
        "    # xgboost Booster\n",
        "    if isinstance(model, xgb.Booster):\n",
        "        dmatrix = xgb.DMatrix(X)\n",
        "        out = model.predict(dmatrix)\n",
        "        return out.ravel()\n",
        "    raise RuntimeError(\"Unknown model API; please provide a sklearn-like or xgboost.Booster model\")\n",
        "\n",
        "# ---------------- run ----------------\n",
        "def run_live(use_ccxt=USE_CCXT):\n",
        "    # 1) get raw bars\n",
        "    if use_ccxt:\n",
        "        print(\"[INFO] fetching OHLCV via ccxt:\", EXCHANGE_ID, SYMBOL, TIMEFRAME)\n",
        "        df = fetch_ohlcv_ccxt(EXCHANGE_ID, SYMBOL, TIMEFRAME, since=SINCE, limit=FETCH_LIMIT)\n",
        "    else:\n",
        "        print(\"[INFO] loading local OHLCV CSV from OUT_DIR\")\n",
        "        df = load_recent_local_ohlcv(OUT_DIR)\n",
        "\n",
        "    print(\"[INFO] bars:\", df.shape)\n",
        "\n",
        "    # 2) locate feature builder & position builder\n",
        "    feat_fn, feat_mod, feat_name = find_callable(FEATURE_MODULES, FEATURE_FUNC_NAMES)\n",
        "    pos_fn, pos_mod, pos_name = find_callable(FEATURE_MODULES, POS_FUNC_NAMES)\n",
        "    vol_fn, vol_mod, vol_name = find_callable(FEATURE_MODULES, VOL_SCALE_FUNC_NAMES)\n",
        "\n",
        "    if feat_fn is None:\n",
        "        raise RuntimeError(\n",
        "            \"Feature builder not found. Please expose a function in one of modules: \"\n",
        "            f\"{FEATURE_MODULES} with name in {FEATURE_FUNC_NAMES}.\\n\"\n",
        "            \"Example to paste in your repo:\\n\\n\"\n",
        "            \"def build_features_live(df_bars):\\n\"\n",
        "            \"    # return DataFrame of features indexed like df_bars (last row = newest)\\n\"\n",
        "            \"    ...\\n\"\n",
        "        )\n",
        "    print(f\"[INFO] using feature builder {feat_name} from module {feat_mod}\")\n",
        "\n",
        "    # 3) build features (causal) for the newest bar(s)\n",
        "    feat_df = feat_fn(df.copy())\n",
        "    # Expect feat_df to have same index and feature columns ordering as model.\n",
        "    if not isinstance(feat_df, pd.DataFrame):\n",
        "        raise RuntimeError(\"build_features_live must return a pandas.DataFrame (rows=indexed by bar datetime)\")\n",
        "    print(\"[INFO] features built shape:\", feat_df.shape)\n",
        "\n",
        "    # 4) load model(s)\n",
        "    model_obj, model_path = load_model_candidate()\n",
        "    if model_obj is None:\n",
        "        raise RuntimeError(\"No model found. Place your model file under OUT_DIR matching patterns or load with joblib/xgboost.\")\n",
        "    print(\"[INFO] loaded model:\", model_path)\n",
        "\n",
        "    # 5) attempt to predict p_long / p_short\n",
        "    # Try: model predicts p_long directly if multi-output; otherwise single model -> we assume it's p_long.\n",
        "    # If your pipeline had separate long/short models named e.g. model_long / model_short, modify to load both.\n",
        "    X = feat_df.fillna(0.0).astype(float)\n",
        "    preds = predict_model(model_obj, X)\n",
        "    # If preds length equals columns of X -> ambiguous. We assume preds is per-row probability/score.\n",
        "    p_long = pd.Series(preds, index=X.index, name=\"p_long\")\n",
        "    # fallback: if model outputs two columns via joblib, handle it earlier (predict_proba handled)\n",
        "    # p_short not available -> assume symmetric: p_short = 1 - p_long if model was binary proba for long vs short\n",
        "    p_short = 1.0 - p_long\n",
        "\n",
        "    # 6) combine to signed signal\n",
        "    signal = (p_long - p_short).rename(\"signal\")\n",
        "    print(\"[INFO] signal computed; head:\", signal.iloc[-5:].to_dict())\n",
        "\n",
        "    # 7) load best params (optuna) if present\n",
        "    best_params, param_path = load_json_latest(BEST_PARAMS_GLOB)\n",
        "    if best_params is None:\n",
        "        print(\"[WARN] best params JSON not found. Using conservative defaults.\")\n",
        "        best_params = {\n",
        "            \"k\": 1.0, \"tau\": 0.2, \"L\": 1.0, \"WINDOW\": 90,\n",
        "            \"threshold\": 0.0, \"tc\": 0.0005, \"target_vol\": 0.4,\n",
        "            \"scale_clip_low\": 0.4, \"scale_clip_high\": 2.0\n",
        "        }\n",
        "    else:\n",
        "        print(\"[INFO] loaded best params from\", param_path)\n",
        "\n",
        "    # 8) compute positions: prefer pipeline's compute_positions_from_params if present\n",
        "    if pos_fn is not None:\n",
        "        print(f\"[INFO] using pipeline position builder {pos_name} from {pos_mod}\")\n",
        "        # try to call pos_fn with signature (params, p_long_series, p_short_series, vol_series_optional)\n",
        "        try:\n",
        "            # compute causal vol for scaling inside function (pass rets if available not known here)\n",
        "            pos_series = pos_fn(best_params, p_long, p_short)  # adapt depending on your function signature\n",
        "            if isinstance(pos_series, pd.DataFrame):\n",
        "                # pick 'position' or first numeric col\n",
        "                cols = [c for c in pos_series.columns if np.issubdtype(pos_series[c].dtype, np.number)]\n",
        "                pos_series = pos_series[cols[0]]\n",
        "            pos_series = pos_series.reindex(signal.index).fillna(0.0).astype(float)\n",
        "        except TypeError:\n",
        "            # fallback: try signature (p_long, p_short, params)\n",
        "            pos_series = pos_fn(p_long, p_short, best_params)\n",
        "    else:\n",
        "        print(\"[WARN] position builder not found in pipeline. Using safe fallback: threshold -> sign*L\")\n",
        "        thr = float(best_params.get(\"threshold\", 0.0))\n",
        "        L = float(best_params.get(\"L\", 1.0))\n",
        "        pos_series = (signal.abs() >= thr).astype(float) * signal.apply(np.sign) * L\n",
        "\n",
        "    # 9) vol-scaling: prefer vol_fn if available\n",
        "    if vol_fn is not None:\n",
        "        print(f\"[INFO] applying pipeline vol-scaling {vol_name} from {vol_mod}\")\n",
        "        try:\n",
        "            pos_scaled = vol_fn(pos_series, best_params)  # adapt to actual signature\n",
        "        except TypeError:\n",
        "            # some pipelines expect (pos_series, target_vol)\n",
        "            pos_scaled = vol_fn(pos_series, best_params.get(\"target_vol\", 0.4))\n",
        "    else:\n",
        "        # simple naive vol-scaling (causal): scale by target_vol / realized_ann_vol of (pos*ret)\n",
        "        print(\"[INFO] applying local naive vol-scaling fallback\")\n",
        "        # we need returns series; try to load meta with tb_ret_8\n",
        "        meta_file = find_latest(OUT_DIR/\"df_meta_shortlist.v*.pkl\")\n",
        "        if meta_file is not None:\n",
        "            meta = pd.read_pickle(meta_file)\n",
        "            meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "            # prefer tb_ret_8\n",
        "            ret_col = next((c for c in meta.columns if c.startswith(\"tb_ret_\")), None)\n",
        "            if ret_col is None:\n",
        "                raise RuntimeError(\"meta found but no tb_ret_* column\")\n",
        "            rets = meta[ret_col].reindex(pos_series.index).fillna(0.0).astype(float)\n",
        "        else:\n",
        "            raise RuntimeError(\"Cannot vol-scale: no meta returns found in OUT_DIR. Set vol_fn in pipeline or provide meta pickle.\")\n",
        "\n",
        "        pnl = (pos_series.shift(EXEC_SHIFT).fillna(0.0) * rets).fillna(0.0)\n",
        "        realized_ann_vol = float(np.nanstd(pnl) * np.sqrt(756)) if np.nanstd(pnl) > 0 else 0.0\n",
        "        target_vol = float(best_params.get(\"target_vol\", 0.4))\n",
        "        if realized_ann_vol > 0:\n",
        "            scale = float(target_vol / realized_ann_vol)\n",
        "        else:\n",
        "            scale = 1.0\n",
        "        scale = float(np.clip(scale, best_params.get(\"scale_clip_low\", 0.4), best_params.get(\"scale_clip_high\", 2.0)))\n",
        "        pos_scaled = (pos_series * scale).clip(-best_params.get(\"L\", 1.0), best_params.get(\"L\", 1.0))\n",
        "\n",
        "    # 10) final human-readable signal: last row\n",
        "    last_ts = pos_scaled.index[-1]\n",
        "    last_pos = float(pos_scaled.iloc[-1])\n",
        "    conf = float(signal.iloc[-1])  # rough confidence\n",
        "    action = \"HOLD\"\n",
        "    if last_pos > 0:\n",
        "        action = f\"BUY  size={last_pos:.4f}\"\n",
        "    elif last_pos < 0:\n",
        "        action = f\"SELL size={last_pos:.4f}\"\n",
        "    print(f\"[SIGNAL @ {last_ts.isoformat()}] {action}  (conf={conf:.4f})\")\n",
        "\n",
        "    # 11) save outputs\n",
        "    out_df = pd.DataFrame({\"position\": pos_scaled, \"signal\": signal, \"p_long\": p_long, \"p_short\": p_short})\n",
        "    ts = time.strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    out_path = OUT_DIR / f\"A_SRC_live_signal_{ts}.csv\"\n",
        "    out_df.to_csv(out_path)\n",
        "    print(\"[SAVED] live signal ->\", out_path)\n",
        "\n",
        "    # 12) quick diagnostics\n",
        "    diag = {\n",
        "        \"model_path\": model_path,\n",
        "        \"best_params\": best_params,\n",
        "        \"last_ts\": str(last_ts),\n",
        "        \"last_pos\": last_pos,\n",
        "        \"conf\": conf,\n",
        "        \"n_bars\": len(out_df)\n",
        "    }\n",
        "    diag_path = OUT_DIR / f\"A_SRC_live_signal_diag_{ts}.json\"\n",
        "    json.dump(diag, open(diag_path, \"w\"), indent=2)\n",
        "    print(\"[SAVED] diag ->\", diag_path)\n",
        "\n",
        "    return out_df, diag\n",
        "\n",
        "# run if script executed\n",
        "if __name__ == \"__main__\":\n",
        "    out, diag = run_live(use_ccxt=USE_CCXT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "YySYeIfAIDZT",
        "outputId": "f1fe966f-7ab5-414b-8c58-61d44d78d580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] fetching OHLCV via ccxt: bitfinex BTC/USDT 1h\n",
            "[INFO] bars: (5000, 5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Feature builder not found. Please expose a function in one of modules: ['step04_final', 'mtb_step04', 'features', 'feature_pipeline'] with name in ['build_features_live', 'build_features', 'make_features'].\nExample to paste in your repo:\n\ndef build_features_live(df_bars):\n    # return DataFrame of features indexed like df_bars (last row = newest)\n    ...\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4045474916.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;31m# run if script executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_ccxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_CCXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4045474916.py\u001b[0m in \u001b[0;36mrun_live\u001b[0;34m(use_ccxt)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeat_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0;34m\"Feature builder not found. Please expose a function in one of modules: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;34mf\"{FEATURE_MODULES} with name in {FEATURE_FUNC_NAMES}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Feature builder not found. Please expose a function in one of modules: ['step04_final', 'mtb_step04', 'features', 'feature_pipeline'] with name in ['build_features_live', 'build_features', 'make_features'].\nExample to paste in your repo:\n\ndef build_features_live(df_bars):\n    # return DataFrame of features indexed like df_bars (last row = newest)\n    ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "LIVE SIGNAL ADAPTER — adapted to user's pipeline\n",
        "- Places to edit: OUT_DIR, EXCHANGE credentials (if using ccxt), SYMBOL, TIMEFRAME\n",
        "- Attempts to auto-discover: feature builder (step04_final / mtb_step04), model artifacts, optuna params\n",
        "- Safe defaults and clear errors/warnings if something missing\n",
        "\n",
        "Run in same environment where your pipeline repo is mounted (Colab / local).\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# optional deps\n",
        "try:\n",
        "    import ccxt\n",
        "except Exception:\n",
        "    ccxt = None\n",
        "\n",
        "try:\n",
        "    import joblib\n",
        "except Exception:\n",
        "    joblib = None\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# patterns used in our pipeline (adapted from conversation)\n",
        "MODEL_GLOB = str(OUT_DIR / \"A_SRC_model_*.joblib\")\n",
        "XGB_GLOB = str(OUT_DIR / \"A_SRC_model_xgb_*.json\")\n",
        "BEST_PARAMS_GLOB = str(OUT_DIR / \"A_SRC_optuna_best_params_penalized_*.json\")\n",
        "META_GLOB = str(OUT_DIR / \"df_meta_shortlist.v*.pkl\")\n",
        "POSITIONS_GLOB = str(OUT_DIR / \"A_SRC_positions_*.csv\")\n",
        "SIGNAL_GLOB = str(OUT_DIR / \"A_SRC_signal_opt_signals_*.csv\")\n",
        "\n",
        "# feature modules we try (from convo)\n",
        "FEATURE_MODULES = [\n",
        "    \"step04_final\",\n",
        "    \"mtb_step04\",\n",
        "    \"features\",\n",
        "    \"feature_pipeline\",\n",
        "]\n",
        "FEATURE_FUNC_NAMES = [\"build_features_live\", \"build_features\", \"make_features\"]\n",
        "POS_FUNC_NAMES = [\"compute_positions_from_params\", \"compute_positions\", \"positions_from_signal\", \"pos_from_signal\"]\n",
        "VOL_SCALE_FUNC_NAMES = [\"vol_scale_positions\", \"apply_vol_scaling\", \"vol_scale\"]\n",
        "\n",
        "# CCXT settings\n",
        "USE_CCXT = True\n",
        "EXCHANGE_ID = \"bitfinex\"\n",
        "SYMBOL = \"BTC/USDT\"\n",
        "TIMEFRAME = \"1h\"\n",
        "SINCE = None\n",
        "FETCH_LIMIT = 1500\n",
        "\n",
        "# conservative execution shift (use previous bar features to avoid lookahead)\n",
        "EXEC_SHIFT = 1\n",
        "\n",
        "# human-friendly defaults\n",
        "TC_DEFAULT = 0.0012  # round-trip fee default (0.12%)\n",
        "TARGET_VOL_DEFAULT = 0.2\n",
        "LEV_DEFAULT = 1.0\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "\n",
        "def find_callable(modules, func_names):\n",
        "    for mod_name in modules:\n",
        "        try:\n",
        "            mod = __import__(mod_name, fromlist=[\"*\"])\n",
        "        except Exception:\n",
        "            continue\n",
        "        for fn in func_names:\n",
        "            if hasattr(mod, fn):\n",
        "                return getattr(mod, fn), mod_name, fn\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "def find_latest_glob(pattern: str) -> Optional[Path]:\n",
        "    hits = sorted(Path().glob(pattern), key=lambda p: p.stat().st_mtime) if list(Path().glob(pattern)) else []\n",
        "    return hits[-1] if hits else None\n",
        "\n",
        "\n",
        "def load_json_latest(pattern: str) -> Tuple[Optional[dict], Optional[Path]]:\n",
        "    p = find_latest_glob(pattern)\n",
        "    if p is None:\n",
        "        return None, None\n",
        "    return json.load(open(p, \"r\")), p\n",
        "\n",
        "\n",
        "def fetch_ohlcv_ccxt(exchange_id, symbol, timeframe, since=None, limit=1500):\n",
        "    if ccxt is None:\n",
        "        raise RuntimeError(\"ccxt not installed. Set USE_CCXT=False or install ccxt in environment.\")\n",
        "    Exchange = getattr(ccxt, exchange_id)\n",
        "    ex = Exchange()\n",
        "    # if credentials needed, user must set ex.apiKey / ex.secret before calling\n",
        "    ohlcv = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=limit)\n",
        "    df = pd.DataFrame(ohlcv, columns=[\"ts\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "    df[\"datetime\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True)\n",
        "    df = df.set_index(\"datetime\").drop(columns=[\"ts\"])\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_recent_local_ohlcv(out_dir: Path, pattern=\"ohlcv_*.csv\"):\n",
        "    hits = sorted(out_dir.glob(pattern), key=lambda p: p.stat().st_mtime)\n",
        "    if not hits:\n",
        "        raise FileNotFoundError(\"No local OHLCV CSV found in OUT_DIR with pattern \" + pattern)\n",
        "    df = pd.read_csv(hits[-1], index_col=0)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_model_candidate() -> Tuple[Optional[object], Optional[Path]]:\n",
        "    p = find_latest_glob(MODEL_GLOB)\n",
        "    if p is not None and joblib is not None:\n",
        "        try:\n",
        "            return joblib.load(p), p\n",
        "        except Exception:\n",
        "            pass\n",
        "    p2 = find_latest_glob(XGB_GLOB)\n",
        "    if p2 is not None and xgb is not None:\n",
        "        try:\n",
        "            booster = xgb.Booster()\n",
        "            booster.load_model(str(p2))\n",
        "            return booster, p2\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def predict_model(model, X: pd.DataFrame) -> np.ndarray:\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"model is None\")\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba = model.predict_proba(X)\n",
        "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
        "            return proba[:, 1]\n",
        "        return proba.ravel()\n",
        "    if hasattr(model, \"predict\"):\n",
        "        return model.predict(X)\n",
        "    if xgb is not None and isinstance(model, xgb.Booster):\n",
        "        dmat = xgb.DMatrix(X)\n",
        "        out = model.predict(dmat)\n",
        "        return out.ravel()\n",
        "    raise RuntimeError(\"Unknown model API; please supply sklearn-like or xgboost Booster model\")\n",
        "\n",
        "# ---------------- main --------------------------------\n",
        "\n",
        "def run_live(use_ccxt: bool = USE_CCXT, out_dir: Path = OUT_DIR, safe_mode: bool = True):\n",
        "    # 1) load bars\n",
        "    if use_ccxt:\n",
        "        print(f\"[INFO] fetching OHLCV via ccxt: {EXCHANGE_ID} {SYMBOL} {TIMEFRAME}\")\n",
        "        bars = fetch_ohlcv_ccxt(EXCHANGE_ID, SYMBOL, TIMEFRAME, since=SINCE, limit=FETCH_LIMIT)\n",
        "    else:\n",
        "        print(\"[INFO] loading local OHLCV CSV from OUT_DIR\")\n",
        "        bars = load_recent_local_ohlcv(out_dir)\n",
        "    print(\"[INFO] bars:\", bars.shape)\n",
        "\n",
        "    # 2) find pipeline functions\n",
        "    feat_fn, feat_mod, feat_name = find_callable(FEATURE_MODULES, FEATURE_FUNC_NAMES)\n",
        "    pos_fn, pos_mod, pos_name = find_callable(FEATURE_MODULES, POS_FUNC_NAMES)\n",
        "    vol_fn, vol_mod, vol_name = find_callable(FEATURE_MODULES, VOL_SCALE_FUNC_NAMES)\n",
        "\n",
        "    if feat_fn is None:\n",
        "        raise RuntimeError(\"Feature builder not found. Expose build_features_live(df) in one of: \" + str(FEATURE_MODULES))\n",
        "    print(f\"[INFO] using feature builder {feat_name} from {feat_mod}\")\n",
        "\n",
        "    # 3) build features (causal) — feature builder expected to be causal\n",
        "    feat_df = feat_fn(bars.copy())\n",
        "    if not isinstance(feat_df, pd.DataFrame):\n",
        "        raise RuntimeError(\"build_features_live must return a pandas.DataFrame indexed by bar datetime\")\n",
        "    feat_df.index = pd.to_datetime(feat_df.index, utc=True)\n",
        "    print(\"[INFO] features built shape:\", feat_df.shape)\n",
        "\n",
        "    # 4) load model\n",
        "    model_obj, model_path = load_model_candidate()\n",
        "    if model_obj is None:\n",
        "        raise RuntimeError(\"No model artifact found in OUT_DIR. Place joblib/xgboost model with naming pattern.\")\n",
        "    print(\"[INFO] loaded model:\", model_path)\n",
        "\n",
        "    # 5) predict\n",
        "    X = feat_df.fillna(0.0).astype(float)\n",
        "    preds = predict_model(model_obj, X)\n",
        "    p_long = pd.Series(preds, index=X.index, name=\"p_long\")\n",
        "    # if predict returned scores outside [0,1], try transform to proba-ish via sigmoid\n",
        "    if p_long.max() > 1.0 or p_long.min() < 0.0:\n",
        "        p_long = 1.0 / (1.0 + np.exp(-p_long))\n",
        "    p_short = 1.0 - p_long\n",
        "    signal = (p_long - p_short).rename(\"signal\")\n",
        "\n",
        "    # 6) load optuna params if present\n",
        "    best_params, param_path = load_json_latest(BEST_PARAMS_GLOB)\n",
        "    if best_params is None:\n",
        "        print(\"[WARN] optuna best-params not found; using conservative defaults\")\n",
        "        best_params = {\n",
        "            \"k\": 1.0, \"tau\": 0.2, \"L\": LEV_DEFAULT, \"WINDOW\": 90,\n",
        "            \"threshold\": 0.0, \"tc\": TC_DEFAULT, \"target_vol\": TARGET_VOL_DEFAULT,\n",
        "            \"scale_clip_low\": 0.4, \"scale_clip_high\": 2.0\n",
        "        }\n",
        "    else:\n",
        "        print(\"[INFO] loaded best params from\", param_path)\n",
        "\n",
        "    # 7) compute positions: prefer pipeline's function\n",
        "    if pos_fn is not None:\n",
        "        print(f\"[INFO] using position builder {pos_name} from {pos_mod}\")\n",
        "        try:\n",
        "            pos_series = pos_fn(best_params, p_long, p_short)\n",
        "        except TypeError:\n",
        "            pos_series = pos_fn(p_long, p_short, best_params)\n",
        "        if isinstance(pos_series, pd.DataFrame):\n",
        "            numeric_cols = [c for c in pos_series.columns if np.issubdtype(pos_series[c].dtype, np.number)]\n",
        "            pos_series = pos_series[numeric_cols[0]]\n",
        "        pos_series = pos_series.reindex(signal.index).fillna(0.0).astype(float)\n",
        "    else:\n",
        "        print(\"[WARN] position builder not found; using simple threshold->sign*L fallback\")\n",
        "        thr = float(best_params.get(\"threshold\", 0.0))\n",
        "        L = float(best_params.get(\"L\", LEV_DEFAULT))\n",
        "        pos_series = (signal.abs() >= thr).astype(float) * np.sign(signal) * L\n",
        "\n",
        "    # 8) vol-scaling: prefer pipeline vol_fn\n",
        "    if vol_fn is not None:\n",
        "        try:\n",
        "            pos_scaled = vol_fn(pos_series, best_params)\n",
        "        except TypeError:\n",
        "            pos_scaled = vol_fn(pos_series, best_params.get(\"target_vol\", TARGET_VOL_DEFAULT))\n",
        "    else:\n",
        "        # fallback naive vol scale using latest meta returns\n",
        "        meta_p = find_latest_glob(META_GLOB)\n",
        "        if meta_p is None:\n",
        "            raise RuntimeError(\"No meta pickle found for vol-scaling fallback. Provide vol_fn or meta pickle in OUT_DIR.\")\n",
        "        meta = pd.read_pickle(meta_p)\n",
        "        meta.index = pd.to_datetime(meta.index, utc=True)\n",
        "        ret_col = next((c for c in meta.columns if c.startswith(\"tb_ret\")), None)\n",
        "        if ret_col is None:\n",
        "            raise RuntimeError(\"meta pickle found but no tb_ret* column for vol-scaling\")\n",
        "        rets = meta[ret_col].reindex(pos_series.index).fillna(0.0).astype(float)\n",
        "        pnl = (pos_series.shift(EXEC_SHIFT).fillna(0.0) * rets).fillna(0.0)\n",
        "        realized_ann_vol = float(np.nanstd(pnl) * np.sqrt(252 * 24)) if np.nanstd(pnl) > 0 else 0.0\n",
        "        target_vol = float(best_params.get(\"target_vol\", TARGET_VOL_DEFAULT))\n",
        "        scale = float(target_vol / realized_ann_vol) if realized_ann_vol > 0 else 1.0\n",
        "        scale = float(np.clip(scale, best_params.get(\"scale_clip_low\", 0.4), best_params.get(\"scale_clip_high\", 2.0)))\n",
        "        pos_scaled = (pos_series * scale).clip(-best_params.get(\"L\", LEV_DEFAULT), best_params.get(\"L\", LEV_DEFAULT))\n",
        "\n",
        "    # 9) outputs\n",
        "    last_ts = pos_scaled.index[-1]\n",
        "    last_pos = float(pos_scaled.iloc[-1])\n",
        "    conf = float(signal.iloc[-1])\n",
        "    action = \"HOLD\"\n",
        "    if last_pos > 0:\n",
        "        action = f\"BUY size={last_pos:.4f}\"\n",
        "    elif last_pos < 0:\n",
        "        action = f\"SELL size={last_pos:.4f}\"\n",
        "    print(f\"[SIGNAL @ {last_ts.isoformat()}] {action} (conf={conf:.4f})\")\n",
        "\n",
        "    out_df = pd.DataFrame({\"position\": pos_scaled, \"signal\": signal, \"p_long\": p_long, \"p_short\": p_short})\n",
        "    ts = time.strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    out_path = out_dir / f\"A_SRC_live_signal_{ts}.csv\"\n",
        "    out_df.to_csv(out_path)\n",
        "    print(\"[SAVED] live signal ->\", out_path)\n",
        "\n",
        "    diag = {\"model_path\": str(model_path), \"best_params\": best_params, \"last_ts\": str(last_ts), \"last_pos\": last_pos, \"conf\": conf, \"n_bars\": len(out_df)}\n",
        "    diag_path = out_dir / f\"A_SRC_live_signal_diag_{ts}.json\"\n",
        "    json.dump(diag, open(diag_path, \"w\"), indent=2)\n",
        "    print(\"[SAVED] diag ->\", diag_path)\n",
        "    return out_df, diag\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_live()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "4ey8gp5VJSUp",
        "outputId": "fe636ecb-ad66-42bf-c12b-2646b188af8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] fetching OHLCV via ccxt: bitfinex BTC/USDT 1h\n",
            "[INFO] bars: (1500, 5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Feature builder not found. Expose build_features_live(df) in one of: ['step04_final', 'mtb_step04', 'features', 'feature_pipeline']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2510744493.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mrun_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2510744493.py\u001b[0m in \u001b[0;36mrun_live\u001b[0;34m(use_ccxt, out_dir, safe_mode)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeat_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature builder not found. Expose build_features_live(df) in one of: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURE_MODULES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] using feature builder {feat_name} from {feat_mod}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Feature builder not found. Expose build_features_live(df) in one of: ['step04_final', 'mtb_step04', 'features', 'feature_pipeline']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ONE-CELL: live_builder_final_v1.py\n",
        "# Usage: paste into notebook cell and run. Adjust OUT_DIR / EXCHANGE / SYMBOL if needed.\n",
        "\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# optional libs\n",
        "try:\n",
        "    import joblib\n",
        "except Exception:\n",
        "    joblib = None\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "try:\n",
        "    import ccxt\n",
        "except Exception:\n",
        "    ccxt = None\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_GLOB_JOBLIB = OUT_DIR.glob(\"A_SRC_model_*.joblib\")\n",
        "MODEL_GLOB_XGB = OUT_DIR.glob(\"A_SRC_model_xgb_*.json\")\n",
        "BEST_PARAMS_GLOB = sorted(list(OUT_DIR.glob(\"A_SRC_optuna_best_params_*.json\")), key=lambda p: p.stat().st_mtime)\n",
        "META_GLOB = sorted(list(OUT_DIR.glob(\"df_meta_shortlist.v*.pkl\")), key=lambda p: p.stat().st_mtime)\n",
        "SIG_OUT_PREFIX = \"A_SRC_live_signal\"\n",
        "TS = time.strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "\n",
        "# CCXT\n",
        "USE_CCXT = True if ccxt is not None else False\n",
        "EXCHANGE_ID = \"bitfinex\"   # change if you want bitfinex etc.\n",
        "SYMBOL = \"BTC/USDT\"\n",
        "TIMEFRAME = \"1h\"\n",
        "FETCH_LIMIT = 1500\n",
        "\n",
        "# execution shift (conservative)\n",
        "EXEC_SHIFT = 1\n",
        "\n",
        "# vol-scaling defaults\n",
        "TARGET_VOL = 0.2\n",
        "SCALE_CLIP_LOW = 0.4\n",
        "SCALE_CLIP_HIGH = 2.0\n",
        "\n",
        "# ----------------- helpers -----------------\n",
        "def find_latest_path(glob_list):\n",
        "    return glob_list[-1] if glob_list else None\n",
        "\n",
        "def load_meta_latest() -> (pd.DataFrame, Path):\n",
        "    p = find_latest_path(META_GLOB)\n",
        "    if p is None:\n",
        "        raise FileNotFoundError(\"No df_meta_shortlist.v*.pkl found in OUT_DIR\")\n",
        "    df = pd.read_pickle(p)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "    return df, p\n",
        "\n",
        "def load_model_latest():\n",
        "    # try joblib first\n",
        "    joblibs = sorted(list(MODEL_GLOB_JOBLIB), key=lambda p: p.stat().st_mtime)\n",
        "    if joblibs:\n",
        "        p = joblibs[-1]\n",
        "        if joblib is None:\n",
        "            raise RuntimeError(\"joblib not available to load joblib model: \" + str(p))\n",
        "        m = joblib.load(p)\n",
        "        return m, p\n",
        "    # try xgb JSON\n",
        "    xgbs = sorted(list(MODEL_GLOB_XGB), key=lambda p: p.stat().st_mtime)\n",
        "    if xgbs:\n",
        "        p = xgbs[-1]\n",
        "        if xgb is None:\n",
        "            raise RuntimeError(\"xgboost not installed to load booster: \" + str(p))\n",
        "        booster = xgb.Booster()\n",
        "        booster.load_model(str(p))\n",
        "        return booster, p\n",
        "    return None, None\n",
        "\n",
        "def fetch_ohlcv_ccxt(exchange_id, symbol, timeframe, limit=1500):\n",
        "    if ccxt is None:\n",
        "        raise RuntimeError(\"ccxt not installed\")\n",
        "    ex = getattr(ccxt, exchange_id)()\n",
        "    ohlcv = ex.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)\n",
        "    df = pd.DataFrame(ohlcv, columns=[\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
        "    df[\"datetime\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True)\n",
        "    df = df.set_index(\"datetime\").drop(columns=[\"ts\"])\n",
        "    return df\n",
        "\n",
        "def predict_model(model, X: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Return 1-d array of score/prob (p_long). Accept joblib sklearn-like or xgboost.Booster\"\"\"\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"model is None\")\n",
        "    # sklearn-like\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba = model.predict_proba(X)\n",
        "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
        "            return proba[:, 1]\n",
        "        return proba.ravel()\n",
        "    if hasattr(model, \"predict\"):\n",
        "        out = model.predict(X)\n",
        "        # if multi-dim, try to reduce\n",
        "        arr = np.asarray(out)\n",
        "        if arr.ndim > 1 and arr.shape[1] >= 2:\n",
        "            return arr[:, 1]\n",
        "        return arr.ravel()\n",
        "    # xgboost booster\n",
        "    if xgb is not None and isinstance(model, xgb.Booster):\n",
        "        d = xgb.DMatrix(X)\n",
        "        out = model.predict(d)\n",
        "        return np.asarray(out).ravel()\n",
        "    raise RuntimeError(\"Unknown model API; please provide sklearn-like or xgboost.Booster\")\n",
        "\n",
        "def simple_compute_from_ohlcv(colname: str, ohlcv: pd.DataFrame):\n",
        "    \"\"\"Best-effort simple feature construction from OHLCV for common patterns.\n",
        "       Returns scalar (last value) or raises if not possible.\n",
        "    \"\"\"\n",
        "    # last N extraction helper\n",
        "    close = ohlcv[\"close\"].astype(float)\n",
        "    ret = close.pct_change().fillna(0.0)\n",
        "    # patterns: ret_N, ema_N, vol_rollN, stdN\n",
        "    import re\n",
        "    # ret_at_break_h4 etc -> fallback to 1-period return\n",
        "    if \"ret\" in colname and any(ch.isdigit() for ch in colname):\n",
        "        # find first number\n",
        "        m = re.search(r\"(\\d+)\", colname)\n",
        "        if m:\n",
        "            n = int(m.group(1))\n",
        "            if n <= 1:\n",
        "                return float(ret.iloc[-1])\n",
        "            # use simple n-period return:\n",
        "            return float((close.iloc[-1] / close.iloc[-min(len(close), n+1)]) - 1.0) if len(close) > n else float(ret.iloc[-1])\n",
        "    if colname.startswith(\"ema\") or \"ema_\" in colname:\n",
        "        m = re.search(r\"(\\d+)\", colname)\n",
        "        n = int(m.group(1)) if m else 20\n",
        "        # pandas ewm with span = n (causal)\n",
        "        return float(close.ewm(span=n, adjust=False).mean().iloc[-1])\n",
        "    if \"vol\" in colname or \"std\" in colname or \"rolling\" in colname:\n",
        "        m = re.search(r\"(\\d+)\", colname)\n",
        "        n = int(m.group(1)) if m else 20\n",
        "        return float(ret.rolling(window=min(len(ret), n), min_periods=1).std().iloc[-1])\n",
        "    # fallback: return last close\n",
        "    return float(close.iloc[-1])\n",
        "\n",
        "# ----------------- main -----------------\n",
        "def run_live_builder(out_dir=OUT_DIR):\n",
        "    # load meta\n",
        "    meta_df, meta_path = load_meta_latest()\n",
        "    print(\"[INFO] loaded meta:\", meta_path, \"shape:\", meta_df.shape)\n",
        "    # check tb_ret presence (leak risk)\n",
        "    tb_ret_cols = [c for c in meta_df.columns if c.startswith(\"tb_ret\") or c.startswith(\"tb_t_break\")]\n",
        "    if tb_ret_cols:\n",
        "        print(\"[WARN] meta contains tb_ret / tb_t_break columns. Ensure model was not trained using future leakage cols:\", tb_ret_cols)\n",
        "    # load model\n",
        "    model_obj, model_path = load_model_latest()\n",
        "    if model_obj is None:\n",
        "        raise RuntimeError(\"No model artifact found in OUT_DIR (joblib or xgb json).\")\n",
        "    print(\"[INFO] loaded model:\", model_path)\n",
        "    # infer model feature names if available\n",
        "    model_feat_names = None\n",
        "    if hasattr(model_obj, \"feature_names_in_\"):\n",
        "        try:\n",
        "            model_feat_names = list(model_obj.feature_names_in_)\n",
        "        except Exception:\n",
        "            model_feat_names = None\n",
        "    elif hasattr(model_obj, \"feature_names\"):\n",
        "        try:\n",
        "            model_feat_names = list(model_obj.feature_names)\n",
        "        except Exception:\n",
        "            model_feat_names = None\n",
        "    # fallback: try to read feature names from best params JSON or saved diag (not required)\n",
        "    if model_feat_names is None:\n",
        "        print(\"[INFO] model does not expose feature_names_in_. We'll use intersection(meta.columns) as features.\")\n",
        "    else:\n",
        "        print(f\"[INFO] model expects {len(model_feat_names)} features (sample): {model_feat_names[:10]}\")\n",
        "\n",
        "    # fetch OHLCV if available (used only for reconstructing missing features)\n",
        "    ohlcv = None\n",
        "    if USE_CCXT and ccxt is not None:\n",
        "        try:\n",
        "            print(\"[INFO] fetching OHLCV via ccxt:\", EXCHANGE_ID, SYMBOL, TIMEFRAME)\n",
        "            ohlcv = fetch_ohlcv_ccxt(EXCHANGE_ID, SYMBOL, TIMEFRAME, limit=FETCH_LIMIT)\n",
        "            print(\"[INFO] fetched OHLCV rows:\", len(ohlcv))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] ccxt fetch failed:\", e)\n",
        "            ohlcv = None\n",
        "\n",
        "    # build feature vector for last index\n",
        "    last_idx = meta_df.dropna(how=\"all\").index[-1]\n",
        "    meta_last = meta_df.reindex([last_idx]).iloc[0]\n",
        "    # decide feature list\n",
        "    if model_feat_names:\n",
        "        feat_cols = model_feat_names\n",
        "    else:\n",
        "        feat_cols = [c for c in meta_df.columns if meta_df[c].dtype.kind in \"fiu\" or meta_df[c].dtype == \"bool\"]\n",
        "    # assemble X_live\n",
        "    X_live = {}\n",
        "    missing = []\n",
        "    for c in feat_cols:\n",
        "        if c in meta_df.columns:\n",
        "            # use last available value (causal)\n",
        "            X_live[c] = float(meta_last[c]) if not pd.isna(meta_last[c]) else None\n",
        "        else:\n",
        "            missing.append(c)\n",
        "            X_live[c] = None\n",
        "\n",
        "    # attempt compute for missing using OHLCV or simple heuristic\n",
        "    if missing and ohlcv is not None:\n",
        "        for c in missing:\n",
        "            try:\n",
        "                X_live[c] = float(simple_compute_from_ohlcv(c, ohlcv))\n",
        "            except Exception:\n",
        "                X_live[c] = None\n",
        "\n",
        "    # final fill: for any None left, fill from meta recent history (ffill) or 0.0\n",
        "    for k, v in list(X_live.items()):\n",
        "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
        "            # try forward-fill from meta_df last valid\n",
        "            if k in meta_df.columns:\n",
        "                val = meta_df[k].ffill().iloc[-1] if meta_df[k].ffill().any() else 0.0\n",
        "                try:\n",
        "                    X_live[k] = float(val)\n",
        "                except Exception:\n",
        "                    X_live[k] = 0.0\n",
        "            else:\n",
        "                X_live[k] = 0.0\n",
        "\n",
        "    # make DataFrame (single row) aligned with model expectation order\n",
        "    X_df = pd.DataFrame([X_live])\n",
        "    if model_feat_names:\n",
        "        # ensure order and columns present\n",
        "        for col in model_feat_names:\n",
        "            if col not in X_df.columns:\n",
        "                X_df[col] = 0.0\n",
        "        X_df = X_df[model_feat_names]\n",
        "    else:\n",
        "        # use X_df as-is\n",
        "        pass\n",
        "\n",
        "    # predict\n",
        "    try:\n",
        "        preds = predict_model(model_obj, X_df.fillna(0.0).astype(float))\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Prediction failed: \" + str(e))\n",
        "\n",
        "    # p_long series: we only have last row -> scalar\n",
        "    p_long_val = float(preds.ravel()[-1]) if np.ndim(preds) > 0 else float(preds)\n",
        "    # treat as probability-like, clamp [0,1]\n",
        "    try:\n",
        "        p_long_val = float(np.clip(p_long_val, 0.0, 1.0))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # p_short fallback\n",
        "    p_short_val = 1.0 - p_long_val\n",
        "\n",
        "    # build signal / signed value\n",
        "    signal_val = p_long_val - p_short_val  # in [-1,1]\n",
        "    # load best params if present\n",
        "    best_params = {}\n",
        "    if BEST_PARAMS_GLOB:\n",
        "        try:\n",
        "            best_params = json.load(open(str(BEST_PARAMS_GLOB[-1]), \"r\"))\n",
        "            print(\"[INFO] loaded best params from\", BEST_PARAMS_GLOB[-1])\n",
        "        except Exception:\n",
        "            best_params = {}\n",
        "    best_params = dict(best_params)\n",
        "    # thresholds / L fallback\n",
        "    thr = float(best_params.get(\"threshold\", best_params.get(\"thr\", 0.0)))\n",
        "    L = float(best_params.get(\"L\", 1.0))\n",
        "    target_vol = float(best_params.get(\"TARGET_VOL\", best_params.get(\"target_vol\", TARGET_VOL)))\n",
        "    scale_clip_low = float(best_params.get(\"scale_clip_low\", SCALE_CLIP_LOW))\n",
        "    scale_clip_high = float(best_params.get(\"scale_clip_high\", SCALE_CLIP_HIGH))\n",
        "\n",
        "    # decide raw position (lev1x requested)\n",
        "    pos_raw = 0.0\n",
        "    if abs(signal_val) >= thr:\n",
        "        pos_raw = np.sign(signal_val) * 1.0  # lev1x\n",
        "    else:\n",
        "        pos_raw = 0.0\n",
        "\n",
        "    # naive vol-scaling: compute realized vol from meta tb_ret if available, else use windowed close returns if ohlcv available\n",
        "    realized_ann_vol = 0.0\n",
        "    rets = None\n",
        "    candidate_ret_col = next((c for c in meta_df.columns if c.startswith(\"tb_ret\")), None)\n",
        "    if candidate_ret_col is not None:\n",
        "        rets = meta_df[candidate_ret_col].reindex(meta_df.index).fillna(0.0).astype(float)\n",
        "        # compute vol of pnl from recent N (causal)\n",
        "        pnl_series = (meta_df.get(\"pos\", pd.Series(0.0, index=meta_df.index)).shift(EXEC_SHIFT).fillna(0.0) * rets).fillna(0.0)\n",
        "        if pnl_series.std() > 0:\n",
        "            realized_ann_vol = float(pnl_series.std() * np.sqrt(252 * 24))  # hourly\n",
        "    elif ohlcv is not None:\n",
        "        close = ohlcv[\"close\"].astype(float)\n",
        "        ret = close.pct_change().fillna(0.0)\n",
        "        # use last 200 bars\n",
        "        window = min(len(ret), 200)\n",
        "        realized_ann_vol = float(ret.iloc[-window:].std() * np.sqrt(252 * 24)) if window > 1 else 0.0\n",
        "    # compute scale\n",
        "    scale = 1.0\n",
        "    if realized_ann_vol > 0:\n",
        "        scale = float(target_vol / realized_ann_vol)\n",
        "    scale = float(np.clip(scale, scale_clip_low, scale_clip_high))\n",
        "    pos_scaled = float(np.clip(pos_raw * scale, -1.0, 1.0))\n",
        "\n",
        "    # final DataFrame to save (index = last_idx)\n",
        "    out_df = pd.DataFrame({\n",
        "        \"position\": [pos_scaled],\n",
        "        \"raw\": [pos_raw],\n",
        "        \"signal\": [signal_val],\n",
        "        \"p_long\": [p_long_val],\n",
        "        \"p_short\": [p_short_val],\n",
        "    }, index=[last_idx])\n",
        "\n",
        "    # save outputs\n",
        "    csv_path = OUT_DIR / f\"{SIG_OUT_PREFIX}_{TS}.csv\"\n",
        "    out_df.to_csv(csv_path)\n",
        "    diag = {\n",
        "        \"meta_path\": str(meta_path),\n",
        "        \"model_path\": str(model_path),\n",
        "        \"last_idx\": str(last_idx),\n",
        "        \"p_long\": float(p_long_val),\n",
        "        \"p_short\": float(p_short_val),\n",
        "        \"signal\": float(signal_val),\n",
        "        \"pos_raw\": float(pos_raw),\n",
        "        \"pos_scaled\": float(pos_scaled),\n",
        "        \"scale\": float(scale),\n",
        "        \"realized_ann_vol\": float(realized_ann_vol),\n",
        "        \"best_params\": best_params,\n",
        "        \"n_features_sent\": int(X_df.shape[1]),\n",
        "        \"feature_sample\": dict(X_df.iloc[0].to_dict())\n",
        "    }\n",
        "    diag_path = OUT_DIR / f\"{SIG_OUT_PREFIX}_diag_{TS}.json\"\n",
        "    json.dump(diag, open(diag_path, \"w\"), indent=2)\n",
        "    print(\"[SAVED] live signal:\", csv_path)\n",
        "    print(\"[SAVED] diag:\", diag_path)\n",
        "    print(f\"[SIGNAL @ {last_idx.isoformat()}] pos={pos_scaled:.6f} raw={pos_raw:.6f} p_long={p_long_val:.4f} thr={thr}\")\n",
        "    # Leakage note\n",
        "    if tb_ret_cols:\n",
        "        print(\"[WARN] meta contained tb_ret/tb_t_break columns. Double-check that your model was trained without using future leakage columns.\")\n",
        "    return out_df, diag\n",
        "\n",
        "# run\n",
        "if __name__ == \"__main__\":\n",
        "    out_df, diag = run_live_builder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "rcE60E-IKO0Z",
        "outputId": "4d2541ef-d9e5-493a-e09c-399eb8f123f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loaded meta: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl shape: (17521, 42)\n",
            "[WARN] meta contains tb_ret / tb_t_break columns. Ensure model was not trained using future leakage cols: ['tb_t_break_h4', 'tb_ret_at_break_h4', 'tb_t_break_h8', 'tb_ret_at_break_h8', 'tb_t_break_h12', 'tb_ret_at_break_h12', 'tb_ret_8']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No model artifact found in OUT_DIR (joblib or xgb json).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3754093034.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mout_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_live_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3754093034.py\u001b[0m in \u001b[0;36mrun_live_builder\u001b[0;34m(out_dir)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_latest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No model artifact found in OUT_DIR (joblib or xgb json).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loaded model:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# infer model feature names if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No model artifact found in OUT_DIR (joblib or xgb json)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "live_signal_from_pipeline.py — One-cell live inference wrapper for your MTB pipeline.\n",
        "\n",
        "Usage:\n",
        " - Edit CONFIG below (OUT_DIR, EXCHANGE_ID/SYMBOL if using CCXT).\n",
        " - Run in same environment where your pipeline artifacts live (df_features_*.pkl, model artifacts).\n",
        " - Produces a CSV with position/signal and diag JSON in OUT_DIR.\n",
        "\n",
        "Design goals: use same features as training (artifact-first), preserve causality (shifted features),\n",
        "automatic leakage guard (drop tb_ret*/tb_t_break*), robust fallback compute for minimal derived features.\n",
        "\"\"\"\n",
        "\n",
        "import os, time, json, warnings\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import importlib\n",
        "import glob\n",
        "import joblib\n",
        "import math\n",
        "import random\n",
        "\n",
        "# optional: xgboost and ccxt (only used if present)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "try:\n",
        "    import ccxt\n",
        "except Exception:\n",
        "    ccxt = None\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")  # adapt to your environment\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_GLOB_JOBLIB = str(OUT_DIR / \"A_SRC_model_*.joblib\")\n",
        "MODEL_GLOB_XGB = str(OUT_DIR / \"A_SRC_model_xgb_*.json\")\n",
        "BEST_PARAMS_GLOB = str(OUT_DIR / \"A_SRC_optuna_best_params_penalized_*.json\")\n",
        "\n",
        "FEATURE_ARTIFACT_PREFER = [\n",
        "    str(OUT_DIR / \"df_features_micro.pkl\"),\n",
        "    str(OUT_DIR / \"df_features_base.pkl\"),\n",
        "    str(OUT_DIR / \"df_features_for_training.pkl\"),\n",
        "]\n",
        "\n",
        "FEATURE_MODULES = [\"step04_final\", \"mtb_step04\", \"features\", \"feature_pipeline\"]\n",
        "FEATURE_FUNC_NAMES = [\"build_features_live\", \"build_features\", \"make_features\"]\n",
        "\n",
        "POS_FUNC_NAMES = [\"compute_positions_from_params\", \"compute_positions\", \"positions_from_signal\"]\n",
        "VOL_FUNC_NAMES = [\"vol_scale_positions\", \"apply_vol_scaling\"]\n",
        "\n",
        "# CCXT live fetchsettings (optional)\n",
        "USE_CCXT = True\n",
        "EXCHANGE_ID = \"bitfinex\"\n",
        "SYMBOL = \"BTC/USDT\"\n",
        "TIMEFRAME = \"1h\"\n",
        "FETCH_LIMIT = 2000\n",
        "SINCE = None  # ms since epoch or None\n",
        "\n",
        "EXEC_SHIFT = 1  # how many bars to shift for execution (causal)\n",
        "TARGET_VOL_DEFAULT = 0.2\n",
        "SCALE_CLIP = (0.4, 2.0)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ------------------ helpers ------------------\n",
        "def _tz_utc(idx: pd.Index) -> pd.DatetimeIndex:\n",
        "    idx = pd.to_datetime(idx)\n",
        "    if getattr(idx, \"tz\", None) is None:\n",
        "        idx = idx.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        idx = idx.tz_convert(\"UTC\")\n",
        "    return idx\n",
        "\n",
        "def find_latest(pattern: str) -> Optional[Path]:\n",
        "    hits = glob.glob(pattern)\n",
        "    if not hits:\n",
        "        return None\n",
        "    hits = sorted(hits, key=lambda p: Path(p).stat().st_mtime)\n",
        "    return Path(hits[-1])\n",
        "\n",
        "def load_json_latest(pattern: str) -> Tuple[Optional[Dict], Optional[Path]]:\n",
        "    p = find_latest(pattern)\n",
        "    if p is None:\n",
        "        return None, None\n",
        "    try:\n",
        "        return json.load(open(p, \"r\")), p\n",
        "    except Exception:\n",
        "        return None, p\n",
        "\n",
        "def drop_leakage_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    drop = [c for c in df.columns if isinstance(c, str) and (c.startswith(\"tb_ret\") or c.startswith(\"tb_t_break\") or c.startswith(\"_meta\"))]\n",
        "    if drop:\n",
        "        warnings.warn(f\"Dropping leakage columns before inference: {drop[:10]}\")\n",
        "        df = df.drop(columns=drop, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "def vwap_per_utc_day(df, high=\"high\", low=\"low\", close=\"close\", vol=\"volume\"):\n",
        "    idx = df.index.tz_convert(\"UTC\") if df.index.tz is not None else df.index.tz_localize(\"UTC\")\n",
        "    days = idx.normalize()\n",
        "    typ = (df[high] + df[low] + df[close]) / 3.0\n",
        "    cum_dollar = (typ * df[vol]).groupby(days).cumsum()\n",
        "    cum_vol = (df[vol]).groupby(days).cumsum()\n",
        "    vwap = cum_dollar / cum_vol.replace({0: np.nan})\n",
        "    vwap.index = df.index\n",
        "    return vwap\n",
        "\n",
        "def try_import_callable(mod_names, fn_names):\n",
        "    for mod in mod_names:\n",
        "        try:\n",
        "            m = importlib.import_module(mod)\n",
        "        except Exception:\n",
        "            continue\n",
        "        for fn in fn_names:\n",
        "            if hasattr(m, fn):\n",
        "                return getattr(m, fn), mod, fn\n",
        "    return None, None, None\n",
        "\n",
        "def load_model_candidate() -> Tuple[Any, Optional[Path]]:\n",
        "    # joblib first\n",
        "    p = find_latest(MODEL_GLOB_JOBLIB)\n",
        "    if p:\n",
        "        try:\n",
        "            m = joblib.load(p)\n",
        "            return m, p\n",
        "        except Exception:\n",
        "            pass\n",
        "    # xgboost JSON\n",
        "    p = find_latest(MODEL_GLOB_XGB)\n",
        "    if p:\n",
        "        if xgb is None:\n",
        "            raise RuntimeError(\"XGBoost JSON model present but xgboost not installed\")\n",
        "        booster = xgb.Booster()\n",
        "        booster.load_model(str(p))\n",
        "        return booster, p\n",
        "    return None, None\n",
        "\n",
        "def predict_model(model, X: pd.DataFrame) -> np.ndarray:\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"model is None\")\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        proba = model.predict_proba(X)\n",
        "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
        "            return proba[:, 1]\n",
        "        return proba.ravel()\n",
        "    if hasattr(model, \"predict\"):\n",
        "        out = model.predict(X)\n",
        "        return np.asarray(out).ravel()\n",
        "    if xgb is not None and isinstance(model, xgb.Booster):\n",
        "        d = xgb.DMatrix(X)\n",
        "        out = model.predict(d)\n",
        "        return out.ravel()\n",
        "    raise RuntimeError(\"Unknown model API\")\n",
        "\n",
        "# ------------------ feature artifact loader / minimal recompute ------------------\n",
        "def load_or_recompute_features_for_bars(bars: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return features DataFrame aligned to bars.index.\n",
        "    Strategy:\n",
        "     - prefer artifact (micro/base). If artifact covers requested index -> subset returned.\n",
        "     - if artifact exists but does not cover new bars -> extend by computing minimal deterministic derived features causally.\n",
        "     - if no artifact -> compute minimal features from bars (best-effort).\n",
        "    \"\"\"\n",
        "    bars = bars.copy()\n",
        "    if not isinstance(bars.index, pd.DatetimeIndex):\n",
        "        bars.index = pd.to_datetime(bars.index, utc=True)\n",
        "    else:\n",
        "        if bars.index.tz is None:\n",
        "            bars.index = bars.index.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            bars.index = bars.index.tz_convert(\"UTC\")\n",
        "\n",
        "    # try artifact\n",
        "    art_path = None\n",
        "    for cand in FEATURE_ARTIFACT_PREFER:\n",
        "        if Path(cand).exists():\n",
        "            art_path = Path(cand); break\n",
        "\n",
        "    if art_path is None:\n",
        "        # no artifact => compute minimal features directly (deterministic, causal)\n",
        "        df = _compute_minimal_features(bars)\n",
        "        df = drop_leakage_cols(df)\n",
        "        return df\n",
        "\n",
        "    df_art = pd.read_pickle(art_path)\n",
        "    # ensure tz\n",
        "    if not isinstance(df_art.index, pd.DatetimeIndex):\n",
        "        df_art.index = pd.to_datetime(df_art.index, utc=True)\n",
        "    else:\n",
        "        if df_art.index.tz is None:\n",
        "            df_art.index = df_art.index.tz_localize(\"UTC\")\n",
        "        else:\n",
        "            df_art.index = df_art.index.tz_convert(\"UTC\")\n",
        "\n",
        "    # if artifact fully covers bars -> return subset\n",
        "    if bars.index.min() >= df_art.index.min() and bars.index.max() <= df_art.index.max():\n",
        "        out = df_art.reindex(bars.index)\n",
        "        out = drop_leakage_cols(out)\n",
        "        return out\n",
        "\n",
        "    # else need to extend: use artifact raw OHLC if available + bars to compute minimal new features\n",
        "    required = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "    if all(c in df_art.columns for c in required):\n",
        "        # take last chunk from artifact to seed rolling computations\n",
        "        seed = df_art[required].iloc[-10000:].copy()  # keep reasonably sized seed\n",
        "        merged = pd.concat([seed, bars[required]], axis=0)\n",
        "        merged = merged[~merged.index.duplicated(keep=\"last\")].sort_index()\n",
        "    else:\n",
        "        merged = bars[required].copy()\n",
        "\n",
        "    # compute minimal deterministic features causally\n",
        "    df_ext = _compute_minimal_features(merged)\n",
        "    # shift derived features by 1 bar (same rule as pipeline)\n",
        "    raw_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"is_imputed\"] if \"is_imputed\" in df_ext.columns else [\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
        "    feat_cols = [c for c in df_ext.columns if c not in raw_cols]\n",
        "    df_ext[feat_cols] = df_ext[feat_cols].shift(1)\n",
        "\n",
        "    # extract requested rows\n",
        "    out = df_ext.reindex(bars.index)\n",
        "    # merge with artifact columns where possible (prefer artifact values for historical rows)\n",
        "    for c in df_art.columns:\n",
        "        if c not in out.columns:\n",
        "            out[c] = df_art.reindex(bars.index)[c]\n",
        "    out = drop_leakage_cols(out)\n",
        "    # ensure numeric floats\n",
        "    for col in out.columns:\n",
        "        if pd.api.types.is_numeric_dtype(out[col].dtype):\n",
        "            out[col] = out[col].astype(float)\n",
        "    return out\n",
        "\n",
        "def _compute_minimal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Deterministic minimal set: ret_1, ema_10/21/50, rolling ret moments, vwap, tick rule, signed_volume, vol_imbalance.\"\"\"\n",
        "    df = df.copy().sort_index()\n",
        "    # ensure numeric types\n",
        "    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(float)\n",
        "    # ret_1\n",
        "    if \"close\" in df.columns:\n",
        "        df[\"ret_1\"] = df[\"close\"].pct_change().fillna(0.0)\n",
        "    else:\n",
        "        df[\"ret_1\"] = 0.0\n",
        "    # EMAs\n",
        "    if \"close\" in df.columns:\n",
        "        for w in (10,21,50):\n",
        "            df[f\"ema_{w}\"] = df[\"close\"].ewm(span=w, adjust=False).mean()\n",
        "    # rolling moments\n",
        "    for w in (24,72,168):\n",
        "        df[f\"ret_mean_{w}\"] = df[\"ret_1\"].rolling(w, min_periods=1).mean()\n",
        "        df[f\"ret_std_{w}\"] = df[\"ret_1\"].rolling(w, min_periods=1).std().fillna(0.0)\n",
        "    # VWAP\n",
        "    if all(c in df.columns for c in (\"high\",\"low\",\"close\",\"volume\")):\n",
        "        try:\n",
        "            df[\"vwap\"] = vwap_per_utc_day(df, high=\"high\", low=\"low\", close=\"close\", vol=\"volume\")\n",
        "        except Exception:\n",
        "            df[\"vwap\"] = np.nan\n",
        "    else:\n",
        "        df[\"vwap\"] = np.nan\n",
        "    # tick rule & signed vol\n",
        "    if \"close\" in df.columns:\n",
        "        tick = np.sign(df[\"close\"].diff().fillna(0))\n",
        "        tick = tick.replace(0, np.nan).ffill().fillna(0).astype(int)\n",
        "    else:\n",
        "        tick = pd.Series(0, index=df.index)\n",
        "    df[\"tick_sign\"] = tick\n",
        "    if \"volume\" in df.columns:\n",
        "        df[\"signed_volume\"] = df[\"volume\"] * df[\"tick_sign\"]\n",
        "        df[\"vol_imbalance_24\"] = df[\"signed_volume\"].rolling(24, min_periods=1).sum()\n",
        "        df[\"vol_imbalance_50\"] = df[\"signed_volume\"].rolling(50, min_periods=1).sum()\n",
        "    else:\n",
        "        df[\"signed_volume\"] = 0.0\n",
        "        df[\"vol_imbalance_24\"] = 0.0\n",
        "        df[\"vol_imbalance_50\"] = 0.0\n",
        "    return df\n",
        "\n",
        "# ------------------ run_live ------------------\n",
        "def run_live(use_ccxt: bool = USE_CCXT) -> Tuple[pd.DataFrame, Dict[str,Any]]:\n",
        "    # 1) fetch bars\n",
        "    if use_ccxt:\n",
        "        if ccxt is None:\n",
        "            raise RuntimeError(\"ccxt not installed; set USE_CCXT=False or install ccxt.\")\n",
        "        ex = getattr(ccxt, EXCHANGE_ID)()\n",
        "        ohlcv = ex.fetch_ohlcv(SYMBOL, timeframe=TIMEFRAME, since=SINCE, limit=FETCH_LIMIT)\n",
        "        df = pd.DataFrame(ohlcv, columns=[\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
        "        df[\"datetime\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True)\n",
        "        df = df.set_index(\"datetime\").drop(columns=[\"ts\"])\n",
        "    else:\n",
        "        # prefer latest aligned artifact if present\n",
        "        # try to load a cached ohlcv CSV in OUT_DIR or else require user to pass df externally\n",
        "        local_csv = find_latest(str(OUT_DIR / \"ohlcv_*.csv\"))\n",
        "        if local_csv is None:\n",
        "            raise RuntimeError(\"USE_CCXT=False and no local ohlcv CSV found in OUT_DIR. Provide bars or enable CCXT.\")\n",
        "        df = pd.read_csv(local_csv, index_col=0)\n",
        "        df.index = _tz_utc(df.index)\n",
        "    # ensure tz\n",
        "    df.index = _tz_utc(df.index)\n",
        "\n",
        "    # 2) build features (artifact-first)\n",
        "    feat_df = load_or_recompute_features_for_bars(df)\n",
        "    if feat_df is None or feat_df.shape[0] == 0:\n",
        "        raise RuntimeError(\"Feature builder returned empty DataFrame\")\n",
        "    # drop leakage guard (again)\n",
        "    feat_df = drop_leakage_cols(feat_df)\n",
        "\n",
        "    # 3) load model\n",
        "    model_obj, model_path = load_model_candidate()\n",
        "    if model_obj is None:\n",
        "        raise RuntimeError(\"No model artifact found in OUT_DIR (joblib or xgb json). Place model under OUT_DIR.\")\n",
        "    # infer feature columns to use: prefer model.feature_names_in_ if available, else intersection\n",
        "    if hasattr(model_obj, \"feature_names_in_\"):\n",
        "        model_feats = list(getattr(model_obj, \"feature_names_in_\"))\n",
        "    else:\n",
        "        # fallback: use intersection of feat_df and train features saved in manifest (if present)\n",
        "        model_feats = [c for c in feat_df.columns]  # conservative: use all available features\n",
        "    # prepare X\n",
        "    X = feat_df.reindex(columns=model_feats, copy=False).fillna(0.0).astype(float)\n",
        "\n",
        "    # 4) predict\n",
        "    preds = predict_model(model_obj, X)\n",
        "    p_long = pd.Series(preds, index=X.index, name=\"p_long\")\n",
        "    # assume binary long-model -> p_short = 1 - p_long\n",
        "    p_short = (1.0 - p_long).rename(\"p_short\")\n",
        "\n",
        "    signal = (p_long - p_short).rename(\"signal\")\n",
        "\n",
        "    # 5) load best params if available\n",
        "    best_params, param_path = load_json_latest(BEST_PARAMS_GLOB)\n",
        "    if best_params is None:\n",
        "        best_params = {\"threshold\": 0.0, \"L\": 1.0, \"tc\": 0.0005, \"target_vol\": TARGET_VOL_DEFAULT, \"scale_clip_low\": SCALE_CLIP[0], \"scale_clip_high\": SCALE_CLIP[1]}\n",
        "    # unify keys used downstream\n",
        "    thr = float(best_params.get(\"threshold\", best_params.get(\"thr\", 0.0)))\n",
        "    L = float(best_params.get(\"L\", 1.0))\n",
        "    target_vol = float(best_params.get(\"target_vol\", best_params.get(\"TARGET_VOL\", TARGET_VOL_DEFAULT)))\n",
        "    tc_rate = float(best_params.get(\"tc\", 0.0005))\n",
        "\n",
        "    # 6) compute positions: prefer pipeline position builder if found\n",
        "    pos_fn, pos_mod, pos_name = try_import_callable(FEATURE_MODULES, POS_FUNC_NAMES)\n",
        "    pos_series = None\n",
        "    if pos_fn is not None:\n",
        "        try:\n",
        "            # try common signatures\n",
        "            try:\n",
        "                pos_out = pos_fn(best_params, p_long, p_short)\n",
        "            except TypeError:\n",
        "                try:\n",
        "                    pos_out = pos_fn(p_long, p_short, best_params)\n",
        "                except TypeError:\n",
        "                    pos_out = pos_fn(best_params, p_long)  # last-ditch\n",
        "            if isinstance(pos_out, pd.Series):\n",
        "                pos_series = pos_out.reindex(signal.index).fillna(0.0).astype(float)\n",
        "            elif isinstance(pos_out, pd.DataFrame):\n",
        "                # pick 'position' or first numeric column\n",
        "                numeric = [c for c in pos_out.columns if np.issubdtype(pos_out[c].dtype, np.number)]\n",
        "                if \"position\" in pos_out.columns:\n",
        "                    pos_series = pos_out[\"position\"].reindex(signal.index).fillna(0.0).astype(float)\n",
        "                elif numeric:\n",
        "                    pos_series = pos_out[numeric[0]].reindex(signal.index).fillna(0.0).astype(float)\n",
        "                else:\n",
        "                    pos_series = pos_out.iloc[:,0].reindex(signal.index).fillna(0.0).astype(float)\n",
        "            else:\n",
        "                pos_series = pd.Series(pos_out, index=signal.index).astype(float)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"pipeline pos builder failed ({e}), falling back to deterministic rule\")\n",
        "            pos_series = None\n",
        "\n",
        "    if pos_series is None:\n",
        "        # deterministic fallback: binary threshold * sign * L, using absolute signal (rank/scale not applied)\n",
        "        pos_series = (signal.abs() >= thr).astype(float) * np.sign(signal) * L\n",
        "        pos_series = pos_series.reindex(signal.index).fillna(0.0).astype(float)\n",
        "\n",
        "    # 7) vol-scaling: prefer pipeline vol-scaling\n",
        "    vol_fn, vol_mod, vol_name = try_import_callable(FEATURE_MODULES, VOL_FUNC_NAMES)\n",
        "    if vol_fn is not None:\n",
        "        try:\n",
        "            pos_scaled = vol_fn(pos_series, best_params)\n",
        "            pos_scaled = pos_scaled.reindex(pos_series.index).fillna(0.0).astype(float)\n",
        "        except Exception:\n",
        "            warnings.warn(\"pipeline vol-scaling failed; falling back to naive vol-scaling\")\n",
        "            vol_fn = None\n",
        "\n",
        "    if vol_fn is None:\n",
        "        # naive causal vol-scaling: estimate realized vol of (pos.shift*ret) using ret_1 in features if available\n",
        "        if next((c for c in feat_df.columns if c.startswith(\"tb_ret\") or c==\"ret_1\"), None):\n",
        "            # prefer tb_ret* if exists (but those might be leakage — prefer ret_1)\n",
        "            ret_col = \"ret_1\" if \"ret_1\" in feat_df.columns else next((c for c in feat_df.columns if c.startswith(\"tb_ret\")), None)\n",
        "            rets = feat_df[ret_col].reindex(pos_series.index).fillna(0.0).astype(float)\n",
        "        else:\n",
        "            rets = pd.Series(0.0, index=pos_series.index)\n",
        "        pnl = (pos_series.shift(EXEC_SHIFT).fillna(0.0) * rets).fillna(0.0)\n",
        "        realized_ann_vol = float(np.nanstd(pnl) * math.sqrt(252*24)) if np.nanstd(pnl) > 0 else 0.0\n",
        "        if realized_ann_vol > 0:\n",
        "            scale = float(target_vol / realized_ann_vol)\n",
        "        else:\n",
        "            scale = 1.0\n",
        "        scale = float(np.clip(scale, best_params.get(\"scale_clip_low\", SCALE_CLIP[0]), best_params.get(\"scale_clip_high\", SCALE_CLIP[1])))\n",
        "        pos_scaled = (pos_series * scale).clip(-L, L).astype(float)\n",
        "\n",
        "    # 8) build output DataFrame and save\n",
        "    out_df = pd.DataFrame(index=signal.index)\n",
        "    out_df[\"p_long\"] = p_long\n",
        "    out_df[\"p_short\"] = p_short\n",
        "    out_df[\"signal\"] = signal\n",
        "    out_df[\"position\"] = pos_scaled\n",
        "    out_df[\"raw_position\"] = pos_series\n",
        "    out_df[\"tc\"] = tc_rate\n",
        "\n",
        "    ts = time.strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    out_path = OUT_DIR / f\"A_SRC_live_signal_{ts}.csv\"\n",
        "    out_df.to_csv(out_path)\n",
        "\n",
        "    diag = {\n",
        "        \"generated_at\": ts,\n",
        "        \"model_path\": str(model_path),\n",
        "        \"best_params_path\": str(param_path) if param_path else None,\n",
        "        \"n_bars\": int(len(out_df)),\n",
        "        \"last_ts\": str(out_df.index[-1]),\n",
        "        \"last_pos\": float(out_df[\"position\"].iloc[-1]),\n",
        "        \"conf\": float(signal.iloc[-1]),\n",
        "        \"scale_used\": float(np.nan if realized_ann_vol==0 else scale) if 'realized_ann_vol' in locals() else None,\n",
        "    }\n",
        "    diag_path = OUT_DIR / f\"A_SRC_live_signal_diag_{ts}.json\"\n",
        "    with open(diag_path, \"w\") as f:\n",
        "        json.dump(diag, f, indent=2)\n",
        "    print(\"[SAVED] live signal:\", out_path)\n",
        "    print(\"[SAVED] diag:\", diag_path)\n",
        "    # warning about leakage if artifacts contain tb_ret*:\n",
        "    leak_cols = [c for c in feat_df.columns if isinstance(c,str) and (c.startswith(\"tb_ret\") or c.startswith(\"tb_t_break\"))]\n",
        "    if leak_cols:\n",
        "        warnings.warn(f\"FEATURE ARTIFACT CONTAINS tb_ret/tb_t_break columns: {leak_cols[:6]} — ensure model not trained with leakage columns.\")\n",
        "    return out_df, diag\n",
        "\n",
        "# ------------------ run as script ------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"RUNNING live_signal_from_pipeline.py\")\n",
        "    try:\n",
        "        out, diag = run_live(use_ccxt=USE_CCXT)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR during run_live:\", e)\n",
        "        raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "id": "2gAW8IyCLocv",
        "outputId": "6843d1b3-1c1c-4042-8278-ce876552a3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING live_signal_from_pipeline.py\n",
            "ERROR during run_live: No model artifact found in OUT_DIR (joblib or xgb json). Place model under OUT_DIR.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No model artifact found in OUT_DIR (joblib or xgb json). Place model under OUT_DIR.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1399947759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RUNNING live_signal_from_pipeline.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_ccxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_CCXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR during run_live:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1399947759.py\u001b[0m in \u001b[0;36mrun_live\u001b[0;34m(use_ccxt)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_candidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No model artifact found in OUT_DIR (joblib or xgb json). Place model under OUT_DIR.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;31m# infer feature columns to use: prefer model.feature_names_in_ if available, else intersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"feature_names_in_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No model artifact found in OUT_DIR (joblib or xgb json). Place model under OUT_DIR."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "list(OUT_DIR.glob(\"A_SRC_model_*\")) + list(OUT_DIR.glob(\"*.joblib\")) + list(OUT_DIR.glob(\"A_SRC_model_xgb_*.json\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFmxdkGiMiBt",
        "outputId": "999f6e6a-ed54-4186-9b5c-e16e682615a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SANITY / PROVENANCE CHECK CELL (paste & run) ===\n",
        "# Purpose: run before broadcasting orders when using signal-only CSV in production.\n",
        "# Behavior: prints summary, writes diag JSON, raises RuntimeError on fatal failures.\n",
        "import os, json, time, hashlib, logging\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# CONFIG: adjust to your environment\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_PARAMS_GLOB = \"A_SRC_optuna_best_params*.json\"\n",
        "SIGNAL_GLOB = \"A_SRC_signal_*.csv\"\n",
        "POSITIONS_GLOB = \"A_SRC_positions_*.csv\"\n",
        "METAPKL_GLOB = \"df_meta_shortlist.v*.pkl\"\n",
        "MANIFEST_PATH = OUT_DIR / \"pipeline_manifest.json\"\n",
        "DIAG_OUT = OUT_DIR / f\"prod_sanity_diag_{int(time.time())}.json\"\n",
        "DIAG_CSV = OUT_DIR / f\"prod_sanity_diag_steps_{int(time.time())}.csv\"\n",
        "\n",
        "# Safety thresholds (tune to your strategy)\n",
        "MAX_DPOS_PER_BAR = 0.5         # max allowed position change per bar (fractional)\n",
        "MAX_DAILY_TURNOVER = 5.0       # max sum |dpos| per day (in position units)\n",
        "MAX_SCALE_DEV = 10.0           # allowed deviation from scale=1 (if vol-scaling used)\n",
        "MIN_COMMON_LEN = 10            # minimal overlap length for alignment checks\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "log = logging.getLogger(\"prod_sanity\")\n",
        "\n",
        "def sha256_file(path: Path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def find_latest(glob_pattern):\n",
        "    hits = sorted(OUT_DIR.glob(glob_pattern), key=lambda p: p.stat().st_mtime)\n",
        "    return hits[-1] if hits else None\n",
        "\n",
        "def load_csv_if_exists(p: Path):\n",
        "    if p is None or not p.exists(): return None\n",
        "    return pd.read_csv(p, index_col=0, parse_dates=True)\n",
        "\n",
        "def load_pickle_if_exists(p: Path):\n",
        "    if p is None or not p.exists(): return None\n",
        "    return pd.read_pickle(p)\n",
        "\n",
        "# discover artifacts\n",
        "sig_path = find_latest(SIGNAL_GLOB)\n",
        "pos_path = find_latest(POSITIONS_GLOB)\n",
        "best_params_path = find_latest(BEST_PARAMS_GLOB)\n",
        "meta_pkl = find_latest(METAPKL_GLOB)\n",
        "sig_eq_path = find_latest(\"signal_only_backtest_equity_*.csv\")\n",
        "pos_eq_path = find_latest(\"backtest_positions_equity_*.csv\")\n",
        "\n",
        "diag = {\"ts\": time.time(), \"found\": {}, \"checks\": {}, \"fatal\": []}\n",
        "\n",
        "# basic discovery\n",
        "for name, p in ((\"signal\", sig_path), (\"positions\", pos_path), (\"best_params\", best_params_path), (\"meta_pkl\", meta_pkl), (\"sig_eq\", sig_eq_path), (\"pos_eq\", pos_eq_path)):\n",
        "    diag[\"found\"][name] = str(p) if p is not None else None\n",
        "log.info(\"Artifacts: %s\", diag[\"found\"])\n",
        "\n",
        "# load\n",
        "sig_df = load_csv_if_exists(sig_path)\n",
        "pos_df = load_csv_if_exists(pos_path)\n",
        "best_params = None\n",
        "if best_params_path is not None:\n",
        "    try:\n",
        "        best_params = json.load(open(best_params_path))\n",
        "    except Exception as e:\n",
        "        log.warning(\"Failed to load best_params JSON: %s\", e)\n",
        "meta_df = load_pickle_if_exists(meta_pkl)\n",
        "\n",
        "# provenance: SHA\n",
        "if sig_path is not None:\n",
        "    try:\n",
        "        diag[\"sig_sha256\"] = sha256_file(sig_path)\n",
        "    except Exception as e:\n",
        "        diag[\"sig_sha256_error\"] = str(e)\n",
        "if pos_path is not None:\n",
        "    try:\n",
        "        diag[\"pos_sha256\"] = sha256_file(pos_path)\n",
        "    except Exception as e:\n",
        "        diag[\"pos_sha256_error\"] = str(e)\n",
        "\n",
        "# basic content checks\n",
        "def check_index_monotonic_and_tz(df, key):\n",
        "    if df is None:\n",
        "        diag[\"checks\"][f\"{key}_index_ok\"] = False\n",
        "        diag[\"fatal\"].append(f\"{key} missing\")\n",
        "        return False\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        try:\n",
        "            df.index = pd.to_datetime(df.index, utc=True)\n",
        "        except Exception as e:\n",
        "            diag[\"checks\"][f\"{key}_index_ok\"] = False\n",
        "            diag[\"fatal\"].append(f\"{key} index not datetime-like: {e}\")\n",
        "            return False\n",
        "    # ensure tz-aware in UTC\n",
        "    if df.index.tz is None:\n",
        "        df.index = df.index.tz_localize(\"UTC\")\n",
        "    else:\n",
        "        df.index = df.index.tz_convert(\"UTC\")\n",
        "    ok_mon = df.index.is_monotonic_increasing\n",
        "    diag[\"checks\"][f\"{key}_index_ok\"] = bool(ok_mon)\n",
        "    if not ok_mon:\n",
        "        diag[\"fatal\"].append(f\"{key} index not monotonic\")\n",
        "    # gaps\n",
        "    diffs = df.index.to_series().diff().dt.total_seconds().dropna()\n",
        "    if len(diffs) > 0:\n",
        "        max_gap = float(diffs.max())\n",
        "        diag[\"checks\"][f\"{key}_max_gap_seconds\"] = max_gap\n",
        "        if max_gap > 3600*4:  # allow 4 bar gap; tune for your freq\n",
        "            diag[\"fatal\"].append(f\"{key} index gap > 4 bars ({max_gap}s)\")\n",
        "    return True\n",
        "\n",
        "check_index_monotonic_and_tz(sig_df, \"signal\")\n",
        "if pos_df is not None:\n",
        "    check_index_monotonic_and_tz(pos_df, \"positions\")\n",
        "if meta_df is not None:\n",
        "    check_index_monotonic_and_tz(meta_df, \"meta\")\n",
        "\n",
        "# leak columns check (tb_*)\n",
        "def find_leak_cols(df, name):\n",
        "    if df is None: return []\n",
        "    leak = [c for c in df.columns if isinstance(c, str) and (c.startswith(\"tb_\") or c.startswith(\"tbret\") or \"tb_ret\" in c)]\n",
        "    diag[\"checks\"][f\"{name}_leak_cols\"] = leak\n",
        "    if leak:\n",
        "        diag[\"fatal\"].append(f\"Leakage columns found in {name}: {leak[:6]}\")\n",
        "    return leak\n",
        "\n",
        "find_leak_cols(sig_df, \"signal\")\n",
        "find_leak_cols(pos_df, \"positions\")\n",
        "find_leak_cols(meta_df, \"meta\")\n",
        "\n",
        "# require at least signal exists\n",
        "if sig_df is None:\n",
        "    raise RuntimeError(\"Signal CSV not found — cannot proceed with production run.\")\n",
        "\n",
        "# fill missing expected cols\n",
        "if \"position\" not in (pos_df.columns if pos_df is not None else []):\n",
        "    # positions may be missing; that's acceptable for signal-only path if we reconstruct later\n",
        "    diag[\"checks\"][\"positions_has_position_col\"] = False\n",
        "else:\n",
        "    diag[\"checks\"][\"positions_has_position_col\"] = True\n",
        "\n",
        "# load params or defaults\n",
        "params = best_params or {}\n",
        "diag[\"params\"] = params\n",
        "\n",
        "L = float(params.get(\"L\", 1.0))\n",
        "target_vol = float(params.get(\"target_vol\", params.get(\"TARGET_VOL\", np.nan))) if params else np.nan\n",
        "diag[\"checks\"][\"L\"] = L\n",
        "diag[\"checks\"][\"target_vol\"] = target_vol\n",
        "\n",
        "# position bounds check (if positions available)\n",
        "if pos_df is not None and \"position\" in pos_df.columns:\n",
        "    pos_series = pos_df[\"position\"].astype(float)\n",
        "    if (pos_series.abs() > (1.0 + 1e-8) * L).any():\n",
        "        diag[\"fatal\"].append(\"position bounds violated: abs(position) > L\")\n",
        "    diag[\"checks\"][\"pos_max_abs\"] = float(pos_series.abs().max())\n",
        "    # per-bar dpos\n",
        "    dpos = pos_series.diff().abs().fillna(0)\n",
        "    diag[\"checks\"][\"dpos_max\"] = float(dpos.max())\n",
        "    if float(dpos.max()) > MAX_DPOS_PER_BAR:\n",
        "        diag[\"fatal\"].append(f\"excessive single-bar turnover: {float(dpos.max())} > {MAX_DPOS_PER_BAR}\")\n",
        "    # daily turnover\n",
        "    daily_turn = dpos.groupby(pos_series.index.normalize()).sum()\n",
        "    diag[\"checks\"][\"daily_turnover_max\"] = float(daily_turn.max())\n",
        "    if float(daily_turn.max()) > MAX_DAILY_TURNOVER:\n",
        "        diag[\"fatal\"].append(f\"daily turnover > MAX_DAILY_TURNOVER: {float(daily_turn.max())} > {MAX_DAILY_TURNOVER}\")\n",
        "else:\n",
        "    diag[\"checks\"][\"positions_present\"] = False\n",
        "    # if positions not present, check signal->pos mapping plausibility\n",
        "    diag[\"checks\"][\"signal_head\"] = sig_df.iloc[:5].to_dict(orient=\"list\")\n",
        "\n",
        "# vol-scale sanity (if params include scale entries)\n",
        "if \"scale\" in params or \"target_vol\" in params:\n",
        "    scale = float(params.get(\"scale\", 1.0))\n",
        "    diag[\"checks\"][\"scale\"] = scale\n",
        "    if abs(scale - 1.0) > MAX_SCALE_DEV:\n",
        "        diag[\"fatal\"].append(f\"scale dev implausible: {scale} far from 1 (MAX_SCALE_DEV={MAX_SCALE_DEV})\")\n",
        "\n",
        "# equity quick-compare if available\n",
        "def load_eq(csvp):\n",
        "    try:\n",
        "        if csvp is None: return None\n",
        "        df = pd.read_csv(csvp, index_col=0, parse_dates=True)\n",
        "        if \"eq\" in df.columns:\n",
        "            s = df[\"eq\"]\n",
        "        elif \"equity\" in df.columns:\n",
        "            s = df[\"equity\"]\n",
        "        else:\n",
        "            # guess first numeric col\n",
        "            s = df.select_dtypes(include=[np.number]).iloc[:,0]\n",
        "        s.index = pd.to_datetime(s.index, utc=True)\n",
        "        return s\n",
        "    except Exception as e:\n",
        "        diag.setdefault(\"errors\", {})[\"load_eq_err\"] = str(e)\n",
        "        return None\n",
        "\n",
        "eq_sig = load_eq(sig_eq_path)\n",
        "eq_pos = load_eq(pos_eq_path)\n",
        "if eq_sig is not None and eq_pos is not None:\n",
        "    # align on common index\n",
        "    common = eq_sig.index.intersection(eq_pos.index)\n",
        "    diag[\"checks\"][\"eq_common_len\"] = int(len(common))\n",
        "    if len(common) >= MIN_COMMON_LEN:\n",
        "        diff = (eq_pos.reindex(common) - eq_sig.reindex(common)).fillna(0)\n",
        "        diag[\"checks\"][\"eq_last_signal\"] = float(eq_sig.reindex(common).iloc[-1])\n",
        "        diag[\"checks\"][\"eq_last_pos\"] = float(eq_pos.reindex(common).iloc[-1])\n",
        "        diag[\"checks\"][\"eq_max_abs_diff\"] = float(diff.abs().max())\n",
        "        diag[\"checks\"][\"eq_mean_abs_diff\"] = float(diff.abs().mean())\n",
        "        # large mismatch threshold (tunable)\n",
        "        if float(diff.abs().max()) > 0.5:\n",
        "            diag[\"checks\"][\"eq_mismatch_warning\"] = \"large mismatch > 0.5 equity units; inspect reconstruction\"\n",
        "    else:\n",
        "        diag[\"checks\"][\"eq_alignment_note\"] = \"too short common overlap to compare equity series\"\n",
        "else:\n",
        "    diag[\"checks\"][\"eq_present\"] = False\n",
        "\n",
        "# signal->position plausibility: if both p_long/p_short exist and position missing, compute naive pos and compare\n",
        "if \"position\" not in (pos_df.columns if pos_df is not None else []):\n",
        "    if {\"p_long\",\"p_short\"}.issubset(set(sig_df.columns)):\n",
        "        p_long = sig_df[\"p_long\"].astype(float)\n",
        "        p_short = sig_df[\"p_short\"].astype(float)\n",
        "        inferred_raw = p_long - p_short\n",
        "        # check percentile range\n",
        "        diag[\"checks\"][\"inferred_raw_mean\"] = float(inferred_raw.mean())\n",
        "        diag[\"checks\"][\"inferred_raw_std\"] = float(inferred_raw.std())\n",
        "    else:\n",
        "        diag[\"checks\"][\"inferred_raw\"] = \"cannot infer: p_long/p_short missing\"\n",
        "\n",
        "# final action: if any fatal in diag -> raise RuntimeError (do not proceed to trade)\n",
        "if diag.get(\"fatal\"):\n",
        "    # save diag and raise\n",
        "    try:\n",
        "        with open(DIAG_OUT, \"w\") as f:\n",
        "            json.dump(diag, f, indent=2, default=lambda o: str(o))\n",
        "    except Exception:\n",
        "        pass\n",
        "    raise RuntimeError(\"Fatal sanity checks failed: \" + \"; \".join(diag[\"fatal\"]))\n",
        "\n",
        "# save diag and small CSV summary for ops\n",
        "with open(DIAG_OUT, \"w\") as f:\n",
        "    json.dump(diag, f, indent=2, default=lambda o: str(o))\n",
        "# small CSV: per-check flat\n",
        "pd.DataFrame.from_dict({k: [v] for k, v in diag[\"checks\"].items()}).T.to_csv(DIAG_CSV)\n",
        "\n",
        "log.info(\"Sanity checks PASSED. Diag saved -> %s ; CSV -> %s\", DIAG_OUT, DIAG_CSV)\n",
        "print(\"SUMMARY (quick):\")\n",
        "print(\" - signal:\", diag[\"found\"].get(\"signal\"))\n",
        "print(\" - positions:\", diag[\"found\"].get(\"positions\"))\n",
        "print(\" - params:\", best_params_path)\n",
        "print(\" - sig_sha256:\", diag.get(\"sig_sha256\"))\n",
        "print(\" - fatal_issues:\", diag.get(\"fatal\", []))\n",
        "\n",
        "# Return diag for notebook consumption\n",
        "diag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "3dYPACmWMy6n",
        "outputId": "728f96ff-dbdf-4bef-e759-e2c1e138abb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Fatal sanity checks failed: Leakage columns found in meta: ['tb_label_h4', 'tb_t_break_h4', 'tb_ret_at_break_h4', 'tb_label_h8', 'tb_t_break_h8', 'tb_ret_at_break_h8']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3531484038.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fatal sanity checks failed: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"; \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fatal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;31m# save diag and small CSV summary for ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Fatal sanity checks failed: Leakage columns found in meta: ['tb_label_h4', 'tb_t_break_h4', 'tb_ret_at_break_h4', 'tb_label_h8', 'tb_t_break_h8', 'tb_ret_at_break_h8']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE-CELL: live single-decision (BUY / SELL / HOLD) — run as-is\n",
        "import os, time, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- optional deps ---\n",
        "try:\n",
        "    import ccxt\n",
        "except Exception:\n",
        "    ccxt = None\n",
        "try:\n",
        "    import joblib\n",
        "except Exception:\n",
        "    joblib = None\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "# --- helpers ---\n",
        "def find_latest(path_glob):\n",
        "    files = sorted(list(OUT_DIR.glob(path_glob)), key=lambda p: p.stat().st_mtime) if isinstance(path_glob, str) else sorted(path_glob, key=lambda p: p.stat().st_mtime)\n",
        "    return files[-1] if files else None\n",
        "\n",
        "def load_model():\n",
        "    # try joblib sklearn-like\n",
        "    m = find_latest(\"A_SRC_model_*.joblib\") or find_latest(\"*model*.joblib\")\n",
        "    if m is not None and joblib is not None:\n",
        "        try:\n",
        "            model = joblib.load(m)\n",
        "            return model, str(m)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # try xgboost JSON\n",
        "    x = find_latest(\"A_SRC_model_xgb_*.json\") or find_latest(\"*.xgb.json\") or find_latest(\"*.json\")\n",
        "    if x is not None and xgboost_available():\n",
        "        try:\n",
        "            booster = xgb.Booster()\n",
        "            booster.load_model(str(x))\n",
        "            return booster, str(x)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # try generic model files\n",
        "    m2 = find_latest(\"*.joblib\")\n",
        "    if m2 is not None and joblib is not None:\n",
        "        try:\n",
        "            return joblib.load(m2), str(m2)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None, None\n",
        "\n",
        "def xgboost_available():\n",
        "    return 'xgboost' in globals() and xgb is not None\n",
        "\n",
        "def predict_model(model, X_df):\n",
        "    X = X_df.values if not hasattr(X_df, \"to_numpy\") else X_df.to_numpy()\n",
        "    # sklearn-like\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        p = model.predict_proba(X_df)\n",
        "        if p.ndim == 2 and p.shape[1] >= 2:\n",
        "            return pd.Series(p[:,1], index=X_df.index)\n",
        "        return pd.Series(p.ravel(), index=X_df.index)\n",
        "    if hasattr(model, \"predict\"):\n",
        "        out = model.predict(X_df)\n",
        "        return pd.Series(out.ravel(), index=X_df.index)\n",
        "    # xgboost Booster\n",
        "    if xgboost_available() and isinstance(model, xgb.Booster):\n",
        "        d = xgb.DMatrix(X_df)\n",
        "        out = model.predict(d)\n",
        "        return pd.Series(out.ravel(), index=X_df.index)\n",
        "    raise RuntimeError(\"Unknown model type (need sklearn-like or xgboost.Booster).\")\n",
        "\n",
        "# --- feature builder discovery ---\n",
        "def find_feature_builder(modules=(\"step04_final\",\"mtb_step04\",\"features\",\"feature_pipeline\"), fnames=(\"build_features_live\",\"build_features\",\"make_features\")):\n",
        "    for m in modules:\n",
        "        try:\n",
        "            mod = __import__(m, fromlist=[\"*\"])\n",
        "        except Exception:\n",
        "            continue\n",
        "        for fn in fnames:\n",
        "            if hasattr(mod, fn):\n",
        "                return getattr(mod, fn), m, fn\n",
        "    return None, None, None\n",
        "\n",
        "# --- fallback minimal feature builder (causal) ---\n",
        "def fallback_features_from_ohlcv(df_bars):\n",
        "    df = df_bars.copy()\n",
        "    df = df.sort_index()\n",
        "    # ensure tz-aware UTC\n",
        "    if df.index.tz is None:\n",
        "        df.index = pd.to_datetime(df.index, utc=True)\n",
        "    else:\n",
        "        df.index = pd.to_datetime(df.index).tz_convert(\"UTC\")\n",
        "    # simple causal features: ret_1, ema50, ema200, rsi14\n",
        "    df[\"ret_1\"] = df[\"close\"].pct_change()\n",
        "    df[\"logret_1\"] = np.log(df[\"close\"]).diff()\n",
        "    df[\"ema_50\"] = df[\"close\"].ewm(span=50, adjust=False).mean()\n",
        "    df[\"ema_200\"] = df[\"close\"].ewm(span=200, adjust=False).mean()\n",
        "    # rsi 14 simple:\n",
        "    delta = df[\"close\"].diff()\n",
        "    up = delta.clip(lower=0)\n",
        "    down = -delta.clip(upper=0)\n",
        "    roll_up = up.rolling(14, min_periods=1).mean()\n",
        "    roll_down = down.rolling(14, min_periods=1).mean().replace(0, 1e-12)\n",
        "    rs = roll_up / roll_down\n",
        "    df[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
        "    # shift derived features by 1 bar to keep causality (same as Step03)\n",
        "    raw_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
        "    feat_cols = [c for c in df.columns if c not in raw_cols]\n",
        "    df[feat_cols] = df[feat_cols].shift(1)\n",
        "    # keep only last row for prediction\n",
        "    return df[feat_cols].iloc[[-1]]\n",
        "\n",
        "# --- get latest OHLCV: ccxt or local ---\n",
        "def get_bars(use_ccxt=True, exchange_id=\"binance\", symbol=\"BTC/USDT\", timeframe=\"1h\", limit=1500):\n",
        "    # try local pickled aligned / ohlcv first\n",
        "    cand = find_latest(\"ohlcv_*.csv\") or find_latest(\"df_aligned_final.pkl\") or find_latest(\"df_aligned_clean.pkl\")\n",
        "    if cand is not None:\n",
        "        try:\n",
        "            if str(cand).endswith(\".pkl\"):\n",
        "                df = pd.read_pickle(cand)\n",
        "            else:\n",
        "                df = pd.read_csv(cand, index_col=0, parse_dates=True)\n",
        "            # ensure columns open/high/low/close/volume exist or raise\n",
        "            if set([\"open\",\"high\",\"low\",\"close\",\"volume\"]).issubset(df.columns):\n",
        "                df.index = pd.to_datetime(df.index, utc=True)\n",
        "                return df\n",
        "        except Exception:\n",
        "            pass\n",
        "    if use_ccxt and ccxt is not None:\n",
        "        ex = getattr(ccxt, exchange_id)()\n",
        "        o = ex.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)\n",
        "        df = pd.DataFrame(o, columns=[\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
        "        df[\"datetime\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True)\n",
        "        df = df.set_index(\"datetime\").drop(columns=[\"ts\"])\n",
        "        return df\n",
        "    raise RuntimeError(\"No OHLCV source available (no local ohlcv file and ccxt not available).\")\n",
        "\n",
        "# --- main run ---\n",
        "ts_now = int(time.time())\n",
        "diag = {\"ts\": ts_now, \"out_dir\": str(OUT_DIR)}\n",
        "# 1) load model\n",
        "model, model_path = load_model()\n",
        "diag[\"model_path\"] = model_path\n",
        "if model is None:\n",
        "    raise RuntimeError(\"No model found in OUT_DIR (joblib or xgb). Place model artifact and retry. Searched patterns: A_SRC_model_*.joblib, *.joblib, A_SRC_model_xgb_*.json\")\n",
        "\n",
        "# 2) build features\n",
        "feat_fn, feat_mod, feat_name = find_feature_builder()\n",
        "use_bars = None\n",
        "if feat_fn is not None:\n",
        "    try:\n",
        "        # try to get bars if function needs them\n",
        "        use_bars = get_bars(use_ccxt=(ccxt is not None))\n",
        "        X_last = feat_fn(use_bars)  # assume builder returns full-frame features\n",
        "        # ensure last row only and preserve index\n",
        "        if isinstance(X_last, pd.DataFrame):\n",
        "            X_last = X_last.reindex(sorted(X_last.index)).iloc[[-1]]\n",
        "        else:\n",
        "            raise RuntimeError(\"feature builder did not return DataFrame\")\n",
        "        diag[\"feature_builder\"] = f\"{feat_mod}.{feat_name}\"\n",
        "    except Exception as e:\n",
        "        # fallback\n",
        "        feat_fn = None\n",
        "        diag[\"feature_builder_error\"] = str(e)\n",
        "\n",
        "if feat_fn is None:\n",
        "    # fallback compute minimal causal features\n",
        "    bars = get_bars(use_ccxt=(ccxt is not None))\n",
        "    X_last = fallback_features_from_ohlcv(bars)\n",
        "    diag[\"feature_builder\"] = \"fallback_minimal\"\n",
        "\n",
        "# 3) safety: drop tb_* / tb_ret / tb_t_break if present\n",
        "to_drop = [c for c in X_last.columns if isinstance(c, str) and (c.startswith(\"tb_\") or \"tb_ret\" in c or \"tb_t_break\" in c)]\n",
        "if to_drop:\n",
        "    X_last = X_last.drop(columns=to_drop, errors=\"ignore\")\n",
        "diag[\"dropped_tb_cols\"] = to_drop\n",
        "\n",
        "# 4) align features expected by model if feature_names present\n",
        "if hasattr(model, \"feature_names_in_\"):\n",
        "    expected = list(model.feature_names_in_)\n",
        "    # keep only intersection and preserve order\n",
        "    inter = [c for c in expected if c in X_last.columns]\n",
        "    if len(inter) == 0:\n",
        "        raise RuntimeError(f\"Model expects features {expected[:10]} but none present in built features (available: {list(X_last.columns)[:20]})\")\n",
        "    X_last = X_last[inter]\n",
        "    diag[\"used_features\"] = inter\n",
        "else:\n",
        "    # model has no feature names; just pass available features\n",
        "    diag[\"used_features\"] = list(X_last.columns)\n",
        "\n",
        "# 5) predict p_long\n",
        "p_long_ser = predict_model(model, X_last)\n",
        "# if predict returns continuous scores (not bounded 0-1) we clip/sigmoid if needed\n",
        "p_long_val = float(p_long_ser.iloc[-1])\n",
        "# heuristics: if model output likely logit/score (outside [0,1]) convert with sigmoid if values large\n",
        "if not (0.0 <= p_long_val <= 1.0):\n",
        "    # apply sigmoid to map to (0,1)\n",
        "    p_long_val = 1.0 / (1.0 + np.exp(-p_long_val))\n",
        "diag[\"p_long_raw\"] = float(p_long_ser.iloc[-1])\n",
        "diag[\"p_long_mapped\"] = p_long_val\n",
        "\n",
        "# 6) derive decision (lev1x, no vol-scaling)\n",
        "p_short_val = 1.0 - p_long_val\n",
        "signed = p_long_val - p_short_val  # in [-1,1]\n",
        "# threshold = 0 -> sign(signed); but to avoid flip in marginal cases use small threshold\n",
        "thr = 0.02\n",
        "if signed > thr:\n",
        "    action = \"BUY\"\n",
        "    size = 1.0   # lev1x\n",
        "elif signed < -thr:\n",
        "    action = \"SELL\"\n",
        "    size = -1.0\n",
        "else:\n",
        "    action = \"HOLD\"\n",
        "    size = 0.0\n",
        "\n",
        "# 7) print + save\n",
        "last_ts = X_last.index[-1] if hasattr(X_last.index, \"__len__\") else None\n",
        "msg = {\n",
        "    \"timestamp\": str(last_ts),\n",
        "    \"action\": action,\n",
        "    \"size\": float(size),\n",
        "    \"p_long\": p_long_val,\n",
        "    \"p_short\": p_short_val,\n",
        "    \"signed\": float(signed),\n",
        "    \"used_features\": diag.get(\"used_features\"),\n",
        "    \"model_path\": model_path\n",
        "}\n",
        "print(\"=== LIVE DECISION ===\")\n",
        "print(f\"@ {msg['timestamp']} -> {msg['action']} size={msg['size']} (p_long={msg['p_long']:.4f}, signed={msg['signed']:.4f})\")\n",
        "# save outputs\n",
        "ts = time.strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "out_csv = OUT_DIR / f\"A_SRC_live_decision_{ts}.csv\"\n",
        "pd.DataFrame([msg]).to_csv(out_csv, index=False)\n",
        "diag_path = OUT_DIR / f\"A_SRC_live_decision_diag_{ts}.json\"\n",
        "json.dump({\"msg\": msg, \"diag\": diag}, open(diag_path, \"w\"), indent=2)\n",
        "print(\"Saved decision ->\", out_csv)\n",
        "print(\"Saved diag ->\", diag_path)\n",
        "\n",
        "# return values for interactive use\n",
        "msg"
      ],
      "metadata": {
        "id": "TM5891k2NUbR",
        "outputId": "cbeed3b0-37c0-4358-8900-aad3bfd67c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No model found in OUT_DIR (joblib or xgb). Place model artifact and retry. Searched patterns: A_SRC_model_*.joblib, *.joblib, A_SRC_model_xgb_*.json",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2462720148.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No model found in OUT_DIR (joblib or xgb). Place model artifact and retry. Searched patterns: A_SRC_model_*.joblib, *.joblib, A_SRC_model_xgb_*.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m# 2) build features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No model found in OUT_DIR (joblib or xgb). Place model artifact and retry. Searched patterns: A_SRC_model_*.joblib, *.joblib, A_SRC_model_xgb_*.json"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE-CELL: Train & export XGBClassifier -> joblib (leak-hardened, seeded, temporal holdout)\n",
        "# Usage: adjust OUT_DIR if needed, then run.\n",
        "import os, time, json, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/quant_pipeline/mtb_out\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) locate artifacts\n",
        "feat_glob = sorted(OUT_DIR.glob(\"df_features_for_training.v*.pkl\")) + sorted(OUT_DIR.glob(\"df_features_for_training.pkl\"))\n",
        "meta_glob = sorted(OUT_DIR.glob(\"df_meta_shortlist.v*.pkl\")) + sorted(OUT_DIR.glob(\"df_meta_shortlist*.pkl\"))\n",
        "\n",
        "if not feat_glob:\n",
        "    raise FileNotFoundError(\"df_features_for_training.pkl not found in OUT_DIR. Run Step03 feature builder first.\")\n",
        "if not meta_glob:\n",
        "    raise FileNotFoundError(\"df_meta_shortlist.v*.pkl not found in OUT_DIR. Provide meta pickle.\")\n",
        "\n",
        "feat_p = feat_glob[-1]\n",
        "meta_p = meta_glob[-1]\n",
        "print(\"[INFO] loading features:\", feat_p)\n",
        "print(\"[INFO] loading meta:\", meta_p)\n",
        "\n",
        "dfX = pd.read_pickle(feat_p)\n",
        "df_meta = pd.read_pickle(meta_p)\n",
        "\n",
        "# ensure tz-aware index and alignment\n",
        "dfX.index = pd.to_datetime(dfX.index, utc=True)\n",
        "df_meta.index = pd.to_datetime(df_meta.index, utc=True)\n",
        "\n",
        "# 2) choose target column (prefer h8)\n",
        "TARGET_CANDIDATES = [\"tb_label_h8\", \"tb_label_h4\", \"tb_label_h12\", \"target\"]\n",
        "target_col = next((c for c in TARGET_CANDIDATES if c in df_meta.columns), None)\n",
        "if target_col is None:\n",
        "    raise RuntimeError(f\"No target column found in meta; expected one of: {TARGET_CANDIDATES}\")\n",
        "\n",
        "y = df_meta[target_col].reindex(dfX.index).astype(float)\n",
        "print(f\"[INFO] using target: {target_col}  (na count: {y.isna().sum()})\")\n",
        "\n",
        "# 3) build X: drop explicit leak & diag columns\n",
        "LEAK_PAT = [\"^tb_\", \"^_meta\", \"proba\", \"_label$\", \"^target$\"]\n",
        "diag_prefix = \"diag_\"\n",
        "import re\n",
        "def is_leak_col(c):\n",
        "    for p in LEAK_PAT:\n",
        "        if re.search(p, c):\n",
        "            return True\n",
        "    if str(c).startswith(diag_prefix):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "drop_cols = [c for c in dfX.columns if is_leak_col(c)]\n",
        "if drop_cols:\n",
        "    warnings.warn(f\"Dropping {len(drop_cols)} leak/diag cols from features (sample): {drop_cols[:8]}\")\n",
        "    dfX = dfX.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "# 4) simple sanity: no tb_* remain\n",
        "tb_cols_remain = [c for c in dfX.columns if str(c).startswith(\"tb_\")]\n",
        "if tb_cols_remain:\n",
        "    raise RuntimeError(f\"Leakage columns still present in features after drop: sample={tb_cols_remain[:10]}\")\n",
        "\n",
        "# 5) align and drop rows with NaN target\n",
        "common_idx = dfX.index.intersection(y.dropna().index)\n",
        "if len(common_idx) == 0:\n",
        "    raise RuntimeError(\"No overlapping index between features and non-null target rows.\")\n",
        "X = dfX.reindex(common_idx).copy()\n",
        "y = y.reindex(common_idx).copy()\n",
        "\n",
        "# 6) quick class balance check\n",
        "uniq, cnts = np.unique(y.dropna().values, return_counts=True)\n",
        "print(\"[INFO] target distribution:\", dict(zip(uniq.tolist(), cnts.tolist())))\n",
        "\n",
        "# 7) temporal split -> keep last 20% as holdout (no shuffling)\n",
        "n = len(X)\n",
        "if n < 50:\n",
        "    raise RuntimeError(\"Too few rows to train reliably (<50).\")\n",
        "holdout_frac = 0.20\n",
        "hold_len = max(1, int(np.floor(n * holdout_frac)))\n",
        "train_idx = X.index[:-hold_len]\n",
        "val_idx = X.index[-hold_len:]\n",
        "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
        "X_val, y_val = X.loc[val_idx], y.loc[val_idx]\n",
        "print(f\"[INFO] n_train={len(X_train):,}  n_val={len(X_val):,}\")\n",
        "\n",
        "# 8) cast numeric, fill minor NaNs (tree-based -> no scaler)\n",
        "X_train = X_train.fillna(0.0).astype(float)\n",
        "X_val   = X_val.fillna(0.0).astype(float)\n",
        "\n",
        "# 9) model config (seeded; small early stopping)\n",
        "RANDOM_STATE = 42\n",
        "model = XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method=\"hist\",\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=4,\n",
        ")\n",
        "\n",
        "# 10) fit with early stopping (use validation)\n",
        "print(\"[INFO] training XGBClassifier ... (early stopping on val)\")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# 11) validate quick metrics\n",
        "pval = model.predict_proba(X_val)[:, 1] if hasattr(model, \"predict_proba\") else model.predict(X_val)\n",
        "pred_class = (pval >= 0.5).astype(int)\n",
        "auc = float(roc_auc_score(y_val, pval)) if len(np.unique(y_val)) > 1 else float(\"nan\")\n",
        "acc = float(accuracy_score(y_val, pred_class))\n",
        "print(f\"[INFO] val_auc={auc:.4f}  val_acc={acc:.4f}  best_ntree_limit={getattr(model, 'best_ntree_limit', None)}\")\n",
        "\n",
        "# 12) save model + metadata + feature list\n",
        "ts = int(time.time())\n",
        "model_path = OUT_DIR / f\"A_SRC_model_{ts}.joblib\"\n",
        "joblib.dump(model, model_path, compress=3)\n",
        "feat_list_path = OUT_DIR / f\"A_SRC_model_features_{ts}.json\"\n",
        "json.dump(list(X.columns), open(feat_list_path, \"w\"), indent=2)\n",
        "meta_save = {\n",
        "    \"saved_at\": ts,\n",
        "    \"model_path\": str(model_path),\n",
        "    \"feature_list\": str(feat_list_path),\n",
        "    \"target_col\": target_col,\n",
        "    \"n_train\": int(len(X_train)),\n",
        "    \"n_val\": int(len(X_val)),\n",
        "    \"val_auc\": auc,\n",
        "    \"val_acc\": acc,\n",
        "    \"random_state\": RANDOM_STATE\n",
        "}\n",
        "meta_path = OUT_DIR / f\"A_SRC_model_meta_{ts}.json\"\n",
        "json.dump(meta_save, open(meta_path, \"w\"), indent=2)\n",
        "\n",
        "print(\"[SAVED] model ->\", model_path)\n",
        "print(\"[SAVED] features ->\", feat_list_path)\n",
        "print(\"[SAVED] meta ->\", meta_path)\n",
        "\n",
        "# 13) final note about leakage\n",
        "if any(c.startswith(\"tb_\") for c in df_meta.columns):\n",
        "    print(\"\\n[WARNING] df_meta contains tb_* columns. Ensure these were NOT used as features during training earlier (we dropped tb_* from X here).\")\n",
        "print(\"\\n[OK] Done. Use live inference script (live_signal_from_existing_pipeline.py) — it will now find joblib model in OUT_DIR.\")"
      ],
      "metadata": {
        "id": "rnQkMdWeTIND",
        "outputId": "5ebf709c-c6c0-4855-ea76-e1a75803d1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading features: /content/drive/MyDrive/quant_pipeline/mtb_out/df_features_for_training.pkl\n",
            "[INFO] loading meta: /content/drive/MyDrive/quant_pipeline/mtb_out/df_meta_shortlist.v1764900731.with_tb_ret_h8.pkl\n",
            "[INFO] using target: tb_label_h8  (na count: 0)\n",
            "[INFO] target distribution: {-1.0: 10976, 0.0: 14, 1.0: 6531}\n",
            "[INFO] n_train=14,017  n_val=3,504\n",
            "[INFO] training XGBClassifier ... (early stopping on val)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4142021407.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# 10) fit with early stopping (use validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training XGBClassifier ... (early stopping on val)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006e782c366841b68dbc7f03ec455491": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009636aaf0f4450bb5668568eaa99e34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "011f2d1167fa4128bb358bd52c084c86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05df991e74944d98970c1cde95892f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08eb86bd76f44e26b5a261dffc59c89d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e224a145c8f465f8de71cd662629375": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_886c836b29d14e60b110cd04eebfddc1",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e5c3ed2184c48c9b727075d55bb0063",
            "value": 38
          }
        },
        "0e9e40b7d6b243969f786ca239b46f23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122876bc7ba64a638a2df119aa989add": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_484fe1a87bb143eebd4ad529a91361e3",
            "placeholder": "​",
            "style": "IPY_MODEL_31560a39ad7b4230826d4507e6bc636b",
            "value": "Best trial: 282. Best value: 1.51362: 100%"
          }
        },
        "1cd74d6c623144fe8d8951bc57a62ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e35dff10ca3458794f081e199a92adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e5c3ed2184c48c9b727075d55bb0063": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20466e108c72449dbf379b5507c93c54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ac154a23b34ff7b42ae0a401d717d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31560a39ad7b4230826d4507e6bc636b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35188bf9b395453c9b182a1e310d591c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_432426f1f5094a1a9b920f994cba7cea",
              "IPY_MODEL_6cbf421b299e4717a2492f0156e02648",
              "IPY_MODEL_b1b60a1b78444469bfa68d8299b87a7d"
            ],
            "layout": "IPY_MODEL_726ce4dae954497eb1fc92de607b3fc1"
          }
        },
        "432426f1f5094a1a9b920f994cba7cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_009636aaf0f4450bb5668568eaa99e34",
            "placeholder": "​",
            "style": "IPY_MODEL_f9395ed788424525965c68ae89cc04f9",
            "value": "Best trial: 167. Best value: 2.05628: 100%"
          }
        },
        "484fe1a87bb143eebd4ad529a91361e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4971843e02b0454cb849825eaed2fcb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59e704ba7d03441eaf065b381fe40377": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae1bd5996f04298bce98d9b15588503": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "628ac448548d436eb1a2220fbeb6af77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08eb86bd76f44e26b5a261dffc59c89d",
            "placeholder": "​",
            "style": "IPY_MODEL_7abe8de868884741aecfc532a1791c47",
            "value": "Best trial: 10. Best value: 0.517676: 100%"
          }
        },
        "68863319ec8243e2a51464501e1dbe18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeac7ea55de64e199c297f1e0f081530",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfb644bb139447dc9f406bedcda13f3e",
            "value": 300
          }
        },
        "6cbf421b299e4717a2492f0156e02648": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ac154a23b34ff7b42ae0a401d717d2",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81ea57ff6c2e489d9d37e2b59d7098ed",
            "value": 300
          }
        },
        "6e3b63b074d14781b80e7d6b9da624c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_011f2d1167fa4128bb358bd52c084c86",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e35dff10ca3458794f081e199a92adc",
            "value": 300
          }
        },
        "726ce4dae954497eb1fc92de607b3fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75feffc232a2445eb31443d3fc0fdce3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78e5bc70b98d41b99ed6c1e6206d54bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8346d86816ff4160a18e9db1401fecb3",
            "placeholder": "​",
            "style": "IPY_MODEL_d0f2432f291e412dac941290aa782263",
            "value": " 38/40 [08:51&lt;00:17,  8.69s/it]"
          }
        },
        "7abe8de868884741aecfc532a1791c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ef1715563ab4e489596e50e7d064e85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ea57ff6c2e489d9d37e2b59d7098ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8346d86816ff4160a18e9db1401fecb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886c836b29d14e60b110cd04eebfddc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88812425d8ef4cd680ff87bfe090d6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "899a4cab4b6943d89c315821db1e29e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f274cb107464eadb18cf23762709ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e9e40b7d6b243969f786ca239b46f23",
            "placeholder": "​",
            "style": "IPY_MODEL_96e53fa4fafc407183b4da867aa6fd68",
            "value": " 24/24 [07:12&lt;00:00, 19.39s/it]"
          }
        },
        "96e53fa4fafc407183b4da867aa6fd68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e1bb7b5c0d0407dbc29445e6ed56613": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbaf77978bb748679710568ff16273bf",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_899a4cab4b6943d89c315821db1e29e6",
            "value": 6
          }
        },
        "aa40a33531414b30ac3f7a24617ebd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ef1715563ab4e489596e50e7d064e85",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88812425d8ef4cd680ff87bfe090d6e2",
            "value": 24
          }
        },
        "ac73da65eb8a43d08d904c5a6602371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8cfbc0aa9044d6795a5244337070f3a",
              "IPY_MODEL_0e224a145c8f465f8de71cd662629375",
              "IPY_MODEL_78e5bc70b98d41b99ed6c1e6206d54bc"
            ],
            "layout": "IPY_MODEL_59e704ba7d03441eaf065b381fe40377"
          }
        },
        "addd303bd705460c9cbbb0ad99fa8760": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_628ac448548d436eb1a2220fbeb6af77",
              "IPY_MODEL_aa40a33531414b30ac3f7a24617ebd86",
              "IPY_MODEL_8f274cb107464eadb18cf23762709ac3"
            ],
            "layout": "IPY_MODEL_20466e108c72449dbf379b5507c93c54"
          }
        },
        "b160f83dc7b9404baf8be9b569082289": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f036160f5af540b6a51653e7e8bad0d0",
              "IPY_MODEL_9e1bb7b5c0d0407dbc29445e6ed56613",
              "IPY_MODEL_e57756ef5e984a61ada1672b0e574fec"
            ],
            "layout": "IPY_MODEL_caf47bf1ddb540beb86c3e6d6dedc993"
          }
        },
        "b1b60a1b78444469bfa68d8299b87a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4971843e02b0454cb849825eaed2fcb2",
            "placeholder": "​",
            "style": "IPY_MODEL_e11e8ad9c8be4797a31ccd882d155120",
            "value": " 300/300 [00:16&lt;00:00, 19.75it/s]"
          }
        },
        "bbaf77978bb748679710568ff16273bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc78bca73c44805bb2abdad1ab207bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdeeb52fd7e84068aedfa1482a11be85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c872171fe6114780b56d1d8ac3c25d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca96e487bd7a4853ad31401ce735defe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_122876bc7ba64a638a2df119aa989add",
              "IPY_MODEL_68863319ec8243e2a51464501e1dbe18",
              "IPY_MODEL_edf5c8297c814604b385de1f1ea2e981"
            ],
            "layout": "IPY_MODEL_db4724a84e7542b8b712e905324f0e23"
          }
        },
        "caf47bf1ddb540beb86c3e6d6dedc993": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0f2432f291e412dac941290aa782263": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db4724a84e7542b8b712e905324f0e23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df095eca0d934f4f9bf4391ae83e5925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f099c1ed249a497096143143dbb83e6a",
              "IPY_MODEL_6e3b63b074d14781b80e7d6b9da624c7",
              "IPY_MODEL_f5965d0f88054e3fa47acfb76df6679b"
            ],
            "layout": "IPY_MODEL_eaecf58e0c2f4b458da0dec793f36b65"
          }
        },
        "dfb644bb139447dc9f406bedcda13f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e11e8ad9c8be4797a31ccd882d155120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3943d268ee54f358e3beb10aaa8d052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e57756ef5e984a61ada1672b0e574fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdeeb52fd7e84068aedfa1482a11be85",
            "placeholder": "​",
            "style": "IPY_MODEL_5ae1bd5996f04298bce98d9b15588503",
            "value": " 6/40 [07:47&lt;52:32, 92.73s/it]"
          }
        },
        "eaecf58e0c2f4b458da0dec793f36b65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edf5c8297c814604b385de1f1ea2e981": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1481a7df2aa41c3abfab04cdf3bd565",
            "placeholder": "​",
            "style": "IPY_MODEL_05df991e74944d98970c1cde95892f7a",
            "value": " 300/300 [00:38&lt;00:00, 12.48it/s]"
          }
        },
        "eeac7ea55de64e199c297f1e0f081530": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f036160f5af540b6a51653e7e8bad0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcc78bca73c44805bb2abdad1ab207bd",
            "placeholder": "​",
            "style": "IPY_MODEL_faa4c132ac2e49a781bfd014690be257",
            "value": "Best trial: 1. Best value: 0.517322:  15%"
          }
        },
        "f099c1ed249a497096143143dbb83e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_006e782c366841b68dbc7f03ec455491",
            "placeholder": "​",
            "style": "IPY_MODEL_e3943d268ee54f358e3beb10aaa8d052",
            "value": "Best trial: 282. Best value: 1.51362: 100%"
          }
        },
        "f1481a7df2aa41c3abfab04cdf3bd565": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19a8504a19b4c8ba7ed5a76f9fd0a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5965d0f88054e3fa47acfb76df6679b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c872171fe6114780b56d1d8ac3c25d7f",
            "placeholder": "​",
            "style": "IPY_MODEL_f19a8504a19b4c8ba7ed5a76f9fd0a19",
            "value": " 300/300 [00:36&lt;00:00,  6.48it/s]"
          }
        },
        "f8cfbc0aa9044d6795a5244337070f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75feffc232a2445eb31443d3fc0fdce3",
            "placeholder": "​",
            "style": "IPY_MODEL_1cd74d6c623144fe8d8951bc57a62ff7",
            "value": "Best trial: 30. Best value: 0.494413:  95%"
          }
        },
        "f9395ed788424525965c68ae89cc04f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faa4c132ac2e49a781bfd014690be257": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}